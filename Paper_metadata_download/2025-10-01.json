[
  {
    "paper": {
      "id": "2509.25182",
      "authors": [
        {
          "_id": "68dc88d34159d1f2418f9a36",
          "name": "Junyu Chen",
          "hidden": false
        },
        {
          "_id": "68dc88d34159d1f2418f9a37",
          "name": "Wenkun He",
          "hidden": false
        },
        {
          "_id": "68dc88d34159d1f2418f9a38",
          "name": "Yuchao Gu",
          "hidden": false
        },
        {
          "_id": "68dc88d34159d1f2418f9a39",
          "name": "Yuyang Zhao",
          "hidden": false
        },
        {
          "_id": "68dc88d34159d1f2418f9a3a",
          "name": "Jincheng Yu",
          "hidden": false
        },
        {
          "_id": "68dc88d34159d1f2418f9a3b",
          "name": "Junsong Chen",
          "hidden": false
        },
        {
          "_id": "68dc88d34159d1f2418f9a3c",
          "name": "Dongyun Zou",
          "hidden": false
        },
        {
          "_id": "68dc88d34159d1f2418f9a3d",
          "name": "Yujun Lin",
          "hidden": false
        },
        {
          "_id": "68dc88d34159d1f2418f9a3e",
          "name": "Zhekai Zhang",
          "hidden": false
        },
        {
          "_id": "68dc88d34159d1f2418f9a3f",
          "name": "Muyang Li",
          "hidden": false
        },
        {
          "_id": "68dc88d34159d1f2418f9a40",
          "name": "Haocheng Xi",
          "hidden": false
        },
        {
          "_id": "68dc88d34159d1f2418f9a41",
          "name": "Ligeng Zhu",
          "hidden": false
        },
        {
          "_id": "68dc88d34159d1f2418f9a42",
          "name": "Enze Xie",
          "hidden": false
        },
        {
          "_id": "68dc88d34159d1f2418f9a43",
          "name": "Song Han",
          "hidden": false
        },
        {
          "_id": "68dc88d34159d1f2418f9a44",
          "name": "Han Cai",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/650e2b14c945dfc9386a7e28/bgdM9-uqshQ2p3QyNbj82.jpeg"
      ],
      "publishedAt": "2025-09-29T17:59:31.000Z",
      "submittedOnDailyAt": "2025-10-01T00:21:53.066Z",
      "title": "DC-VideoGen: Efficient Video Generation with Deep Compression Video\n  Autoencoder",
      "submittedOnDailyBy": {
        "_id": "650e2b14c945dfc9386a7e28",
        "avatarUrl": "/avatars/0a9be20aa53f4c52a2d1a8b02d4093ea.svg",
        "isPro": false,
        "fullname": "Han Cai",
        "user": "han-cai",
        "type": "user"
      },
      "summary": "We introduce DC-VideoGen, a post-training acceleration framework for\nefficient video generation. DC-VideoGen can be applied to any pre-trained video\ndiffusion model, improving efficiency by adapting it to a deep compression\nlatent space with lightweight fine-tuning. The framework builds on two key\ninnovations: (i) a Deep Compression Video Autoencoder with a novel chunk-causal\ntemporal design that achieves 32x/64x spatial and 4x temporal compression while\npreserving reconstruction quality and generalization to longer videos; and (ii)\nAE-Adapt-V, a robust adaptation strategy that enables rapid and stable transfer\nof pre-trained models into the new latent space. Adapting the pre-trained\nWan-2.1-14B model with DC-VideoGen requires only 10 GPU days on the NVIDIA H100\nGPU. The accelerated models achieve up to 14.8x lower inference latency than\ntheir base counterparts without compromising quality, and further enable\n2160x3840 video generation on a single GPU. Code:\nhttps://github.com/dc-ai-projects/DC-VideoGen.",
      "upvotes": 14,
      "discussionId": "68dc88d34159d1f2418f9a45",
      "ai_summary": "DC-VideoGen accelerates video generation by adapting pre-trained diffusion models to a deep compression latent space, reducing inference latency and enabling high-resolution video generation.",
      "ai_keywords": [
        "DC-VideoGen",
        "video diffusion model",
        "deep compression latent space",
        "lightweight fine-tuning",
        "Deep Compression Video Autoencoder",
        "chunk-causal temporal design",
        "AE-Adapt-V",
        "Wan-2.1-14B model",
        "inference latency",
        "high-resolution video generation"
      ]
    },
    "publishedAt": "2025-09-29T13:59:31.000Z",
    "title": "DC-VideoGen: Efficient Video Generation with Deep Compression Video\n  Autoencoder",
    "summary": "We introduce DC-VideoGen, a post-training acceleration framework for\nefficient video generation. DC-VideoGen can be applied to any pre-trained video\ndiffusion model, improving efficiency by adapting it to a deep compression\nlatent space with lightweight fine-tuning. The framework builds on two key\ninnovations: (i) a Deep Compression Video Autoencoder with a novel chunk-causal\ntemporal design that achieves 32x/64x spatial and 4x temporal compression while\npreserving reconstruction quality and generalization to longer videos; and (ii)\nAE-Adapt-V, a robust adaptation strategy that enables rapid and stable transfer\nof pre-trained models into the new latent space. Adapting the pre-trained\nWan-2.1-14B model with DC-VideoGen requires only 10 GPU days on the NVIDIA H100\nGPU. The accelerated models achieve up to 14.8x lower inference latency than\ntheir base counterparts without compromising quality, and further enable\n2160x3840 video generation on a single GPU. Code:\nhttps://github.com/dc-ai-projects/DC-VideoGen.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/650e2b14c945dfc9386a7e28/bgdM9-uqshQ2p3QyNbj82.jpeg"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.25182.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "650e2b14c945dfc9386a7e28",
      "avatarUrl": "/avatars/0a9be20aa53f4c52a2d1a8b02d4093ea.svg",
      "fullname": "Han Cai",
      "name": "han-cai",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 11
    },
    "submitterOrganization": {
      "_id": "60262b67268c201cdc8b7d43",
      "name": "nvidia",
      "fullname": "NVIDIA",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2509.25760",
      "authors": [
        {
          "_id": "68dc90b84159d1f2418f9aa7",
          "name": "Zhepei Wei",
          "hidden": false
        },
        {
          "_id": "68dc90b84159d1f2418f9aa8",
          "name": "Xiao Yang",
          "hidden": false
        },
        {
          "_id": "68dc90b84159d1f2418f9aa9",
          "name": "Kai Sun",
          "hidden": false
        },
        {
          "_id": "68dc90b84159d1f2418f9aaa",
          "name": "Jiaqi Wang",
          "hidden": false
        },
        {
          "_id": "68dc90b84159d1f2418f9aab",
          "name": "Rulin Shao",
          "hidden": false
        },
        {
          "_id": "68dc90b84159d1f2418f9aac",
          "name": "Sean Chen",
          "hidden": false
        },
        {
          "_id": "68dc90b84159d1f2418f9aad",
          "name": "Mohammad Kachuee",
          "hidden": false
        },
        {
          "_id": "68dc90b84159d1f2418f9aae",
          "name": "Teja Gollapudi",
          "hidden": false
        },
        {
          "_id": "68dc90b84159d1f2418f9aaf",
          "name": "Tony Liao",
          "hidden": false
        },
        {
          "_id": "68dc90b84159d1f2418f9ab0",
          "name": "Nicolas Scheffer",
          "hidden": false
        },
        {
          "_id": "68dc90b84159d1f2418f9ab1",
          "name": "Rakesh Wanga",
          "hidden": false
        },
        {
          "_id": "68dc90b84159d1f2418f9ab2",
          "name": "Anuj Kumar",
          "hidden": false
        },
        {
          "_id": "68dc90b84159d1f2418f9ab3",
          "name": "Yu Meng",
          "hidden": false
        },
        {
          "_id": "68dc90b84159d1f2418f9ab4",
          "name": "Wen-tau Yih",
          "hidden": false
        },
        {
          "_id": "68dc90b84159d1f2418f9ab5",
          "name": "Xin Luna Dong",
          "hidden": false
        }
      ],
      "publishedAt": "2025-09-30T04:25:17.000Z",
      "submittedOnDailyAt": "2025-10-01T00:56:26.188Z",
      "title": "TruthRL: Incentivizing Truthful LLMs via Reinforcement Learning",
      "submittedOnDailyBy": {
        "_id": "6526307af06ac0cf9a922e86",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/nchCipX-XWw2cnzYsU_Cv.jpeg",
        "isPro": false,
        "fullname": "Zhepei Wei",
        "user": "weizhepei",
        "type": "user"
      },
      "summary": "While large language models (LLMs) have demonstrated strong performance on\nfactoid question answering, they are still prone to hallucination and\nuntruthful responses, particularly when tasks demand information outside their\nparametric knowledge. Indeed, truthfulness requires more than accuracy --\nmodels must also recognize uncertainty and abstain when unsure to avoid\nhallucinations. This presents a fundamental challenge for existing methods:\napproaches that optimize for accuracy often amplify hallucinations, while those\nthat encourage abstention can become overly conservative, sacrificing correct\nanswers. Both extremes ultimately compromise truthfulness. In this work, we\npresent TruthRL, a general reinforcement learning (RL) framework that directly\noptimizes the truthfulness of LLMs. Specifically, we implement TruthRL using\nGRPO with a simple yet effective ternary reward that distinguishes correct\nanswers, hallucinations, and abstentions. It incentivizes models to reduce\nhallucinations not only by providing correct responses, but also by enabling\nabstention when uncertain, thereby improving truthfulness. Extensive\nexperiments across four knowledge-intensive benchmarks show that, compared to\nvanilla RL, TruthRL significantly reduces hallucinations by 28.9% and improves\ntruthfulness by 21.1%, with consistent gains across various backbone models\n(e.g., Qwen, Llama) under both retrieval and non-retrieval setups. In-depth\nablation study demonstrates that vanilla accuracy-driven methods, such as\nsupervised fine-tuning or RL with a binary reward, struggle to balance factual\ncorrectness and uncertainty. In contrast, our proposed truthfulness-driven\nTruthRL achieves strong performance in both accuracy and truthfulness,\nunderscoring the importance of learning objective design for developing\ntruthful LLMs.",
      "upvotes": 9,
      "discussionId": "68dc90b84159d1f2418f9ab6",
      "ai_summary": "TruthRL, a reinforcement learning framework, enhances the truthfulness of large language models by balancing accuracy and abstention, significantly reducing hallucinations and improving performance across benchmarks.",
      "ai_keywords": [
        "large language models",
        "LLMs",
        "hallucination",
        "untruthful responses",
        "parametric knowledge",
        "truthfulness",
        "reinforcement learning",
        "RL",
        "GRPO",
        "ternary reward",
        "abstention",
        "accuracy-driven methods",
        "supervised fine-tuning",
        "binary reward",
        "knowledge-intensive benchmarks",
        "Qwen",
        "Llama",
        "retrieval",
        "non-retrieval setups",
        "ablation study"
      ]
    },
    "publishedAt": "2025-09-30T00:25:17.000Z",
    "title": "TruthRL: Incentivizing Truthful LLMs via Reinforcement Learning",
    "summary": "While large language models (LLMs) have demonstrated strong performance on\nfactoid question answering, they are still prone to hallucination and\nuntruthful responses, particularly when tasks demand information outside their\nparametric knowledge. Indeed, truthfulness requires more than accuracy --\nmodels must also recognize uncertainty and abstain when unsure to avoid\nhallucinations. This presents a fundamental challenge for existing methods:\napproaches that optimize for accuracy often amplify hallucinations, while those\nthat encourage abstention can become overly conservative, sacrificing correct\nanswers. Both extremes ultimately compromise truthfulness. In this work, we\npresent TruthRL, a general reinforcement learning (RL) framework that directly\noptimizes the truthfulness of LLMs. Specifically, we implement TruthRL using\nGRPO with a simple yet effective ternary reward that distinguishes correct\nanswers, hallucinations, and abstentions. It incentivizes models to reduce\nhallucinations not only by providing correct responses, but also by enabling\nabstention when uncertain, thereby improving truthfulness. Extensive\nexperiments across four knowledge-intensive benchmarks show that, compared to\nvanilla RL, TruthRL significantly reduces hallucinations by 28.9% and improves\ntruthfulness by 21.1%, with consistent gains across various backbone models\n(e.g., Qwen, Llama) under both retrieval and non-retrieval setups. In-depth\nablation study demonstrates that vanilla accuracy-driven methods, such as\nsupervised fine-tuning or RL with a binary reward, struggle to balance factual\ncorrectness and uncertainty. In contrast, our proposed truthfulness-driven\nTruthRL achieves strong performance in both accuracy and truthfulness,\nunderscoring the importance of learning objective design for developing\ntruthful LLMs.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.25760.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6526307af06ac0cf9a922e86",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/nchCipX-XWw2cnzYsU_Cv.jpeg",
      "fullname": "Zhepei Wei",
      "name": "weizhepei",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "submitterOrganization": {
      "_id": "5e63d8713071d5be688861b8",
      "name": "facebook",
      "fullname": "AI at Meta",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1592839207516-noauth.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2509.26490",
      "authors": [
        {
          "_id": "68dc8ca34159d1f2418f9a7e",
          "name": "Wei He",
          "hidden": false
        },
        {
          "_id": "68dc8ca34159d1f2418f9a7f",
          "name": "Yueqing Sun",
          "hidden": false
        },
        {
          "_id": "68dc8ca34159d1f2418f9a80",
          "name": "Hongyan Hao",
          "hidden": false
        },
        {
          "_id": "68dc8ca34159d1f2418f9a81",
          "name": "Xueyuan Hao",
          "hidden": false
        },
        {
          "_id": "68dc8ca34159d1f2418f9a82",
          "name": "Zhikang Xia",
          "hidden": false
        },
        {
          "_id": "68dc8ca34159d1f2418f9a83",
          "name": "Qi Gu",
          "hidden": false
        },
        {
          "_id": "68dc8ca34159d1f2418f9a84",
          "name": "Chengcheng Han",
          "hidden": false
        },
        {
          "_id": "68dc8ca34159d1f2418f9a85",
          "name": "Dengchang Zhao",
          "hidden": false
        },
        {
          "_id": "68dc8ca34159d1f2418f9a86",
          "name": "Hui Su",
          "hidden": false
        },
        {
          "_id": "68dc8ca34159d1f2418f9a87",
          "name": "Kefeng Zhang",
          "hidden": false
        },
        {
          "_id": "68dc8ca34159d1f2418f9a88",
          "name": "Man Gao",
          "hidden": false
        },
        {
          "_id": "68dc8ca34159d1f2418f9a89",
          "name": "Xi Su",
          "hidden": false
        },
        {
          "_id": "68dc8ca34159d1f2418f9a8a",
          "name": "Xiaodong Cai",
          "hidden": false
        },
        {
          "_id": "68dc8ca34159d1f2418f9a8b",
          "name": "Xunliang Cai",
          "hidden": false
        },
        {
          "_id": "68dc8ca34159d1f2418f9a8c",
          "name": "Yu Yang",
          "hidden": false
        },
        {
          "_id": "68dc8ca34159d1f2418f9a8d",
          "name": "Yunke Zhao",
          "hidden": false
        }
      ],
      "publishedAt": "2025-09-30T16:33:49.000Z",
      "submittedOnDailyAt": "2025-10-01T00:38:43.265Z",
      "title": "VitaBench: Benchmarking LLM Agents with Versatile Interactive Tasks in\n  Real-world Applications",
      "submittedOnDailyBy": {
        "_id": "66ecee857264238429a1211f",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66ecee857264238429a1211f/TbuM7ToLBrSxDF8mOccpK.jpeg",
        "isPro": false,
        "fullname": "Wei He",
        "user": "hewei2001",
        "type": "user"
      },
      "summary": "As LLM-based agents are increasingly deployed in real-life scenarios,\nexisting benchmarks fail to capture their inherent complexity of handling\nextensive information, leveraging diverse resources, and managing dynamic user\ninteractions. To address this gap, we introduce VitaBench, a challenging\nbenchmark that evaluates agents on versatile interactive tasks grounded in\nreal-world settings. Drawing from daily applications in food delivery, in-store\nconsumption, and online travel services, VitaBench presents agents with the\nmost complex life-serving simulation environment to date, comprising 66 tools.\nThrough a framework that eliminates domain-specific policies, we enable\nflexible composition of these scenarios and tools, yielding 100 cross-scenario\ntasks (main results) and 300 single-scenario tasks. Each task is derived from\nmultiple real user requests and requires agents to reason across temporal and\nspatial dimensions, utilize complex tool sets, proactively clarify ambiguous\ninstructions, and track shifting user intent throughout multi-turn\nconversations. Moreover, we propose a rubric-based sliding window evaluator,\nenabling robust assessment of diverse solution pathways in complex environments\nand stochastic interactions. Our comprehensive evaluation reveals that even the\nmost advanced models achieve only 30% success rate on cross-scenario tasks, and\nless than 50% success rate on others. Overall, we believe VitaBench will serve\nas a valuable resource for advancing the development of AI agents in practical\nreal-world applications. The code, dataset, and leaderboard are available at\nhttps://vitabench.github.io/",
      "upvotes": 6,
      "discussionId": "68dc8ca44159d1f2418f9a8e",
      "projectPage": "https://vitabench.github.io/",
      "githubRepo": "https://github.com/meituan/vitabench",
      "ai_summary": "VitaBench is a benchmark for evaluating LLM-based agents in complex, real-world interactive tasks using a diverse set of tools and scenarios.",
      "ai_keywords": [
        "LLM-based agents",
        "VitaBench",
        "interactive tasks",
        "real-world settings",
        "food delivery",
        "in-store consumption",
        "online travel services",
        "life-serving simulation environment",
        "domain-specific policies",
        "flexible composition",
        "cross-scenario tasks",
        "single-scenario tasks",
        "real user requests",
        "temporal dimensions",
        "spatial dimensions",
        "complex tool sets",
        "ambiguous instructions",
        "shifting user intent",
        "multi-turn conversations",
        "rubric-based sliding window evaluator",
        "stochastic interactions"
      ]
    },
    "publishedAt": "2025-09-30T12:33:49.000Z",
    "title": "VitaBench: Benchmarking LLM Agents with Versatile Interactive Tasks in\n  Real-world Applications",
    "summary": "As LLM-based agents are increasingly deployed in real-life scenarios,\nexisting benchmarks fail to capture their inherent complexity of handling\nextensive information, leveraging diverse resources, and managing dynamic user\ninteractions. To address this gap, we introduce VitaBench, a challenging\nbenchmark that evaluates agents on versatile interactive tasks grounded in\nreal-world settings. Drawing from daily applications in food delivery, in-store\nconsumption, and online travel services, VitaBench presents agents with the\nmost complex life-serving simulation environment to date, comprising 66 tools.\nThrough a framework that eliminates domain-specific policies, we enable\nflexible composition of these scenarios and tools, yielding 100 cross-scenario\ntasks (main results) and 300 single-scenario tasks. Each task is derived from\nmultiple real user requests and requires agents to reason across temporal and\nspatial dimensions, utilize complex tool sets, proactively clarify ambiguous\ninstructions, and track shifting user intent throughout multi-turn\nconversations. Moreover, we propose a rubric-based sliding window evaluator,\nenabling robust assessment of diverse solution pathways in complex environments\nand stochastic interactions. Our comprehensive evaluation reveals that even the\nmost advanced models achieve only 30% success rate on cross-scenario tasks, and\nless than 50% success rate on others. Overall, we believe VitaBench will serve\nas a valuable resource for advancing the development of AI agents in practical\nreal-world applications. The code, dataset, and leaderboard are available at\nhttps://vitabench.github.io/",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.26490.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66ecee857264238429a1211f",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66ecee857264238429a1211f/TbuM7ToLBrSxDF8mOccpK.jpeg",
      "fullname": "Wei He",
      "name": "hewei2001",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "submitterOrganization": {
      "_id": "68b28d79a176a9beb30d2049",
      "name": "meituan-longcat",
      "fullname": "LongCat",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/68a2a29ab9d4c5698e02c747/CDCAx7X7rXDt7xjI-DoxG.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2509.26488",
      "authors": [
        {
          "_id": "68dc88d74159d1f2418f9a47",
          "name": "Zigeng Chen",
          "hidden": false
        },
        {
          "_id": "68dc88d74159d1f2418f9a48",
          "name": "Gongfan Fang",
          "hidden": false
        },
        {
          "_id": "68dc88d74159d1f2418f9a49",
          "name": "Xinyin Ma",
          "hidden": false
        },
        {
          "_id": "68dc88d74159d1f2418f9a4a",
          "name": "Ruonan Yu",
          "hidden": false
        },
        {
          "_id": "68dc88d74159d1f2418f9a4b",
          "name": "Xinchao Wang",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/65811eeaa2284a018e51f1ba/0NK-gFnfpLThLyS0atsLH.mp4"
      ],
      "publishedAt": "2025-09-30T16:32:52.000Z",
      "submittedOnDailyAt": "2025-10-01T00:34:29.943Z",
      "title": "dParallel: Learnable Parallel Decoding for dLLMs",
      "submittedOnDailyBy": {
        "_id": "65811eeaa2284a018e51f1ba",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/dH8UZj6Kk5HJkI1DItCNm.jpeg",
        "isPro": false,
        "fullname": "Zigeng Chen",
        "user": "Zigeng",
        "type": "user"
      },
      "summary": "Diffusion large language models (dLLMs) have recently drawn considerable\nattention within the research community as a promising alternative to\nautoregressive generation, offering parallel token prediction and lower\ninference latency. Yet, their parallel decoding potential remains largely\nunderexplored, as existing open-source models still require nearly token-length\ndecoding steps to ensure performance. To address this, we introduce dParallel,\na simple and effective method that unlocks the inherent parallelism of dLLMs\nfor fast sampling. We identify that the key bottleneck to parallel decoding\narises from the sequential certainty convergence for masked tokens. Building on\nthis insight, we introduce the core of our approach: certainty-forcing\ndistillation, a novel training strategy that distills the model to follow its\noriginal sampling trajectories while enforcing it to achieve high certainty on\nmasked tokens more rapidly and in parallel. Extensive experiments across\nvarious benchmarks demonstrate that our method can dramatically reduce the\nnumber of decoding steps while maintaining performance. When applied to the\nLLaDA-8B-Instruct model, dParallel reduces decoding steps from 256 to 30 on\nGSM8K, achieving an 8.5x speedup without performance degradation. On the MBPP\nbenchmark, it cuts decoding steps from 256 to 24, resulting in a 10.5x speedup\nwhile maintaining accuracy. Our code is available at\nhttps://github.com/czg1225/dParallel",
      "upvotes": 4,
      "discussionId": "68dc88d74159d1f2418f9a4c",
      "githubRepo": "https://github.com/czg1225/dParallel",
      "ai_summary": "dParallel is a method that enhances the parallel decoding of diffusion large language models, significantly reducing decoding steps without compromising performance.",
      "ai_keywords": [
        "diffusion large language models",
        "dLLMs",
        "autoregressive generation",
        "parallel token prediction",
        "parallel decoding",
        "masked tokens",
        "certainty-forcing distillation",
        "LLaDA-8B-Instruct",
        "GSM8K",
        "MBPP benchmark"
      ],
      "githubStars": 3
    },
    "publishedAt": "2025-09-30T12:32:52.000Z",
    "title": "dParallel: Learnable Parallel Decoding for dLLMs",
    "summary": "Diffusion large language models (dLLMs) have recently drawn considerable\nattention within the research community as a promising alternative to\nautoregressive generation, offering parallel token prediction and lower\ninference latency. Yet, their parallel decoding potential remains largely\nunderexplored, as existing open-source models still require nearly token-length\ndecoding steps to ensure performance. To address this, we introduce dParallel,\na simple and effective method that unlocks the inherent parallelism of dLLMs\nfor fast sampling. We identify that the key bottleneck to parallel decoding\narises from the sequential certainty convergence for masked tokens. Building on\nthis insight, we introduce the core of our approach: certainty-forcing\ndistillation, a novel training strategy that distills the model to follow its\noriginal sampling trajectories while enforcing it to achieve high certainty on\nmasked tokens more rapidly and in parallel. Extensive experiments across\nvarious benchmarks demonstrate that our method can dramatically reduce the\nnumber of decoding steps while maintaining performance. When applied to the\nLLaDA-8B-Instruct model, dParallel reduces decoding steps from 256 to 30 on\nGSM8K, achieving an 8.5x speedup without performance degradation. On the MBPP\nbenchmark, it cuts decoding steps from 256 to 24, resulting in a 10.5x speedup\nwhile maintaining accuracy. Our code is available at\nhttps://github.com/czg1225/dParallel",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/65811eeaa2284a018e51f1ba/0NK-gFnfpLThLyS0atsLH.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.26488.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65811eeaa2284a018e51f1ba",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/dH8UZj6Kk5HJkI1DItCNm.jpeg",
      "fullname": "Zigeng Chen",
      "name": "Zigeng",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 7
    },
    "submitterOrganization": {
      "_id": "6508ab2b349930913196378b",
      "name": "NationalUniversityofSingapore",
      "fullname": "National University of Singapore",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/630ca0817dacb93b33506ce7/ZYUmpSMsa5Whihw3me2Bw.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2509.26628",
      "authors": [
        {
          "_id": "68dc8bf34159d1f2418f9a6e",
          "name": "Runze Liu",
          "hidden": false
        },
        {
          "_id": "68dc8bf34159d1f2418f9a6f",
          "name": "Jiakang Wang",
          "hidden": false
        },
        {
          "_id": "68dc8bf34159d1f2418f9a70",
          "name": "Yuling Shi",
          "hidden": false
        },
        {
          "_id": "68dc8bf34159d1f2418f9a71",
          "name": "Zhihui Xie",
          "hidden": false
        },
        {
          "_id": "68dc8bf34159d1f2418f9a72",
          "name": "Chenxin An",
          "hidden": false
        },
        {
          "_id": "68dc8bf34159d1f2418f9a73",
          "name": "Kaiyan Zhang",
          "hidden": false
        },
        {
          "_id": "68dc8bf34159d1f2418f9a74",
          "name": "Jian Zhao",
          "hidden": false
        },
        {
          "_id": "68dc8bf34159d1f2418f9a75",
          "name": "Xiaodong Gu",
          "hidden": false
        },
        {
          "_id": "68dc8bf34159d1f2418f9a76",
          "name": "Lei Lin",
          "hidden": false
        },
        {
          "_id": "68dc8bf34159d1f2418f9a77",
          "name": "Wenping Hu",
          "hidden": false
        },
        {
          "_id": "68dc8bf34159d1f2418f9a78",
          "name": "Xiu Li",
          "hidden": false
        },
        {
          "_id": "68dc8bf34159d1f2418f9a79",
          "name": "Fuzheng Zhang",
          "hidden": false
        },
        {
          "_id": "68dc8bf34159d1f2418f9a7a",
          "name": "Guorui Zhou",
          "hidden": false
        },
        {
          "_id": "68dc8bf34159d1f2418f9a7b",
          "name": "Kun Gai",
          "hidden": false
        }
      ],
      "publishedAt": "2025-09-30T17:58:34.000Z",
      "submittedOnDailyAt": "2025-10-01T00:35:07.177Z",
      "title": "Attention as a Compass: Efficient Exploration for Process-Supervised RL\n  in Reasoning Models",
      "submittedOnDailyBy": {
        "_id": "667187ba9ab144eb3ac43a1b",
        "avatarUrl": "/avatars/db5558aa1c5160b9aee8b58573271959.svg",
        "isPro": false,
        "fullname": "Runze Liu",
        "user": "RyanLiu112",
        "type": "user"
      },
      "summary": "Reinforcement Learning (RL) has shown remarkable success in enhancing the\nreasoning capabilities of Large Language Models (LLMs). Process-Supervised RL\n(PSRL) has emerged as a more effective paradigm compared to outcome-based RL.\nHowever, existing PSRL approaches suffer from limited exploration efficiency,\nboth in terms of branching positions and sampling. In this paper, we introduce\na novel PSRL framework (AttnRL), which enables efficient exploration for\nreasoning models. Motivated by preliminary observations that steps exhibiting\nhigh attention scores correlate with reasoning behaviors, we propose to branch\nfrom positions with high values. Furthermore, we develop an adaptive sampling\nstrategy that accounts for problem difficulty and historical batch size,\nensuring that the whole training batch maintains non-zero advantage values. To\nfurther improve sampling efficiency, we design a one-step off-policy training\npipeline for PSRL. Extensive experiments on multiple challenging mathematical\nreasoning benchmarks demonstrate that our method consistently outperforms prior\napproaches in terms of performance and sampling and training efficiency.",
      "upvotes": 3,
      "discussionId": "68dc8bf34159d1f2418f9a7c",
      "ai_summary": "A novel PSRL framework (AttnRL) enhances exploration efficiency in reasoning models by branching from high attention positions and using an adaptive sampling strategy, outperforming prior methods in mathematical reasoning benchmarks.",
      "ai_keywords": [
        "Reinforcement Learning",
        "Large Language Models",
        "Process-Supervised RL",
        "PSRL",
        "AttnRL",
        "attention scores",
        "reasoning behaviors",
        "adaptive sampling strategy",
        "one-step off-policy training",
        "mathematical reasoning benchmarks"
      ]
    },
    "publishedAt": "2025-09-30T13:58:34.000Z",
    "title": "Attention as a Compass: Efficient Exploration for Process-Supervised RL\n  in Reasoning Models",
    "summary": "Reinforcement Learning (RL) has shown remarkable success in enhancing the\nreasoning capabilities of Large Language Models (LLMs). Process-Supervised RL\n(PSRL) has emerged as a more effective paradigm compared to outcome-based RL.\nHowever, existing PSRL approaches suffer from limited exploration efficiency,\nboth in terms of branching positions and sampling. In this paper, we introduce\na novel PSRL framework (AttnRL), which enables efficient exploration for\nreasoning models. Motivated by preliminary observations that steps exhibiting\nhigh attention scores correlate with reasoning behaviors, we propose to branch\nfrom positions with high values. Furthermore, we develop an adaptive sampling\nstrategy that accounts for problem difficulty and historical batch size,\nensuring that the whole training batch maintains non-zero advantage values. To\nfurther improve sampling efficiency, we design a one-step off-policy training\npipeline for PSRL. Extensive experiments on multiple challenging mathematical\nreasoning benchmarks demonstrate that our method consistently outperforms prior\napproaches in terms of performance and sampling and training efficiency.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.26628.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "667187ba9ab144eb3ac43a1b",
      "avatarUrl": "/avatars/db5558aa1c5160b9aee8b58573271959.svg",
      "fullname": "Runze Liu",
      "name": "RyanLiu112",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "submitterOrganization": {
      "_id": "628735cbc83a2d6ab8d14a66",
      "name": "Tsinghua",
      "fullname": "Tsinghua University"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2509.23610",
      "authors": [
        {
          "_id": "68dba99cd2bf1f4b15ec7950",
          "name": "Kai Li",
          "hidden": false
        },
        {
          "_id": "68dba99cd2bf1f4b15ec7951",
          "name": "Kejun Gao",
          "hidden": false
        },
        {
          "_id": "68dba99cd2bf1f4b15ec7952",
          "name": "Xiaolin Hu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-09-28T03:25:34.000Z",
      "submittedOnDailyAt": "2025-10-01T00:42:52.519Z",
      "title": "Efficient Audio-Visual Speech Separation with Discrete Lip Semantics and\n  Multi-Scale Global-Local Attention",
      "submittedOnDailyBy": {
        "_id": "6387676c23da90491eb9fb16",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1669818175965-noauth.jpeg",
        "isPro": true,
        "fullname": "Kai Li",
        "user": "JusperLee",
        "type": "user"
      },
      "summary": "Audio-visual speech separation (AVSS) methods leverage visual cues to extract\ntarget speech and have demonstrated strong separation quality in noisy acoustic\nenvironments. However, these methods usually involve a large number of\nparameters and require high computational cost, which is unacceptable in many\napplications where speech separation serves as only a preprocessing step for\nfurther speech processing. To address this issue, we propose an efficient AVSS\nmethod, named Dolphin. For visual feature extraction, we develop DP-LipCoder, a\ndual-path lightweight video encoder that transforms lip-motion into discrete\naudio-aligned semantic tokens. For audio separation, we construct a lightweight\nencoder-decoder separator, in which each layer incorporates a global-local\nattention (GLA) block to efficiently capture multi-scale dependencies.\nExperiments on three benchmark datasets showed that Dolphin not only surpassed\nthe current state-of-the-art (SOTA) model in separation quality but also\nachieved remarkable improvements in efficiency: over 50% fewer parameters, more\nthan 2.4x reduction in MACs, and over 6x faster GPU inference speed. These\nresults indicate that Dolphin offers a practical and deployable solution for\nhigh-performance AVSS in real-world scenarios. Our code and demo page are\npublicly available at http://cslikai.cn/Dolphin/.",
      "upvotes": 3,
      "discussionId": "68dba99cd2bf1f4b15ec7953",
      "projectPage": "https://cslikai.cn/Dolphin",
      "githubRepo": "https://github.com/JusperLee/Dolphin",
      "ai_summary": "Dolphin, an efficient AVSS method, uses a dual-path lightweight video encoder and a lightweight encoder-decoder separator with global-local attention blocks to achieve high separation quality and significant computational efficiency.",
      "ai_keywords": [
        "dual-path lightweight video encoder",
        "DP-LipCoder",
        "discrete audio-aligned semantic tokens",
        "lightweight encoder-decoder separator",
        "global-local attention (GLA) block",
        "multi-scale dependencies",
        "state-of-the-art (SOTA)",
        "MACs",
        "GPU inference speed"
      ]
    },
    "publishedAt": "2025-09-27T23:25:34.000Z",
    "title": "Efficient Audio-Visual Speech Separation with Discrete Lip Semantics and\n  Multi-Scale Global-Local Attention",
    "summary": "Audio-visual speech separation (AVSS) methods leverage visual cues to extract\ntarget speech and have demonstrated strong separation quality in noisy acoustic\nenvironments. However, these methods usually involve a large number of\nparameters and require high computational cost, which is unacceptable in many\napplications where speech separation serves as only a preprocessing step for\nfurther speech processing. To address this issue, we propose an efficient AVSS\nmethod, named Dolphin. For visual feature extraction, we develop DP-LipCoder, a\ndual-path lightweight video encoder that transforms lip-motion into discrete\naudio-aligned semantic tokens. For audio separation, we construct a lightweight\nencoder-decoder separator, in which each layer incorporates a global-local\nattention (GLA) block to efficiently capture multi-scale dependencies.\nExperiments on three benchmark datasets showed that Dolphin not only surpassed\nthe current state-of-the-art (SOTA) model in separation quality but also\nachieved remarkable improvements in efficiency: over 50% fewer parameters, more\nthan 2.4x reduction in MACs, and over 6x faster GPU inference speed. These\nresults indicate that Dolphin offers a practical and deployable solution for\nhigh-performance AVSS in real-world scenarios. Our code and demo page are\npublicly available at http://cslikai.cn/Dolphin/.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.23610.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6387676c23da90491eb9fb16",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1669818175965-noauth.jpeg",
      "fullname": "Kai Li",
      "name": "JusperLee",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 29
    },
    "submitterOrganization": {
      "_id": "628735cbc83a2d6ab8d14a66",
      "name": "Tsinghua",
      "fullname": "Tsinghua University"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2509.26625",
      "authors": [
        {
          "_id": "68dc87ff4159d1f2418f9a1a",
          "name": "Junlin Han",
          "hidden": false
        },
        {
          "_id": "68dc87ff4159d1f2418f9a1b",
          "name": "Shengbang Tong",
          "hidden": false
        },
        {
          "_id": "68dc87ff4159d1f2418f9a1c",
          "name": "David Fan",
          "hidden": false
        },
        {
          "_id": "68dc87ff4159d1f2418f9a1d",
          "name": "Yufan Ren",
          "hidden": false
        },
        {
          "_id": "68dc87ff4159d1f2418f9a1e",
          "name": "Koustuv Sinha",
          "hidden": false
        },
        {
          "_id": "68dc87ff4159d1f2418f9a1f",
          "name": "Philip Torr",
          "hidden": false
        },
        {
          "_id": "68dc87ff4159d1f2418f9a20",
          "name": "Filippos Kokkinos",
          "hidden": false
        }
      ],
      "publishedAt": "2025-09-30T17:57:44.000Z",
      "submittedOnDailyAt": "2025-10-01T00:18:03.860Z",
      "title": "Learning to See Before Seeing: Demystifying LLM Visual Priors from\n  Language Pre-training",
      "submittedOnDailyBy": {
        "_id": "636e6ee287545ca5a136b4c3",
        "avatarUrl": "/avatars/208d32b1202e2da210146027212dbdd3.svg",
        "isPro": false,
        "fullname": "Junlin Han",
        "user": "Junlinh",
        "type": "user"
      },
      "summary": "Large Language Models (LLMs), despite being trained on text alone,\nsurprisingly develop rich visual priors. These priors allow latent visual\ncapabilities to be unlocked for vision tasks with a relatively small amount of\nmultimodal data, and in some cases, to perform visual tasks without ever having\nseen an image. Through systematic analysis, we reveal that visual priors-the\nimplicit, emergent knowledge about the visual world acquired during language\npre-training-are composed of separable perception and reasoning priors with\nunique scaling trends and origins. We show that an LLM's latent visual\nreasoning ability is predominantly developed by pre-training on\nreasoning-centric data (e.g., code, math, academia) and scales progressively.\nThis reasoning prior acquired from language pre-training is transferable and\nuniversally applicable to visual reasoning. In contrast, a perception prior\nemerges more diffusely from broad corpora, and perception ability is more\nsensitive to the vision encoder and visual instruction tuning data. In\nparallel, text describing the visual world proves crucial, though its\nperformance impact saturates rapidly. Leveraging these insights, we propose a\ndata-centric recipe for pre-training vision-aware LLMs and verify it in 1T\ntoken scale pre-training. Our findings are grounded in over 100 controlled\nexperiments consuming 500,000 GPU-hours, spanning the full MLLM construction\npipeline-from LLM pre-training to visual alignment and supervised multimodal\nfine-tuning-across five model scales, a wide range of data categories and\nmixtures, and multiple adaptation setups. Along with our main findings, we\npropose and investigate several hypotheses, and introduce the Multi-Level\nExistence Bench (MLE-Bench). Together, this work provides a new way of\ndeliberately cultivating visual priors from language pre-training, paving the\nway for the next generation of multimodal LLMs.",
      "upvotes": 2,
      "discussionId": "68dc87ff4159d1f2418f9a21",
      "ai_summary": "LLMs develop visual priors during language pre-training, which can be leveraged for vision tasks with minimal additional data, and these priors are composed of separable perception and reasoning components.",
      "ai_keywords": [
        "Large Language Models",
        "LLMs",
        "visual priors",
        "latent visual capabilities",
        "multimodal data",
        "visual tasks",
        "implicit knowledge",
        "visual world",
        "perception priors",
        "reasoning priors",
        "pre-training",
        "reasoning-centric data",
        "transferable",
        "visual reasoning",
        "perception ability",
        "vision encoder",
        "visual instruction tuning",
        "text describing the visual world",
        "vision-aware LLMs",
        "data-centric recipe",
        "pre-training",
        "visual alignment",
        "supervised multimodal fine-tuning",
        "Multi-Level Existence Bench",
        "MLE-Bench"
      ]
    },
    "publishedAt": "2025-09-30T13:57:44.000Z",
    "title": "Learning to See Before Seeing: Demystifying LLM Visual Priors from\n  Language Pre-training",
    "summary": "Large Language Models (LLMs), despite being trained on text alone,\nsurprisingly develop rich visual priors. These priors allow latent visual\ncapabilities to be unlocked for vision tasks with a relatively small amount of\nmultimodal data, and in some cases, to perform visual tasks without ever having\nseen an image. Through systematic analysis, we reveal that visual priors-the\nimplicit, emergent knowledge about the visual world acquired during language\npre-training-are composed of separable perception and reasoning priors with\nunique scaling trends and origins. We show that an LLM's latent visual\nreasoning ability is predominantly developed by pre-training on\nreasoning-centric data (e.g., code, math, academia) and scales progressively.\nThis reasoning prior acquired from language pre-training is transferable and\nuniversally applicable to visual reasoning. In contrast, a perception prior\nemerges more diffusely from broad corpora, and perception ability is more\nsensitive to the vision encoder and visual instruction tuning data. In\nparallel, text describing the visual world proves crucial, though its\nperformance impact saturates rapidly. Leveraging these insights, we propose a\ndata-centric recipe for pre-training vision-aware LLMs and verify it in 1T\ntoken scale pre-training. Our findings are grounded in over 100 controlled\nexperiments consuming 500,000 GPU-hours, spanning the full MLLM construction\npipeline-from LLM pre-training to visual alignment and supervised multimodal\nfine-tuning-across five model scales, a wide range of data categories and\nmixtures, and multiple adaptation setups. Along with our main findings, we\npropose and investigate several hypotheses, and introduce the Multi-Level\nExistence Bench (MLE-Bench). Together, this work provides a new way of\ndeliberately cultivating visual priors from language pre-training, paving the\nway for the next generation of multimodal LLMs.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.26625.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "636e6ee287545ca5a136b4c3",
      "avatarUrl": "/avatars/208d32b1202e2da210146027212dbdd3.svg",
      "fullname": "Junlin Han",
      "name": "Junlinh",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2509.25758",
      "authors": [
        {
          "_id": "68dc8a1f4159d1f2418f9a64",
          "name": "Yein Park",
          "hidden": false
        },
        {
          "_id": "68dc8a1f4159d1f2418f9a65",
          "name": "Minbyul Jeong",
          "hidden": false
        },
        {
          "_id": "68dc8a1f4159d1f2418f9a66",
          "name": "Jaewoo Kang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-09-30T04:23:43.000Z",
      "submittedOnDailyAt": "2025-10-01T00:26:22.625Z",
      "title": "Thinking Sparks!: Emergent Attention Heads in Reasoning Models During\n  Post Training",
      "submittedOnDailyBy": {
        "_id": "64587be872b60ae7a3817858",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64587be872b60ae7a3817858/BbdOOxOCEzWTvEpkWp8MM.png",
        "isPro": false,
        "fullname": "Minbyul Jeong",
        "user": "Minbyul",
        "type": "user"
      },
      "summary": "The remarkable capabilities of modern large reasoning models are largely\nunlocked through post-training techniques such as supervised fine-tuning and\nreinforcement learning. However, the architectural mechanisms behind such\nimprovements remain largely opaque. In this work, we use circuit analysis to\ndemonstrate that post-training for complex reasoning sparks the emergence of\nnovel, functionally specialized attention heads. These heads collectively\nsupport structured reasoning and computation. Our comparative analysis across\nQwen families and DeepSeek-distilled model reveals that these emergent heads\nevolve differently under different training regimes. Distillation and SFT\nfoster a cumulative addition of stable reasoning heads. In contrast, group\nrelative policy optimization operates in a dynamic search mode: relatively few\nattention heads are iteratively activated, evaluated, and pruned, with their\nsurvival closely tracking fluctuations in the task reward signal. Furthermore,\nwe find that controllable think on/off models do not possess dedicated thinking\nheads. Instead, turning off explicit reasoning triggers a broader-but less\nefficient-set of compensatory heads. Through ablation and qualitative analyses,\nwe connect these circuit-level dynamics to a crucial performance trade-off:\nstrengthened heads enable sophisticated problem-solving strategies for\ndifficult problems but can also introduce over-thinking failure modes, such as\ncalculation errors or logical loops on simpler tasks. These findings connect\ncircuit-level dynamics to macro-level performance, identifying an inherent\ntension where complex reasoning comes at the cost of elementary computations.\nMore broadly, our work points to future directions for training policy design,\nemphasizing the need to balance the development of effective reasoning\nstrategies with the assurance of reliable, flawless execution.",
      "upvotes": 2,
      "discussionId": "68dc8a1f4159d1f2418f9a67",
      "ai_summary": "Post-training techniques like supervised fine-tuning and reinforcement learning lead to the emergence of specialized attention heads that support structured reasoning, with different training regimes affecting their evolution and performance.",
      "ai_keywords": [
        "supervised fine-tuning",
        "reinforcement learning",
        "circuit analysis",
        "attention heads",
        "structured reasoning",
        "Qwen families",
        "DeepSeek-distilled model",
        "group relative policy optimization",
        "think on/off models",
        "ablation analysis",
        "qualitative analysis",
        "over-thinking failure modes"
      ]
    },
    "publishedAt": "2025-09-30T00:23:43.000Z",
    "title": "Thinking Sparks!: Emergent Attention Heads in Reasoning Models During\n  Post Training",
    "summary": "The remarkable capabilities of modern large reasoning models are largely\nunlocked through post-training techniques such as supervised fine-tuning and\nreinforcement learning. However, the architectural mechanisms behind such\nimprovements remain largely opaque. In this work, we use circuit analysis to\ndemonstrate that post-training for complex reasoning sparks the emergence of\nnovel, functionally specialized attention heads. These heads collectively\nsupport structured reasoning and computation. Our comparative analysis across\nQwen families and DeepSeek-distilled model reveals that these emergent heads\nevolve differently under different training regimes. Distillation and SFT\nfoster a cumulative addition of stable reasoning heads. In contrast, group\nrelative policy optimization operates in a dynamic search mode: relatively few\nattention heads are iteratively activated, evaluated, and pruned, with their\nsurvival closely tracking fluctuations in the task reward signal. Furthermore,\nwe find that controllable think on/off models do not possess dedicated thinking\nheads. Instead, turning off explicit reasoning triggers a broader-but less\nefficient-set of compensatory heads. Through ablation and qualitative analyses,\nwe connect these circuit-level dynamics to a crucial performance trade-off:\nstrengthened heads enable sophisticated problem-solving strategies for\ndifficult problems but can also introduce over-thinking failure modes, such as\ncalculation errors or logical loops on simpler tasks. These findings connect\ncircuit-level dynamics to macro-level performance, identifying an inherent\ntension where complex reasoning comes at the cost of elementary computations.\nMore broadly, our work points to future directions for training policy design,\nemphasizing the need to balance the development of effective reasoning\nstrategies with the assurance of reliable, flawless execution.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.25758.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64587be872b60ae7a3817858",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64587be872b60ae7a3817858/BbdOOxOCEzWTvEpkWp8MM.png",
      "fullname": "Minbyul Jeong",
      "name": "Minbyul",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "submitterOrganization": {
      "_id": "6621bc39e774284ec1742ab8",
      "name": "KoreaUniversity",
      "fullname": "Korea University"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2509.26542",
      "authors": [
        {
          "_id": "68dc885c4159d1f2418f9a23",
          "name": "Yueqian Lin",
          "hidden": false
        },
        {
          "_id": "68dc885c4159d1f2418f9a24",
          "name": "Zhengmian Hu",
          "hidden": false
        },
        {
          "_id": "68dc885c4159d1f2418f9a25",
          "name": "Qinsi Wang",
          "hidden": false
        },
        {
          "_id": "68dc885c4159d1f2418f9a26",
          "name": "Yudong Liu",
          "hidden": false
        },
        {
          "_id": "68dc885c4159d1f2418f9a27",
          "name": "Hengfan Zhang",
          "hidden": false
        },
        {
          "_id": "68dc885c4159d1f2418f9a28",
          "name": "Jayakumar Subramanian",
          "hidden": false
        },
        {
          "_id": "68dc885c4159d1f2418f9a29",
          "name": "Nikos Vlassis",
          "hidden": false
        },
        {
          "_id": "68dc885c4159d1f2418f9a2a",
          "name": "Hai Helen Li",
          "hidden": false
        },
        {
          "_id": "68dc885c4159d1f2418f9a2b",
          "name": "Yiran Chen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-09-30T17:17:09.000Z",
      "submittedOnDailyAt": "2025-10-01T00:34:18.271Z",
      "title": "Voice Evaluation of Reasoning Ability: Diagnosing the Modality-Induced\n  Performance Gap",
      "submittedOnDailyBy": {
        "_id": "64b5198c25882acb62fb77ef",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b5198c25882acb62fb77ef/HX9pfMEPQlfjvSAgSLplY.png",
        "isPro": false,
        "fullname": "Yueqian Lin",
        "user": "linyueqian",
        "type": "user"
      },
      "summary": "We present Voice Evaluation of Reasoning Ability (VERA), a benchmark for\nevaluating reasoning ability in voice-interactive systems under real-time\nconversational constraints. VERA comprises 2,931 voice-native episodes derived\nfrom established text benchmarks and organized into five tracks (Math, Web,\nScience, Long-Context, Factual). Each item is adapted for speech interaction\nwhile preserving reasoning difficulty. VERA enables direct text-voice\ncomparison within model families and supports analysis of how architectural\nchoices affect reliability. We assess 12 contemporary voice systems alongside\nstrong text baselines and observe large, consistent modality gaps: on\ncompetition mathematics a leading text model attains 74.8% accuracy while its\nvoice counterpart reaches 6.1%; macro-averaged across tracks the best text\nmodels achieve 54.0% versus 11.3% for voice. Latency-accuracy analyses reveal a\nlow-latency plateau, where fast voice systems cluster around ~10% accuracy,\nwhile approaching text performance requires sacrificing real-time interaction.\nDiagnostic experiments indicate that common mitigations are insufficient.\nIncreasing \"thinking time\" yields negligible gains; a decoupled cascade that\nseparates reasoning from narration improves accuracy but still falls well short\nof text and introduces characteristic grounding/consistency errors. Failure\nanalyses further show distinct error signatures across native streaming,\nend-to-end, and cascade designs. VERA provides a reproducible testbed and\ntargeted diagnostics for architectures that decouple thinking from speaking,\noffering a principled way to measure progress toward real-time voice assistants\nthat are both fluent and reliably reasoned.",
      "upvotes": 1,
      "discussionId": "68dc885c4159d1f2418f9a2c",
      "ai_summary": "VERA is a benchmark for evaluating reasoning ability in voice-interactive systems, revealing significant performance gaps compared to text models and highlighting challenges in real-time interaction.",
      "ai_keywords": [
        "voice-interactive systems",
        "reasoning ability",
        "real-time conversational constraints",
        "voice-native episodes",
        "text benchmarks",
        "speech interaction",
        "text-voice comparison",
        "architectural choices",
        "reliability",
        "latency-accuracy analyses",
        "low-latency plateau",
        "thinking time",
        "decoupled cascade",
        "error signatures",
        "native streaming",
        "end-to-end",
        "cascade designs",
        "real-time voice assistants",
        "fluent reasoning"
      ]
    },
    "publishedAt": "2025-09-30T13:17:09.000Z",
    "title": "Voice Evaluation of Reasoning Ability: Diagnosing the Modality-Induced\n  Performance Gap",
    "summary": "We present Voice Evaluation of Reasoning Ability (VERA), a benchmark for\nevaluating reasoning ability in voice-interactive systems under real-time\nconversational constraints. VERA comprises 2,931 voice-native episodes derived\nfrom established text benchmarks and organized into five tracks (Math, Web,\nScience, Long-Context, Factual). Each item is adapted for speech interaction\nwhile preserving reasoning difficulty. VERA enables direct text-voice\ncomparison within model families and supports analysis of how architectural\nchoices affect reliability. We assess 12 contemporary voice systems alongside\nstrong text baselines and observe large, consistent modality gaps: on\ncompetition mathematics a leading text model attains 74.8% accuracy while its\nvoice counterpart reaches 6.1%; macro-averaged across tracks the best text\nmodels achieve 54.0% versus 11.3% for voice. Latency-accuracy analyses reveal a\nlow-latency plateau, where fast voice systems cluster around ~10% accuracy,\nwhile approaching text performance requires sacrificing real-time interaction.\nDiagnostic experiments indicate that common mitigations are insufficient.\nIncreasing \"thinking time\" yields negligible gains; a decoupled cascade that\nseparates reasoning from narration improves accuracy but still falls well short\nof text and introduces characteristic grounding/consistency errors. Failure\nanalyses further show distinct error signatures across native streaming,\nend-to-end, and cascade designs. VERA provides a reproducible testbed and\ntargeted diagnostics for architectures that decouple thinking from speaking,\noffering a principled way to measure progress toward real-time voice assistants\nthat are both fluent and reliably reasoned.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.26542.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64b5198c25882acb62fb77ef",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b5198c25882acb62fb77ef/HX9pfMEPQlfjvSAgSLplY.png",
      "fullname": "Yueqian Lin",
      "name": "linyueqian",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2509.26539",
      "authors": [
        {
          "_id": "68dc8f6f4159d1f2418f9a95",
          "name": "Zhen Yang",
          "hidden": false
        },
        {
          "_id": "68dc8f6f4159d1f2418f9a96",
          "name": "Zi-Yi Dou",
          "hidden": false
        },
        {
          "_id": "68dc8f6f4159d1f2418f9a97",
          "name": "Di Feng",
          "hidden": false
        },
        {
          "_id": "68dc8f6f4159d1f2418f9a98",
          "name": "Forrest Huang",
          "hidden": false
        },
        {
          "_id": "68dc8f6f4159d1f2418f9a99",
          "name": "Anh Nguyen",
          "hidden": false
        },
        {
          "_id": "68dc8f6f4159d1f2418f9a9a",
          "name": "Keen You",
          "hidden": false
        },
        {
          "_id": "68dc8f6f4159d1f2418f9a9b",
          "name": "Omar Attia",
          "hidden": false
        },
        {
          "_id": "68dc8f6f4159d1f2418f9a9c",
          "name": "Yuhao Yang",
          "hidden": false
        },
        {
          "_id": "68dc8f6f4159d1f2418f9a9d",
          "name": "Michael Feng",
          "hidden": false
        },
        {
          "_id": "68dc8f6f4159d1f2418f9a9e",
          "name": "Haotian Zhang",
          "hidden": false
        },
        {
          "_id": "68dc8f6f4159d1f2418f9a9f",
          "name": "Ram Ramrakhya",
          "hidden": false
        },
        {
          "_id": "68dc8f6f4159d1f2418f9aa0",
          "name": "Chao Jia",
          "hidden": false
        },
        {
          "_id": "68dc8f6f4159d1f2418f9aa1",
          "name": "Jeffrey Nichols",
          "hidden": false
        },
        {
          "_id": "68dc8f6f4159d1f2418f9aa2",
          "name": "Alexander Toshev",
          "hidden": false
        },
        {
          "_id": "68dc8f6f4159d1f2418f9aa3",
          "name": "Yinfei Yang",
          "hidden": false
        },
        {
          "_id": "68dc8f6f4159d1f2418f9aa4",
          "name": "Zhe Gan",
          "hidden": false
        }
      ],
      "publishedAt": "2025-09-30T17:13:56.000Z",
      "submittedOnDailyAt": "2025-10-01T00:48:59.196Z",
      "title": "Ferret-UI Lite: Lessons from Building Small On-Device GUI Agents",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Developing autonomous agents that effectively interact with Graphic User\nInterfaces (GUIs) remains a challenging open problem, especially for small\non-device models. In this paper, we present Ferret-UI Lite, a compact,\nend-to-end GUI agent that operates across diverse platforms, including mobile,\nweb, and desktop. Utilizing techniques optimized for developing small models,\nwe build our 3B Ferret-UI Lite agent through curating a diverse GUI data\nmixture from real and synthetic sources, strengthening inference-time\nperformance through chain-of-thought reasoning and visual tool-use, and\nreinforcement learning with designed rewards. Ferret-UI Lite achieves\ncompetitive performance with other small-scale GUI agents. In GUI grounding,\nFerret-UI Lite attains scores of 91.6%, 53.3%, and 61.2% on the\nScreenSpot-V2, ScreenSpot-Pro, and OSWorld-G benchmarks, respectively. For GUI\nnavigation, Ferret-UI Lite achieves success rates of 28.0% on AndroidWorld\nand 19.8% on OSWorld. We share our methods and lessons learned from\ndeveloping compact, on-device GUI agents.",
      "upvotes": 1,
      "discussionId": "68dc8f704159d1f2418f9aa5",
      "ai_summary": "Ferret-UI Lite, a compact end-to-end GUI agent, achieves competitive performance across diverse platforms using chain-of-thought reasoning, visual tool-use, and reinforcement learning.",
      "ai_keywords": [
        "chain-of-thought reasoning",
        "visual tool-use",
        "reinforcement learning",
        "GUI agent",
        "GUI grounding",
        "GUI navigation",
        "ScreenSpot-V2",
        "ScreenSpot-Pro",
        "OSWorld-G",
        "AndroidWorld",
        "OSWorld"
      ]
    },
    "publishedAt": "2025-09-30T13:13:56.000Z",
    "title": "Ferret-UI Lite: Lessons from Building Small On-Device GUI Agents",
    "summary": "Developing autonomous agents that effectively interact with Graphic User\nInterfaces (GUIs) remains a challenging open problem, especially for small\non-device models. In this paper, we present Ferret-UI Lite, a compact,\nend-to-end GUI agent that operates across diverse platforms, including mobile,\nweb, and desktop. Utilizing techniques optimized for developing small models,\nwe build our 3B Ferret-UI Lite agent through curating a diverse GUI data\nmixture from real and synthetic sources, strengthening inference-time\nperformance through chain-of-thought reasoning and visual tool-use, and\nreinforcement learning with designed rewards. Ferret-UI Lite achieves\ncompetitive performance with other small-scale GUI agents. In GUI grounding,\nFerret-UI Lite attains scores of 91.6%, 53.3%, and 61.2% on the\nScreenSpot-V2, ScreenSpot-Pro, and OSWorld-G benchmarks, respectively. For GUI\nnavigation, Ferret-UI Lite achieves success rates of 28.0% on AndroidWorld\nand 19.8% on OSWorld. We share our methods and lessons learned from\ndeveloping compact, on-device GUI agents.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.26539.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 115
    },
    "submitterOrganization": {
      "_id": "628cbd99ef14f971b69948ab",
      "name": "apple",
      "fullname": "Apple",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1653390727490-5dd96eb166059660ed1ee413.jpeg"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2509.26476",
      "authors": [
        {
          "_id": "68dc92b64159d1f2418f9ae2",
          "name": "Yash Akhauri",
          "hidden": false
        },
        {
          "_id": "68dc92b64159d1f2418f9ae3",
          "name": "Xingyou Song",
          "hidden": false
        },
        {
          "_id": "68dc92b64159d1f2418f9ae4",
          "name": "Arissa Wongpanich",
          "hidden": false
        },
        {
          "_id": "68dc92b64159d1f2418f9ae5",
          "name": "Bryan Lewandowski",
          "hidden": false
        },
        {
          "_id": "68dc92b64159d1f2418f9ae6",
          "name": "Mohamed S. Abdelfattah",
          "hidden": false
        }
      ],
      "publishedAt": "2025-09-30T16:25:23.000Z",
      "submittedOnDailyAt": "2025-10-01T01:02:49.611Z",
      "title": "Regression Language Models for Code",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "We study code-to-metric regression: predicting numeric outcomes of code\nexecutions, a challenging task due to the open-ended nature of programming\nlanguages. While prior methods have resorted to heavy and domain-specific\nfeature engineering, we show that a single unified Regression Language Model\n(RLM) can simultaneously predict directly from text, (i) the memory footprint\nof code across multiple high-level languages such as Python and C++, (ii) the\nlatency of Triton GPU kernels, and (iii) the accuracy and speed of trained\nneural networks represented in ONNX. In particular, a relatively small 300M\nparameter RLM initialized from T5Gemma, obtains > 0.9 Spearman-rank on\ncompetitive programming submissions from APPS, and a single unified model\nachieves > 0.5 average Spearman-rank across 17 separate languages from CodeNet.\nFurthermore, the RLM can obtain the highest average Kendall-Tau of 0.46 on five\nclassic NAS design spaces previously dominated by graph neural networks, and\nsimultaneously predict architecture latencies on numerous hardware platforms.",
      "upvotes": 1,
      "discussionId": "68dc92b64159d1f2418f9ae7",
      "ai_summary": "A unified Regression Language Model (RLM) predicts numeric outcomes of code executions, including memory footprint, latency, and neural network performance, across multiple languages and hardware platforms.",
      "ai_keywords": [
        "Regression Language Model (RLM)",
        "T5Gemma",
        "Spearman-rank",
        "Kendall-Tau",
        "NAS design spaces",
        "graph neural networks"
      ]
    },
    "publishedAt": "2025-09-30T12:25:23.000Z",
    "title": "Regression Language Models for Code",
    "summary": "We study code-to-metric regression: predicting numeric outcomes of code\nexecutions, a challenging task due to the open-ended nature of programming\nlanguages. While prior methods have resorted to heavy and domain-specific\nfeature engineering, we show that a single unified Regression Language Model\n(RLM) can simultaneously predict directly from text, (i) the memory footprint\nof code across multiple high-level languages such as Python and C++, (ii) the\nlatency of Triton GPU kernels, and (iii) the accuracy and speed of trained\nneural networks represented in ONNX. In particular, a relatively small 300M\nparameter RLM initialized from T5Gemma, obtains > 0.9 Spearman-rank on\ncompetitive programming submissions from APPS, and a single unified model\nachieves > 0.5 average Spearman-rank across 17 separate languages from CodeNet.\nFurthermore, the RLM can obtain the highest average Kendall-Tau of 0.46 on five\nclassic NAS design spaces previously dominated by graph neural networks, and\nsimultaneously predict architecture latencies on numerous hardware platforms.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.26476.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 115
    },
    "submitterOrganization": {
      "_id": "5e6aca39878b8b2bf9806447",
      "name": "google",
      "fullname": "Google",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/WtA3YYitedOr9n02eHfJe.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2509.26391",
      "authors": [
        {
          "_id": "68dc90ca4159d1f2418f9ab8",
          "name": "Chenhui Zhu",
          "hidden": false
        },
        {
          "_id": "68dc90ca4159d1f2418f9ab9",
          "name": "Yilu Wu",
          "hidden": false
        },
        {
          "_id": "68dc90ca4159d1f2418f9aba",
          "name": "Shuai Wang",
          "hidden": false
        },
        {
          "_id": "68dc90ca4159d1f2418f9abb",
          "name": "Gangshan Wu",
          "hidden": false
        },
        {
          "_id": "68dc90ca4159d1f2418f9abc",
          "name": "Limin Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-09-30T15:26:04.000Z",
      "submittedOnDailyAt": "2025-10-01T01:00:58.800Z",
      "title": "MotionRAG: Motion Retrieval-Augmented Image-to-Video Generation",
      "submittedOnDailyBy": {
        "_id": "65e55ca4a0681de63022843e",
        "avatarUrl": "/avatars/5b1ac4a81f0c38fda6f47b392f7474c8.svg",
        "isPro": false,
        "fullname": "zhu chenhui",
        "user": "flateon",
        "type": "user"
      },
      "summary": "Image-to-video generation has made remarkable progress with the advancements\nin diffusion models, yet generating videos with realistic motion remains highly\nchallenging. This difficulty arises from the complexity of accurately modeling\nmotion, which involves capturing physical constraints, object interactions, and\ndomain-specific dynamics that are not easily generalized across diverse\nscenarios. To address this, we propose MotionRAG, a retrieval-augmented\nframework that enhances motion realism by adapting motion priors from relevant\nreference videos through Context-Aware Motion Adaptation (CAMA). The key\ntechnical innovations include: (i) a retrieval-based pipeline extracting\nhigh-level motion features using video encoder and specialized resamplers to\ndistill semantic motion representations; (ii) an in-context learning approach\nfor motion adaptation implemented through a causal transformer architecture;\n(iii) an attention-based motion injection adapter that seamlessly integrates\ntransferred motion features into pretrained video diffusion models. Extensive\nexperiments demonstrate that our method achieves significant improvements\nacross multiple domains and various base models, all with negligible\ncomputational overhead during inference. Furthermore, our modular design\nenables zero-shot generalization to new domains by simply updating the\nretrieval database without retraining any components. This research enhances\nthe core capability of video generation systems by enabling the effective\nretrieval and transfer of motion priors, facilitating the synthesis of\nrealistic motion dynamics.",
      "upvotes": 1,
      "discussionId": "68dc90ca4159d1f2418f9abd",
      "githubRepo": "https://github.com/MCG-NJU/MotionRAG",
      "ai_summary": "MotionRAG enhances video generation by integrating motion priors from reference videos using a retrieval-augmented framework, improving motion realism with negligible computational overhead.",
      "ai_keywords": [
        "diffusion models",
        "MotionRAG",
        "Context-Aware Motion Adaptation (CAMA)",
        "video encoder",
        "specialized resamplers",
        "causal transformer architecture",
        "attention-based motion injection adapter",
        "zero-shot generalization"
      ]
    },
    "publishedAt": "2025-09-30T11:26:04.000Z",
    "title": "MotionRAG: Motion Retrieval-Augmented Image-to-Video Generation",
    "summary": "Image-to-video generation has made remarkable progress with the advancements\nin diffusion models, yet generating videos with realistic motion remains highly\nchallenging. This difficulty arises from the complexity of accurately modeling\nmotion, which involves capturing physical constraints, object interactions, and\ndomain-specific dynamics that are not easily generalized across diverse\nscenarios. To address this, we propose MotionRAG, a retrieval-augmented\nframework that enhances motion realism by adapting motion priors from relevant\nreference videos through Context-Aware Motion Adaptation (CAMA). The key\ntechnical innovations include: (i) a retrieval-based pipeline extracting\nhigh-level motion features using video encoder and specialized resamplers to\ndistill semantic motion representations; (ii) an in-context learning approach\nfor motion adaptation implemented through a causal transformer architecture;\n(iii) an attention-based motion injection adapter that seamlessly integrates\ntransferred motion features into pretrained video diffusion models. Extensive\nexperiments demonstrate that our method achieves significant improvements\nacross multiple domains and various base models, all with negligible\ncomputational overhead during inference. Furthermore, our modular design\nenables zero-shot generalization to new domains by simply updating the\nretrieval database without retraining any components. This research enhances\nthe core capability of video generation systems by enabling the effective\nretrieval and transfer of motion priors, facilitating the synthesis of\nrealistic motion dynamics.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.26391.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65e55ca4a0681de63022843e",
      "avatarUrl": "/avatars/5b1ac4a81f0c38fda6f47b392f7474c8.svg",
      "fullname": "zhu chenhui",
      "name": "flateon",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2509.25541",
      "authors": [
        {
          "_id": "68dc90f34159d1f2418f9abf",
          "name": "Qinsi Wang",
          "hidden": false
        },
        {
          "_id": "68dc90f34159d1f2418f9ac0",
          "name": "Bo Liu",
          "hidden": false
        },
        {
          "_id": "68dc90f34159d1f2418f9ac1",
          "name": "Tianyi Zhou",
          "hidden": false
        },
        {
          "_id": "68dc90f34159d1f2418f9ac2",
          "name": "Jing Shi",
          "hidden": false
        },
        {
          "_id": "68dc90f34159d1f2418f9ac3",
          "name": "Yueqian Lin",
          "hidden": false
        },
        {
          "_id": "68dc90f34159d1f2418f9ac4",
          "name": "Yiran Chen",
          "hidden": false
        },
        {
          "_id": "68dc90f34159d1f2418f9ac5",
          "name": "Hai Helen Li",
          "hidden": false
        },
        {
          "_id": "68dc90f34159d1f2418f9ac6",
          "name": "Kun Wan",
          "hidden": false
        },
        {
          "_id": "68dc90f34159d1f2418f9ac7",
          "name": "Wentian Zhao",
          "hidden": false
        }
      ],
      "publishedAt": "2025-09-29T21:55:55.000Z",
      "submittedOnDailyAt": "2025-10-01T00:55:06.135Z",
      "title": "Vision-Zero: Scalable VLM Self-Improvement via Strategic Gamified\n  Self-Play",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Although reinforcement learning (RL) can effectively enhance the reasoning\ncapabilities of vision-language models (VLMs), current methods remain heavily\ndependent on labor-intensive datasets that require extensive manual\nconstruction and verification, leading to extremely high training costs and\nconsequently constraining the practical deployment of VLMs. To address this\nchallenge, we propose Vision-Zero, a domain-agnostic framework enabling VLM\nself-improvement through competitive visual games generated from arbitrary\nimage pairs. Specifically, Vision-Zero encompasses three main attributes: (1)\nStrategic Self-Play Framework: Vision-Zero trains VLMs in \"Who Is the\nSpy\"-style games, where the models engage in strategic reasoning and actions\nacross multiple roles. Through interactive gameplay, models autonomously\ngenerate their training data without human annotation. (2) Gameplay from\nArbitrary Images: Unlike existing gamified frameworks, Vision-Zero can generate\ngames from arbitrary images, thereby enhancing the model's reasoning ability\nacross diverse domains and showing strong generalization to different tasks. We\ndemonstrate this versatility using three distinct types of image datasets:\nCLEVR-based synthetic scenes, charts, and real-world images. (3) Sustainable\nPerformance Gain: We introduce Iterative Self-Play Policy Optimization\n(Iterative-SPO), a novel training algorithm that alternates between Self-Play\nand reinforcement learning with verifiable rewards (RLVR), mitigating the\nperformance plateau often seen in self-play-only training and achieving\nsustained long-term improvements. Despite using label-free data, Vision-Zero\nachieves state-of-the-art performance on reasoning, chart question answering,\nand vision-centric understanding tasks, surpassing other annotation-based\nmethods. Models and code has been released at\nhttps://github.com/wangqinsi1/Vision-Zero.",
      "upvotes": 1,
      "discussionId": "68dc90f34159d1f2418f9ac8",
      "githubRepo": "https://github.com/wangqinsi1/Vision-Zero",
      "ai_summary": "Vision-Zero is a domain-agnostic framework that enhances vision-language models through self-improvement in competitive visual games, using Iterative Self-Play Policy Optimization and achieving state-of-the-art performance without human annotation.",
      "ai_keywords": [
        "reinforcement learning",
        "vision-language models",
        "strategic self-play framework",
        "self-play",
        "reinforcement learning with verifiable rewards",
        "Iterative Self-Play Policy Optimization"
      ]
    },
    "publishedAt": "2025-09-29T17:55:55.000Z",
    "title": "Vision-Zero: Scalable VLM Self-Improvement via Strategic Gamified\n  Self-Play",
    "summary": "Although reinforcement learning (RL) can effectively enhance the reasoning\ncapabilities of vision-language models (VLMs), current methods remain heavily\ndependent on labor-intensive datasets that require extensive manual\nconstruction and verification, leading to extremely high training costs and\nconsequently constraining the practical deployment of VLMs. To address this\nchallenge, we propose Vision-Zero, a domain-agnostic framework enabling VLM\nself-improvement through competitive visual games generated from arbitrary\nimage pairs. Specifically, Vision-Zero encompasses three main attributes: (1)\nStrategic Self-Play Framework: Vision-Zero trains VLMs in \"Who Is the\nSpy\"-style games, where the models engage in strategic reasoning and actions\nacross multiple roles. Through interactive gameplay, models autonomously\ngenerate their training data without human annotation. (2) Gameplay from\nArbitrary Images: Unlike existing gamified frameworks, Vision-Zero can generate\ngames from arbitrary images, thereby enhancing the model's reasoning ability\nacross diverse domains and showing strong generalization to different tasks. We\ndemonstrate this versatility using three distinct types of image datasets:\nCLEVR-based synthetic scenes, charts, and real-world images. (3) Sustainable\nPerformance Gain: We introduce Iterative Self-Play Policy Optimization\n(Iterative-SPO), a novel training algorithm that alternates between Self-Play\nand reinforcement learning with verifiable rewards (RLVR), mitigating the\nperformance plateau often seen in self-play-only training and achieving\nsustained long-term improvements. Despite using label-free data, Vision-Zero\nachieves state-of-the-art performance on reasoning, chart question answering,\nand vision-centric understanding tasks, surpassing other annotation-based\nmethods. Models and code has been released at\nhttps://github.com/wangqinsi1/Vision-Zero.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.25541.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 115
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2509.25339",
      "authors": [
        {
          "_id": "68dc916b4159d1f2418f9aca",
          "name": "Paul Gavrikov",
          "hidden": false
        },
        {
          "_id": "68dc916b4159d1f2418f9acb",
          "name": "Wei Lin",
          "hidden": false
        },
        {
          "_id": "68dc916b4159d1f2418f9acc",
          "name": "M. Jehanzeb Mirza",
          "hidden": false
        },
        {
          "_id": "68dc916b4159d1f2418f9acd",
          "name": "Soumya Jahagirdar",
          "hidden": false
        },
        {
          "_id": "68dc916b4159d1f2418f9ace",
          "name": "Muhammad Huzaifa",
          "hidden": false
        },
        {
          "_id": "68dc916b4159d1f2418f9acf",
          "name": "Sivan Doveh",
          "hidden": false
        },
        {
          "_id": "68dc916b4159d1f2418f9ad0",
          "name": "Serena Yeung-Levy",
          "hidden": false
        },
        {
          "_id": "68dc916b4159d1f2418f9ad1",
          "name": "James Glass",
          "hidden": false
        },
        {
          "_id": "68dc916b4159d1f2418f9ad2",
          "name": "Hilde Kuehne",
          "hidden": false
        }
      ],
      "publishedAt": "2025-09-29T18:00:25.000Z",
      "submittedOnDailyAt": "2025-10-01T00:57:09.721Z",
      "title": "VisualOverload: Probing Visual Understanding of VLMs in Really Dense\n  Scenes",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Is basic visual understanding really solved in state-of-the-art VLMs? We\npresent VisualOverload, a slightly different visual question answering (VQA)\nbenchmark comprising 2,720 question-answer pairs, with privately held\nground-truth responses. Unlike prior VQA datasets that typically focus on near\nglobal image understanding, VisualOverload challenges models to perform simple,\nknowledge-free vision tasks in densely populated (or, overloaded) scenes. Our\ndataset consists of high-resolution scans of public-domain paintings that are\npopulated with multiple figures, actions, and unfolding subplots set against\nelaborately detailed backdrops. We manually annotated these images with\nquestions across six task categories to probe for a thorough understanding of\nthe scene. We hypothesize that current benchmarks overestimate the performance\nof VLMs, and encoding and reasoning over details is still a challenging task\nfor them, especially if they are confronted with densely populated scenes.\nIndeed, we observe that even the best model (o3) out of 37 tested models only\nachieves 19.6% accuracy on our hardest test split and overall 69.5% accuracy on\nall questions. Beyond a thorough evaluation, we complement our benchmark with\nan error analysis that reveals multiple failure modes, including a lack of\ncounting skills, failure in OCR, and striking logical inconsistencies under\ncomplex tasks. Altogether, VisualOverload exposes a critical gap in current\nvision models and offers a crucial resource for the community to develop better\nmodels.\n  Benchmark: http://paulgavrikov.github.io/visualoverload",
      "upvotes": 1,
      "discussionId": "68dc916b4159d1f2418f9ad3",
      "projectPage": "https://paulgavrikov.github.io/visualoverload/",
      "githubRepo": "https://github.com/paulgavrikov/visualoverload",
      "ai_summary": "VisualOverload is a VQA benchmark that challenges models with simple vision tasks in densely populated scenes, revealing gaps in current VLMs' performance and offering insights into their failure modes.",
      "ai_keywords": [
        "visual question answering",
        "VQA",
        "densely populated scenes",
        "high-resolution scans",
        "public-domain paintings",
        "manual annotation",
        "task categories",
        "error analysis",
        "counting skills",
        "OCR",
        "logical inconsistencies",
        "vision models"
      ]
    },
    "publishedAt": "2025-09-29T14:00:25.000Z",
    "title": "VisualOverload: Probing Visual Understanding of VLMs in Really Dense\n  Scenes",
    "summary": "Is basic visual understanding really solved in state-of-the-art VLMs? We\npresent VisualOverload, a slightly different visual question answering (VQA)\nbenchmark comprising 2,720 question-answer pairs, with privately held\nground-truth responses. Unlike prior VQA datasets that typically focus on near\nglobal image understanding, VisualOverload challenges models to perform simple,\nknowledge-free vision tasks in densely populated (or, overloaded) scenes. Our\ndataset consists of high-resolution scans of public-domain paintings that are\npopulated with multiple figures, actions, and unfolding subplots set against\nelaborately detailed backdrops. We manually annotated these images with\nquestions across six task categories to probe for a thorough understanding of\nthe scene. We hypothesize that current benchmarks overestimate the performance\nof VLMs, and encoding and reasoning over details is still a challenging task\nfor them, especially if they are confronted with densely populated scenes.\nIndeed, we observe that even the best model (o3) out of 37 tested models only\nachieves 19.6% accuracy on our hardest test split and overall 69.5% accuracy on\nall questions. Beyond a thorough evaluation, we complement our benchmark with\nan error analysis that reveals multiple failure modes, including a lack of\ncounting skills, failure in OCR, and striking logical inconsistencies under\ncomplex tasks. Altogether, VisualOverload exposes a critical gap in current\nvision models and offers a crucial resource for the community to develop better\nmodels.\n  Benchmark: http://paulgavrikov.github.io/visualoverload",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.25339.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 115
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2509.22613",
      "authors": [
        {
          "_id": "68dc86ef4159d1f2418f9a09",
          "name": "Siwei Wang",
          "hidden": false
        },
        {
          "_id": "68dc86ef4159d1f2418f9a0a",
          "name": "Yifei Shen",
          "hidden": false
        },
        {
          "_id": "68dc86ef4159d1f2418f9a0b",
          "name": "Haoran Sun",
          "hidden": false
        },
        {
          "_id": "68dc86ef4159d1f2418f9a0c",
          "name": "Shi Feng",
          "hidden": false
        },
        {
          "_id": "68dc86ef4159d1f2418f9a0d",
          "name": "Shang-Hua Teng",
          "hidden": false
        },
        {
          "_id": "68dc86ef4159d1f2418f9a0e",
          "name": "Li Dong",
          "hidden": false
        },
        {
          "_id": "68dc86ef4159d1f2418f9a0f",
          "name": "Yaru Hao",
          "hidden": false
        },
        {
          "_id": "68dc86ef4159d1f2418f9a10",
          "name": "Wei Chen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-09-26T17:39:48.000Z",
      "submittedOnDailyAt": "2025-10-01T00:20:26.292Z",
      "title": "Benefits and Pitfalls of Reinforcement Learning for Language Model\n  Planning: A Theoretical Perspective",
      "submittedOnDailyBy": {
        "_id": "649aa367c6cf3cc95bc1b7f6",
        "avatarUrl": "/avatars/4bf5446c261eab08fc06caebf4c5779a.svg",
        "isPro": false,
        "fullname": "Yifei Shen",
        "user": "yshenaw",
        "type": "user"
      },
      "summary": "Recent reinforcement learning (RL) methods have substantially enhanced the\nplanning capabilities of Large Language Models (LLMs), yet the theoretical\nbasis for their effectiveness remains elusive. In this work, we investigate\nRL's benefits and limitations through a tractable graph-based abstraction,\nfocusing on policy gradient (PG) and Q-learning methods. Our theoretical\nanalyses reveal that supervised fine-tuning (SFT) may introduce\nco-occurrence-based spurious solutions, whereas RL achieves correct planning\nprimarily through exploration, underscoring exploration's role in enabling\nbetter generalization. However, we also show that PG suffers from diversity\ncollapse, where output diversity decreases during training and persists even\nafter perfect accuracy is attained. By contrast, Q-learning provides two key\nadvantages: off-policy learning and diversity preservation at convergence. We\nfurther demonstrate that careful reward design is necessary to prevent reward\nhacking in Q-learning. Finally, applying our framework to the real-world\nplanning benchmark Blocksworld, we confirm that these behaviors manifest in\npractice.",
      "upvotes": 1,
      "discussionId": "68dc86f04159d1f2418f9a11",
      "ai_summary": "Theoretical analysis of reinforcement learning methods in enhancing LLM planning reveals that while RL improves generalization through exploration, policy gradient suffers from diversity collapse, whereas Q-learning maintains diversity and requires careful reward design.",
      "ai_keywords": [
        "reinforcement learning",
        "Large Language Models",
        "policy gradient",
        "Q-learning",
        "supervised fine-tuning",
        "co-occurrence-based spurious solutions",
        "diversity collapse",
        "off-policy learning",
        "reward hacking",
        "Blocksworld"
      ]
    },
    "publishedAt": "2025-09-26T13:39:48.000Z",
    "title": "Benefits and Pitfalls of Reinforcement Learning for Language Model\n  Planning: A Theoretical Perspective",
    "summary": "Recent reinforcement learning (RL) methods have substantially enhanced the\nplanning capabilities of Large Language Models (LLMs), yet the theoretical\nbasis for their effectiveness remains elusive. In this work, we investigate\nRL's benefits and limitations through a tractable graph-based abstraction,\nfocusing on policy gradient (PG) and Q-learning methods. Our theoretical\nanalyses reveal that supervised fine-tuning (SFT) may introduce\nco-occurrence-based spurious solutions, whereas RL achieves correct planning\nprimarily through exploration, underscoring exploration's role in enabling\nbetter generalization. However, we also show that PG suffers from diversity\ncollapse, where output diversity decreases during training and persists even\nafter perfect accuracy is attained. By contrast, Q-learning provides two key\nadvantages: off-policy learning and diversity preservation at convergence. We\nfurther demonstrate that careful reward design is necessary to prevent reward\nhacking in Q-learning. Finally, applying our framework to the real-world\nplanning benchmark Blocksworld, we confirm that these behaviors manifest in\npractice.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.22613.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "649aa367c6cf3cc95bc1b7f6",
      "avatarUrl": "/avatars/4bf5446c261eab08fc06caebf4c5779a.svg",
      "fullname": "Yifei Shen",
      "name": "yshenaw",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "submitterOrganization": {
      "_id": "68151d0f51add3813f3f7d1b",
      "name": "MicrosoftResearch",
      "fullname": "Microsoft Research",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6529a4f2f1205983224fa513/PeuVr7jSuJflmDBBGxoDX.png"
    },
    "isAuthorParticipating": false
  }
]