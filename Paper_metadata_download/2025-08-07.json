[
  {
    "paper": {
      "id": "2508.04586",
      "authors": [
        {
          "_id": "6893fe53741a16f544fbce21",
          "name": "Nuo Chen",
          "hidden": false
        },
        {
          "_id": "6893fe53741a16f544fbce22",
          "name": "Moming Duan",
          "hidden": false
        },
        {
          "_id": "6893fe53741a16f544fbce23",
          "name": "Andre Huikai Lin",
          "hidden": false
        },
        {
          "_id": "6893fe53741a16f544fbce24",
          "name": "Qian Wang",
          "hidden": false
        },
        {
          "_id": "6893fe53741a16f544fbce25",
          "name": "Jiaying Wu",
          "hidden": false
        },
        {
          "_id": "6893fe53741a16f544fbce26",
          "name": "Bingsheng He",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/641ac2207c21ab946bf036e8/yyBhKvXaRlW-2Z24Q0vJ1.gif"
      ],
      "publishedAt": "2025-08-06T16:08:27.000Z",
      "submittedOnDailyAt": "2025-08-07T01:14:52.604Z",
      "title": "Position: The Current AI Conference Model is Unsustainable! Diagnosing\n  the Crisis of Centralized AI Conference",
      "submittedOnDailyBy": {
        "_id": "641ac2207c21ab946bf036e8",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/641ac2207c21ab946bf036e8/r6c9gpOrul0eC59d9e2Mo.png",
        "isPro": true,
        "fullname": "Nuo Chen",
        "user": "nuojohnchen",
        "type": "user"
      },
      "summary": "Artificial Intelligence (AI) conferences are essential for advancing\nresearch, sharing knowledge, and fostering academic community. However, their\nrapid expansion has rendered the centralized conference model increasingly\nunsustainable. This paper offers a data-driven diagnosis of a structural crisis\nthat threatens the foundational goals of scientific dissemination, equity, and\ncommunity well-being. We identify four key areas of strain: (1) scientifically,\nwith per-author publication rates more than doubling over the past decade to\nover 4.5 papers annually; (2) environmentally, with the carbon footprint of a\nsingle conference exceeding the daily emissions of its host city; (3)\npsychologically, with 71% of online community discourse reflecting negative\nsentiment and 35% referencing mental health concerns; and (4) logistically,\nwith attendance at top conferences such as NeurIPS 2024 beginning to outpace\nvenue capacity. These pressures point to a system that is misaligned with its\ncore mission. In response, we propose the Community-Federated Conference (CFC)\nmodel, which separates peer review, presentation, and networking into globally\ncoordinated but locally organized components, offering a more sustainable,\ninclusive, and resilient path forward for AI research.",
      "upvotes": 3,
      "discussionId": "6893fe53741a16f544fbce27",
      "ai_summary": "The paper diagnoses structural issues in AI conferences, including publication rates, carbon footprint, negative community sentiment, and logistical challenges, and proposes a Community-Federated Conference model to address these issues.",
      "ai_keywords": [
        ""
      ]
    },
    "publishedAt": "2025-08-06T12:08:27.000Z",
    "title": "Position: The Current AI Conference Model is Unsustainable! Diagnosing\n  the Crisis of Centralized AI Conference",
    "summary": "Artificial Intelligence (AI) conferences are essential for advancing\nresearch, sharing knowledge, and fostering academic community. However, their\nrapid expansion has rendered the centralized conference model increasingly\nunsustainable. This paper offers a data-driven diagnosis of a structural crisis\nthat threatens the foundational goals of scientific dissemination, equity, and\ncommunity well-being. We identify four key areas of strain: (1) scientifically,\nwith per-author publication rates more than doubling over the past decade to\nover 4.5 papers annually; (2) environmentally, with the carbon footprint of a\nsingle conference exceeding the daily emissions of its host city; (3)\npsychologically, with 71% of online community discourse reflecting negative\nsentiment and 35% referencing mental health concerns; and (4) logistically,\nwith attendance at top conferences such as NeurIPS 2024 beginning to outpace\nvenue capacity. These pressures point to a system that is misaligned with its\ncore mission. In response, we propose the Community-Federated Conference (CFC)\nmodel, which separates peer review, presentation, and networking into globally\ncoordinated but locally organized components, offering a more sustainable,\ninclusive, and resilient path forward for AI research.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/641ac2207c21ab946bf036e8/yyBhKvXaRlW-2Z24Q0vJ1.gif"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.04586.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "641ac2207c21ab946bf036e8",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/641ac2207c21ab946bf036e8/r6c9gpOrul0eC59d9e2Mo.png",
      "fullname": "Nuo Chen",
      "name": "nuojohnchen",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 8
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2508.03789",
      "authors": [
        {
          "_id": "6894142c741a16f544fbcec2",
          "name": "Yuhang Ma",
          "hidden": false
        },
        {
          "_id": "6894142c741a16f544fbcec3",
          "name": "Xiaoshi Wu",
          "hidden": false
        },
        {
          "_id": "6894142c741a16f544fbcec4",
          "name": "Keqiang Sun",
          "hidden": false
        },
        {
          "_id": "6894142c741a16f544fbcec5",
          "name": "Hongsheng Li",
          "hidden": false
        }
      ],
      "publishedAt": "2025-08-05T17:17:13.000Z",
      "submittedOnDailyAt": "2025-08-07T01:20:19.742Z",
      "title": "HPSv3: Towards Wide-Spectrum Human Preference Score",
      "submittedOnDailyBy": {
        "_id": "645a46df5e6871b4b2d39bb8",
        "avatarUrl": "/avatars/5687fdca5bec77677377b57a06d64b9e.svg",
        "isPro": false,
        "fullname": "Yunhaoshui",
        "user": "xilanhua12138",
        "type": "user"
      },
      "summary": "Evaluating text-to-image generation models requires alignment with human\nperception, yet existing human-centric metrics are constrained by limited data\ncoverage, suboptimal feature extraction, and inefficient loss functions. To\naddress these challenges, we introduce Human Preference Score v3 (HPSv3). (1)\nWe release HPDv3, the first wide-spectrum human preference dataset integrating\n1.08M text-image pairs and 1.17M annotated pairwise comparisons from\nstate-of-the-art generative models and low to high-quality real-world images.\n(2) We introduce a VLM-based preference model trained using an\nuncertainty-aware ranking loss for fine-grained ranking. Besides, we propose\nChain-of-Human-Preference (CoHP), an iterative image refinement method that\nenhances quality without extra data, using HPSv3 to select the best image at\neach step. Extensive experiments demonstrate that HPSv3 serves as a robust\nmetric for wide-spectrum image evaluation, and CoHP offers an efficient and\nhuman-aligned approach to improve image generation quality. The code and\ndataset are available at the HPSv3 Homepage.",
      "upvotes": 1,
      "discussionId": "6894142d741a16f544fbcec6",
      "ai_summary": "HPSv3, a human preference score using a wide-spectrum dataset and uncertainty-aware ranking loss, enhances text-to-image generation quality through iterative refinement.",
      "ai_keywords": [
        "human preference score",
        "HPDv3",
        "text-image pairs",
        "pairwise comparisons",
        "VLM-based preference model",
        "uncertainty-aware ranking loss",
        "Chain-of-Human-Preference",
        "CoHP",
        "image refinement",
        "image generation quality"
      ]
    },
    "publishedAt": "2025-08-05T13:17:13.000Z",
    "title": "HPSv3: Towards Wide-Spectrum Human Preference Score",
    "summary": "Evaluating text-to-image generation models requires alignment with human\nperception, yet existing human-centric metrics are constrained by limited data\ncoverage, suboptimal feature extraction, and inefficient loss functions. To\naddress these challenges, we introduce Human Preference Score v3 (HPSv3). (1)\nWe release HPDv3, the first wide-spectrum human preference dataset integrating\n1.08M text-image pairs and 1.17M annotated pairwise comparisons from\nstate-of-the-art generative models and low to high-quality real-world images.\n(2) We introduce a VLM-based preference model trained using an\nuncertainty-aware ranking loss for fine-grained ranking. Besides, we propose\nChain-of-Human-Preference (CoHP), an iterative image refinement method that\nenhances quality without extra data, using HPSv3 to select the best image at\neach step. Extensive experiments demonstrate that HPSv3 serves as a robust\nmetric for wide-spectrum image evaluation, and CoHP offers an efficient and\nhuman-aligned approach to improve image generation quality. The code and\ndataset are available at the HPSv3 Homepage.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.03789.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "645a46df5e6871b4b2d39bb8",
      "avatarUrl": "/avatars/5687fdca5bec77677377b57a06d64b9e.svg",
      "fullname": "Yunhaoshui",
      "name": "xilanhua12138",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 8
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2508.02215",
      "authors": [
        {
          "_id": "6894106d741a16f544fbcea6",
          "name": "Yike Zhang",
          "hidden": false
        },
        {
          "_id": "6894106d741a16f544fbcea7",
          "name": "Zhiyuan He",
          "hidden": false
        },
        {
          "_id": "6894106d741a16f544fbcea8",
          "name": "Huiqiang Jiang",
          "hidden": false
        },
        {
          "_id": "6894106d741a16f544fbcea9",
          "name": "Chengruidong Zhang",
          "hidden": false
        },
        {
          "_id": "6894106d741a16f544fbceaa",
          "name": "Yuqing Yang",
          "hidden": false
        },
        {
          "_id": "6894106d741a16f544fbceab",
          "name": "Jianyong Wang",
          "hidden": false
        },
        {
          "_id": "6894106d741a16f544fbceac",
          "name": "Lili Qiu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-08-04T09:08:43.000Z",
      "submittedOnDailyAt": "2025-08-07T01:18:44.229Z",
      "title": "LeanK: Learnable K Cache Channel Pruning for Efficient Decoding",
      "submittedOnDailyBy": {
        "_id": "6541159afcbd1aa00682ec66",
        "avatarUrl": "/avatars/94fd9ce0ea534e5d84736c1a1eb949e7.svg",
        "isPro": false,
        "fullname": "zhangyik21",
        "user": "zhangyik21",
        "type": "user"
      },
      "summary": "Large language models (LLMs) enable long-context tasks but face efficiency\nchallenges due to the growing key-value (KV) cache. We propose LeanK, a\nlearning-based method that prunes unimportant key (K) cache channels by\nleveraging static channel sparsity. With a novel two-stage training process,\nLeanK learns channel-wise static mask that could satisfy specific sparsity\nratio and hardware alignment requirement. LeanK reduces GPU memory and\naccelerates decoding without sacrificing accuracy. Experiments demonstrate up\nto 70% K cache and 16%-18% V cache memory reduction. Custom decoding kernel\nenables 1.3x speedup for attention computation. We also provide insights into\nmodel channels and attention heads during long-context inference by analyzing\nthe learned importance distribution. Our code is available at\nhttps://aka.ms/LeanK.",
      "upvotes": 1,
      "discussionId": "6894106d741a16f544fbcead",
      "projectPage": "https://aka.ms/LeanK",
      "ai_summary": "LeanK, a learning-based method, prunes unimportant key cache channels in large language models to reduce memory usage and accelerate decoding without sacrificing accuracy.",
      "ai_keywords": [
        "large language models",
        "key-value cache",
        "channel sparsity",
        "two-stage training",
        "channel-wise static mask",
        "GPU memory",
        "decoding speedup",
        "attention computation",
        "long-context inference",
        "importance distribution"
      ]
    },
    "publishedAt": "2025-08-04T05:08:43.000Z",
    "title": "LeanK: Learnable K Cache Channel Pruning for Efficient Decoding",
    "summary": "Large language models (LLMs) enable long-context tasks but face efficiency\nchallenges due to the growing key-value (KV) cache. We propose LeanK, a\nlearning-based method that prunes unimportant key (K) cache channels by\nleveraging static channel sparsity. With a novel two-stage training process,\nLeanK learns channel-wise static mask that could satisfy specific sparsity\nratio and hardware alignment requirement. LeanK reduces GPU memory and\naccelerates decoding without sacrificing accuracy. Experiments demonstrate up\nto 70% K cache and 16%-18% V cache memory reduction. Custom decoding kernel\nenables 1.3x speedup for attention computation. We also provide insights into\nmodel channels and attention heads during long-context inference by analyzing\nthe learned importance distribution. Our code is available at\nhttps://aka.ms/LeanK.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.02215.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6541159afcbd1aa00682ec66",
      "avatarUrl": "/avatars/94fd9ce0ea534e5d84736c1a1eb949e7.svg",
      "fullname": "zhangyik21",
      "name": "zhangyik21",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2508.01197",
      "authors": [
        {
          "_id": "6894158a741a16f544fbcedd",
          "name": "Zhan Shi",
          "hidden": false
        },
        {
          "_id": "6894158a741a16f544fbcede",
          "name": "Song Wang",
          "hidden": false
        },
        {
          "_id": "6894158a741a16f544fbcedf",
          "name": "Junbo Chen",
          "hidden": false
        },
        {
          "_id": "6894158a741a16f544fbcee0",
          "name": "Jianke Zhu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-08-02T05:05:50.000Z",
      "submittedOnDailyAt": "2025-08-07T01:25:50.658Z",
      "title": "A Coarse-to-Fine Approach to Multi-Modality 3D Occupancy Grounding",
      "submittedOnDailyBy": {
        "_id": "66863d26e2b71e3d09189ae9",
        "avatarUrl": "/avatars/3c0e6f30e053f2e622ae75e1dc43edba.svg",
        "isPro": false,
        "fullname": "Song Wang",
        "user": "songw-zju",
        "type": "user"
      },
      "summary": "Visual grounding aims to identify objects or regions in a scene based on\nnatural language descriptions, essential for spatially aware perception in\nautonomous driving. However, existing visual grounding tasks typically depend\non bounding boxes that often fail to capture fine-grained details. Not all\nvoxels within a bounding box are occupied, resulting in inaccurate object\nrepresentations. To address this, we introduce a benchmark for 3D occupancy\ngrounding in challenging outdoor scenes. Built on the nuScenes dataset, it\nintegrates natural language with voxel-level occupancy annotations, offering\nmore precise object perception compared to the traditional grounding task.\nMoreover, we propose GroundingOcc, an end-to-end model designed for 3D\noccupancy grounding through multi-modal learning. It combines visual, textual,\nand point cloud features to predict object location and occupancy information\nfrom coarse to fine. Specifically, GroundingOcc comprises a multimodal encoder\nfor feature extraction, an occupancy head for voxel-wise predictions, and a\ngrounding head to refine localization. Additionally, a 2D grounding module and\na depth estimation module enhance geometric understanding, thereby boosting\nmodel performance. Extensive experiments on the benchmark demonstrate that our\nmethod outperforms existing baselines on 3D occupancy grounding. The dataset is\navailable at https://github.com/RONINGOD/GroundingOcc.",
      "upvotes": 1,
      "discussionId": "6894158a741a16f544fbcee1",
      "githubRepo": "https://github.com/RONINGOD/GroundingOcc",
      "ai_summary": "A benchmark and model for 3D occupancy grounding using natural language and voxel-level annotations improve object perception in autonomous driving.",
      "ai_keywords": [
        "visual grounding",
        "3D occupancy grounding",
        "nuScenes dataset",
        "voxel-level occupancy",
        "multimodal learning",
        "multimodal encoder",
        "occupancy head",
        "grounding head",
        "2D grounding module",
        "depth estimation module"
      ]
    },
    "publishedAt": "2025-08-02T01:05:50.000Z",
    "title": "A Coarse-to-Fine Approach to Multi-Modality 3D Occupancy Grounding",
    "summary": "Visual grounding aims to identify objects or regions in a scene based on\nnatural language descriptions, essential for spatially aware perception in\nautonomous driving. However, existing visual grounding tasks typically depend\non bounding boxes that often fail to capture fine-grained details. Not all\nvoxels within a bounding box are occupied, resulting in inaccurate object\nrepresentations. To address this, we introduce a benchmark for 3D occupancy\ngrounding in challenging outdoor scenes. Built on the nuScenes dataset, it\nintegrates natural language with voxel-level occupancy annotations, offering\nmore precise object perception compared to the traditional grounding task.\nMoreover, we propose GroundingOcc, an end-to-end model designed for 3D\noccupancy grounding through multi-modal learning. It combines visual, textual,\nand point cloud features to predict object location and occupancy information\nfrom coarse to fine. Specifically, GroundingOcc comprises a multimodal encoder\nfor feature extraction, an occupancy head for voxel-wise predictions, and a\ngrounding head to refine localization. Additionally, a 2D grounding module and\na depth estimation module enhance geometric understanding, thereby boosting\nmodel performance. Extensive experiments on the benchmark demonstrate that our\nmethod outperforms existing baselines on 3D occupancy grounding. The dataset is\navailable at https://github.com/RONINGOD/GroundingOcc.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.01197.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66863d26e2b71e3d09189ae9",
      "avatarUrl": "/avatars/3c0e6f30e053f2e622ae75e1dc43edba.svg",
      "fullname": "Song Wang",
      "name": "songw-zju",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2508.03905",
      "authors": [
        {
          "_id": "6894156a741a16f544fbced2",
          "name": "Haofei Yu",
          "hidden": false
        },
        {
          "_id": "6894156a741a16f544fbced3",
          "name": "Zhengyang Qi",
          "hidden": false
        },
        {
          "_id": "6894156a741a16f544fbced4",
          "name": "Yining Zhao",
          "hidden": false
        },
        {
          "_id": "6894156a741a16f544fbced5",
          "name": "Kolby Nottingham",
          "hidden": false
        },
        {
          "_id": "6894156a741a16f544fbced6",
          "name": "Keyang Xuan",
          "hidden": false
        },
        {
          "_id": "6894156a741a16f544fbced7",
          "name": "Bodhisattwa Prasad Majumder",
          "hidden": false
        },
        {
          "_id": "6894156a741a16f544fbced8",
          "name": "Hao Zhu",
          "hidden": false
        },
        {
          "_id": "6894156a741a16f544fbced9",
          "name": "Paul Pu Liang",
          "hidden": false
        },
        {
          "_id": "6894156a741a16f544fbceda",
          "name": "Jiaxuan You",
          "hidden": false
        }
      ],
      "publishedAt": "2025-08-05T20:43:42.000Z",
      "submittedOnDailyAt": "2025-08-07T01:30:32.752Z",
      "title": "Sotopia-RL: Reward Design for Social Intelligence",
      "submittedOnDailyBy": {
        "_id": "636453547cf2c0b4f0a3ee1e",
        "avatarUrl": "/avatars/a453340b44d08eec2281ecbe5e993707.svg",
        "isPro": false,
        "fullname": "Haofei Yu",
        "user": "lwaekfjlk",
        "type": "user"
      },
      "summary": "Social intelligence has become a critical capability for large language\nmodels (LLMs), enabling them to engage effectively in real-world social tasks\nsuch as accommodation, persuasion, collaboration, and negotiation.\nReinforcement learning (RL) is a natural fit for training socially intelligent\nagents because it allows models to learn sophisticated strategies directly\nthrough social interactions. However, social interactions have two key\ncharacteristics that set barriers for RL training: (1) partial observability,\nwhere utterances have indirect and delayed effects that complicate credit\nassignment, and (2) multi-dimensionality, where behaviors such as\nrapport-building or knowledge-seeking contribute indirectly to goal\nachievement. These characteristics make Markov decision process (MDP)-based RL\nwith single-dimensional episode-level rewards inefficient and unstable. To\naddress these challenges, we propose Sotopia-RL, a novel framework that refines\ncoarse episode-level feedback into utterance-level, multi-dimensional rewards.\nUtterance-level credit assignment mitigates partial observability by\nattributing outcomes to individual utterances, while multi-dimensional rewards\ncapture the full richness of social interactions and reduce reward hacking.\nExperiments in Sotopia, an open-ended social learning environment, demonstrate\nthat Sotopia-RL achieves state-of-the-art social goal completion scores (7.17\non Sotopia-hard and 8.31 on Sotopia-full), significantly outperforming existing\napproaches. Ablation studies confirm the necessity of both utterance-level\ncredit assignment and multi-dimensional reward design for RL training. Our\nimplementation is publicly available at:\nhttps://github.com/sotopia-lab/sotopia-rl.",
      "upvotes": 0,
      "discussionId": "6894156b741a16f544fbcedb",
      "ai_summary": "Sotopia-RL, a novel reinforcement learning framework, enhances social intelligence in large language models by refining feedback into utterance-level, multi-dimensional rewards, improving performance in social tasks.",
      "ai_keywords": [
        "reinforcement learning",
        "socially intelligent agents",
        "partial observability",
        "multi-dimensionality",
        "Markov decision process",
        "episode-level rewards",
        "utterance-level credit assignment",
        "multi-dimensional rewards",
        "reward hacking",
        "social goal completion scores",
        "Sotopia",
        "Sotopia-RL"
      ]
    },
    "publishedAt": "2025-08-05T16:43:42.000Z",
    "title": "Sotopia-RL: Reward Design for Social Intelligence",
    "summary": "Social intelligence has become a critical capability for large language\nmodels (LLMs), enabling them to engage effectively in real-world social tasks\nsuch as accommodation, persuasion, collaboration, and negotiation.\nReinforcement learning (RL) is a natural fit for training socially intelligent\nagents because it allows models to learn sophisticated strategies directly\nthrough social interactions. However, social interactions have two key\ncharacteristics that set barriers for RL training: (1) partial observability,\nwhere utterances have indirect and delayed effects that complicate credit\nassignment, and (2) multi-dimensionality, where behaviors such as\nrapport-building or knowledge-seeking contribute indirectly to goal\nachievement. These characteristics make Markov decision process (MDP)-based RL\nwith single-dimensional episode-level rewards inefficient and unstable. To\naddress these challenges, we propose Sotopia-RL, a novel framework that refines\ncoarse episode-level feedback into utterance-level, multi-dimensional rewards.\nUtterance-level credit assignment mitigates partial observability by\nattributing outcomes to individual utterances, while multi-dimensional rewards\ncapture the full richness of social interactions and reduce reward hacking.\nExperiments in Sotopia, an open-ended social learning environment, demonstrate\nthat Sotopia-RL achieves state-of-the-art social goal completion scores (7.17\non Sotopia-hard and 8.31 on Sotopia-full), significantly outperforming existing\napproaches. Ablation studies confirm the necessity of both utterance-level\ncredit assignment and multi-dimensional reward design for RL training. Our\nimplementation is publicly available at:\nhttps://github.com/sotopia-lab/sotopia-rl.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.03905.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "636453547cf2c0b4f0a3ee1e",
      "avatarUrl": "/avatars/a453340b44d08eec2281ecbe5e993707.svg",
      "fullname": "Haofei Yu",
      "name": "lwaekfjlk",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2508.01191",
      "authors": [
        {
          "_id": "689414b0741a16f544fbcec8",
          "name": "Chengshuai Zhao",
          "hidden": false
        },
        {
          "_id": "689414b0741a16f544fbcec9",
          "name": "Zhen Tan",
          "hidden": false
        },
        {
          "_id": "689414b0741a16f544fbceca",
          "name": "Pingchuan Ma",
          "hidden": false
        },
        {
          "_id": "689414b0741a16f544fbcecb",
          "name": "Dawei Li",
          "hidden": false
        },
        {
          "_id": "689414b0741a16f544fbcecc",
          "name": "Bohan Jiang",
          "hidden": false
        },
        {
          "_id": "689414b0741a16f544fbcecd",
          "name": "Yancheng Wang",
          "hidden": false
        },
        {
          "_id": "689414b0741a16f544fbcece",
          "name": "Yingzhen Yang",
          "hidden": false
        },
        {
          "_id": "689414b0741a16f544fbcecf",
          "name": "Huan Liu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-08-02T04:37:28.000Z",
      "submittedOnDailyAt": "2025-08-07T01:29:05.289Z",
      "title": "Is Chain-of-Thought Reasoning of LLMs a Mirage? A Data Distribution Lens",
      "submittedOnDailyBy": {
        "_id": "65b2fae679954e21ac426aec",
        "avatarUrl": "/avatars/c495c4ee9afb140a87ebc04602aa8c3e.svg",
        "isPro": false,
        "fullname": "Chengshuai Zhao",
        "user": "chengshuaizhao",
        "type": "user"
      },
      "summary": "Chain-of-Thought (CoT) prompting has been shown to improve Large Language\nModel (LLM) performance on various tasks. With this approach, LLMs appear to\nproduce human-like reasoning steps before providing answers (a.k.a., CoT\nreasoning), which often leads to the perception that they engage in deliberate\ninferential processes. However, some initial findings suggest that CoT\nreasoning may be more superficial than it appears, motivating us to explore\nfurther. In this paper, we study CoT reasoning via a data distribution lens and\ninvestigate if CoT reasoning reflects a structured inductive bias learned from\nin-distribution data, allowing the model to conditionally generate reasoning\npaths that approximate those seen during training. Thus, its effectiveness is\nfundamentally bounded by the degree of distribution discrepancy between the\ntraining data and the test queries. With this lens, we dissect CoT reasoning\nvia three dimensions: task, length, and format. To investigate each dimension,\nwe design DataAlchemy, an isolated and controlled environment to train LLMs\nfrom scratch and systematically probe them under various distribution\nconditions. Our results reveal that CoT reasoning is a brittle mirage that\nvanishes when it is pushed beyond training distributions. This work offers a\ndeeper understanding of why and when CoT reasoning fails, emphasizing the\nongoing challenge of achieving genuine and generalizable reasoning.",
      "upvotes": 0,
      "discussionId": "689414b0741a16f544fbced0",
      "githubRepo": "https://github.com/ChengshuaiZhao0/DataAlchemy",
      "ai_summary": "CoT reasoning in LLMs is found to be limited by the distribution discrepancy between training and test data, suggesting it is not a robust form of reasoning.",
      "ai_keywords": [
        "Chain-of-Thought",
        "Large Language Model",
        "CoT reasoning",
        "inductive bias",
        "DataAlchemy",
        "distribution discrepancy",
        "reasoning paths",
        "generalizable reasoning"
      ]
    },
    "publishedAt": "2025-08-02T00:37:28.000Z",
    "title": "Is Chain-of-Thought Reasoning of LLMs a Mirage? A Data Distribution Lens",
    "summary": "Chain-of-Thought (CoT) prompting has been shown to improve Large Language\nModel (LLM) performance on various tasks. With this approach, LLMs appear to\nproduce human-like reasoning steps before providing answers (a.k.a., CoT\nreasoning), which often leads to the perception that they engage in deliberate\ninferential processes. However, some initial findings suggest that CoT\nreasoning may be more superficial than it appears, motivating us to explore\nfurther. In this paper, we study CoT reasoning via a data distribution lens and\ninvestigate if CoT reasoning reflects a structured inductive bias learned from\nin-distribution data, allowing the model to conditionally generate reasoning\npaths that approximate those seen during training. Thus, its effectiveness is\nfundamentally bounded by the degree of distribution discrepancy between the\ntraining data and the test queries. With this lens, we dissect CoT reasoning\nvia three dimensions: task, length, and format. To investigate each dimension,\nwe design DataAlchemy, an isolated and controlled environment to train LLMs\nfrom scratch and systematically probe them under various distribution\nconditions. Our results reveal that CoT reasoning is a brittle mirage that\nvanishes when it is pushed beyond training distributions. This work offers a\ndeeper understanding of why and when CoT reasoning fails, emphasizing the\nongoing challenge of achieving genuine and generalizable reasoning.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.01191.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65b2fae679954e21ac426aec",
      "avatarUrl": "/avatars/c495c4ee9afb140a87ebc04602aa8c3e.svg",
      "fullname": "Chengshuai Zhao",
      "name": "chengshuaizhao",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.23785",
      "authors": [
        {
          "_id": "6892cf048da45ffb0a2b24b6",
          "user": {
            "_id": "6237f25f16004228e6c74e01",
            "avatarUrl": "/avatars/cc63ce464a25702c8155610d2a708595.svg",
            "isPro": false,
            "fullname": "Bowen Zhang",
            "user": "BwZhang",
            "type": "user"
          },
          "name": "Bowen Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-08-06T19:16:22.835Z",
          "hidden": false
        },
        {
          "_id": "6892cf048da45ffb0a2b24b7",
          "name": "Sicheng Xu",
          "hidden": false
        },
        {
          "_id": "6892cf048da45ffb0a2b24b8",
          "name": "Chuxin Wang",
          "hidden": false
        },
        {
          "_id": "6892cf048da45ffb0a2b24b9",
          "name": "Jiaolong Yang",
          "hidden": false
        },
        {
          "_id": "6892cf048da45ffb0a2b24ba",
          "name": "Feng Zhao",
          "hidden": false
        },
        {
          "_id": "6892cf048da45ffb0a2b24bb",
          "name": "Dong Chen",
          "hidden": false
        },
        {
          "_id": "6892cf048da45ffb0a2b24bc",
          "name": "Baining Guo",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-31T17:59:51.000Z",
      "submittedOnDailyAt": "2025-08-07T01:26:55.033Z",
      "title": "Gaussian Variation Field Diffusion for High-fidelity Video-to-4D\n  Synthesis",
      "submittedOnDailyBy": {
        "_id": "6237f25f16004228e6c74e01",
        "avatarUrl": "/avatars/cc63ce464a25702c8155610d2a708595.svg",
        "isPro": false,
        "fullname": "Bowen Zhang",
        "user": "BwZhang",
        "type": "user"
      },
      "summary": "In this paper, we present a novel framework for video-to-4D generation that\ncreates high-quality dynamic 3D content from single video inputs. Direct 4D\ndiffusion modeling is extremely challenging due to costly data construction and\nthe high-dimensional nature of jointly representing 3D shape, appearance, and\nmotion. We address these challenges by introducing a Direct 4DMesh-to-GS\nVariation Field VAE that directly encodes canonical Gaussian Splats (GS) and\ntheir temporal variations from 3D animation data without per-instance fitting,\nand compresses high-dimensional animations into a compact latent space.\nBuilding upon this efficient representation, we train a Gaussian Variation\nField diffusion model with temporal-aware Diffusion Transformer conditioned on\ninput videos and canonical GS. Trained on carefully-curated animatable 3D\nobjects from the Objaverse dataset, our model demonstrates superior generation\nquality compared to existing methods. It also exhibits remarkable\ngeneralization to in-the-wild video inputs despite being trained exclusively on\nsynthetic data, paving the way for generating high-quality animated 3D content.\nProject page: https://gvfdiffusion.github.io/.",
      "upvotes": 0,
      "discussionId": "6892cf058da45ffb0a2b24bd",
      "projectPage": "https://gvfdiffusion.github.io/",
      "githubRepo": "https://github.com/ForeverFancy/gvfdiffusion",
      "ai_summary": "A novel framework uses a Direct 4DMesh-to-GS Variation Field VAE and Gaussian Variation Field diffusion model to generate high-quality dynamic 3D content from single video inputs, demonstrating superior quality and generalization.",
      "ai_keywords": [
        "Direct 4D diffusion modeling",
        "Gaussian Splats (GS)",
        "Direct 4DMesh-to-GS Variation Field VAE",
        "Gaussian Variation Field diffusion model",
        "Diffusion Transformer",
        "Objaverse dataset"
      ],
      "githubStars": 45
    },
    "publishedAt": "2025-07-31T13:59:51.000Z",
    "title": "Gaussian Variation Field Diffusion for High-fidelity Video-to-4D\n  Synthesis",
    "summary": "In this paper, we present a novel framework for video-to-4D generation that\ncreates high-quality dynamic 3D content from single video inputs. Direct 4D\ndiffusion modeling is extremely challenging due to costly data construction and\nthe high-dimensional nature of jointly representing 3D shape, appearance, and\nmotion. We address these challenges by introducing a Direct 4DMesh-to-GS\nVariation Field VAE that directly encodes canonical Gaussian Splats (GS) and\ntheir temporal variations from 3D animation data without per-instance fitting,\nand compresses high-dimensional animations into a compact latent space.\nBuilding upon this efficient representation, we train a Gaussian Variation\nField diffusion model with temporal-aware Diffusion Transformer conditioned on\ninput videos and canonical GS. Trained on carefully-curated animatable 3D\nobjects from the Objaverse dataset, our model demonstrates superior generation\nquality compared to existing methods. It also exhibits remarkable\ngeneralization to in-the-wild video inputs despite being trained exclusively on\nsynthetic data, paving the way for generating high-quality animated 3D content.\nProject page: https://gvfdiffusion.github.io/.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.23785.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6237f25f16004228e6c74e01",
      "avatarUrl": "/avatars/cc63ce464a25702c8155610d2a708595.svg",
      "fullname": "Bowen Zhang",
      "name": "BwZhang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": true
  }
]