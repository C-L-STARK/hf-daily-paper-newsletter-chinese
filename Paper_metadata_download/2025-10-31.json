[
  {
    "paper": {
      "id": "2510.26583",
      "authors": [
        {
          "_id": "690414cc2c556835fa67efb5",
          "name": "Yufeng Cui",
          "hidden": false
        },
        {
          "_id": "690414cc2c556835fa67efb6",
          "name": "Honghao Chen",
          "hidden": false
        },
        {
          "_id": "690414cc2c556835fa67efb7",
          "name": "Haoge Deng",
          "hidden": false
        },
        {
          "_id": "690414cc2c556835fa67efb8",
          "name": "Xu Huang",
          "hidden": false
        },
        {
          "_id": "690414cc2c556835fa67efb9",
          "name": "Xinghang Li",
          "hidden": false
        },
        {
          "_id": "690414cc2c556835fa67efba",
          "name": "Jirong Liu",
          "hidden": false
        },
        {
          "_id": "690414cc2c556835fa67efbb",
          "name": "Yang Liu",
          "hidden": false
        },
        {
          "_id": "690414cc2c556835fa67efbc",
          "name": "Zhuoyan Luo",
          "hidden": false
        },
        {
          "_id": "690414cc2c556835fa67efbd",
          "name": "Jinsheng Wang",
          "hidden": false
        },
        {
          "_id": "690414cc2c556835fa67efbe",
          "name": "Wenxuan Wang",
          "hidden": false
        },
        {
          "_id": "690414cc2c556835fa67efbf",
          "name": "Yueze Wang",
          "hidden": false
        },
        {
          "_id": "690414cc2c556835fa67efc0",
          "name": "Chengyuan Wang",
          "hidden": false
        },
        {
          "_id": "690414cc2c556835fa67efc1",
          "name": "Fan Zhang",
          "hidden": false
        },
        {
          "_id": "690414cc2c556835fa67efc2",
          "name": "Yingli Zhao",
          "hidden": false
        },
        {
          "_id": "690414cc2c556835fa67efc3",
          "name": "Ting Pan",
          "hidden": false
        },
        {
          "_id": "690414cc2c556835fa67efc4",
          "name": "Xianduo Li",
          "hidden": false
        },
        {
          "_id": "690414cc2c556835fa67efc5",
          "name": "Zecheng Hao",
          "hidden": false
        },
        {
          "_id": "690414cc2c556835fa67efc6",
          "name": "Wenxuan Ma",
          "hidden": false
        },
        {
          "_id": "690414cc2c556835fa67efc7",
          "name": "Zhuo Chen",
          "hidden": false
        },
        {
          "_id": "690414cc2c556835fa67efc8",
          "name": "Yulong Ao",
          "hidden": false
        },
        {
          "_id": "690414cc2c556835fa67efc9",
          "name": "Tiejun Huang",
          "hidden": false
        },
        {
          "_id": "690414cc2c556835fa67efca",
          "name": "Zhongyuan Wang",
          "hidden": false
        },
        {
          "_id": "690414cc2c556835fa67efcb",
          "name": "Xinlong Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-30T15:11:16.000Z",
      "submittedOnDailyAt": "2025-10-31T00:20:30.779Z",
      "title": "Emu3.5: Native Multimodal Models are World Learners",
      "submittedOnDailyBy": {
        "_id": "63ca558304c979828311c5a5",
        "avatarUrl": "/avatars/2a439d79fba2f987cabe780d10c94d25.svg",
        "isPro": false,
        "fullname": "Xinlong Wang",
        "user": "xinlongwang",
        "type": "user"
      },
      "summary": "We introduce Emu3.5, a large-scale multimodal world model that natively\npredicts the next state across vision and language. Emu3.5 is pre-trained\nend-to-end with a unified next-token prediction objective on a corpus of\nvision-language interleaved data containing over 10 trillion tokens, primarily\nderived from sequential frames and transcripts of internet videos. The model\nnaturally accepts interleaved vision-language inputs and generates interleaved\nvision-language outputs. Emu3.5 is further post-trained with large-scale\nreinforcement learning to enhance multimodal reasoning and generation. To\nimprove inference efficiency, we propose Discrete Diffusion Adaptation (DiDA),\nwhich converts token-by-token decoding into bidirectional parallel prediction,\naccelerating per-image inference by about 20x without sacrificing performance.\nEmu3.5 exhibits strong native multimodal capabilities, including long-horizon\nvision-language generation, any-to-image (X2I) generation, and complex\ntext-rich image generation. It also exhibits generalizable world-modeling\nabilities, enabling spatiotemporally consistent world exploration and\nopen-world embodied manipulation across diverse scenarios and tasks. For\ncomparison, Emu3.5 achieves performance comparable to Gemini 2.5 Flash Image\n(Nano Banana) on image generation and editing tasks and demonstrates superior\nresults on a suite of interleaved generation tasks. We open-source Emu3.5 at\nhttps://github.com/baaivision/Emu3.5 to support community research.",
      "upvotes": 5,
      "discussionId": "690414cc2c556835fa67efcc",
      "projectPage": "https://emu.world/",
      "githubRepo": "https://github.com/baaivision/Emu3.5",
      "githubStars": 555
    },
    "publishedAt": "2025-10-30T11:11:16.000Z",
    "title": "Emu3.5: Native Multimodal Models are World Learners",
    "summary": "We introduce Emu3.5, a large-scale multimodal world model that natively\npredicts the next state across vision and language. Emu3.5 is pre-trained\nend-to-end with a unified next-token prediction objective on a corpus of\nvision-language interleaved data containing over 10 trillion tokens, primarily\nderived from sequential frames and transcripts of internet videos. The model\nnaturally accepts interleaved vision-language inputs and generates interleaved\nvision-language outputs. Emu3.5 is further post-trained with large-scale\nreinforcement learning to enhance multimodal reasoning and generation. To\nimprove inference efficiency, we propose Discrete Diffusion Adaptation (DiDA),\nwhich converts token-by-token decoding into bidirectional parallel prediction,\naccelerating per-image inference by about 20x without sacrificing performance.\nEmu3.5 exhibits strong native multimodal capabilities, including long-horizon\nvision-language generation, any-to-image (X2I) generation, and complex\ntext-rich image generation. It also exhibits generalizable world-modeling\nabilities, enabling spatiotemporally consistent world exploration and\nopen-world embodied manipulation across diverse scenarios and tasks. For\ncomparison, Emu3.5 achieves performance comparable to Gemini 2.5 Flash Image\n(Nano Banana) on image generation and editing tasks and demonstrates superior\nresults on a suite of interleaved generation tasks. We open-source Emu3.5 at\nhttps://github.com/baaivision/Emu3.5 to support community research.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.26583.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63ca558304c979828311c5a5",
      "avatarUrl": "/avatars/2a439d79fba2f987cabe780d10c94d25.svg",
      "fullname": "Xinlong Wang",
      "name": "xinlongwang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 13
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.26692",
      "authors": [
        {
          "_id": "690414082c556835fa67ef77",
          "name": "Kimi Team",
          "hidden": false
        },
        {
          "_id": "690414082c556835fa67ef78",
          "name": "Yu Zhang",
          "hidden": false
        },
        {
          "_id": "690414082c556835fa67ef79",
          "name": "Zongyu Lin",
          "hidden": false
        },
        {
          "_id": "690414082c556835fa67ef7a",
          "name": "Xingcheng Yao",
          "hidden": false
        },
        {
          "_id": "690414082c556835fa67ef7b",
          "name": "Jiaxi Hu",
          "hidden": false
        },
        {
          "_id": "690414082c556835fa67ef7c",
          "name": "Fanqing Meng",
          "hidden": false
        },
        {
          "_id": "690414082c556835fa67ef7d",
          "name": "Chengyin Liu",
          "hidden": false
        },
        {
          "_id": "690414082c556835fa67ef7e",
          "name": "Xin Men",
          "hidden": false
        },
        {
          "_id": "690414082c556835fa67ef7f",
          "name": "Songlin Yang",
          "hidden": false
        },
        {
          "_id": "690414082c556835fa67ef80",
          "name": "Zhiyuan Li",
          "hidden": false
        },
        {
          "_id": "690414082c556835fa67ef81",
          "name": "Wentao Li",
          "hidden": false
        },
        {
          "_id": "690414082c556835fa67ef82",
          "name": "Enzhe Lu",
          "hidden": false
        },
        {
          "_id": "690414082c556835fa67ef83",
          "name": "Weizhou Liu",
          "hidden": false
        },
        {
          "_id": "690414082c556835fa67ef84",
          "name": "Yanru Chen",
          "hidden": false
        },
        {
          "_id": "690414082c556835fa67ef85",
          "name": "Weixin Xu",
          "hidden": false
        },
        {
          "_id": "690414082c556835fa67ef86",
          "name": "Longhui Yu",
          "hidden": false
        },
        {
          "_id": "690414082c556835fa67ef87",
          "name": "Yejie Wang",
          "hidden": false
        },
        {
          "_id": "690414082c556835fa67ef88",
          "name": "Yu Fan",
          "hidden": false
        },
        {
          "_id": "690414082c556835fa67ef89",
          "name": "Longguang Zhong",
          "hidden": false
        },
        {
          "_id": "690414082c556835fa67ef8a",
          "name": "Enming Yuan",
          "hidden": false
        },
        {
          "_id": "690414082c556835fa67ef8b",
          "name": "Dehao Zhang",
          "hidden": false
        },
        {
          "_id": "690414082c556835fa67ef8c",
          "name": "Yizhi Zhang",
          "hidden": false
        },
        {
          "_id": "690414082c556835fa67ef8d",
          "name": "T. Y. Liu",
          "hidden": false
        },
        {
          "_id": "690414082c556835fa67ef8e",
          "name": "Haiming Wang",
          "hidden": false
        },
        {
          "_id": "690414082c556835fa67ef8f",
          "name": "Shengjun Fang",
          "hidden": false
        },
        {
          "_id": "690414082c556835fa67ef90",
          "name": "Weiran He",
          "hidden": false
        },
        {
          "_id": "690414082c556835fa67ef91",
          "name": "Shaowei Liu",
          "hidden": false
        },
        {
          "_id": "690414082c556835fa67ef92",
          "name": "Yiwei Li",
          "hidden": false
        },
        {
          "_id": "690414082c556835fa67ef93",
          "name": "Jianlin Su",
          "hidden": false
        },
        {
          "_id": "690414082c556835fa67ef94",
          "name": "Jiezhong Qiu",
          "hidden": false
        },
        {
          "_id": "690414082c556835fa67ef95",
          "name": "Bo Pang",
          "hidden": false
        },
        {
          "_id": "690414082c556835fa67ef96",
          "name": "Junjie Yan",
          "hidden": false
        },
        {
          "_id": "690414082c556835fa67ef97",
          "name": "Zhejun Jiang",
          "hidden": false
        },
        {
          "_id": "690414082c556835fa67ef98",
          "name": "Weixiao Huang",
          "hidden": false
        },
        {
          "_id": "690414082c556835fa67ef99",
          "name": "Bohong Yin",
          "hidden": false
        },
        {
          "_id": "690414082c556835fa67ef9a",
          "name": "Jiacheng You",
          "hidden": false
        },
        {
          "_id": "690414082c556835fa67ef9b",
          "name": "Chu Wei",
          "hidden": false
        },
        {
          "_id": "690414082c556835fa67ef9c",
          "name": "Zhengtao Wang",
          "hidden": false
        },
        {
          "_id": "690414082c556835fa67ef9d",
          "name": "Chao Hong",
          "hidden": false
        },
        {
          "_id": "690414082c556835fa67ef9e",
          "name": "Yutian Chen",
          "hidden": false
        },
        {
          "_id": "690414082c556835fa67ef9f",
          "name": "Guanduo Chen",
          "hidden": false
        },
        {
          "_id": "690414082c556835fa67efa0",
          "name": "Yucheng Wang",
          "hidden": false
        },
        {
          "_id": "690414082c556835fa67efa1",
          "name": "Huabin Zheng",
          "hidden": false
        },
        {
          "_id": "690414082c556835fa67efa2",
          "name": "Feng Wang",
          "hidden": false
        },
        {
          "_id": "690414082c556835fa67efa3",
          "name": "Yibo Liu",
          "hidden": false
        },
        {
          "_id": "690414082c556835fa67efa4",
          "name": "Mengnan Dong",
          "hidden": false
        },
        {
          "_id": "690414082c556835fa67efa5",
          "name": "Zheng Zhang",
          "hidden": false
        },
        {
          "_id": "690414082c556835fa67efa6",
          "name": "Siyuan Pan",
          "hidden": false
        },
        {
          "_id": "690414082c556835fa67efa7",
          "name": "Wenhao Wu",
          "hidden": false
        },
        {
          "_id": "690414082c556835fa67efa8",
          "name": "Yuhao Wu",
          "hidden": false
        },
        {
          "_id": "690414082c556835fa67efa9",
          "name": "Longyu Guan",
          "hidden": false
        },
        {
          "_id": "690414082c556835fa67efaa",
          "name": "Jiawen Tao",
          "hidden": false
        },
        {
          "_id": "690414082c556835fa67efab",
          "name": "Guohong Fu",
          "hidden": false
        },
        {
          "_id": "690414082c556835fa67efac",
          "name": "Xinran Xu",
          "hidden": false
        },
        {
          "_id": "690414082c556835fa67efad",
          "name": "Yuzhi Wang",
          "hidden": false
        },
        {
          "_id": "690414082c556835fa67efae",
          "name": "Guokun Lai",
          "hidden": false
        },
        {
          "_id": "690414082c556835fa67efaf",
          "name": "Yuxin Wu",
          "hidden": false
        },
        {
          "_id": "690414082c556835fa67efb0",
          "name": "Xinyu Zhou",
          "hidden": false
        },
        {
          "_id": "690414082c556835fa67efb1",
          "name": "Zhilin Yang",
          "hidden": false
        },
        {
          "_id": "690414082c556835fa67efb2",
          "name": "Yulun Du",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-30T16:59:43.000Z",
      "submittedOnDailyAt": "2025-10-31T00:12:50.143Z",
      "title": "Kimi Linear: An Expressive, Efficient Attention Architecture",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "We introduce Kimi Linear, a hybrid linear attention architecture that, for\nthe first time, outperforms full attention under fair comparisons across\nvarious scenarios -- including short-context, long-context, and reinforcement\nlearning (RL) scaling regimes. At its core lies Kimi Delta Attention (KDA), an\nexpressive linear attention module that extends Gated DeltaNet with a\nfiner-grained gating mechanism, enabling more effective use of limited\nfinite-state RNN memory. Our bespoke chunkwise algorithm achieves high hardware\nefficiency through a specialized variant of the Diagonal-Plus-Low-Rank (DPLR)\ntransition matrices, which substantially reduces computation compared to the\ngeneral DPLR formulation while remaining more consistent with the classical\ndelta rule.\n  We pretrain a Kimi Linear model with 3B activated parameters and 48B total\nparameters, based on a layerwise hybrid of KDA and Multi-Head Latent Attention\n(MLA). Our experiments show that with an identical training recipe, Kimi Linear\noutperforms full MLA with a sizeable margin across all evaluated tasks, while\nreducing KV cache usage by up to 75% and achieving up to 6 times decoding\nthroughput for a 1M context. These results demonstrate that Kimi Linear can be\na drop-in replacement for full attention architectures with superior\nperformance and efficiency, including tasks with longer input and output\nlengths.\n  To support further research, we open-source the KDA kernel and vLLM\nimplementations, and release the pre-trained and instruction-tuned model\ncheckpoints.",
      "upvotes": 4,
      "discussionId": "690414092c556835fa67efb3",
      "githubRepo": "https://github.com/MoonshotAI/Kimi-Linear",
      "githubStars": 246
    },
    "publishedAt": "2025-10-30T12:59:43.000Z",
    "title": "Kimi Linear: An Expressive, Efficient Attention Architecture",
    "summary": "We introduce Kimi Linear, a hybrid linear attention architecture that, for\nthe first time, outperforms full attention under fair comparisons across\nvarious scenarios -- including short-context, long-context, and reinforcement\nlearning (RL) scaling regimes. At its core lies Kimi Delta Attention (KDA), an\nexpressive linear attention module that extends Gated DeltaNet with a\nfiner-grained gating mechanism, enabling more effective use of limited\nfinite-state RNN memory. Our bespoke chunkwise algorithm achieves high hardware\nefficiency through a specialized variant of the Diagonal-Plus-Low-Rank (DPLR)\ntransition matrices, which substantially reduces computation compared to the\ngeneral DPLR formulation while remaining more consistent with the classical\ndelta rule.\n  We pretrain a Kimi Linear model with 3B activated parameters and 48B total\nparameters, based on a layerwise hybrid of KDA and Multi-Head Latent Attention\n(MLA). Our experiments show that with an identical training recipe, Kimi Linear\noutperforms full MLA with a sizeable margin across all evaluated tasks, while\nreducing KV cache usage by up to 75% and achieving up to 6 times decoding\nthroughput for a 1M context. These results demonstrate that Kimi Linear can be\na drop-in replacement for full attention architectures with superior\nperformance and efficiency, including tasks with longer input and output\nlengths.\n  To support further research, we open-source the KDA kernel and vLLM\nimplementations, and release the pre-trained and instruction-tuned model\ncheckpoints.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.26692.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 152
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.26802",
      "authors": [
        {
          "_id": "690417142c556835fa67f021",
          "name": "Ziyu Guo",
          "hidden": false
        },
        {
          "_id": "690417142c556835fa67f022",
          "name": "Xinyan Chen",
          "hidden": false
        },
        {
          "_id": "690417142c556835fa67f023",
          "name": "Renrui Zhang",
          "hidden": false
        },
        {
          "_id": "690417142c556835fa67f024",
          "name": "Ruichuan An",
          "hidden": false
        },
        {
          "_id": "690417142c556835fa67f025",
          "name": "Yu Qi",
          "hidden": false
        },
        {
          "_id": "690417142c556835fa67f026",
          "name": "Dongzhi Jiang",
          "hidden": false
        },
        {
          "_id": "690417142c556835fa67f027",
          "name": "Xiangtai Li",
          "hidden": false
        },
        {
          "_id": "690417142c556835fa67f028",
          "name": "Manyuan Zhang",
          "hidden": false
        },
        {
          "_id": "690417142c556835fa67f029",
          "name": "Hongsheng Li",
          "hidden": false
        },
        {
          "_id": "690417142c556835fa67f02a",
          "name": "Pheng-Ann Heng",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-30T17:59:55.000Z",
      "submittedOnDailyAt": "2025-10-31T00:26:10.415Z",
      "title": "Are Video Models Ready as Zero-Shot Reasoners? An Empirical Study with\n  the MME-CoF Benchmark",
      "submittedOnDailyBy": {
        "_id": "645b8b2687c79b6ec0bb3b7a",
        "avatarUrl": "/avatars/00a9db32a42dc950112bf2593bb109cb.svg",
        "isPro": false,
        "fullname": "Renrui",
        "user": "ZrrSkywalker",
        "type": "user"
      },
      "summary": "Recent video generation models can produce high-fidelity, temporally coherent\nvideos, indicating that they may encode substantial world knowledge. Beyond\nrealistic synthesis, they also exhibit emerging behaviors indicative of visual\nperception, modeling, and manipulation. Yet, an important question still\nremains: Are video models ready to serve as zero-shot reasoners in challenging\nvisual reasoning scenarios? In this work, we conduct an empirical study to\ncomprehensively investigate this question, focusing on the leading and popular\nVeo-3. We evaluate its reasoning behavior across 12 dimensions, including\nspatial, geometric, physical, temporal, and embodied logic, systematically\ncharacterizing both its strengths and failure modes. To standardize this study,\nwe curate the evaluation data into MME-CoF, a compact benchmark that enables\nin-depth and thorough assessment of Chain-of-Frame (CoF) reasoning. Our\nfindings reveal that while current video models demonstrate promising reasoning\npatterns on short-horizon spatial coherence, fine-grained grounding, and\nlocally consistent dynamics, they remain limited in long-horizon causal\nreasoning, strict geometric constraints, and abstract logic. Overall, they are\nnot yet reliable as standalone zero-shot reasoners, but exhibit encouraging\nsigns as complementary visual engines alongside dedicated reasoning models.\nProject page: https://video-cof.github.io",
      "upvotes": 2,
      "discussionId": "690417152c556835fa67f02b"
    },
    "publishedAt": "2025-10-30T13:59:55.000Z",
    "title": "Are Video Models Ready as Zero-Shot Reasoners? An Empirical Study with\n  the MME-CoF Benchmark",
    "summary": "Recent video generation models can produce high-fidelity, temporally coherent\nvideos, indicating that they may encode substantial world knowledge. Beyond\nrealistic synthesis, they also exhibit emerging behaviors indicative of visual\nperception, modeling, and manipulation. Yet, an important question still\nremains: Are video models ready to serve as zero-shot reasoners in challenging\nvisual reasoning scenarios? In this work, we conduct an empirical study to\ncomprehensively investigate this question, focusing on the leading and popular\nVeo-3. We evaluate its reasoning behavior across 12 dimensions, including\nspatial, geometric, physical, temporal, and embodied logic, systematically\ncharacterizing both its strengths and failure modes. To standardize this study,\nwe curate the evaluation data into MME-CoF, a compact benchmark that enables\nin-depth and thorough assessment of Chain-of-Frame (CoF) reasoning. Our\nfindings reveal that while current video models demonstrate promising reasoning\npatterns on short-horizon spatial coherence, fine-grained grounding, and\nlocally consistent dynamics, they remain limited in long-horizon causal\nreasoning, strict geometric constraints, and abstract logic. Overall, they are\nnot yet reliable as standalone zero-shot reasoners, but exhibit encouraging\nsigns as complementary visual engines alongside dedicated reasoning models.\nProject page: https://video-cof.github.io",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.26802.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "645b8b2687c79b6ec0bb3b7a",
      "avatarUrl": "/avatars/00a9db32a42dc950112bf2593bb109cb.svg",
      "fullname": "Renrui",
      "name": "ZrrSkywalker",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 7
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.25992",
      "authors": [
        {
          "_id": "6904169e2c556835fa67f004",
          "name": "Yihe Deng",
          "hidden": false
        },
        {
          "_id": "6904169e2c556835fa67f005",
          "name": "I-Hung Hsu",
          "hidden": false
        },
        {
          "_id": "6904169e2c556835fa67f006",
          "name": "Jun Yan",
          "hidden": false
        },
        {
          "_id": "6904169e2c556835fa67f007",
          "name": "Zifeng Wang",
          "hidden": false
        },
        {
          "_id": "6904169e2c556835fa67f008",
          "name": "Rujun Han",
          "hidden": false
        },
        {
          "_id": "6904169e2c556835fa67f009",
          "name": "Gufeng Zhang",
          "hidden": false
        },
        {
          "_id": "6904169e2c556835fa67f00a",
          "name": "Yanfei Chen",
          "hidden": false
        },
        {
          "_id": "6904169e2c556835fa67f00b",
          "name": "Wei Wang",
          "hidden": false
        },
        {
          "_id": "6904169e2c556835fa67f00c",
          "name": "Tomas Pfister",
          "hidden": false
        },
        {
          "_id": "6904169e2c556835fa67f00d",
          "name": "Chen-Yu Lee",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-29T22:05:08.000Z",
      "submittedOnDailyAt": "2025-10-31T00:25:45.081Z",
      "title": "Supervised Reinforcement Learning: From Expert Trajectories to Step-wise\n  Reasoning",
      "submittedOnDailyBy": {
        "_id": "64dc08f9e7bc8544f9b1ac32",
        "avatarUrl": "/avatars/e2ceccaf12dbdc643396c56f9a80ab8b.svg",
        "isPro": false,
        "fullname": "I-Hung Hsu",
        "user": "alexhsu",
        "type": "user"
      },
      "summary": "Large Language Models (LLMs) often struggle with problems that require\nmulti-step reasoning. For small-scale open-source models, Reinforcement\nLearning with Verifiable Rewards (RLVR) fails when correct solutions are rarely\nsampled even after many attempts, while Supervised Fine-Tuning (SFT) tends to\noverfit long demonstrations through rigid token-by-token imitation. To address\nthis gap, we propose Supervised Reinforcement Learning (SRL), a framework that\nreformulates problem solving as generating a sequence of logical \"actions\". SRL\ntrains the model to generate an internal reasoning monologue before committing\nto each action. It provides smoother rewards based on the similarity between\nthe model's actions and expert actions extracted from the SFT dataset in a\nstep-wise manner. This supervision offers richer learning signals even when all\nrollouts are incorrect, while encouraging flexible reasoning guided by expert\ndemonstrations. As a result, SRL enables small models to learn challenging\nproblems previously unlearnable by SFT or RLVR. Moreover, initializing training\nwith SRL before refining with RLVR yields the strongest overall performance.\nBeyond reasoning benchmarks, SRL generalizes effectively to agentic software\nengineering tasks, establishing it as a robust and versatile training framework\nfor reasoning-oriented LLMs.",
      "upvotes": 2,
      "discussionId": "6904169f2c556835fa67f00e",
      "organization": {
        "_id": "5e6aca39878b8b2bf9806447",
        "name": "google",
        "fullname": "Google",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/WtA3YYitedOr9n02eHfJe.png"
      }
    },
    "publishedAt": "2025-10-29T18:05:08.000Z",
    "title": "Supervised Reinforcement Learning: From Expert Trajectories to Step-wise\n  Reasoning",
    "summary": "Large Language Models (LLMs) often struggle with problems that require\nmulti-step reasoning. For small-scale open-source models, Reinforcement\nLearning with Verifiable Rewards (RLVR) fails when correct solutions are rarely\nsampled even after many attempts, while Supervised Fine-Tuning (SFT) tends to\noverfit long demonstrations through rigid token-by-token imitation. To address\nthis gap, we propose Supervised Reinforcement Learning (SRL), a framework that\nreformulates problem solving as generating a sequence of logical \"actions\". SRL\ntrains the model to generate an internal reasoning monologue before committing\nto each action. It provides smoother rewards based on the similarity between\nthe model's actions and expert actions extracted from the SFT dataset in a\nstep-wise manner. This supervision offers richer learning signals even when all\nrollouts are incorrect, while encouraging flexible reasoning guided by expert\ndemonstrations. As a result, SRL enables small models to learn challenging\nproblems previously unlearnable by SFT or RLVR. Moreover, initializing training\nwith SRL before refining with RLVR yields the strongest overall performance.\nBeyond reasoning benchmarks, SRL generalizes effectively to agentic software\nengineering tasks, establishing it as a robust and versatile training framework\nfor reasoning-oriented LLMs.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.25992.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64dc08f9e7bc8544f9b1ac32",
      "avatarUrl": "/avatars/e2ceccaf12dbdc643396c56f9a80ab8b.svg",
      "fullname": "I-Hung Hsu",
      "name": "alexhsu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "organization": {
      "_id": "5e6aca39878b8b2bf9806447",
      "name": "google",
      "fullname": "Google",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/WtA3YYitedOr9n02eHfJe.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.26787",
      "authors": [
        {
          "_id": "690415cf2c556835fa67efd3",
          "name": "Mantas Mazeika",
          "hidden": false
        },
        {
          "_id": "690415cf2c556835fa67efd4",
          "name": "Alice Gatti",
          "hidden": false
        },
        {
          "_id": "690415cf2c556835fa67efd5",
          "name": "Cristina Menghini",
          "hidden": false
        },
        {
          "_id": "690415cf2c556835fa67efd6",
          "name": "Udari Madhushani Sehwag",
          "hidden": false
        },
        {
          "_id": "690415cf2c556835fa67efd7",
          "name": "Shivam Singhal",
          "hidden": false
        },
        {
          "_id": "690415cf2c556835fa67efd8",
          "name": "Yury Orlovskiy",
          "hidden": false
        },
        {
          "_id": "690415cf2c556835fa67efd9",
          "name": "Steven Basart",
          "hidden": false
        },
        {
          "_id": "690415cf2c556835fa67efda",
          "name": "Manasi Sharma",
          "hidden": false
        },
        {
          "_id": "690415cf2c556835fa67efdb",
          "name": "Denis Peskoff",
          "hidden": false
        },
        {
          "_id": "690415cf2c556835fa67efdc",
          "name": "Elaine Lau",
          "hidden": false
        },
        {
          "_id": "690415cf2c556835fa67efdd",
          "name": "Jaehyuk Lim",
          "hidden": false
        },
        {
          "_id": "690415cf2c556835fa67efde",
          "name": "Lachlan Carroll",
          "hidden": false
        },
        {
          "_id": "690415cf2c556835fa67efdf",
          "name": "Alice Blair",
          "hidden": false
        },
        {
          "_id": "690415cf2c556835fa67efe0",
          "name": "Vinaya Sivakumar",
          "hidden": false
        },
        {
          "_id": "690415cf2c556835fa67efe1",
          "name": "Sumana Basu",
          "hidden": false
        },
        {
          "_id": "690415cf2c556835fa67efe2",
          "name": "Brad Kenstler",
          "hidden": false
        },
        {
          "_id": "690415cf2c556835fa67efe3",
          "name": "Yuntao Ma",
          "hidden": false
        },
        {
          "_id": "690415cf2c556835fa67efe4",
          "name": "Julian Michael",
          "hidden": false
        },
        {
          "_id": "690415cf2c556835fa67efe5",
          "name": "Xiaoke Li",
          "hidden": false
        },
        {
          "_id": "690415cf2c556835fa67efe6",
          "name": "Oliver Ingebretsen",
          "hidden": false
        },
        {
          "_id": "690415cf2c556835fa67efe7",
          "name": "Aditya Mehta",
          "hidden": false
        },
        {
          "_id": "690415cf2c556835fa67efe8",
          "name": "Jean Mottola",
          "hidden": false
        },
        {
          "_id": "690415cf2c556835fa67efe9",
          "name": "John Teichmann",
          "hidden": false
        },
        {
          "_id": "690415cf2c556835fa67efea",
          "name": "Kevin Yu",
          "hidden": false
        },
        {
          "_id": "690415cf2c556835fa67efeb",
          "name": "Zaina Shaik",
          "hidden": false
        },
        {
          "_id": "690415cf2c556835fa67efec",
          "name": "Adam Khoja",
          "hidden": false
        },
        {
          "_id": "690415cf2c556835fa67efed",
          "name": "Richard Ren",
          "hidden": false
        },
        {
          "_id": "690415cf2c556835fa67efee",
          "name": "Jason Hausenloy",
          "hidden": false
        },
        {
          "_id": "690415cf2c556835fa67efef",
          "name": "Long Phan",
          "hidden": false
        },
        {
          "_id": "690415cf2c556835fa67eff0",
          "name": "Ye Htet",
          "hidden": false
        },
        {
          "_id": "690415cf2c556835fa67eff1",
          "name": "Ankit Aich",
          "hidden": false
        },
        {
          "_id": "690415cf2c556835fa67eff2",
          "name": "Tahseen Rabbani",
          "hidden": false
        },
        {
          "_id": "690415cf2c556835fa67eff3",
          "name": "Vivswan Shah",
          "hidden": false
        },
        {
          "_id": "690415cf2c556835fa67eff4",
          "name": "Andriy Novykov",
          "hidden": false
        },
        {
          "_id": "690415cf2c556835fa67eff5",
          "name": "Felix Binder",
          "hidden": false
        },
        {
          "_id": "690415cf2c556835fa67eff6",
          "name": "Kirill Chugunov",
          "hidden": false
        },
        {
          "_id": "690415cf2c556835fa67eff7",
          "name": "Luis Ramirez",
          "hidden": false
        },
        {
          "_id": "690415cf2c556835fa67eff8",
          "name": "Matias Geralnik",
          "hidden": false
        },
        {
          "_id": "690415cf2c556835fa67eff9",
          "name": "Hernán Mesura",
          "hidden": false
        },
        {
          "_id": "690415cf2c556835fa67effa",
          "name": "Dean Lee",
          "hidden": false
        },
        {
          "_id": "690415cf2c556835fa67effb",
          "name": "Ed-Yeremai Hernandez Cardona",
          "hidden": false
        },
        {
          "_id": "690415cf2c556835fa67effc",
          "name": "Annette Diamond",
          "hidden": false
        },
        {
          "_id": "690415cf2c556835fa67effd",
          "name": "Summer Yue",
          "hidden": false
        },
        {
          "_id": "690415cf2c556835fa67effe",
          "name": "Alexandr Wang",
          "hidden": false
        },
        {
          "_id": "690415cf2c556835fa67efff",
          "name": "Bing Liu",
          "hidden": false
        },
        {
          "_id": "690415cf2c556835fa67f000",
          "name": "Ernesto Hernandez",
          "hidden": false
        },
        {
          "_id": "690415cf2c556835fa67f001",
          "name": "Dan Hendrycks",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-30T17:58:04.000Z",
      "submittedOnDailyAt": "2025-10-31T00:20:15.697Z",
      "title": "Remote Labor Index: Measuring AI Automation of Remote Work",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "AIs have made rapid progress on research-oriented benchmarks of knowledge and\nreasoning, but it remains unclear how these gains translate into economic value\nand automation. To measure this, we introduce the Remote Labor Index (RLI), a\nbroadly multi-sector benchmark comprising real-world, economically valuable\nprojects designed to evaluate end-to-end agent performance in practical\nsettings. AI agents perform near the floor on RLI, with the highest-performing\nagent achieving an automation rate of 2.5%. These results help ground\ndiscussions of AI automation in empirical evidence, setting a common basis for\ntracking AI impacts and enabling stakeholders to proactively navigate AI-driven\nlabor automation.",
      "upvotes": 1,
      "discussionId": "690415cf2c556835fa67f002"
    },
    "publishedAt": "2025-10-30T13:58:04.000Z",
    "title": "Remote Labor Index: Measuring AI Automation of Remote Work",
    "summary": "AIs have made rapid progress on research-oriented benchmarks of knowledge and\nreasoning, but it remains unclear how these gains translate into economic value\nand automation. To measure this, we introduce the Remote Labor Index (RLI), a\nbroadly multi-sector benchmark comprising real-world, economically valuable\nprojects designed to evaluate end-to-end agent performance in practical\nsettings. AI agents perform near the floor on RLI, with the highest-performing\nagent achieving an automation rate of 2.5%. These results help ground\ndiscussions of AI automation in empirical evidence, setting a common basis for\ntracking AI impacts and enabling stakeholders to proactively navigate AI-driven\nlabor automation.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.26787.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 152
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.26298",
      "authors": [
        {
          "_id": "690414fd2c556835fa67efce",
          "name": "Jingran Zhang",
          "hidden": false
        },
        {
          "_id": "690414fd2c556835fa67efcf",
          "name": "Ning Li",
          "hidden": false
        },
        {
          "_id": "690414fd2c556835fa67efd0",
          "name": "Justin Cui",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/XQsuSRT7f95axrkHL3DnO.mp4"
      ],
      "publishedAt": "2025-10-30T09:35:51.000Z",
      "submittedOnDailyAt": "2025-10-31T00:16:51.181Z",
      "title": "Can Agent Conquer Web? Exploring the Frontiers of ChatGPT Atlas Agent in\n  Web Games",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "OpenAI's ChatGPT Atlas introduces new capabilities for web interaction,\nenabling the model to analyze webpages, process user intents, and execute\ncursor and keyboard inputs directly within the browser. While its capacity for\ninformation retrieval tasks has been demonstrated, its performance in dynamic,\ninteractive environments remains less explored. In this study, we conduct an\nearly evaluation of Atlas's web interaction capabilities using browser-based\ngames as test scenarios, including Google's T-Rex Runner, Sudoku, Flappy Bird,\nand Stein.world. We employ in-game performance scores as quantitative metrics\nto assess performance across different task types. Our results show that Atlas\nperforms strongly in logical reasoning tasks like Sudoku, completing puzzles\nsignificantly faster than human baselines, but struggles substantially in\nreal-time games requiring precise timing and motor control, often failing to\nprogress beyond initial obstacles. These findings suggest that while Atlas\ndemonstrates capable analytical processing, there remain notable limitations in\ndynamic web environments requiring real-time interaction. The website of our\nproject can be found at https://atlas-game-eval.github.io.",
      "upvotes": 1,
      "discussionId": "690414fe2c556835fa67efd1",
      "projectPage": "https://atlas-game-eval.github.io/"
    },
    "publishedAt": "2025-10-30T05:35:51.000Z",
    "title": "Can Agent Conquer Web? Exploring the Frontiers of ChatGPT Atlas Agent in\n  Web Games",
    "summary": "OpenAI's ChatGPT Atlas introduces new capabilities for web interaction,\nenabling the model to analyze webpages, process user intents, and execute\ncursor and keyboard inputs directly within the browser. While its capacity for\ninformation retrieval tasks has been demonstrated, its performance in dynamic,\ninteractive environments remains less explored. In this study, we conduct an\nearly evaluation of Atlas's web interaction capabilities using browser-based\ngames as test scenarios, including Google's T-Rex Runner, Sudoku, Flappy Bird,\nand Stein.world. We employ in-game performance scores as quantitative metrics\nto assess performance across different task types. Our results show that Atlas\nperforms strongly in logical reasoning tasks like Sudoku, completing puzzles\nsignificantly faster than human baselines, but struggles substantially in\nreal-time games requiring precise timing and motor control, often failing to\nprogress beyond initial obstacles. These findings suggest that while Atlas\ndemonstrates capable analytical processing, there remain notable limitations in\ndynamic web environments requiring real-time interaction. The website of our\nproject can be found at https://atlas-game-eval.github.io.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/XQsuSRT7f95axrkHL3DnO.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.26298.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 152
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.26800",
      "authors": [
        {
          "_id": "6904193e2c556835fa67f063",
          "name": "Yukun Huang",
          "hidden": false
        },
        {
          "_id": "6904193e2c556835fa67f064",
          "name": "Jiwen Yu",
          "hidden": false
        },
        {
          "_id": "6904193e2c556835fa67f065",
          "name": "Yanning Zhou",
          "hidden": false
        },
        {
          "_id": "6904193e2c556835fa67f066",
          "name": "Jianan Wang",
          "hidden": false
        },
        {
          "_id": "6904193e2c556835fa67f067",
          "name": "Xintao Wang",
          "hidden": false
        },
        {
          "_id": "6904193e2c556835fa67f068",
          "name": "Pengfei Wan",
          "hidden": false
        },
        {
          "_id": "6904193e2c556835fa67f069",
          "name": "Xihui Liu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-30T17:59:51.000Z",
      "submittedOnDailyAt": "2025-10-31T00:35:27.539Z",
      "title": "OmniX: From Unified Panoramic Generation and Perception to\n  Graphics-Ready 3D Scenes",
      "submittedOnDailyBy": {
        "_id": "638ee900ee7e45e0474a5712",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/638ee900ee7e45e0474a5712/KLli_eCbWwffKR7oLDmV3.jpeg",
        "isPro": false,
        "fullname": "Yukun Huang",
        "user": "KevinHuang",
        "type": "user"
      },
      "summary": "There are two prevalent ways to constructing 3D scenes: procedural generation\nand 2D lifting. Among them, panorama-based 2D lifting has emerged as a\npromising technique, leveraging powerful 2D generative priors to produce\nimmersive, realistic, and diverse 3D environments. In this work, we advance\nthis technique to generate graphics-ready 3D scenes suitable for physically\nbased rendering (PBR), relighting, and simulation. Our key insight is to\nrepurpose 2D generative models for panoramic perception of geometry, textures,\nand PBR materials. Unlike existing 2D lifting approaches that emphasize\nappearance generation and ignore the perception of intrinsic properties, we\npresent OmniX, a versatile and unified framework. Based on a lightweight and\nefficient cross-modal adapter structure, OmniX reuses 2D generative priors for\na broad range of panoramic vision tasks, including panoramic perception,\ngeneration, and completion. Furthermore, we construct a large-scale synthetic\npanorama dataset containing high-quality multimodal panoramas from diverse\nindoor and outdoor scenes. Extensive experiments demonstrate the effectiveness\nof our model in panoramic visual perception and graphics-ready 3D scene\ngeneration, opening new possibilities for immersive and physically realistic\nvirtual world generation.",
      "upvotes": 0,
      "discussionId": "6904193e2c556835fa67f06a",
      "projectPage": "https://yukun-huang.github.io/OmniX/",
      "githubRepo": "https://github.com/HKU-MMLab/OmniX",
      "githubStars": 0
    },
    "publishedAt": "2025-10-30T13:59:51.000Z",
    "title": "OmniX: From Unified Panoramic Generation and Perception to\n  Graphics-Ready 3D Scenes",
    "summary": "There are two prevalent ways to constructing 3D scenes: procedural generation\nand 2D lifting. Among them, panorama-based 2D lifting has emerged as a\npromising technique, leveraging powerful 2D generative priors to produce\nimmersive, realistic, and diverse 3D environments. In this work, we advance\nthis technique to generate graphics-ready 3D scenes suitable for physically\nbased rendering (PBR), relighting, and simulation. Our key insight is to\nrepurpose 2D generative models for panoramic perception of geometry, textures,\nand PBR materials. Unlike existing 2D lifting approaches that emphasize\nappearance generation and ignore the perception of intrinsic properties, we\npresent OmniX, a versatile and unified framework. Based on a lightweight and\nefficient cross-modal adapter structure, OmniX reuses 2D generative priors for\na broad range of panoramic vision tasks, including panoramic perception,\ngeneration, and completion. Furthermore, we construct a large-scale synthetic\npanorama dataset containing high-quality multimodal panoramas from diverse\nindoor and outdoor scenes. Extensive experiments demonstrate the effectiveness\nof our model in panoramic visual perception and graphics-ready 3D scene\ngeneration, opening new possibilities for immersive and physically realistic\nvirtual world generation.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.26800.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "638ee900ee7e45e0474a5712",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/638ee900ee7e45e0474a5712/KLli_eCbWwffKR7oLDmV3.jpeg",
      "fullname": "Yukun Huang",
      "name": "KevinHuang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 7
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.26474",
      "authors": [
        {
          "_id": "6904182a2c556835fa67f02d",
          "name": "Xin Guo",
          "hidden": false
        },
        {
          "_id": "6904182a2c556835fa67f02e",
          "name": "Zhiheng Xi",
          "hidden": false
        },
        {
          "_id": "6904182a2c556835fa67f02f",
          "name": "Yiwen Ding",
          "hidden": false
        },
        {
          "_id": "6904182a2c556835fa67f030",
          "name": "Yitao Zhai",
          "hidden": false
        },
        {
          "_id": "6904182a2c556835fa67f031",
          "name": "Xiaowei Shi",
          "hidden": false
        },
        {
          "_id": "6904182a2c556835fa67f032",
          "name": "Xunliang Cai",
          "hidden": false
        },
        {
          "_id": "6904182a2c556835fa67f033",
          "name": "Tao Gui",
          "hidden": false
        },
        {
          "_id": "6904182a2c556835fa67f034",
          "name": "Qi Zhang",
          "hidden": false
        },
        {
          "_id": "6904182a2c556835fa67f035",
          "name": "Xuanjing Huang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-30T13:26:58.000Z",
      "submittedOnDailyAt": "2025-10-31T00:30:19.141Z",
      "title": "Counteracting Matthew Effect in Self-Improvement of LVLMs through\n  Head-Tail Re-balancing",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Self-improvement has emerged as a mainstream paradigm for advancing the\nreasoning capabilities of large vision-language models (LVLMs), where models\nexplore and learn from successful trajectories iteratively. However, we\nidentify a critical issue during this process: the model excels at generating\nhigh-quality trajectories for simple queries (i.e., head data) but struggles\nwith more complex ones (i.e., tail data). This leads to an imbalanced\noptimization that drives the model to prioritize simple reasoning skills, while\nhindering its ability to tackle more complex reasoning tasks. Over iterations,\nthis imbalance becomes increasingly pronounced--a dynamic we term the \"Matthew\neffect\"--which ultimately hinders further model improvement and leads to\nperformance bottlenecks. To counteract this challenge, we introduce four\nefficient strategies from two perspectives: distribution-reshaping and\ntrajectory-resampling, to achieve head-tail re-balancing during the\nexploration-and-learning self-improvement process. Extensive experiments on\nQwen2-VL-7B-Instruct and InternVL2.5-4B models across visual reasoning tasks\ndemonstrate that our methods consistently improve visual reasoning\ncapabilities, outperforming vanilla self-improvement by 3.86 points on average.",
      "upvotes": 0,
      "discussionId": "6904182a2c556835fa67f036"
    },
    "publishedAt": "2025-10-30T09:26:58.000Z",
    "title": "Counteracting Matthew Effect in Self-Improvement of LVLMs through\n  Head-Tail Re-balancing",
    "summary": "Self-improvement has emerged as a mainstream paradigm for advancing the\nreasoning capabilities of large vision-language models (LVLMs), where models\nexplore and learn from successful trajectories iteratively. However, we\nidentify a critical issue during this process: the model excels at generating\nhigh-quality trajectories for simple queries (i.e., head data) but struggles\nwith more complex ones (i.e., tail data). This leads to an imbalanced\noptimization that drives the model to prioritize simple reasoning skills, while\nhindering its ability to tackle more complex reasoning tasks. Over iterations,\nthis imbalance becomes increasingly pronounced--a dynamic we term the \"Matthew\neffect\"--which ultimately hinders further model improvement and leads to\nperformance bottlenecks. To counteract this challenge, we introduce four\nefficient strategies from two perspectives: distribution-reshaping and\ntrajectory-resampling, to achieve head-tail re-balancing during the\nexploration-and-learning self-improvement process. Extensive experiments on\nQwen2-VL-7B-Instruct and InternVL2.5-4B models across visual reasoning tasks\ndemonstrate that our methods consistently improve visual reasoning\ncapabilities, outperforming vanilla self-improvement by 3.86 points on average.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.26474.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 152
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.26160",
      "authors": [
        {
          "_id": "690419172c556835fa67f038",
          "name": "Jiaqi Wang",
          "hidden": false
        },
        {
          "_id": "690419172c556835fa67f039",
          "name": "Xiao Yang",
          "hidden": false
        },
        {
          "_id": "690419172c556835fa67f03a",
          "name": "Kai Sun",
          "hidden": false
        },
        {
          "_id": "690419172c556835fa67f03b",
          "name": "Parth Suresh",
          "hidden": false
        },
        {
          "_id": "690419172c556835fa67f03c",
          "name": "Sanat Sharma",
          "hidden": false
        },
        {
          "_id": "690419172c556835fa67f03d",
          "name": "Adam Czyzewski",
          "hidden": false
        },
        {
          "_id": "690419172c556835fa67f03e",
          "name": "Derek Andersen",
          "hidden": false
        },
        {
          "_id": "690419172c556835fa67f03f",
          "name": "Surya Appini",
          "hidden": false
        },
        {
          "_id": "690419172c556835fa67f040",
          "name": "Arkav Banerjee",
          "hidden": false
        },
        {
          "_id": "690419172c556835fa67f041",
          "name": "Sajal Choudhary",
          "hidden": false
        },
        {
          "_id": "690419172c556835fa67f042",
          "name": "Shervin Ghasemlou",
          "hidden": false
        },
        {
          "_id": "690419172c556835fa67f043",
          "name": "Ziqiang Guan",
          "hidden": false
        },
        {
          "_id": "690419172c556835fa67f044",
          "name": "Akil Iyer",
          "hidden": false
        },
        {
          "_id": "690419172c556835fa67f045",
          "name": "Haidar Khan",
          "hidden": false
        },
        {
          "_id": "690419172c556835fa67f046",
          "name": "Lingkun Kong",
          "hidden": false
        },
        {
          "_id": "690419172c556835fa67f047",
          "name": "Roy Luo",
          "hidden": false
        },
        {
          "_id": "690419172c556835fa67f048",
          "name": "Tiffany Ma",
          "hidden": false
        },
        {
          "_id": "690419172c556835fa67f049",
          "name": "Zhen Qiao",
          "hidden": false
        },
        {
          "_id": "690419172c556835fa67f04a",
          "name": "David Tran",
          "hidden": false
        },
        {
          "_id": "690419172c556835fa67f04b",
          "name": "Wenfang Xu",
          "hidden": false
        },
        {
          "_id": "690419172c556835fa67f04c",
          "name": "Skyler Yeatman",
          "hidden": false
        },
        {
          "_id": "690419172c556835fa67f04d",
          "name": "Chen Zhou",
          "hidden": false
        },
        {
          "_id": "690419172c556835fa67f04e",
          "name": "Gunveer Gujral",
          "hidden": false
        },
        {
          "_id": "690419172c556835fa67f04f",
          "name": "Yinglong Xia",
          "hidden": false
        },
        {
          "_id": "690419172c556835fa67f050",
          "name": "Shane Moon",
          "hidden": false
        },
        {
          "_id": "690419172c556835fa67f051",
          "name": "Nicolas Scheffer",
          "hidden": false
        },
        {
          "_id": "690419172c556835fa67f052",
          "name": "Nirav Shah",
          "hidden": false
        },
        {
          "_id": "690419172c556835fa67f053",
          "name": "Eun Chang",
          "hidden": false
        },
        {
          "_id": "690419172c556835fa67f054",
          "name": "Yue Liu",
          "hidden": false
        },
        {
          "_id": "690419172c556835fa67f055",
          "name": "Florian Metze",
          "hidden": false
        },
        {
          "_id": "690419172c556835fa67f056",
          "name": "Tammy Stark",
          "hidden": false
        },
        {
          "_id": "690419172c556835fa67f057",
          "name": "Zhaleh Feizollahi",
          "hidden": false
        },
        {
          "_id": "690419172c556835fa67f058",
          "name": "Andrea Jessee",
          "hidden": false
        },
        {
          "_id": "690419172c556835fa67f059",
          "name": "Mangesh Pujari",
          "hidden": false
        },
        {
          "_id": "690419172c556835fa67f05a",
          "name": "Ahmed Aly",
          "hidden": false
        },
        {
          "_id": "690419172c556835fa67f05b",
          "name": "Babak Damavandi",
          "hidden": false
        },
        {
          "_id": "690419172c556835fa67f05c",
          "name": "Rakesh Wanga",
          "hidden": false
        },
        {
          "_id": "690419172c556835fa67f05d",
          "name": "Anuj Kumar",
          "hidden": false
        },
        {
          "_id": "690419172c556835fa67f05e",
          "name": "Rohit Patel",
          "hidden": false
        },
        {
          "_id": "690419172c556835fa67f05f",
          "name": "Wen-tau Yih",
          "hidden": false
        },
        {
          "_id": "690419172c556835fa67f060",
          "name": "Xin Luna Dong",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-30T05:50:48.000Z",
      "submittedOnDailyAt": "2025-10-31T00:34:25.527Z",
      "title": "CRAG-MM: Multi-modal Multi-turn Comprehensive RAG Benchmark",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Wearable devices such as smart glasses are transforming the way people\ninteract with their surroundings, enabling users to seek information regarding\nentities in their view. Multi-Modal Retrieval-Augmented Generation (MM-RAG)\nplays a key role in supporting such questions, yet there is still no\ncomprehensive benchmark for this task, especially regarding wearables\nscenarios. To fill this gap, we present CRAG-MM -- a Comprehensive RAG\nbenchmark for Multi-modal Multi-turn conversations. CRAG-MM contains a diverse\nset of 6.5K (image, question, answer) triplets and 2K visual-based multi-turn\nconversations across 13 domains, including 6.2K egocentric images designed to\nmimic captures from wearable devices. We carefully constructed the questions to\nreflect real-world scenarios and challenges, including five types of\nimage-quality issues, six question types, varying entity popularity, differing\ninformation dynamism, and different conversation turns. We design three tasks:\nsingle-source augmentation, multi-source augmentation, and multi-turn\nconversations -- each paired with an associated retrieval corpus and APIs for\nboth image-KG retrieval and webpage retrieval. Our evaluation shows that\nstraightforward RAG approaches achieve only 32% and 43% truthfulness on CRAG-MM\nsingle- and multi-turn QA, respectively, whereas state-of-the-art industry\nsolutions have similar quality (32%/45%), underscoring ample room for\nimprovement. The benchmark has hosted KDD Cup 2025, attracting about 1K\nparticipants and 5K submissions, with winning solutions improving baseline\nperformance by 28%, highlighting its early impact on advancing the field.",
      "upvotes": 0,
      "discussionId": "690419172c556835fa67f061",
      "organization": {
        "_id": "5e63d8713071d5be688861b8",
        "name": "facebook",
        "fullname": "AI at Meta",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1592839207516-noauth.png"
      }
    },
    "publishedAt": "2025-10-30T01:50:48.000Z",
    "title": "CRAG-MM: Multi-modal Multi-turn Comprehensive RAG Benchmark",
    "summary": "Wearable devices such as smart glasses are transforming the way people\ninteract with their surroundings, enabling users to seek information regarding\nentities in their view. Multi-Modal Retrieval-Augmented Generation (MM-RAG)\nplays a key role in supporting such questions, yet there is still no\ncomprehensive benchmark for this task, especially regarding wearables\nscenarios. To fill this gap, we present CRAG-MM -- a Comprehensive RAG\nbenchmark for Multi-modal Multi-turn conversations. CRAG-MM contains a diverse\nset of 6.5K (image, question, answer) triplets and 2K visual-based multi-turn\nconversations across 13 domains, including 6.2K egocentric images designed to\nmimic captures from wearable devices. We carefully constructed the questions to\nreflect real-world scenarios and challenges, including five types of\nimage-quality issues, six question types, varying entity popularity, differing\ninformation dynamism, and different conversation turns. We design three tasks:\nsingle-source augmentation, multi-source augmentation, and multi-turn\nconversations -- each paired with an associated retrieval corpus and APIs for\nboth image-KG retrieval and webpage retrieval. Our evaluation shows that\nstraightforward RAG approaches achieve only 32% and 43% truthfulness on CRAG-MM\nsingle- and multi-turn QA, respectively, whereas state-of-the-art industry\nsolutions have similar quality (32%/45%), underscoring ample room for\nimprovement. The benchmark has hosted KDD Cup 2025, attracting about 1K\nparticipants and 5K submissions, with winning solutions improving baseline\nperformance by 28%, highlighting its early impact on advancing the field.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.26160.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 152
    },
    "organization": {
      "_id": "5e63d8713071d5be688861b8",
      "name": "facebook",
      "fullname": "AI at Meta",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1592839207516-noauth.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.26140",
      "authors": [
        {
          "_id": "69041ae72c556835fa67f06c",
          "name": "Lihe Ding",
          "hidden": false
        },
        {
          "_id": "69041ae72c556835fa67f06d",
          "name": "Shaocong Dong",
          "hidden": false
        },
        {
          "_id": "69041ae72c556835fa67f06e",
          "name": "Yaokun Li",
          "hidden": false
        },
        {
          "_id": "69041ae72c556835fa67f06f",
          "name": "Chenjian Gao",
          "hidden": false
        },
        {
          "_id": "69041ae72c556835fa67f070",
          "name": "Xiao Chen",
          "hidden": false
        },
        {
          "_id": "69041ae72c556835fa67f071",
          "name": "Rui Han",
          "hidden": false
        },
        {
          "_id": "69041ae72c556835fa67f072",
          "name": "Yihao Kuang",
          "hidden": false
        },
        {
          "_id": "69041ae72c556835fa67f073",
          "name": "Hong Zhang",
          "hidden": false
        },
        {
          "_id": "69041ae72c556835fa67f074",
          "name": "Bo Huang",
          "hidden": false
        },
        {
          "_id": "69041ae72c556835fa67f075",
          "name": "Zhanpeng Huang",
          "hidden": false
        },
        {
          "_id": "69041ae72c556835fa67f076",
          "name": "Zibin Wang",
          "hidden": false
        },
        {
          "_id": "69041ae72c556835fa67f077",
          "name": "Dan Xu",
          "hidden": false
        },
        {
          "_id": "69041ae72c556835fa67f078",
          "name": "Tianfan Xue",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-30T04:51:05.000Z",
      "submittedOnDailyAt": "2025-10-31T00:46:19.339Z",
      "title": "FullPart: Generating each 3D Part at Full Resolution",
      "submittedOnDailyBy": {
        "_id": "63ae91af2314b93f9e6dde42",
        "avatarUrl": "/avatars/792ce138cbee85b8754fdcec7fb1ff52.svg",
        "isPro": true,
        "fullname": "Shaocong Dong",
        "user": "dscdyc",
        "type": "user"
      },
      "summary": "Part-based 3D generation holds great potential for various applications.\nPrevious part generators that represent parts using implicit vector-set tokens\noften suffer from insufficient geometric details. Another line of work adopts\nan explicit voxel representation but shares a global voxel grid among all\nparts; this often causes small parts to occupy too few voxels, leading to\ndegraded quality. In this paper, we propose FullPart, a novel framework that\ncombines both implicit and explicit paradigms. It first derives the bounding\nbox layout through an implicit box vector-set diffusion process, a task that\nimplicit diffusion handles effectively since box tokens contain little\ngeometric detail. Then, it generates detailed parts, each within its own fixed\nfull-resolution voxel grid. Instead of sharing a global low-resolution space,\neach part in our method - even small ones - is generated at full resolution,\nenabling the synthesis of intricate details. We further introduce a\ncenter-point encoding strategy to address the misalignment issue when\nexchanging information between parts of different actual sizes, thereby\nmaintaining global coherence. Moreover, to tackle the scarcity of reliable part\ndata, we present PartVerse-XL, the largest human-annotated 3D part dataset to\ndate with 40K objects and 320K parts. Extensive experiments demonstrate that\nFullPart achieves state-of-the-art results in 3D part generation. We will\nrelease all code, data, and model to benefit future research in 3D part\ngeneration.",
      "upvotes": 0,
      "discussionId": "69041ae72c556835fa67f079",
      "projectPage": "https://fullpart3d.github.io/",
      "githubRepo": "https://github.com/hkdsc/fullpart"
    },
    "publishedAt": "2025-10-30T00:51:05.000Z",
    "title": "FullPart: Generating each 3D Part at Full Resolution",
    "summary": "Part-based 3D generation holds great potential for various applications.\nPrevious part generators that represent parts using implicit vector-set tokens\noften suffer from insufficient geometric details. Another line of work adopts\nan explicit voxel representation but shares a global voxel grid among all\nparts; this often causes small parts to occupy too few voxels, leading to\ndegraded quality. In this paper, we propose FullPart, a novel framework that\ncombines both implicit and explicit paradigms. It first derives the bounding\nbox layout through an implicit box vector-set diffusion process, a task that\nimplicit diffusion handles effectively since box tokens contain little\ngeometric detail. Then, it generates detailed parts, each within its own fixed\nfull-resolution voxel grid. Instead of sharing a global low-resolution space,\neach part in our method - even small ones - is generated at full resolution,\nenabling the synthesis of intricate details. We further introduce a\ncenter-point encoding strategy to address the misalignment issue when\nexchanging information between parts of different actual sizes, thereby\nmaintaining global coherence. Moreover, to tackle the scarcity of reliable part\ndata, we present PartVerse-XL, the largest human-annotated 3D part dataset to\ndate with 40K objects and 320K parts. Extensive experiments demonstrate that\nFullPart achieves state-of-the-art results in 3D part generation. We will\nrelease all code, data, and model to benefit future research in 3D part\ngeneration.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.26140.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63ae91af2314b93f9e6dde42",
      "avatarUrl": "/avatars/792ce138cbee85b8754fdcec7fb1ff52.svg",
      "fullname": "Shaocong Dong",
      "name": "dscdyc",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  }
]