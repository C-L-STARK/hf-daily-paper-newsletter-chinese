[
  {
    "paper": {
      "id": "2602.22897",
      "authors": [
        {
          "_id": "69a0fda9a13deaa4494488e3",
          "name": "Xiaoxi Li",
          "hidden": false
        },
        {
          "_id": "69a0fda9a13deaa4494488e4",
          "name": "Wenxiang Jiao",
          "hidden": false
        },
        {
          "_id": "69a0fda9a13deaa4494488e5",
          "name": "Jiarui Jin",
          "hidden": false
        },
        {
          "_id": "69a0fda9a13deaa4494488e6",
          "name": "Shijian Wang",
          "hidden": false
        },
        {
          "_id": "69a0fda9a13deaa4494488e7",
          "name": "Guanting Dong",
          "hidden": false
        },
        {
          "_id": "69a0fda9a13deaa4494488e8",
          "name": "Jiajie Jin",
          "hidden": false
        },
        {
          "_id": "69a0fda9a13deaa4494488e9",
          "name": "Hao Wang",
          "hidden": false
        },
        {
          "_id": "69a0fda9a13deaa4494488ea",
          "name": "Yinuo Wang",
          "hidden": false
        },
        {
          "_id": "69a0fda9a13deaa4494488eb",
          "name": "Ji-Rong Wen",
          "hidden": false
        },
        {
          "_id": "69a0fda9a13deaa4494488ec",
          "name": "Yuan Lu",
          "hidden": false
        },
        {
          "_id": "69a0fda9a13deaa4494488ed",
          "name": "Zhicheng Dou",
          "hidden": false
        }
      ],
      "publishedAt": "2026-02-26T11:35:04.000Z",
      "submittedOnDailyAt": "2026-02-27T00:29:04.184Z",
      "title": "OmniGAIA: Towards Native Omni-Modal AI Agents",
      "submittedOnDailyBy": {
        "_id": "66e03eace17fb5ff054b7686",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66e03eace17fb5ff054b7686/PpSV0Qo5lwTyxIZMp57xq.jpeg",
        "isPro": false,
        "fullname": "Xiaoxi Li",
        "user": "lixiaoxi45",
        "type": "user"
      },
      "summary": "Human intelligence naturally intertwines omni-modal perception -- spanning vision, audio, and language -- with complex reasoning and tool usage to interact with the world. However, current multi-modal LLMs are primarily confined to bi-modal interactions (e.g., vision-language), lacking the unified cognitive capabilities required for general AI assistants. To bridge this gap, we introduce OmniGAIA, a comprehensive benchmark designed to evaluate omni-modal agents on tasks necessitating deep reasoning and multi-turn tool execution across video, audio, and image modalities. Constructed via a novel omni-modal event graph approach, OmniGAIA synthesizes complex, multi-hop queries derived from real-world data that require cross-modal reasoning and external tool integration. Furthermore, we propose OmniAtlas, a native omni-modal foundation agent under tool-integrated reasoning paradigm with active omni-modal perception. Trained on trajectories synthesized via a hindsight-guided tree exploration strategy and OmniDPO for fine-grained error correction, OmniAtlas effectively enhances the tool-use capabilities of existing open-source models. This work marks a step towards next-generation native omni-modal AI assistants for real-world scenarios.",
      "upvotes": 13,
      "discussionId": "69a0fda9a13deaa4494488ee",
      "githubRepo": "https://github.com/RUC-NLPIR/OmniGAIA",
      "githubRepoAddedBy": "user",
      "ai_summary": "OmniGAIA benchmark evaluates multi-modal agents on complex reasoning tasks across video, audio, and image modalities, while OmniAtlas agent improves tool-use capabilities through hindsight-guided tree exploration and OmniDPO fine-tuning.",
      "ai_keywords": [
        "multi-modal LLMs",
        "omni-modal perception",
        "cross-modal reasoning",
        "tool-integrated reasoning",
        "hindsight-guided tree exploration",
        "OmniDPO"
      ],
      "githubStars": 9
    },
    "publishedAt": "2026-02-26T06:35:04.000Z",
    "title": "OmniGAIA: Towards Native Omni-Modal AI Agents",
    "summary": "Human intelligence naturally intertwines omni-modal perception -- spanning vision, audio, and language -- with complex reasoning and tool usage to interact with the world. However, current multi-modal LLMs are primarily confined to bi-modal interactions (e.g., vision-language), lacking the unified cognitive capabilities required for general AI assistants. To bridge this gap, we introduce OmniGAIA, a comprehensive benchmark designed to evaluate omni-modal agents on tasks necessitating deep reasoning and multi-turn tool execution across video, audio, and image modalities. Constructed via a novel omni-modal event graph approach, OmniGAIA synthesizes complex, multi-hop queries derived from real-world data that require cross-modal reasoning and external tool integration. Furthermore, we propose OmniAtlas, a native omni-modal foundation agent under tool-integrated reasoning paradigm with active omni-modal perception. Trained on trajectories synthesized via a hindsight-guided tree exploration strategy and OmniDPO for fine-grained error correction, OmniAtlas effectively enhances the tool-use capabilities of existing open-source models. This work marks a step towards next-generation native omni-modal AI assistants for real-world scenarios.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.22897.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66e03eace17fb5ff054b7686",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66e03eace17fb5ff054b7686/PpSV0Qo5lwTyxIZMp57xq.jpeg",
      "fullname": "Xiaoxi Li",
      "name": "lixiaoxi45",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 19,
      "isUserFollowing": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2602.23152",
      "authors": [
        {
          "_id": "69a106faa13deaa449448917",
          "name": "Jingxuan Wei",
          "hidden": false
        },
        {
          "_id": "69a106faa13deaa449448918",
          "name": "Siyuan Li",
          "hidden": false
        },
        {
          "_id": "69a106faa13deaa449448919",
          "name": "Yuhang Xu",
          "hidden": false
        },
        {
          "_id": "69a106faa13deaa44944891a",
          "name": "Zheng Sun",
          "hidden": false
        },
        {
          "_id": "69a106faa13deaa44944891b",
          "name": "Junjie Jiang",
          "hidden": false
        },
        {
          "_id": "69a106faa13deaa44944891c",
          "name": "Hexuan Jin",
          "hidden": false
        },
        {
          "_id": "69a106faa13deaa44944891d",
          "name": "Caijun Jia",
          "hidden": false
        },
        {
          "_id": "69a106faa13deaa44944891e",
          "name": "Honghao He",
          "hidden": false
        },
        {
          "_id": "69a106faa13deaa44944891f",
          "name": "Xinglong Xu",
          "hidden": false
        },
        {
          "_id": "69a106faa13deaa449448920",
          "name": "Xi bai",
          "hidden": false
        },
        {
          "_id": "69a106faa13deaa449448921",
          "name": "Chang Yu",
          "hidden": false
        },
        {
          "_id": "69a106faa13deaa449448922",
          "name": "Yumou Liu",
          "hidden": false
        },
        {
          "_id": "69a106faa13deaa449448923",
          "name": "Junnan Zhu",
          "hidden": false
        },
        {
          "_id": "69a106faa13deaa449448924",
          "name": "Xuanhe Zhou",
          "hidden": false
        },
        {
          "_id": "69a106faa13deaa449448925",
          "name": "Jintao Chen",
          "hidden": false
        },
        {
          "_id": "69a106faa13deaa449448926",
          "name": "Xiaobin Hu",
          "hidden": false
        },
        {
          "_id": "69a106faa13deaa449448927",
          "name": "Shancheng Pang",
          "hidden": false
        },
        {
          "_id": "69a106faa13deaa449448928",
          "name": "Bihui Yu",
          "hidden": false
        },
        {
          "_id": "69a106faa13deaa449448929",
          "name": "Ran He",
          "hidden": false
        },
        {
          "_id": "69a106faa13deaa44944892a",
          "name": "Zhen Lei",
          "hidden": false
        },
        {
          "_id": "69a106faa13deaa44944892b",
          "name": "Stan Z. Li",
          "hidden": false
        },
        {
          "_id": "69a106faa13deaa44944892c",
          "name": "Conghui He",
          "hidden": false
        },
        {
          "_id": "69a106faa13deaa44944892d",
          "name": "Shuicheng Yan",
          "hidden": false
        },
        {
          "_id": "69a106faa13deaa44944892e",
          "name": "Cheng Tan",
          "hidden": false
        }
      ],
      "publishedAt": "2026-02-26T16:15:55.000Z",
      "submittedOnDailyAt": "2026-02-27T00:24:05.560Z",
      "title": "The Trinity of Consistency as a Defining Principle for General World Models",
      "submittedOnDailyBy": {
        "_id": "64be296a46cc3cdfbb057f7e",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64be296a46cc3cdfbb057f7e/jSHeNY2AcPifCZzJyFhr4.jpeg",
        "isPro": false,
        "fullname": "Cheng Tan",
        "user": "chengtan9907",
        "type": "user"
      },
      "summary": "The construction of World Models capable of learning, simulating, and reasoning about objective physical laws constitutes a foundational challenge in the pursuit of Artificial General Intelligence. Recent advancements represented by video generation models like Sora have demonstrated the potential of data-driven scaling laws to approximate physical dynamics, while the emerging Unified Multimodal Model (UMM) offers a promising architectural paradigm for integrating perception, language, and reasoning. Despite these advances, the field still lacks a principled theoretical framework that defines the essential properties requisite for a General World Model. In this paper, we propose that a World Model must be grounded in the Trinity of Consistency: Modal Consistency as the semantic interface, Spatial Consistency as the geometric basis, and Temporal Consistency as the causal engine. Through this tripartite lens, we systematically review the evolution of multimodal learning, revealing a trajectory from loosely coupled specialized modules toward unified architectures that enable the synergistic emergence of internal world simulators. To complement this conceptual framework, we introduce CoW-Bench, a benchmark centered on multi-frame reasoning and generation scenarios. CoW-Bench evaluates both video generation models and UMMs under a unified evaluation protocol. Our work establishes a principled pathway toward general world models, clarifying both the limitations of current systems and the architectural requirements for future progress.",
      "upvotes": 9,
      "discussionId": "69a106faa13deaa44944892f",
      "projectPage": "https://openraiser.github.io/CoW-Bench/",
      "githubRepo": "https://github.com/openraiser/awesome-world-model-evolution",
      "githubRepoAddedBy": "user",
      "ai_summary": "World Models require three consistency principles—modal, spatial, and temporal—for general artificial intelligence, with a proposed benchmark evaluating multimodal learning systems.",
      "ai_keywords": [
        "World Models",
        "video generation models",
        "Unified Multimodal Model",
        "multimodal learning",
        "multi-frame reasoning",
        "CoW-Bench",
        "modal consistency",
        "spatial consistency",
        "temporal consistency"
      ],
      "githubStars": 10,
      "organization": {
        "_id": "66ce9d1f5e180b9b9c8e6f31",
        "name": "opendatalab",
        "fullname": "OpenDataLab",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/639c3afa7432f2f5d16b7296/yqxxBknyeqkGnYsjoaR4M.png"
      }
    },
    "publishedAt": "2026-02-26T11:15:55.000Z",
    "title": "The Trinity of Consistency as a Defining Principle for General World Models",
    "summary": "The construction of World Models capable of learning, simulating, and reasoning about objective physical laws constitutes a foundational challenge in the pursuit of Artificial General Intelligence. Recent advancements represented by video generation models like Sora have demonstrated the potential of data-driven scaling laws to approximate physical dynamics, while the emerging Unified Multimodal Model (UMM) offers a promising architectural paradigm for integrating perception, language, and reasoning. Despite these advances, the field still lacks a principled theoretical framework that defines the essential properties requisite for a General World Model. In this paper, we propose that a World Model must be grounded in the Trinity of Consistency: Modal Consistency as the semantic interface, Spatial Consistency as the geometric basis, and Temporal Consistency as the causal engine. Through this tripartite lens, we systematically review the evolution of multimodal learning, revealing a trajectory from loosely coupled specialized modules toward unified architectures that enable the synergistic emergence of internal world simulators. To complement this conceptual framework, we introduce CoW-Bench, a benchmark centered on multi-frame reasoning and generation scenarios. CoW-Bench evaluates both video generation models and UMMs under a unified evaluation protocol. Our work establishes a principled pathway toward general world models, clarifying both the limitations of current systems and the architectural requirements for future progress.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.23152.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64be296a46cc3cdfbb057f7e",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64be296a46cc3cdfbb057f7e/jSHeNY2AcPifCZzJyFhr4.jpeg",
      "fullname": "Cheng Tan",
      "name": "chengtan9907",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "66ce9d1f5e180b9b9c8e6f31",
      "name": "opendatalab",
      "fullname": "OpenDataLab",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/639c3afa7432f2f5d16b7296/yqxxBknyeqkGnYsjoaR4M.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2602.22594",
      "authors": [
        {
          "_id": "69a10abaa13deaa44944895d",
          "name": "Qing Yu",
          "hidden": false
        },
        {
          "_id": "69a10abaa13deaa44944895e",
          "name": "Akihisa Watanabe",
          "hidden": false
        },
        {
          "_id": "69a10abaa13deaa44944895f",
          "name": "Kent Fujiwara",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/cE21YhRnaLWjMplc3cQfe.mp4"
      ],
      "publishedAt": "2026-02-26T03:58:25.000Z",
      "submittedOnDailyAt": "2026-02-27T00:40:13.183Z",
      "title": "Causal Motion Diffusion Models for Autoregressive Motion Generation",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Recent advances in motion diffusion models have substantially improved the realism of human motion synthesis. However, existing approaches either rely on full-sequence diffusion models with bidirectional generation, which limits temporal causality and real-time applicability, or autoregressive models that suffer from instability and cumulative errors. In this work, we present Causal Motion Diffusion Models (CMDM), a unified framework for autoregressive motion generation based on a causal diffusion transformer that operates in a semantically aligned latent space. CMDM builds upon a Motion-Language-Aligned Causal VAE (MAC-VAE), which encodes motion sequences into temporally causal latent representations. On top of this latent representation, an autoregressive diffusion transformer is trained using causal diffusion forcing to perform temporally ordered denoising across motion frames. To achieve fast inference, we introduce a frame-wise sampling schedule with causal uncertainty, where each subsequent frame is predicted from partially denoised previous frames. The resulting framework supports high-quality text-to-motion generation, streaming synthesis, and long-horizon motion generation at interactive rates. Experiments on HumanML3D and SnapMoGen demonstrate that CMDM outperforms existing diffusion and autoregressive models in both semantic fidelity and temporal smoothness, while substantially reducing inference latency.",
      "upvotes": 1,
      "discussionId": "69a10abaa13deaa449448960",
      "projectPage": "https://yu1ut.com/CMDM-HP/",
      "ai_summary": "Causal Motion Diffusion Models introduce a unified framework for autoregressive motion generation using a causal diffusion transformer in a semantically aligned latent space, enabling fast, high-quality text-to-motion synthesis with improved temporal smoothness.",
      "ai_keywords": [
        "motion diffusion models",
        "causal diffusion transformer",
        "latent space",
        "Motion-Language-Aligned Causal VAE",
        "autoregressive motion generation",
        "causal diffusion forcing",
        "temporally ordered denoising",
        "frame-wise sampling schedule",
        "causal uncertainty",
        "text-to-motion generation",
        "streaming synthesis",
        "long-horizon motion generation"
      ]
    },
    "publishedAt": "2026-02-25T22:58:25.000Z",
    "title": "Causal Motion Diffusion Models for Autoregressive Motion Generation",
    "summary": "Recent advances in motion diffusion models have substantially improved the realism of human motion synthesis. However, existing approaches either rely on full-sequence diffusion models with bidirectional generation, which limits temporal causality and real-time applicability, or autoregressive models that suffer from instability and cumulative errors. In this work, we present Causal Motion Diffusion Models (CMDM), a unified framework for autoregressive motion generation based on a causal diffusion transformer that operates in a semantically aligned latent space. CMDM builds upon a Motion-Language-Aligned Causal VAE (MAC-VAE), which encodes motion sequences into temporally causal latent representations. On top of this latent representation, an autoregressive diffusion transformer is trained using causal diffusion forcing to perform temporally ordered denoising across motion frames. To achieve fast inference, we introduce a frame-wise sampling schedule with causal uncertainty, where each subsequent frame is predicted from partially denoised previous frames. The resulting framework supports high-quality text-to-motion generation, streaming synthesis, and long-horizon motion generation at interactive rates. Experiments on HumanML3D and SnapMoGen demonstrate that CMDM outperforms existing diffusion and autoregressive models in both semantic fidelity and temporal smoothness, while substantially reducing inference latency.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/cE21YhRnaLWjMplc3cQfe.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.22594.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 241,
      "isUserFollowing": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2602.23259",
      "authors": [
        {
          "_id": "69a10708a13deaa449448931",
          "name": "Jiangxin Sun",
          "hidden": false
        },
        {
          "_id": "69a10708a13deaa449448932",
          "name": "Feng Xue",
          "hidden": false
        },
        {
          "_id": "69a10708a13deaa449448933",
          "name": "Teng Long",
          "hidden": false
        },
        {
          "_id": "69a10708a13deaa449448934",
          "name": "Chang Liu",
          "hidden": false
        },
        {
          "_id": "69a10708a13deaa449448935",
          "name": "Jian-Fang Hu",
          "hidden": false
        },
        {
          "_id": "69a10708a13deaa449448936",
          "name": "Wei-Shi Zheng",
          "hidden": false
        },
        {
          "_id": "69a10708a13deaa449448937",
          "name": "Nicu Sebe",
          "hidden": false
        }
      ],
      "publishedAt": "2026-02-26T17:32:30.000Z",
      "submittedOnDailyAt": "2026-02-27T00:23:05.206Z",
      "title": "Risk-Aware World Model Predictive Control for Generalizable End-to-End Autonomous Driving",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "With advances in imitation learning (IL) and large-scale driving datasets, end-to-end autonomous driving (E2E-AD) has made great progress recently. Currently, IL-based methods have become a mainstream paradigm: models rely on standard driving behaviors given by experts, and learn to minimize the discrepancy between their actions and expert actions. However, this objective of \"only driving like the expert\" suffers from limited generalization: when encountering rare or unseen long-tail scenarios outside the distribution of expert demonstrations, models tend to produce unsafe decisions in the absence of prior experience. This raises a fundamental question: Can an E2E-AD system make reliable decisions without any expert action supervision? Motivated by this, we propose a unified framework named Risk-aware World Model Predictive Control (RaWMPC) to address this generalization dilemma through robust control, without reliance on expert demonstrations. Practically, RaWMPC leverages a world model to predict the consequences of multiple candidate actions and selects low-risk actions through explicit risk evaluation. To endow the world model with the ability to predict the outcomes of risky driving behaviors, we design a risk-aware interaction strategy that systematically exposes the world model to hazardous behaviors, making catastrophic outcomes predictable and thus avoidable. Furthermore, to generate low-risk candidate actions at test time, we introduce a self-evaluation distillation method to distill riskavoidance capabilities from the well-trained world model into a generative action proposal network without any expert demonstration. Extensive experiments show that RaWMPC outperforms state-of-the-art methods in both in-distribution and out-of-distribution scenarios, while providing superior decision interpretability.",
      "upvotes": 0,
      "discussionId": "69a10708a13deaa449448938",
      "ai_summary": "A risk-aware framework for autonomous driving that uses world modeling and risk evaluation to generalize beyond expert demonstrations without requiring explicit expert supervision.",
      "ai_keywords": [
        "imitation learning",
        "end-to-end autonomous driving",
        "world model",
        "predictive control",
        "risk-aware",
        "expert demonstrations",
        "generalization",
        "self-evaluation distillation",
        "action proposal network"
      ]
    },
    "publishedAt": "2026-02-26T12:32:30.000Z",
    "title": "Risk-Aware World Model Predictive Control for Generalizable End-to-End Autonomous Driving",
    "summary": "With advances in imitation learning (IL) and large-scale driving datasets, end-to-end autonomous driving (E2E-AD) has made great progress recently. Currently, IL-based methods have become a mainstream paradigm: models rely on standard driving behaviors given by experts, and learn to minimize the discrepancy between their actions and expert actions. However, this objective of \"only driving like the expert\" suffers from limited generalization: when encountering rare or unseen long-tail scenarios outside the distribution of expert demonstrations, models tend to produce unsafe decisions in the absence of prior experience. This raises a fundamental question: Can an E2E-AD system make reliable decisions without any expert action supervision? Motivated by this, we propose a unified framework named Risk-aware World Model Predictive Control (RaWMPC) to address this generalization dilemma through robust control, without reliance on expert demonstrations. Practically, RaWMPC leverages a world model to predict the consequences of multiple candidate actions and selects low-risk actions through explicit risk evaluation. To endow the world model with the ability to predict the outcomes of risky driving behaviors, we design a risk-aware interaction strategy that systematically exposes the world model to hazardous behaviors, making catastrophic outcomes predictable and thus avoidable. Furthermore, to generate low-risk candidate actions at test time, we introduce a self-evaluation distillation method to distill riskavoidance capabilities from the well-trained world model into a generative action proposal network without any expert demonstration. Extensive experiments show that RaWMPC outperforms state-of-the-art methods in both in-distribution and out-of-distribution scenarios, while providing superior decision interpretability.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.23259.png",
    "numComments": 0,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 241,
      "isUserFollowing": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2602.23165",
      "authors": [
        {
          "_id": "69a10783a13deaa44944893a",
          "name": "Yichen Peng",
          "hidden": false
        },
        {
          "_id": "69a10783a13deaa44944893b",
          "name": "Jyun-Ting Song",
          "hidden": false
        },
        {
          "_id": "69a10783a13deaa44944893c",
          "name": "Siyeol Jung",
          "hidden": false
        },
        {
          "_id": "69a10783a13deaa44944893d",
          "name": "Ruofan Liu",
          "hidden": false
        },
        {
          "_id": "69a10783a13deaa44944893e",
          "name": "Haiyang Liu",
          "hidden": false
        },
        {
          "_id": "69a10783a13deaa44944893f",
          "name": "Xuangeng Chu",
          "hidden": false
        },
        {
          "_id": "69a10783a13deaa449448940",
          "name": "Ruicong Liu",
          "hidden": false
        },
        {
          "_id": "69a10783a13deaa449448941",
          "name": "Erwin Wu",
          "hidden": false
        },
        {
          "_id": "69a10783a13deaa449448942",
          "name": "Hideki Koike",
          "hidden": false
        },
        {
          "_id": "69a10783a13deaa449448943",
          "name": "Kris Kitani",
          "hidden": false
        }
      ],
      "publishedAt": "2026-02-26T16:30:07.000Z",
      "submittedOnDailyAt": "2026-02-27T00:25:02.702Z",
      "title": "DyaDiT: A Multi-Modal Diffusion Transformer for Socially Favorable Dyadic Gesture Generation",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Generating realistic conversational gestures are essential for achieving natural, socially engaging interactions with digital humans. However, existing methods typically map a single audio stream to a single speaker's motion, without considering social context or modeling the mutual dynamics between two people engaging in conversation. We present DyaDiT, a multi-modal diffusion transformer that generates contextually appropriate human motion from dyadic audio signals. Trained on Seamless Interaction Dataset, DyaDiT takes dyadic audio with optional social-context tokens to produce context-appropriate motion. It fuses information from both speakers to capture interaction dynamics, uses a motion dictionary to encode motion priors, and can optionally utilize the conversational partner's gestures to produce more responsive motion. We evaluate DyaDiT on standard motion generation metrics and conduct quantitative user studies, demonstrating that it not only surpasses existing methods on objective metrics but is also strongly preferred by users, highlighting its robustness and socially favorable motion generation. Code and models will be released upon acceptance.",
      "upvotes": 0,
      "discussionId": "69a10783a13deaa449448944",
      "ai_summary": "DyaDiT is a multi-modal diffusion transformer that generates contextually appropriate human motion from dyadic audio signals by capturing interaction dynamics between two speakers.",
      "ai_keywords": [
        "diffusion transformer",
        "dyadic audio signals",
        "motion generation",
        "social context",
        "motion dictionary",
        "conversational partner gestures",
        "Seamless Interaction Dataset"
      ]
    },
    "publishedAt": "2026-02-26T11:30:07.000Z",
    "title": "DyaDiT: A Multi-Modal Diffusion Transformer for Socially Favorable Dyadic Gesture Generation",
    "summary": "Generating realistic conversational gestures are essential for achieving natural, socially engaging interactions with digital humans. However, existing methods typically map a single audio stream to a single speaker's motion, without considering social context or modeling the mutual dynamics between two people engaging in conversation. We present DyaDiT, a multi-modal diffusion transformer that generates contextually appropriate human motion from dyadic audio signals. Trained on Seamless Interaction Dataset, DyaDiT takes dyadic audio with optional social-context tokens to produce context-appropriate motion. It fuses information from both speakers to capture interaction dynamics, uses a motion dictionary to encode motion priors, and can optionally utilize the conversational partner's gestures to produce more responsive motion. We evaluate DyaDiT on standard motion generation metrics and conduct quantitative user studies, demonstrating that it not only surpasses existing methods on objective metrics but is also strongly preferred by users, highlighting its robustness and socially favorable motion generation. Code and models will be released upon acceptance.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.23165.png",
    "numComments": 0,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 241,
      "isUserFollowing": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2602.23058",
      "authors": [
        {
          "_id": "69a108a6a13deaa449448946",
          "name": "Zeyu Zhang",
          "hidden": false
        },
        {
          "_id": "69a108a6a13deaa449448947",
          "name": "Danning Li",
          "hidden": false
        },
        {
          "_id": "69a108a6a13deaa449448948",
          "name": "Ian Reid",
          "hidden": false
        },
        {
          "_id": "69a108a6a13deaa449448949",
          "name": "Richard Hartley",
          "hidden": false
        }
      ],
      "publishedAt": "2026-02-26T14:42:53.000Z",
      "submittedOnDailyAt": "2026-02-27T00:30:33.142Z",
      "title": "GeoWorld: Geometric World Models",
      "submittedOnDailyBy": {
        "_id": "64ec877bb93654d4ca5c92e9",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ec877bb93654d4ca5c92e9/GvHk_KSdE9Rhnk_o-NaZX.jpeg",
        "isPro": false,
        "fullname": "Zeyu Zhang",
        "user": "SteveZeyuZhang",
        "type": "user"
      },
      "summary": "Energy-based predictive world models provide a powerful approach for multi-step visual planning by reasoning over latent energy landscapes rather than generating pixels. However, existing approaches face two major challenges: (i) their latent representations are typically learned in Euclidean space, neglecting the underlying geometric and hierarchical structure among states, and (ii) they struggle with long-horizon prediction, which leads to rapid degradation across extended rollouts. To address these challenges, we introduce GeoWorld, a geometric world model that preserves geometric structure and hierarchical relations through a Hyperbolic JEPA, which maps latent representations from Euclidean space onto hyperbolic manifolds. We further introduce Geometric Reinforcement Learning for energy-based optimization, enabling stable multi-step planning in hyperbolic latent space. Extensive experiments on CrossTask and COIN demonstrate around 3% SR improvement in 3-step planning and 2% SR improvement in 4-step planning compared to the state-of-the-art V-JEPA 2. Project website: https://steve-zeyu-zhang.github.io/GeoWorld.",
      "upvotes": 0,
      "discussionId": "69a108a7a13deaa44944894a",
      "projectPage": "https://steve-zeyu-zhang.github.io/GeoWorld",
      "ai_summary": "GeoWorld addresses limitations in energy-based predictive world models by utilizing hyperbolic geometry to preserve latent state structures and improve long-horizon prediction performance.",
      "ai_keywords": [
        "energy-based predictive world models",
        "latent energy landscapes",
        "Euclidean space",
        "geometric structure",
        "hierarchical relations",
        "Hyperbolic JEPA",
        "hyperbolic manifolds",
        "geometric reinforcement learning",
        "multi-step planning",
        "long-horizon prediction"
      ]
    },
    "publishedAt": "2026-02-26T09:42:53.000Z",
    "title": "GeoWorld: Geometric World Models",
    "summary": "Energy-based predictive world models provide a powerful approach for multi-step visual planning by reasoning over latent energy landscapes rather than generating pixels. However, existing approaches face two major challenges: (i) their latent representations are typically learned in Euclidean space, neglecting the underlying geometric and hierarchical structure among states, and (ii) they struggle with long-horizon prediction, which leads to rapid degradation across extended rollouts. To address these challenges, we introduce GeoWorld, a geometric world model that preserves geometric structure and hierarchical relations through a Hyperbolic JEPA, which maps latent representations from Euclidean space onto hyperbolic manifolds. We further introduce Geometric Reinforcement Learning for energy-based optimization, enabling stable multi-step planning in hyperbolic latent space. Extensive experiments on CrossTask and COIN demonstrate around 3% SR improvement in 3-step planning and 2% SR improvement in 4-step planning compared to the state-of-the-art V-JEPA 2. Project website: https://steve-zeyu-zhang.github.io/GeoWorld.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.23058.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64ec877bb93654d4ca5c92e9",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ec877bb93654d4ca5c92e9/GvHk_KSdE9Rhnk_o-NaZX.jpeg",
      "fullname": "Zeyu Zhang",
      "name": "SteveZeyuZhang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4,
      "isUserFollowing": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2602.22437",
      "authors": [
        {
          "_id": "69a108daa13deaa44944894c",
          "name": "Zezhou Wang",
          "hidden": false
        },
        {
          "_id": "69a108daa13deaa44944894d",
          "name": "Youjie Li",
          "hidden": false
        },
        {
          "_id": "69a108daa13deaa44944894e",
          "name": "Zhiqi Lin",
          "hidden": false
        },
        {
          "_id": "69a108daa13deaa44944894f",
          "name": "Jiacheng Yang",
          "hidden": false
        },
        {
          "_id": "69a108daa13deaa449448950",
          "name": "Cong Xie",
          "hidden": false
        },
        {
          "_id": "69a108daa13deaa449448951",
          "name": "Guanyu Feng",
          "hidden": false
        },
        {
          "_id": "69a108daa13deaa449448952",
          "name": "Zheng Zhong",
          "hidden": false
        },
        {
          "_id": "69a108daa13deaa449448953",
          "name": "Ziyue Huang",
          "hidden": false
        },
        {
          "_id": "69a108daa13deaa449448954",
          "name": "Hongyu Zhu",
          "hidden": false
        },
        {
          "_id": "69a108daa13deaa449448955",
          "name": "Zhi Zhang",
          "hidden": false
        },
        {
          "_id": "69a108daa13deaa449448956",
          "name": "Yanghua Peng",
          "hidden": false
        },
        {
          "_id": "69a108daa13deaa449448957",
          "name": "Xin Liu",
          "hidden": false
        }
      ],
      "publishedAt": "2026-02-25T21:55:43.000Z",
      "submittedOnDailyAt": "2026-02-27T00:30:57.506Z",
      "title": "veScale-FSDP: Flexible and High-Performance FSDP at Scale",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Fully Sharded Data Parallel (FSDP), also known as ZeRO, is widely used for training large-scale models, featuring its flexibility and minimal intrusion on model code. However, current FSDP systems struggle with structure-aware training methods (e.g., block-wise quantized training) and with non-element-wise optimizers (e.g., Shampoo and Muon) used in cutting-edge models (e.g., Gemini, Kimi K2). FSDP's fixed element- or row-wise sharding formats conflict with the block-structured computations. In addition, today's implementations fall short in communication and memory efficiency, limiting scaling to tens of thousands of GPUs. We introduce veScale-FSDP, a redesigned FSDP system that couples a flexible sharding format, RaggedShard, with a structure-aware planning algorithm to deliver both flexibility and performance at scale. veScale-FSDP natively supports efficient data placement required by FSDP, empowering block-wise quantization and non-element-wise optimizers. As a result, veScale-FSDP achieves 5~66% higher throughput and 16~30% lower memory usage than existing FSDP systems, while scaling efficiently to tens of thousands of GPUs.",
      "upvotes": 0,
      "discussionId": "69a108dba13deaa449448958",
      "ai_summary": "veScale-FSDP introduces a redesigned fully sharded data parallel system with flexible sharding and structure-aware planning to improve scalability and efficiency for large-scale model training.",
      "ai_keywords": [
        "Fully Sharded Data Parallel",
        "ZeRO",
        "block-wise quantized training",
        "non-element-wise optimizers",
        "Shampoo",
        "Muon",
        "RaggedShard",
        "structure-aware planning",
        "communication efficiency",
        "memory efficiency",
        "scalability"
      ]
    },
    "publishedAt": "2026-02-25T16:55:43.000Z",
    "title": "veScale-FSDP: Flexible and High-Performance FSDP at Scale",
    "summary": "Fully Sharded Data Parallel (FSDP), also known as ZeRO, is widely used for training large-scale models, featuring its flexibility and minimal intrusion on model code. However, current FSDP systems struggle with structure-aware training methods (e.g., block-wise quantized training) and with non-element-wise optimizers (e.g., Shampoo and Muon) used in cutting-edge models (e.g., Gemini, Kimi K2). FSDP's fixed element- or row-wise sharding formats conflict with the block-structured computations. In addition, today's implementations fall short in communication and memory efficiency, limiting scaling to tens of thousands of GPUs. We introduce veScale-FSDP, a redesigned FSDP system that couples a flexible sharding format, RaggedShard, with a structure-aware planning algorithm to deliver both flexibility and performance at scale. veScale-FSDP natively supports efficient data placement required by FSDP, empowering block-wise quantization and non-element-wise optimizers. As a result, veScale-FSDP achieves 5~66% higher throughput and 16~30% lower memory usage than existing FSDP systems, while scaling efficiently to tens of thousands of GPUs.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.22437.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 241,
      "isUserFollowing": false
    },
    "isAuthorParticipating": false
  }
]