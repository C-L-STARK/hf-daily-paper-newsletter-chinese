[
  {
    "paper": {
      "id": "2507.21045",
      "authors": [
        {
          "_id": "68882eddaf872d625c10c5d4",
          "name": "Yukang Cao",
          "hidden": false
        },
        {
          "_id": "68882eddaf872d625c10c5d5",
          "name": "Jiahao Lu",
          "hidden": false
        },
        {
          "_id": "68882eddaf872d625c10c5d6",
          "name": "Zhisheng Huang",
          "hidden": false
        },
        {
          "_id": "68882eddaf872d625c10c5d7",
          "name": "Zhuowei Shen",
          "hidden": false
        },
        {
          "_id": "68882eddaf872d625c10c5d8",
          "name": "Chengfeng Zhao",
          "hidden": false
        },
        {
          "_id": "68882eddaf872d625c10c5d9",
          "name": "Fangzhou Hong",
          "hidden": false
        },
        {
          "_id": "68882eddaf872d625c10c5da",
          "name": "Zhaoxi Chen",
          "hidden": false
        },
        {
          "_id": "68882eddaf872d625c10c5db",
          "name": "Xin Li",
          "hidden": false
        },
        {
          "_id": "68882eddaf872d625c10c5dc",
          "name": "Wenping Wang",
          "hidden": false
        },
        {
          "_id": "68882eddaf872d625c10c5dd",
          "name": "Yuan Liu",
          "hidden": false
        },
        {
          "_id": "68882eddaf872d625c10c5de",
          "name": "Ziwei Liu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-28T17:59:02.000Z",
      "submittedOnDailyAt": "2025-07-29T00:46:42.998Z",
      "title": "Reconstructing 4D Spatial Intelligence: A Survey",
      "submittedOnDailyBy": {
        "_id": "63a07c3ab5515dccd40fdb71",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a07c3ab5515dccd40fdb71/ly3pwhjWVge25LAeVgriV.png",
        "isPro": false,
        "fullname": "Yukang Cao",
        "user": "yukangcao",
        "type": "user"
      },
      "summary": "Reconstructing 4D spatial intelligence from visual observations has long been\na central yet challenging task in computer vision, with broad real-world\napplications. These range from entertainment domains like movies, where the\nfocus is often on reconstructing fundamental visual elements, to embodied AI,\nwhich emphasizes interaction modeling and physical realism. Fueled by rapid\nadvances in 3D representations and deep learning architectures, the field has\nevolved quickly, outpacing the scope of previous surveys. Additionally,\nexisting surveys rarely offer a comprehensive analysis of the hierarchical\nstructure of 4D scene reconstruction. To address this gap, we present a new\nperspective that organizes existing methods into five progressive levels of 4D\nspatial intelligence: (1) Level 1 -- reconstruction of low-level 3D attributes\n(e.g., depth, pose, and point maps); (2) Level 2 -- reconstruction of 3D scene\ncomponents (e.g., objects, humans, structures); (3) Level 3 -- reconstruction\nof 4D dynamic scenes; (4) Level 4 -- modeling of interactions among scene\ncomponents; and (5) Level 5 -- incorporation of physical laws and constraints.\nWe conclude the survey by discussing the key challenges at each level and\nhighlighting promising directions for advancing toward even richer levels of 4D\nspatial intelligence. To track ongoing developments, we maintain an up-to-date\nproject page: https://github.com/yukangcao/Awesome-4D-Spatial-Intelligence.",
      "upvotes": 15,
      "discussionId": "68882eddaf872d625c10c5df",
      "ai_summary": "A survey organizes methods for reconstructing 4D spatial intelligence from visual observations into five progressive levels, offering analysis and identifying future research directions.",
      "ai_keywords": [
        "3D representations",
        "deep learning architectures",
        "4D scene reconstruction",
        "3D attributes",
        "depth",
        "pose",
        "point maps",
        "3D scene components",
        "objects",
        "humans",
        "structures",
        "4D dynamic scenes",
        "modeling of interactions",
        "physical laws",
        "constraints"
      ]
    },
    "publishedAt": "2025-07-28T13:59:02.000Z",
    "title": "Reconstructing 4D Spatial Intelligence: A Survey",
    "summary": "Reconstructing 4D spatial intelligence from visual observations has long been\na central yet challenging task in computer vision, with broad real-world\napplications. These range from entertainment domains like movies, where the\nfocus is often on reconstructing fundamental visual elements, to embodied AI,\nwhich emphasizes interaction modeling and physical realism. Fueled by rapid\nadvances in 3D representations and deep learning architectures, the field has\nevolved quickly, outpacing the scope of previous surveys. Additionally,\nexisting surveys rarely offer a comprehensive analysis of the hierarchical\nstructure of 4D scene reconstruction. To address this gap, we present a new\nperspective that organizes existing methods into five progressive levels of 4D\nspatial intelligence: (1) Level 1 -- reconstruction of low-level 3D attributes\n(e.g., depth, pose, and point maps); (2) Level 2 -- reconstruction of 3D scene\ncomponents (e.g., objects, humans, structures); (3) Level 3 -- reconstruction\nof 4D dynamic scenes; (4) Level 4 -- modeling of interactions among scene\ncomponents; and (5) Level 5 -- incorporation of physical laws and constraints.\nWe conclude the survey by discussing the key challenges at each level and\nhighlighting promising directions for advancing toward even richer levels of 4D\nspatial intelligence. To track ongoing developments, we maintain an up-to-date\nproject page: https://github.com/yukangcao/Awesome-4D-Spatial-Intelligence.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.21045.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63a07c3ab5515dccd40fdb71",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a07c3ab5515dccd40fdb71/ly3pwhjWVge25LAeVgriV.png",
      "fullname": "Yukang Cao",
      "name": "yukangcao",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.20939",
      "authors": [
        {
          "_id": "68882c7eaf872d625c10c5c0",
          "name": "Yuying Ge",
          "hidden": false
        },
        {
          "_id": "68882c7eaf872d625c10c5c1",
          "name": "Yixiao Ge",
          "hidden": false
        },
        {
          "_id": "68882c7eaf872d625c10c5c2",
          "name": "Chen Li",
          "hidden": false
        },
        {
          "_id": "68882c7eaf872d625c10c5c3",
          "name": "Teng Wang",
          "hidden": false
        },
        {
          "_id": "68882c7eaf872d625c10c5c4",
          "name": "Junfu Pu",
          "hidden": false
        },
        {
          "_id": "68882c7eaf872d625c10c5c5",
          "name": "Yizhuo Li",
          "hidden": false
        },
        {
          "_id": "68882c7eaf872d625c10c5c6",
          "name": "Lu Qiu",
          "hidden": false
        },
        {
          "_id": "68882c7eaf872d625c10c5c7",
          "name": "Jin Ma",
          "hidden": false
        },
        {
          "_id": "68882c7eaf872d625c10c5c8",
          "name": "Lisheng Duan",
          "hidden": false
        },
        {
          "_id": "68882c7eaf872d625c10c5c9",
          "name": "Xinyu Zuo",
          "hidden": false
        },
        {
          "_id": "68882c7eaf872d625c10c5ca",
          "name": "Jinwen Luo",
          "hidden": false
        },
        {
          "_id": "68882c7eaf872d625c10c5cb",
          "name": "Weibo Gu",
          "hidden": false
        },
        {
          "_id": "68882c7eaf872d625c10c5cc",
          "name": "Zexuan Li",
          "hidden": false
        },
        {
          "_id": "68882c7eaf872d625c10c5cd",
          "name": "Xiaojing Zhang",
          "hidden": false
        },
        {
          "_id": "68882c7eaf872d625c10c5ce",
          "name": "Yangyu Tao",
          "hidden": false
        },
        {
          "_id": "68882c7eaf872d625c10c5cf",
          "name": "Han Hu",
          "hidden": false
        },
        {
          "_id": "68882c7eaf872d625c10c5d0",
          "name": "Di Wang",
          "hidden": false
        },
        {
          "_id": "68882c7eaf872d625c10c5d1",
          "name": "Ying Shan",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-28T15:52:36.000Z",
      "submittedOnDailyAt": "2025-07-29T00:45:07.316Z",
      "title": "ARC-Hunyuan-Video-7B: Structured Video Comprehension of Real-World\n  Shorts",
      "submittedOnDailyBy": {
        "_id": "6455cc8f654d8bccae50e4d4",
        "avatarUrl": "/avatars/506a9992e5bf52e06d37cc22e4b307c0.svg",
        "isPro": false,
        "fullname": "Yuying Ge",
        "user": "tttoaster",
        "type": "user"
      },
      "summary": "Real-world user-generated short videos, especially those distributed on\nplatforms such as WeChat Channel and TikTok, dominate the mobile internet.\nHowever, current large multimodal models lack essential temporally-structured,\ndetailed, and in-depth video comprehension capabilities, which are the\ncornerstone of effective video search and recommendation, as well as emerging\nvideo applications. Understanding real-world shorts is actually challenging due\nto their complex visual elements, high information density in both visuals and\naudio, and fast pacing that focuses on emotional expression and viewpoint\ndelivery. This requires advanced reasoning to effectively integrate multimodal\ninformation, including visual, audio, and text. In this work, we introduce\nARC-Hunyuan-Video, a multimodal model that processes visual, audio, and textual\nsignals from raw video inputs end-to-end for structured comprehension. The\nmodel is capable of multi-granularity timestamped video captioning and\nsummarization, open-ended video question answering, temporal video grounding,\nand video reasoning. Leveraging high-quality data from an automated annotation\npipeline, our compact 7B-parameter model is trained through a comprehensive\nregimen: pre-training, instruction fine-tuning, cold start, reinforcement\nlearning (RL) post-training, and final instruction fine-tuning. Quantitative\nevaluations on our introduced benchmark ShortVid-Bench and qualitative\ncomparisons demonstrate its strong performance in real-world video\ncomprehension, and it supports zero-shot or fine-tuning with a few samples for\ndiverse downstream applications. The real-world production deployment of our\nmodel has yielded tangible and measurable improvements in user engagement and\nsatisfaction, a success supported by its remarkable efficiency, with stress\ntests indicating an inference time of just 10 seconds for a one-minute video on\nH20 GPU.",
      "upvotes": 15,
      "discussionId": "68882c7eaf872d625c10c5d2",
      "projectPage": "https://tencentarc.github.io/posts/arc-video-announcement/",
      "githubRepo": "https://github.com/TencentARC/ARC-Hunyuan-Video-7B",
      "ai_summary": "A multimodal model that processes visual, audio, and textual signals for structured comprehension of real-world short videos improves video search, recommendation, and engagement.",
      "ai_keywords": [
        "multimodal model",
        "timestamped video captioning",
        "summarization",
        "video question answering",
        "temporal video grounding",
        "video reasoning",
        "instruction fine-tuning",
        "reinforcement learning",
        "ShortVid-Bench",
        "zero-shot learning",
        "H20 GPU"
      ]
    },
    "publishedAt": "2025-07-28T11:52:36.000Z",
    "title": "ARC-Hunyuan-Video-7B: Structured Video Comprehension of Real-World\n  Shorts",
    "summary": "Real-world user-generated short videos, especially those distributed on\nplatforms such as WeChat Channel and TikTok, dominate the mobile internet.\nHowever, current large multimodal models lack essential temporally-structured,\ndetailed, and in-depth video comprehension capabilities, which are the\ncornerstone of effective video search and recommendation, as well as emerging\nvideo applications. Understanding real-world shorts is actually challenging due\nto their complex visual elements, high information density in both visuals and\naudio, and fast pacing that focuses on emotional expression and viewpoint\ndelivery. This requires advanced reasoning to effectively integrate multimodal\ninformation, including visual, audio, and text. In this work, we introduce\nARC-Hunyuan-Video, a multimodal model that processes visual, audio, and textual\nsignals from raw video inputs end-to-end for structured comprehension. The\nmodel is capable of multi-granularity timestamped video captioning and\nsummarization, open-ended video question answering, temporal video grounding,\nand video reasoning. Leveraging high-quality data from an automated annotation\npipeline, our compact 7B-parameter model is trained through a comprehensive\nregimen: pre-training, instruction fine-tuning, cold start, reinforcement\nlearning (RL) post-training, and final instruction fine-tuning. Quantitative\nevaluations on our introduced benchmark ShortVid-Bench and qualitative\ncomparisons demonstrate its strong performance in real-world video\ncomprehension, and it supports zero-shot or fine-tuning with a few samples for\ndiverse downstream applications. The real-world production deployment of our\nmodel has yielded tangible and measurable improvements in user engagement and\nsatisfaction, a success supported by its remarkable efficiency, with stress\ntests indicating an inference time of just 10 seconds for a one-minute video on\nH20 GPU.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.20939.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6455cc8f654d8bccae50e4d4",
      "avatarUrl": "/avatars/506a9992e5bf52e06d37cc22e4b307c0.svg",
      "fullname": "Yuying Ge",
      "name": "tttoaster",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.20025",
      "authors": [
        {
          "_id": "68883088af872d625c10c5eb",
          "name": "Yin Xie",
          "hidden": false
        },
        {
          "_id": "68883088af872d625c10c5ec",
          "name": "Kaicheng Yang",
          "hidden": false
        },
        {
          "_id": "68883088af872d625c10c5ed",
          "name": "Xiang An",
          "hidden": false
        },
        {
          "_id": "68883088af872d625c10c5ee",
          "name": "Kun Wu",
          "hidden": false
        },
        {
          "_id": "68883088af872d625c10c5ef",
          "name": "Yongle Zhao",
          "hidden": false
        },
        {
          "_id": "68883088af872d625c10c5f0",
          "name": "Weimo Deng",
          "hidden": false
        },
        {
          "_id": "68883088af872d625c10c5f1",
          "name": "Zimin Ran",
          "hidden": false
        },
        {
          "_id": "68883088af872d625c10c5f2",
          "name": "Yumeng Wang",
          "hidden": false
        },
        {
          "_id": "68883088af872d625c10c5f3",
          "name": "Ziyong Feng",
          "hidden": false
        },
        {
          "_id": "68883088af872d625c10c5f4",
          "name": "Roy Miles",
          "hidden": false
        },
        {
          "_id": "68883088af872d625c10c5f5",
          "name": "Ismail Elezi",
          "hidden": false
        },
        {
          "_id": "68883088af872d625c10c5f6",
          "name": "Jiankang Deng",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-26T17:47:09.000Z",
      "submittedOnDailyAt": "2025-07-29T00:56:42.617Z",
      "title": "Region-based Cluster Discrimination for Visual Representation Learning",
      "submittedOnDailyBy": {
        "_id": "6478679d7b370854241b2ad8",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6478679d7b370854241b2ad8/dBczWYYdfEt9tQcnVGhQk.jpeg",
        "isPro": false,
        "fullname": "xiangan",
        "user": "xiangan",
        "type": "user"
      },
      "summary": "Learning visual representations is foundational for a broad spectrum of\ndownstream tasks. Although recent vision-language contrastive models, such as\nCLIP and SigLIP, have achieved impressive zero-shot performance via large-scale\nvision-language alignment, their reliance on global representations constrains\ntheir effectiveness for dense prediction tasks, such as grounding, OCR, and\nsegmentation. To address this gap, we introduce Region-Aware Cluster\nDiscrimination (RICE), a novel method that enhances region-level visual and OCR\ncapabilities. We first construct a billion-scale candidate region dataset and\npropose a Region Transformer layer to extract rich regional semantics. We\nfurther design a unified region cluster discrimination loss that jointly\nsupports object and OCR learning within a single classification framework,\nenabling efficient and scalable distributed training on large-scale data.\nExtensive experiments show that RICE consistently outperforms previous methods\non tasks, including segmentation, dense detection, and visual perception for\nMultimodal Large Language Models (MLLMs). The pre-trained models have been\nreleased at https://github.com/deepglint/MVT.",
      "upvotes": 8,
      "discussionId": "68883088af872d625c10c5f7",
      "githubRepo": "https://github.com/deepglint/MVT",
      "ai_summary": "RICE enhances region-level visual and OCR capabilities through a novel Region Transformer and cluster discrimination loss, achieving superior performance across dense prediction and perception tasks.",
      "ai_keywords": [
        "Region-Aware Cluster Discrimination (RICE)",
        "Region Transformer",
        "cluster discrimination loss",
        "segmentation",
        "dense detection",
        "Multimodal Large Language Models (MLLMs)"
      ]
    },
    "publishedAt": "2025-07-26T13:47:09.000Z",
    "title": "Region-based Cluster Discrimination for Visual Representation Learning",
    "summary": "Learning visual representations is foundational for a broad spectrum of\ndownstream tasks. Although recent vision-language contrastive models, such as\nCLIP and SigLIP, have achieved impressive zero-shot performance via large-scale\nvision-language alignment, their reliance on global representations constrains\ntheir effectiveness for dense prediction tasks, such as grounding, OCR, and\nsegmentation. To address this gap, we introduce Region-Aware Cluster\nDiscrimination (RICE), a novel method that enhances region-level visual and OCR\ncapabilities. We first construct a billion-scale candidate region dataset and\npropose a Region Transformer layer to extract rich regional semantics. We\nfurther design a unified region cluster discrimination loss that jointly\nsupports object and OCR learning within a single classification framework,\nenabling efficient and scalable distributed training on large-scale data.\nExtensive experiments show that RICE consistently outperforms previous methods\non tasks, including segmentation, dense detection, and visual perception for\nMultimodal Large Language Models (MLLMs). The pre-trained models have been\nreleased at https://github.com/deepglint/MVT.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.20025.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "6478679d7b370854241b2ad8",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6478679d7b370854241b2ad8/dBczWYYdfEt9tQcnVGhQk.jpeg",
      "fullname": "xiangan",
      "name": "xiangan",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.19804",
      "authors": [
        {
          "_id": "68883519af872d625c10c5f9",
          "name": "Peng Cai",
          "hidden": false
        },
        {
          "_id": "68883519af872d625c10c5fa",
          "name": "Qiang Li",
          "hidden": false
        },
        {
          "_id": "68883519af872d625c10c5fb",
          "name": "Kaicheng Yang",
          "hidden": false
        },
        {
          "_id": "68883519af872d625c10c5fc",
          "name": "Dong Guo",
          "hidden": false
        },
        {
          "_id": "68883519af872d625c10c5fd",
          "name": "Jia Li",
          "hidden": false
        },
        {
          "_id": "68883519af872d625c10c5fe",
          "name": "Nan Zhou",
          "hidden": false
        },
        {
          "_id": "68883519af872d625c10c5ff",
          "name": "Xiang An",
          "hidden": false
        },
        {
          "_id": "68883519af872d625c10c600",
          "name": "Ninghua Yang",
          "hidden": false
        },
        {
          "_id": "68883519af872d625c10c601",
          "name": "Jiankang Deng",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-26T05:36:48.000Z",
      "submittedOnDailyAt": "2025-07-29T01:14:03.210Z",
      "title": "ForCenNet: Foreground-Centric Network for Document Image Rectification",
      "submittedOnDailyBy": {
        "_id": "63e202f352b7578dba448ab5",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63e202f352b7578dba448ab5/8itVBLcv14m7OVsoF8h1o.jpeg",
        "isPro": false,
        "fullname": "Yang",
        "user": "Kaichengalex",
        "type": "user"
      },
      "summary": "Document image rectification aims to eliminate geometric deformation in\nphotographed documents to facilitate text recognition. However, existing\nmethods often neglect the significance of foreground elements, which provide\nessential geometric references and layout information for document image\ncorrection. In this paper, we introduce Foreground-Centric Network (ForCenNet)\nto eliminate geometric distortions in document images. Specifically, we\ninitially propose a foreground-centric label generation method, which extracts\ndetailed foreground elements from an undistorted image. Then we introduce a\nforeground-centric mask mechanism to enhance the distinction between readable\nand background regions. Furthermore, we design a curvature consistency loss to\nleverage the detailed foreground labels to help the model understand the\ndistorted geometric distribution. Extensive experiments demonstrate that\nForCenNet achieves new state-of-the-art on four real-world benchmarks, such as\nDocUNet, DIR300, WarpDoc, and DocReal. Quantitative analysis shows that the\nproposed method effectively undistorts layout elements, such as text lines and\ntable borders. The resources for further comparison are provided at\nhttps://github.com/caipeng328/ForCenNet.",
      "upvotes": 3,
      "discussionId": "68883519af872d625c10c602",
      "githubRepo": "https://github.com/caipeng328/ForCenNet",
      "ai_summary": "A Foreground-Centric Network for document image rectification improves state-of-the-art by effectively handling foreground elements and layout distortions.",
      "ai_keywords": [
        "Foreground-Centric Network",
        "ForCenNet",
        "curvature consistency loss"
      ]
    },
    "publishedAt": "2025-07-26T01:36:48.000Z",
    "title": "ForCenNet: Foreground-Centric Network for Document Image Rectification",
    "summary": "Document image rectification aims to eliminate geometric deformation in\nphotographed documents to facilitate text recognition. However, existing\nmethods often neglect the significance of foreground elements, which provide\nessential geometric references and layout information for document image\ncorrection. In this paper, we introduce Foreground-Centric Network (ForCenNet)\nto eliminate geometric distortions in document images. Specifically, we\ninitially propose a foreground-centric label generation method, which extracts\ndetailed foreground elements from an undistorted image. Then we introduce a\nforeground-centric mask mechanism to enhance the distinction between readable\nand background regions. Furthermore, we design a curvature consistency loss to\nleverage the detailed foreground labels to help the model understand the\ndistorted geometric distribution. Extensive experiments demonstrate that\nForCenNet achieves new state-of-the-art on four real-world benchmarks, such as\nDocUNet, DIR300, WarpDoc, and DocReal. Quantitative analysis shows that the\nproposed method effectively undistorts layout elements, such as text lines and\ntable borders. The resources for further comparison are provided at\nhttps://github.com/caipeng328/ForCenNet.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.19804.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63e202f352b7578dba448ab5",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63e202f352b7578dba448ab5/8itVBLcv14m7OVsoF8h1o.jpeg",
      "fullname": "Yang",
      "name": "Kaichengalex",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 7
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.20900",
      "authors": [
        {
          "_id": "68883026af872d625c10c5e1",
          "name": "Yonghyun Kim",
          "hidden": false
        },
        {
          "_id": "68883026af872d625c10c5e2",
          "name": "Wayne Chi",
          "hidden": false
        },
        {
          "_id": "68883026af872d625c10c5e3",
          "name": "Anastasios N. Angelopoulos",
          "hidden": false
        },
        {
          "_id": "68883026af872d625c10c5e4",
          "name": "Wei-Lin Chiang",
          "hidden": false
        },
        {
          "_id": "68883026af872d625c10c5e5",
          "name": "Koichi Saito",
          "hidden": false
        },
        {
          "_id": "68883026af872d625c10c5e6",
          "name": "Shinji Watanabe",
          "hidden": false
        },
        {
          "_id": "68883026af872d625c10c5e7",
          "name": "Yuki Mitsufuji",
          "hidden": false
        },
        {
          "_id": "68883026af872d625c10c5e8",
          "user": {
            "_id": "63b1b5f034b91b8272aac6c8",
            "avatarUrl": "/avatars/155351aa15967b5346144c03fc87d1ca.svg",
            "isPro": false,
            "fullname": "Chris Donahue",
            "user": "chrisdonahue",
            "type": "user"
          },
          "name": "Chris Donahue",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-29T02:34:38.516Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-28T14:52:57.000Z",
      "submittedOnDailyAt": "2025-07-29T01:01:25.876Z",
      "title": "Music Arena: Live Evaluation for Text-to-Music",
      "submittedOnDailyBy": {
        "_id": "63b1b5f034b91b8272aac6c8",
        "avatarUrl": "/avatars/155351aa15967b5346144c03fc87d1ca.svg",
        "isPro": false,
        "fullname": "Chris Donahue",
        "user": "chrisdonahue",
        "type": "user"
      },
      "summary": "We present Music Arena, an open platform for scalable human preference\nevaluation of text-to-music (TTM) models. Soliciting human preferences via\nlistening studies is the gold standard for evaluation in TTM, but these studies\nare expensive to conduct and difficult to compare, as study protocols may\ndiffer across systems. Moreover, human preferences might help researchers align\ntheir TTM systems or improve automatic evaluation metrics, but an open and\nrenewable source of preferences does not currently exist. We aim to fill these\ngaps by offering *live* evaluation for TTM. In Music Arena, real-world users\ninput text prompts of their choosing and compare outputs from two TTM systems,\nand their preferences are used to compile a leaderboard. While Music Arena\nfollows recent evaluation trends in other AI domains, we also design it with\nkey features tailored to music: an LLM-based routing system to navigate the\nheterogeneous type signatures of TTM systems, and the collection of *detailed*\npreferences including listening data and natural language feedback. We also\npropose a rolling data release policy with user privacy guarantees, providing a\nrenewable source of preference data and increasing platform transparency.\nThrough its standardized evaluation protocol, transparent data access policies,\nand music-specific features, Music Arena not only addresses key challenges in\nthe TTM ecosystem but also demonstrates how live evaluation can be thoughtfully\nadapted to unique characteristics of specific AI domains.\n  Music Arena is available at: https://music-arena.org",
      "upvotes": 0,
      "discussionId": "68883027af872d625c10c5e9",
      "ai_summary": "Music Arena provides a scalable, interactive platform for evaluating text-to-music models through user-generated preferences and detailed feedback.",
      "ai_keywords": [
        "LLM-based routing system",
        "text-to-music models",
        "live evaluation",
        "heterogeneous type signatures",
        "detailed preferences",
        "listening data",
        "natural language feedback",
        "rolling data release policy",
        "user privacy guarantees",
        "standardized evaluation protocol",
        "transparent data access policies"
      ]
    },
    "publishedAt": "2025-07-28T10:52:57.000Z",
    "title": "Music Arena: Live Evaluation for Text-to-Music",
    "summary": "We present Music Arena, an open platform for scalable human preference\nevaluation of text-to-music (TTM) models. Soliciting human preferences via\nlistening studies is the gold standard for evaluation in TTM, but these studies\nare expensive to conduct and difficult to compare, as study protocols may\ndiffer across systems. Moreover, human preferences might help researchers align\ntheir TTM systems or improve automatic evaluation metrics, but an open and\nrenewable source of preferences does not currently exist. We aim to fill these\ngaps by offering *live* evaluation for TTM. In Music Arena, real-world users\ninput text prompts of their choosing and compare outputs from two TTM systems,\nand their preferences are used to compile a leaderboard. While Music Arena\nfollows recent evaluation trends in other AI domains, we also design it with\nkey features tailored to music: an LLM-based routing system to navigate the\nheterogeneous type signatures of TTM systems, and the collection of *detailed*\npreferences including listening data and natural language feedback. We also\npropose a rolling data release policy with user privacy guarantees, providing a\nrenewable source of preference data and increasing platform transparency.\nThrough its standardized evaluation protocol, transparent data access policies,\nand music-specific features, Music Arena not only addresses key challenges in\nthe TTM ecosystem but also demonstrates how live evaluation can be thoughtfully\nadapted to unique characteristics of specific AI domains.\n  Music Arena is available at: https://music-arena.org",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.20900.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63b1b5f034b91b8272aac6c8",
      "avatarUrl": "/avatars/155351aa15967b5346144c03fc87d1ca.svg",
      "fullname": "Chris Donahue",
      "name": "chrisdonahue",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2507.20880",
      "authors": [
        {
          "_id": "688826c8af872d625c10c5a1",
          "name": "Renhang Liu",
          "hidden": false
        },
        {
          "_id": "688826c8af872d625c10c5a2",
          "name": "Chia-Yu Hung",
          "hidden": false
        },
        {
          "_id": "688826c8af872d625c10c5a3",
          "name": "Navonil Majumder",
          "hidden": false
        },
        {
          "_id": "688826c8af872d625c10c5a4",
          "name": "Taylor Gautreaux",
          "hidden": false
        },
        {
          "_id": "688826c8af872d625c10c5a5",
          "name": "Amir Ali Bagherzadeh",
          "hidden": false
        },
        {
          "_id": "688826c8af872d625c10c5a6",
          "name": "Chuan Li",
          "hidden": false
        },
        {
          "_id": "688826c8af872d625c10c5a7",
          "name": "Dorien Herremans",
          "hidden": false
        },
        {
          "_id": "688826c8af872d625c10c5a8",
          "name": "Soujanya Poria",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-28T14:34:02.000Z",
      "submittedOnDailyAt": "2025-07-29T00:39:10.042Z",
      "title": "JAM: A Tiny Flow-based Song Generator with Fine-grained Controllability\n  and Aesthetic Alignment",
      "submittedOnDailyBy": {
        "_id": "626b626405fe1cb65725aca1",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/626b626405fe1cb65725aca1/aa-Lata46I3fXOmMetvXH.jpeg",
        "isPro": false,
        "fullname": "Soujanya Poria",
        "user": "soujanyaporia",
        "type": "user"
      },
      "summary": "Diffusion and flow-matching models have revolutionized automatic\ntext-to-audio generation in recent times. These models are increasingly capable\nof generating high quality and faithful audio outputs capturing to speech and\nacoustic events. However, there is still much room for improvement in creative\naudio generation that primarily involves music and songs. Recent open\nlyrics-to-song models, such as, DiffRhythm, ACE-Step, and LeVo, have set an\nacceptable standard in automatic song generation for recreational use. However,\nthese models lack fine-grained word-level controllability often desired by\nmusicians in their workflows. To the best of our knowledge, our\nflow-matching-based JAM is the first effort toward endowing word-level timing\nand duration control in song generation, allowing fine-grained vocal control.\nTo enhance the quality of generated songs to better align with human\npreferences, we implement aesthetic alignment through Direct Preference\nOptimization, which iteratively refines the model using a synthetic dataset,\neliminating the need or manual data annotations. Furthermore, we aim to\nstandardize the evaluation of such lyrics-to-song models through our public\nevaluation dataset JAME. We show that JAM outperforms the existing models in\nterms of the music-specific attributes.",
      "upvotes": 0,
      "discussionId": "688826c8af872d625c10c5a9",
      "projectPage": "https://declare-lab.github.io/jamify",
      "githubRepo": "https://github.com/declare-lab/jamify",
      "ai_summary": "A flow-matching-based model enhances lyrics-to-song generation by providing word-level control over vocal timing and duration, improving quality through aesthetic alignment and surpassing current models in music-specific attributes.",
      "ai_keywords": [
        "diffusion models",
        "flow-matching models",
        "text-to-audio generation",
        "lyrics-to-song models",
        "DiffRhythm",
        "ACE-Step",
        "LeVo",
        "word-level timing",
        "duration control",
        "fine-grained vocal control",
        "Direct Preference Optimization",
        "aesthetic alignment",
        "public evaluation dataset",
        "JAME"
      ],
      "githubStars": 0
    },
    "publishedAt": "2025-07-28T10:34:02.000Z",
    "title": "JAM: A Tiny Flow-based Song Generator with Fine-grained Controllability\n  and Aesthetic Alignment",
    "summary": "Diffusion and flow-matching models have revolutionized automatic\ntext-to-audio generation in recent times. These models are increasingly capable\nof generating high quality and faithful audio outputs capturing to speech and\nacoustic events. However, there is still much room for improvement in creative\naudio generation that primarily involves music and songs. Recent open\nlyrics-to-song models, such as, DiffRhythm, ACE-Step, and LeVo, have set an\nacceptable standard in automatic song generation for recreational use. However,\nthese models lack fine-grained word-level controllability often desired by\nmusicians in their workflows. To the best of our knowledge, our\nflow-matching-based JAM is the first effort toward endowing word-level timing\nand duration control in song generation, allowing fine-grained vocal control.\nTo enhance the quality of generated songs to better align with human\npreferences, we implement aesthetic alignment through Direct Preference\nOptimization, which iteratively refines the model using a synthetic dataset,\neliminating the need or manual data annotations. Furthermore, we aim to\nstandardize the evaluation of such lyrics-to-song models through our public\nevaluation dataset JAME. We show that JAM outperforms the existing models in\nterms of the music-specific attributes.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.20880.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "626b626405fe1cb65725aca1",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/626b626405fe1cb65725aca1/aa-Lata46I3fXOmMetvXH.jpeg",
      "fullname": "Soujanya Poria",
      "name": "soujanyaporia",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 8
    },
    "isAuthorParticipating": false
  }
]