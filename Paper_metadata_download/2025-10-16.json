[
  {
    "paper": {
      "id": "2510.13621",
      "authors": [
        {
          "_id": "68f0515b36f8b025381e18f5",
          "name": "Yuexing Hao",
          "hidden": false
        },
        {
          "_id": "68f0515b36f8b025381e18f6",
          "name": "Yue Huang",
          "hidden": false
        },
        {
          "_id": "68f0515b36f8b025381e18f7",
          "name": "Haoran Zhang",
          "hidden": false
        },
        {
          "_id": "68f0515b36f8b025381e18f8",
          "name": "Chenyang Zhao",
          "hidden": false
        },
        {
          "_id": "68f0515b36f8b025381e18f9",
          "name": "Zhenwen Liang",
          "hidden": false
        },
        {
          "_id": "68f0515b36f8b025381e18fa",
          "name": "Paul Pu Liang",
          "hidden": false
        },
        {
          "_id": "68f0515b36f8b025381e18fb",
          "name": "Yue Zhao",
          "hidden": false
        },
        {
          "_id": "68f0515b36f8b025381e18fc",
          "name": "Lichao Sun",
          "hidden": false
        },
        {
          "_id": "68f0515b36f8b025381e18fd",
          "name": "Saleh Kalantari",
          "hidden": false
        },
        {
          "_id": "68f0515b36f8b025381e18fe",
          "name": "Xiangliang Zhang",
          "hidden": false
        },
        {
          "_id": "68f0515b36f8b025381e18ff",
          "name": "Marzyeh Ghassemi",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-15T14:50:45.000Z",
      "submittedOnDailyAt": "2025-10-16T00:29:52.071Z",
      "title": "The Role of Computing Resources in Publishing Foundation Model Research",
      "submittedOnDailyBy": {
        "_id": "639d94ab7145123e0d44e48a",
        "avatarUrl": "/avatars/5bb6a65b306d1383c4a8bcd9334b470a.svg",
        "isPro": false,
        "fullname": "Yue Huang",
        "user": "HowieHwong",
        "type": "user"
      },
      "summary": "Cutting-edge research in Artificial Intelligence (AI) requires considerable\nresources, including Graphics Processing Units (GPUs), data, and human\nresources. In this paper, we evaluate of the relationship between these\nresources and the scientific advancement of foundation models (FM). We reviewed\n6517 FM papers published between 2022 to 2024, and surveyed 229 first-authors\nto the impact of computing resources on scientific output. We find that\nincreased computing is correlated with national funding allocations and\ncitations, but our findings don't observe the strong correlations with research\nenvironment (academic or industrial), domain, or study methodology. We advise\nthat individuals and institutions focus on creating shared and affordable\ncomputing opportunities to lower the entry barrier for under-resourced\nresearchers. These steps can help expand participation in FM research, foster\ndiversity of ideas and contributors, and sustain innovation and progress in AI.\nThe data will be available at: https://mit-calc.csail.mit.edu/",
      "upvotes": 5,
      "discussionId": "68f0515b36f8b025381e1900",
      "ai_summary": "Increased computing resources are correlated with national funding and citations in foundation model research, but not with research environment, domain, or methodology.",
      "ai_keywords": [
        "foundation models",
        "computing resources",
        "national funding",
        "citations",
        "research environment",
        "domain",
        "study methodology",
        "shared computing",
        "affordable computing",
        "under-resourced researchers",
        "diversity of ideas",
        "innovation",
        "progress in AI"
      ]
    },
    "publishedAt": "2025-10-15T10:50:45.000Z",
    "title": "The Role of Computing Resources in Publishing Foundation Model Research",
    "summary": "Cutting-edge research in Artificial Intelligence (AI) requires considerable\nresources, including Graphics Processing Units (GPUs), data, and human\nresources. In this paper, we evaluate of the relationship between these\nresources and the scientific advancement of foundation models (FM). We reviewed\n6517 FM papers published between 2022 to 2024, and surveyed 229 first-authors\nto the impact of computing resources on scientific output. We find that\nincreased computing is correlated with national funding allocations and\ncitations, but our findings don't observe the strong correlations with research\nenvironment (academic or industrial), domain, or study methodology. We advise\nthat individuals and institutions focus on creating shared and affordable\ncomputing opportunities to lower the entry barrier for under-resourced\nresearchers. These steps can help expand participation in FM research, foster\ndiversity of ideas and contributors, and sustain innovation and progress in AI.\nThe data will be available at: https://mit-calc.csail.mit.edu/",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.13621.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "639d94ab7145123e0d44e48a",
      "avatarUrl": "/avatars/5bb6a65b306d1383c4a8bcd9334b470a.svg",
      "fullname": "Yue Huang",
      "name": "HowieHwong",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.13802",
      "authors": [
        {
          "_id": "68f04f2d36f8b025381e18e3",
          "name": "Xinhang Liu",
          "hidden": false
        },
        {
          "_id": "68f04f2d36f8b025381e18e4",
          "name": "Yuxi Xiao",
          "hidden": false
        },
        {
          "_id": "68f04f2d36f8b025381e18e5",
          "name": "Donny Y. Chen",
          "hidden": false
        },
        {
          "_id": "68f04f2d36f8b025381e18e6",
          "name": "Jiashi Feng",
          "hidden": false
        },
        {
          "_id": "68f04f2d36f8b025381e18e7",
          "name": "Yu-Wing Tai",
          "hidden": false
        },
        {
          "_id": "68f04f2d36f8b025381e18e8",
          "name": "Chi-Keung Tang",
          "hidden": false
        },
        {
          "_id": "68f04f2d36f8b025381e18e9",
          "name": "Bingyi Kang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-15T17:59:04.000Z",
      "submittedOnDailyAt": "2025-10-16T00:19:47.936Z",
      "title": "Trace Anything: Representing Any Video in 4D via Trajectory Fields",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Effective spatio-temporal representation is fundamental to modeling,\nunderstanding, and predicting dynamics in videos. The atomic unit of a video,\nthe pixel, traces a continuous 3D trajectory over time, serving as the\nprimitive element of dynamics. Based on this principle, we propose representing\nany video as a Trajectory Field: a dense mapping that assigns a continuous 3D\ntrajectory function of time to each pixel in every frame. With this\nrepresentation, we introduce Trace Anything, a neural network that predicts the\nentire trajectory field in a single feed-forward pass. Specifically, for each\npixel in each frame, our model predicts a set of control points that\nparameterizes a trajectory (i.e., a B-spline), yielding its 3D position at\narbitrary query time instants. We trained the Trace Anything model on\nlarge-scale 4D data, including data from our new platform, and our experiments\ndemonstrate that: (i) Trace Anything achieves state-of-the-art performance on\nour new benchmark for trajectory field estimation and performs competitively on\nestablished point-tracking benchmarks; (ii) it offers significant efficiency\ngains thanks to its one-pass paradigm, without requiring iterative optimization\nor auxiliary estimators; and (iii) it exhibits emergent abilities, including\ngoal-conditioned manipulation, motion forecasting, and spatio-temporal fusion.\nProject page: https://trace-anything.github.io/.",
      "upvotes": 4,
      "discussionId": "68f04f2d36f8b025381e18ea",
      "ai_summary": "Trace Anything, a neural network, predicts video trajectories in a single pass, achieving state-of-the-art performance and demonstrating efficiency and emergent abilities like motion forecasting.",
      "ai_keywords": [
        "Trajectory Field",
        "Trace Anything",
        "B-spline",
        "trajectory field estimation",
        "point-tracking benchmarks",
        "goal-conditioned manipulation",
        "motion forecasting",
        "spatio-temporal fusion"
      ],
      "organization": {
        "_id": "67d1140985ea0644e2f14b99",
        "name": "ByteDance-Seed",
        "fullname": "ByteDance Seed",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"
      }
    },
    "publishedAt": "2025-10-15T13:59:04.000Z",
    "title": "Trace Anything: Representing Any Video in 4D via Trajectory Fields",
    "summary": "Effective spatio-temporal representation is fundamental to modeling,\nunderstanding, and predicting dynamics in videos. The atomic unit of a video,\nthe pixel, traces a continuous 3D trajectory over time, serving as the\nprimitive element of dynamics. Based on this principle, we propose representing\nany video as a Trajectory Field: a dense mapping that assigns a continuous 3D\ntrajectory function of time to each pixel in every frame. With this\nrepresentation, we introduce Trace Anything, a neural network that predicts the\nentire trajectory field in a single feed-forward pass. Specifically, for each\npixel in each frame, our model predicts a set of control points that\nparameterizes a trajectory (i.e., a B-spline), yielding its 3D position at\narbitrary query time instants. We trained the Trace Anything model on\nlarge-scale 4D data, including data from our new platform, and our experiments\ndemonstrate that: (i) Trace Anything achieves state-of-the-art performance on\nour new benchmark for trajectory field estimation and performs competitively on\nestablished point-tracking benchmarks; (ii) it offers significant efficiency\ngains thanks to its one-pass paradigm, without requiring iterative optimization\nor auxiliary estimators; and (iii) it exhibits emergent abilities, including\ngoal-conditioned manipulation, motion forecasting, and spatio-temporal fusion.\nProject page: https://trace-anything.github.io/.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.13802.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 127
    },
    "organization": {
      "_id": "67d1140985ea0644e2f14b99",
      "name": "ByteDance-Seed",
      "fullname": "ByteDance Seed",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.13515",
      "authors": [
        {
          "_id": "68f052c836f8b025381e190a",
          "name": "Tiancheng Gu",
          "hidden": false
        },
        {
          "_id": "68f052c836f8b025381e190b",
          "name": "Kaicheng Yang",
          "hidden": false
        },
        {
          "_id": "68f052c836f8b025381e190c",
          "name": "Kaichen Zhang",
          "hidden": false
        },
        {
          "_id": "68f052c836f8b025381e190d",
          "name": "Xiang An",
          "hidden": false
        },
        {
          "_id": "68f052c836f8b025381e190e",
          "name": "Ziyong Feng",
          "hidden": false
        },
        {
          "_id": "68f052c836f8b025381e190f",
          "name": "Yueyi Zhang",
          "hidden": false
        },
        {
          "_id": "68f052c836f8b025381e1910",
          "name": "Weidong Cai",
          "hidden": false
        },
        {
          "_id": "68f052c836f8b025381e1911",
          "name": "Jiankang Deng",
          "hidden": false
        },
        {
          "_id": "68f052c836f8b025381e1912",
          "name": "Lidong Bing",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-15T13:07:00.000Z",
      "submittedOnDailyAt": "2025-10-16T00:36:48.179Z",
      "title": "UniME-V2: MLLM-as-a-Judge for Universal Multimodal Embedding Learning",
      "submittedOnDailyBy": {
        "_id": "63e202f352b7578dba448ab5",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63e202f352b7578dba448ab5/8itVBLcv14m7OVsoF8h1o.jpeg",
        "isPro": false,
        "fullname": "Yang",
        "user": "Kaichengalex",
        "type": "user"
      },
      "summary": "Universal multimodal embedding models are foundational to various tasks.\nExisting approaches typically employ in-batch negative mining by measuring the\nsimilarity of query-candidate pairs. However, these methods often struggle to\ncapture subtle semantic differences among candidates and lack diversity in\nnegative samples. Moreover, the embeddings exhibit limited discriminative\nability in distinguishing false and hard negatives. In this paper, we leverage\nthe advanced understanding capabilities of MLLMs to enhance representation\nlearning and present a novel Universal Multimodal Embedding (UniME-V2) model.\nOur approach first constructs a potential hard negative set through global\nretrieval. We then introduce the MLLM-as-a-Judge mechanism, which utilizes\nMLLMs to assess the semantic alignment of query-candidate pairs and generate\nsoft semantic matching scores. These scores serve as a foundation for hard\nnegative mining, mitigating the impact of false negatives and enabling the\nidentification of diverse, high-quality hard negatives. Furthermore, the\nsemantic matching scores are used as soft labels to mitigate the rigid\none-to-one mapping constraint. By aligning the similarity matrix with the soft\nsemantic matching score matrix, the model learns semantic distinctions among\ncandidates, significantly enhancing its discriminative capacity. To further\nimprove performance, we propose UniME-V2-Reranker, a reranking model trained on\nour mined hard negatives through a joint pairwise and listwise optimization\napproach. We conduct comprehensive experiments on the MMEB benchmark and\nmultiple retrieval tasks, demonstrating that our method achieves\nstate-of-the-art performance on average across all tasks.",
      "upvotes": 4,
      "discussionId": "68f052c836f8b025381e1913",
      "projectPage": "https://garygutc.github.io/UniME-v2/",
      "githubRepo": "https://github.com/GaryGuTC/UniME-v2",
      "ai_summary": "A novel Universal Multimodal Embedding (UniME-V2) model uses MLLMs to enhance representation learning by identifying diverse, high-quality hard negatives and improving discriminative capacity through soft semantic matching scores.",
      "ai_keywords": [
        "Universal Multimodal Embedding",
        "UniME-V2",
        "MLLMs",
        "in-batch negative mining",
        "global retrieval",
        "hard negative mining",
        "semantic alignment",
        "soft semantic matching scores",
        "soft labels",
        "joint pairwise and listwise optimization",
        "MMEB benchmark",
        "retrieval tasks"
      ],
      "githubStars": 6
    },
    "publishedAt": "2025-10-15T09:07:00.000Z",
    "title": "UniME-V2: MLLM-as-a-Judge for Universal Multimodal Embedding Learning",
    "summary": "Universal multimodal embedding models are foundational to various tasks.\nExisting approaches typically employ in-batch negative mining by measuring the\nsimilarity of query-candidate pairs. However, these methods often struggle to\ncapture subtle semantic differences among candidates and lack diversity in\nnegative samples. Moreover, the embeddings exhibit limited discriminative\nability in distinguishing false and hard negatives. In this paper, we leverage\nthe advanced understanding capabilities of MLLMs to enhance representation\nlearning and present a novel Universal Multimodal Embedding (UniME-V2) model.\nOur approach first constructs a potential hard negative set through global\nretrieval. We then introduce the MLLM-as-a-Judge mechanism, which utilizes\nMLLMs to assess the semantic alignment of query-candidate pairs and generate\nsoft semantic matching scores. These scores serve as a foundation for hard\nnegative mining, mitigating the impact of false negatives and enabling the\nidentification of diverse, high-quality hard negatives. Furthermore, the\nsemantic matching scores are used as soft labels to mitigate the rigid\none-to-one mapping constraint. By aligning the similarity matrix with the soft\nsemantic matching score matrix, the model learns semantic distinctions among\ncandidates, significantly enhancing its discriminative capacity. To further\nimprove performance, we propose UniME-V2-Reranker, a reranking model trained on\nour mined hard negatives through a joint pairwise and listwise optimization\napproach. We conduct comprehensive experiments on the MMEB benchmark and\nmultiple retrieval tasks, demonstrating that our method achieves\nstate-of-the-art performance on average across all tasks.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.13515.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63e202f352b7578dba448ab5",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63e202f352b7578dba448ab5/8itVBLcv14m7OVsoF8h1o.jpeg",
      "fullname": "Yang",
      "name": "Kaichengalex",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 9
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.13809",
      "authors": [
        {
          "_id": "68f04e9936f8b025381e18d2",
          "name": "Sihui Ji",
          "hidden": false
        },
        {
          "_id": "68f04e9936f8b025381e18d3",
          "name": "Xi Chen",
          "hidden": false
        },
        {
          "_id": "68f04e9936f8b025381e18d4",
          "name": "Xin Tao",
          "hidden": false
        },
        {
          "_id": "68f04e9936f8b025381e18d5",
          "name": "Pengfei Wan",
          "hidden": false
        },
        {
          "_id": "68f04e9936f8b025381e18d6",
          "name": "Hengshuang Zhao",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-15T17:59:59.000Z",
      "submittedOnDailyAt": "2025-10-16T00:17:16.195Z",
      "title": "PhysMaster: Mastering Physical Representation for Video Generation via\n  Reinforcement Learning",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Video generation models nowadays are capable of generating visually realistic\nvideos, but often fail to adhere to physical laws, limiting their ability to\ngenerate physically plausible videos and serve as ''world models''. To address\nthis issue, we propose PhysMaster, which captures physical knowledge as a\nrepresentation for guiding video generation models to enhance their\nphysics-awareness. Specifically, PhysMaster is based on the image-to-video task\nwhere the model is expected to predict physically plausible dynamics from the\ninput image. Since the input image provides physical priors like relative\npositions and potential interactions of objects in the scenario, we devise\nPhysEncoder to encode physical information from it as an extra condition to\ninject physical knowledge into the video generation process. The lack of proper\nsupervision on the model's physical performance beyond mere appearance\nmotivates PhysEncoder to apply reinforcement learning with human feedback to\nphysical representation learning, which leverages feedback from generation\nmodels to optimize physical representations with Direct Preference Optimization\n(DPO) in an end-to-end manner. PhysMaster provides a feasible solution for\nimproving physics-awareness of PhysEncoder and thus of video generation,\nproving its ability on a simple proxy task and generalizability to wide-ranging\nphysical scenarios. This implies that our PhysMaster, which unifies solutions\nfor various physical processes via representation learning in the reinforcement\nlearning paradigm, can act as a generic and plug-in solution for physics-aware\nvideo generation and broader applications.",
      "upvotes": 2,
      "discussionId": "68f04e9936f8b025381e18d7",
      "projectPage": "https://sihuiji.github.io/PhysMaster-Page/",
      "ai_summary": "PhysMaster enhances video generation by integrating physical knowledge through PhysEncoder, using reinforcement learning and Direct Preference Optimization to improve physics-awareness.",
      "ai_keywords": [
        "PhysMaster",
        "PhysEncoder",
        "reinforcement learning",
        "Direct Preference Optimization",
        "physics-aware video generation"
      ]
    },
    "publishedAt": "2025-10-15T13:59:59.000Z",
    "title": "PhysMaster: Mastering Physical Representation for Video Generation via\n  Reinforcement Learning",
    "summary": "Video generation models nowadays are capable of generating visually realistic\nvideos, but often fail to adhere to physical laws, limiting their ability to\ngenerate physically plausible videos and serve as ''world models''. To address\nthis issue, we propose PhysMaster, which captures physical knowledge as a\nrepresentation for guiding video generation models to enhance their\nphysics-awareness. Specifically, PhysMaster is based on the image-to-video task\nwhere the model is expected to predict physically plausible dynamics from the\ninput image. Since the input image provides physical priors like relative\npositions and potential interactions of objects in the scenario, we devise\nPhysEncoder to encode physical information from it as an extra condition to\ninject physical knowledge into the video generation process. The lack of proper\nsupervision on the model's physical performance beyond mere appearance\nmotivates PhysEncoder to apply reinforcement learning with human feedback to\nphysical representation learning, which leverages feedback from generation\nmodels to optimize physical representations with Direct Preference Optimization\n(DPO) in an end-to-end manner. PhysMaster provides a feasible solution for\nimproving physics-awareness of PhysEncoder and thus of video generation,\nproving its ability on a simple proxy task and generalizability to wide-ranging\nphysical scenarios. This implies that our PhysMaster, which unifies solutions\nfor various physical processes via representation learning in the reinforcement\nlearning paradigm, can act as a generic and plug-in solution for physics-aware\nvideo generation and broader applications.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.13809.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 127
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.13804",
      "authors": [
        {
          "_id": "68f04ef636f8b025381e18d9",
          "name": "Xinchen Zhang",
          "hidden": false
        },
        {
          "_id": "68f04ef636f8b025381e18da",
          "name": "Xiaoying Zhang",
          "hidden": false
        },
        {
          "_id": "68f04ef636f8b025381e18db",
          "name": "Youbin Wu",
          "hidden": false
        },
        {
          "_id": "68f04ef636f8b025381e18dc",
          "name": "Yanbin Cao",
          "hidden": false
        },
        {
          "_id": "68f04ef636f8b025381e18dd",
          "name": "Renrui Zhang",
          "hidden": false
        },
        {
          "_id": "68f04ef636f8b025381e18de",
          "name": "Ruihang Chu",
          "hidden": false
        },
        {
          "_id": "68f04ef636f8b025381e18df",
          "name": "Ling Yang",
          "hidden": false
        },
        {
          "_id": "68f04ef636f8b025381e18e0",
          "name": "Yujiu Yang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-15T17:59:24.000Z",
      "submittedOnDailyAt": "2025-10-16T00:18:57.663Z",
      "title": "Generative Universal Verifier as Multimodal Meta-Reasoner",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "We introduce Generative Universal Verifier, a novel concept and plugin\ndesigned for next-generation multimodal reasoning in vision-language models and\nunified multimodal models, providing the fundamental capability of reflection\nand refinement on visual outcomes during the reasoning and generation process.\nThis work makes three main contributions: (1) We build ViVerBench, a\ncomprehensive benchmark spanning 16 categories of critical tasks for evaluating\nvisual outcomes in multimodal reasoning. Results show that existing VLMs\nconsistently underperform across these tasks, underscoring a substantial gap\nfrom human-level capability in reliable visual verification. (2) We design two\nautomated pipelines to construct large-scale visual verification data and train\nOmniVerifier-7B, the first omni-capable generative verifier trained for\nuniversal visual verification and achieves notable gains on ViVerBench(+8.3).\nThrough training, we identify three atomic capabilities in visual verification\nand demonstrate how they generalize and interact synergistically. (3) We\npropose OmniVerifier-TTS, a sequential test-time scaling paradigm that\nleverages the universal verifier to bridge image generation and editing within\nunified models, enhancing the upper bound of generative ability through\niterative fine-grained optimization. Beyond generation, we extend universal\nverifier to broader world-modeling interleaved reasoning scenarios.\nEmpirically, OmniVerifier-TTS achieves improvements on T2I-ReasonBench(+3.7),\nand GenEval++(+4.3), outperforming existing parallel test-time scaling methods,\nsuch as Best-of-N. By endowing multimodal reasoning with reliable visual\nverification, OmniVerifier advances both reliable reflection during generation\nand scalable test-time refinement, marking a step toward more trustworthy and\ncontrollable next-generation reasoning systems.",
      "upvotes": 2,
      "discussionId": "68f04ef636f8b025381e18e1",
      "projectPage": "https://omniverifier.github.io/",
      "ai_summary": "Generative Universal Verifier enhances multimodal reasoning by providing reliable visual verification through ViVerBench, OmniVerifier-7B, and OmniVerifier-TTS, improving generation and refinement capabilities.",
      "ai_keywords": [
        "Generative Universal Verifier",
        "ViVerBench",
        "OmniVerifier-7B",
        "OmniVerifier-TTS",
        "multimodal reasoning",
        "vision-language models",
        "visual verification",
        "atomic capabilities",
        "sequential test-time scaling",
        "T2I-ReasonBench",
        "GenEval++"
      ],
      "organization": {
        "_id": "67d1140985ea0644e2f14b99",
        "name": "ByteDance-Seed",
        "fullname": "ByteDance Seed",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"
      }
    },
    "publishedAt": "2025-10-15T13:59:24.000Z",
    "title": "Generative Universal Verifier as Multimodal Meta-Reasoner",
    "summary": "We introduce Generative Universal Verifier, a novel concept and plugin\ndesigned for next-generation multimodal reasoning in vision-language models and\nunified multimodal models, providing the fundamental capability of reflection\nand refinement on visual outcomes during the reasoning and generation process.\nThis work makes three main contributions: (1) We build ViVerBench, a\ncomprehensive benchmark spanning 16 categories of critical tasks for evaluating\nvisual outcomes in multimodal reasoning. Results show that existing VLMs\nconsistently underperform across these tasks, underscoring a substantial gap\nfrom human-level capability in reliable visual verification. (2) We design two\nautomated pipelines to construct large-scale visual verification data and train\nOmniVerifier-7B, the first omni-capable generative verifier trained for\nuniversal visual verification and achieves notable gains on ViVerBench(+8.3).\nThrough training, we identify three atomic capabilities in visual verification\nand demonstrate how they generalize and interact synergistically. (3) We\npropose OmniVerifier-TTS, a sequential test-time scaling paradigm that\nleverages the universal verifier to bridge image generation and editing within\nunified models, enhancing the upper bound of generative ability through\niterative fine-grained optimization. Beyond generation, we extend universal\nverifier to broader world-modeling interleaved reasoning scenarios.\nEmpirically, OmniVerifier-TTS achieves improvements on T2I-ReasonBench(+3.7),\nand GenEval++(+4.3), outperforming existing parallel test-time scaling methods,\nsuch as Best-of-N. By endowing multimodal reasoning with reliable visual\nverification, OmniVerifier advances both reliable reflection during generation\nand scalable test-time refinement, marking a step toward more trustworthy and\ncontrollable next-generation reasoning systems.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.13804.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 127
    },
    "organization": {
      "_id": "67d1140985ea0644e2f14b99",
      "name": "ByteDance-Seed",
      "fullname": "ByteDance Seed",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.13678",
      "authors": [
        {
          "_id": "68f051ed36f8b025381e1902",
          "name": "Xinyang Li",
          "hidden": false
        },
        {
          "_id": "68f051ed36f8b025381e1903",
          "name": "Tengfei Wang",
          "hidden": false
        },
        {
          "_id": "68f051ed36f8b025381e1904",
          "name": "Zixiao Gu",
          "hidden": false
        },
        {
          "_id": "68f051ed36f8b025381e1905",
          "name": "Shengchuan Zhang",
          "hidden": false
        },
        {
          "_id": "68f051ed36f8b025381e1906",
          "name": "Chunchao Guo",
          "hidden": false
        },
        {
          "_id": "68f051ed36f8b025381e1907",
          "name": "Liujuan Cao",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/629631565de6e0eb3292afed/WhnRuWrdLfTMHJ2hXMj_P.mp4"
      ],
      "publishedAt": "2025-10-15T15:35:48.000Z",
      "submittedOnDailyAt": "2025-10-16T00:36:24.608Z",
      "title": "FlashWorld: High-quality 3D Scene Generation within Seconds",
      "submittedOnDailyBy": {
        "_id": "629631565de6e0eb3292afed",
        "avatarUrl": "/avatars/0ae1080eebce0f747f650bfc292c46ca.svg",
        "isPro": false,
        "fullname": "Xinyang Li",
        "user": "imlixinyang",
        "type": "user"
      },
      "summary": "We propose FlashWorld, a generative model that produces 3D scenes from a\nsingle image or text prompt in seconds, 10~100times faster than previous\nworks while possessing superior rendering quality. Our approach shifts from the\nconventional multi-view-oriented (MV-oriented) paradigm, which generates\nmulti-view images for subsequent 3D reconstruction, to a 3D-oriented approach\nwhere the model directly produces 3D Gaussian representations during multi-view\ngeneration. While ensuring 3D consistency, 3D-oriented method typically suffers\npoor visual quality. FlashWorld includes a dual-mode pre-training phase\nfollowed by a cross-mode post-training phase, effectively integrating the\nstrengths of both paradigms. Specifically, leveraging the prior from a video\ndiffusion model, we first pre-train a dual-mode multi-view diffusion model,\nwhich jointly supports MV-oriented and 3D-oriented generation modes. To bridge\nthe quality gap in 3D-oriented generation, we further propose a cross-mode\npost-training distillation by matching distribution from consistent 3D-oriented\nmode to high-quality MV-oriented mode. This not only enhances visual quality\nwhile maintaining 3D consistency, but also reduces the required denoising steps\nfor inference. Also, we propose a strategy to leverage massive single-view\nimages and text prompts during this process to enhance the model's\ngeneralization to out-of-distribution inputs. Extensive experiments demonstrate\nthe superiority and efficiency of our method.",
      "upvotes": 2,
      "discussionId": "68f051ed36f8b025381e1908",
      "projectPage": "https://imlixinyang.github.io/FlashWorld-Project-Page/",
      "githubRepo": "https://github.com/imlixinyang/FlashWorld",
      "ai_summary": "FlashWorld generates 3D scenes from single images or text prompts quickly and with high quality by combining MV-oriented and 3D-oriented generation methods.",
      "ai_keywords": [
        "generative model",
        "3D scenes",
        "single image",
        "text prompt",
        "3D Gaussian representations",
        "MV-oriented paradigm",
        "3D-oriented approach",
        "dual-mode pre-training",
        "cross-mode post-training",
        "video diffusion model",
        "multi-view diffusion model",
        "cross-mode post-training distillation",
        "denoising steps",
        "single-view images",
        "out-of-distribution inputs"
      ],
      "githubStars": 5
    },
    "publishedAt": "2025-10-15T11:35:48.000Z",
    "title": "FlashWorld: High-quality 3D Scene Generation within Seconds",
    "summary": "We propose FlashWorld, a generative model that produces 3D scenes from a\nsingle image or text prompt in seconds, 10~100times faster than previous\nworks while possessing superior rendering quality. Our approach shifts from the\nconventional multi-view-oriented (MV-oriented) paradigm, which generates\nmulti-view images for subsequent 3D reconstruction, to a 3D-oriented approach\nwhere the model directly produces 3D Gaussian representations during multi-view\ngeneration. While ensuring 3D consistency, 3D-oriented method typically suffers\npoor visual quality. FlashWorld includes a dual-mode pre-training phase\nfollowed by a cross-mode post-training phase, effectively integrating the\nstrengths of both paradigms. Specifically, leveraging the prior from a video\ndiffusion model, we first pre-train a dual-mode multi-view diffusion model,\nwhich jointly supports MV-oriented and 3D-oriented generation modes. To bridge\nthe quality gap in 3D-oriented generation, we further propose a cross-mode\npost-training distillation by matching distribution from consistent 3D-oriented\nmode to high-quality MV-oriented mode. This not only enhances visual quality\nwhile maintaining 3D consistency, but also reduces the required denoising steps\nfor inference. Also, we propose a strategy to leverage massive single-view\nimages and text prompts during this process to enhance the model's\ngeneralization to out-of-distribution inputs. Extensive experiments demonstrate\nthe superiority and efficiency of our method.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/629631565de6e0eb3292afed/WhnRuWrdLfTMHJ2hXMj_P.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.13678.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "629631565de6e0eb3292afed",
      "avatarUrl": "/avatars/0ae1080eebce0f747f650bfc292c46ca.svg",
      "fullname": "Xinyang Li",
      "name": "imlixinyang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.10977",
      "authors": [
        {
          "_id": "68ef0dce486b78128f0e342f",
          "name": "Taiqiang Wu",
          "hidden": false
        },
        {
          "_id": "68ef0dce486b78128f0e3430",
          "name": "Runming Yang",
          "hidden": false
        },
        {
          "_id": "68ef0dce486b78128f0e3431",
          "name": "Tao Liu",
          "hidden": false
        },
        {
          "_id": "68ef0dce486b78128f0e3432",
          "name": "Jiahao Wang",
          "hidden": false
        },
        {
          "_id": "68ef0dce486b78128f0e3433",
          "name": "Ngai Wong",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6621cea88850e38ffbb1854f/PzaCBVbRjJxzByjU92Cma.png"
      ],
      "publishedAt": "2025-10-13T03:30:01.000Z",
      "submittedOnDailyAt": "2025-10-16T00:46:01.871Z",
      "title": "Revisiting Model Interpolation for Efficient Reasoning",
      "submittedOnDailyBy": {
        "_id": "6621cea88850e38ffbb1854f",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6621cea88850e38ffbb1854f/LeytEEjSwnnqB-zFN1Tgt.jpeg",
        "isPro": false,
        "fullname": "Taki WU",
        "user": "taki555",
        "type": "user"
      },
      "summary": "Model merging, typically on Instruct and Thinking models, has shown\nremarkable performance for efficient reasoning. In this paper, we\nsystematically revisit the simplest merging method that interpolates two\nweights directly. Particularly, we observe that model interpolation follows a\nthree-stage evolutionary paradigm with distinct behaviors on the reasoning\ntrajectory. These dynamics provide a principled guide for navigating the\nperformance-cost trade-off. Empirical results demonstrate that a strategically\ninterpolated model surprisingly surpasses sophisticated model merging baselines\non both efficiency and effectiveness. We further validate our findings with\nextensive ablation studies on model layers, modules, and decoding strategies.\nUltimately, this work demystifies model interpolation and offers a practical\nframework for crafting models with precisely targeted reasoning capabilities.\nCode is available at https://github.com/wutaiqiang/MI{Github}.",
      "upvotes": 2,
      "discussionId": "68ef0dcf486b78128f0e3434",
      "githubRepo": "https://github.com/wutaiqiang/MI",
      "organization": {
        "_id": "67ea9ecfc234715db8dbf339",
        "name": "hkuhk",
        "fullname": "The University of Hong Kong",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67ea9e8d2d95c10a0da11b0c/FNnR4M7YqKRuG43N5771B.png"
      }
    },
    "publishedAt": "2025-10-12T23:30:01.000Z",
    "title": "Revisiting Model Interpolation for Efficient Reasoning",
    "summary": "Model merging, typically on Instruct and Thinking models, has shown\nremarkable performance for efficient reasoning. In this paper, we\nsystematically revisit the simplest merging method that interpolates two\nweights directly. Particularly, we observe that model interpolation follows a\nthree-stage evolutionary paradigm with distinct behaviors on the reasoning\ntrajectory. These dynamics provide a principled guide for navigating the\nperformance-cost trade-off. Empirical results demonstrate that a strategically\ninterpolated model surprisingly surpasses sophisticated model merging baselines\non both efficiency and effectiveness. We further validate our findings with\nextensive ablation studies on model layers, modules, and decoding strategies.\nUltimately, this work demystifies model interpolation and offers a practical\nframework for crafting models with precisely targeted reasoning capabilities.\nCode is available at https://github.com/wutaiqiang/MI{Github}.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6621cea88850e38ffbb1854f/PzaCBVbRjJxzByjU92Cma.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.10977.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6621cea88850e38ffbb1854f",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6621cea88850e38ffbb1854f/LeytEEjSwnnqB-zFN1Tgt.jpeg",
      "fullname": "Taki WU",
      "name": "taki555",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "organization": {
      "_id": "67ea9ecfc234715db8dbf339",
      "name": "hkuhk",
      "fullname": "The University of Hong Kong",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67ea9e8d2d95c10a0da11b0c/FNnR4M7YqKRuG43N5771B.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.13778",
      "authors": [
        {
          "_id": "68f0545236f8b025381e191a",
          "name": "Xinyi Chen",
          "hidden": false
        },
        {
          "_id": "68f0545236f8b025381e191b",
          "name": "Yilun Chen",
          "hidden": false
        },
        {
          "_id": "68f0545236f8b025381e191c",
          "name": "Yanwei Fu",
          "hidden": false
        },
        {
          "_id": "68f0545236f8b025381e191d",
          "name": "Ning Gao",
          "hidden": false
        },
        {
          "_id": "68f0545236f8b025381e191e",
          "name": "Jiaya Jia",
          "hidden": false
        },
        {
          "_id": "68f0545236f8b025381e191f",
          "name": "Weiyang Jin",
          "hidden": false
        },
        {
          "_id": "68f0545236f8b025381e1920",
          "name": "Hao Li",
          "hidden": false
        },
        {
          "_id": "68f0545236f8b025381e1921",
          "name": "Yao Mu",
          "hidden": false
        },
        {
          "_id": "68f0545236f8b025381e1922",
          "name": "Jiangmiao Pang",
          "hidden": false
        },
        {
          "_id": "68f0545236f8b025381e1923",
          "name": "Yu Qiao",
          "hidden": false
        },
        {
          "_id": "68f0545236f8b025381e1924",
          "name": "Yang Tian",
          "hidden": false
        },
        {
          "_id": "68f0545236f8b025381e1925",
          "name": "Bin Wang",
          "hidden": false
        },
        {
          "_id": "68f0545236f8b025381e1926",
          "name": "Bolun Wang",
          "hidden": false
        },
        {
          "_id": "68f0545236f8b025381e1927",
          "name": "Fangjing Wang",
          "hidden": false
        },
        {
          "_id": "68f0545236f8b025381e1928",
          "name": "Hanqing Wang",
          "hidden": false
        },
        {
          "_id": "68f0545236f8b025381e1929",
          "name": "Tai Wang",
          "hidden": false
        },
        {
          "_id": "68f0545236f8b025381e192a",
          "name": "Ziqin Wang",
          "hidden": false
        },
        {
          "_id": "68f0545236f8b025381e192b",
          "name": "Xueyuan Wei",
          "hidden": false
        },
        {
          "_id": "68f0545236f8b025381e192c",
          "name": "Chao Wu",
          "hidden": false
        },
        {
          "_id": "68f0545236f8b025381e192d",
          "name": "Shuai Yang",
          "hidden": false
        },
        {
          "_id": "68f0545236f8b025381e192e",
          "name": "Jinhui Ye",
          "hidden": false
        },
        {
          "_id": "68f0545236f8b025381e192f",
          "name": "Junqiu Yu",
          "hidden": false
        },
        {
          "_id": "68f0545236f8b025381e1930",
          "name": "Jia Zeng",
          "hidden": false
        },
        {
          "_id": "68f0545236f8b025381e1931",
          "name": "Jingjing Zhang",
          "hidden": false
        },
        {
          "_id": "68f0545236f8b025381e1932",
          "name": "Jinyu Zhang",
          "hidden": false
        },
        {
          "_id": "68f0545236f8b025381e1933",
          "name": "Shi Zhang",
          "hidden": false
        },
        {
          "_id": "68f0545236f8b025381e1934",
          "name": "Feng Zheng",
          "hidden": false
        },
        {
          "_id": "68f0545236f8b025381e1935",
          "name": "Bowen Zhou",
          "hidden": false
        },
        {
          "_id": "68f0545236f8b025381e1936",
          "name": "Yangkun Zhu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-15T17:30:05.000Z",
      "submittedOnDailyAt": "2025-10-16T00:42:25.171Z",
      "title": "InternVLA-M1: A Spatially Guided Vision-Language-Action Framework for\n  Generalist Robot Policy",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "We introduce InternVLA-M1, a unified framework for spatial grounding and\nrobot control that advances instruction-following robots toward scalable,\ngeneral-purpose intelligence. Its core idea is spatially guided\nvision-language-action training, where spatial grounding serves as the critical\nlink between instructions and robot actions. InternVLA-M1 employs a two-stage\npipeline: (i) spatial grounding pre-training on over 2.3M spatial reasoning\ndata to determine ``where to act'' by aligning instructions with visual,\nembodiment-agnostic positions, and (ii) spatially guided action post-training\nto decide ``how to act'' by generating embodiment-aware actions through\nplug-and-play spatial prompting. This spatially guided training recipe yields\nconsistent gains: InternVLA-M1 outperforms its variant without spatial guidance\nby +14.6% on SimplerEnv Google Robot, +17% on WidowX, and +4.3% on LIBERO\nFranka, while demonstrating stronger spatial reasoning capability in box,\npoint, and trace prediction. To further scale instruction following, we built a\nsimulation engine to collect 244K generalizable pick-and-place episodes,\nenabling a 6.2% average improvement across 200 tasks and 3K+ objects. In\nreal-world clustered pick-and-place, InternVLA-M1 improved by 7.3%, and with\nsynthetic co-training, achieved +20.6% on unseen objects and novel\nconfigurations. Moreover, in long-horizon reasoning-intensive scenarios, it\nsurpassed existing works by over 10%. These results highlight spatially guided\ntraining as a unifying principle for scalable and resilient generalist robots.\nCode and models are available at\nhttps://github.com/InternRobotics/InternVLA-M1.",
      "upvotes": 1,
      "discussionId": "68f0545236f8b025381e1937",
      "projectPage": "https://internrobotics.github.io/internvla-m1.github.io/",
      "githubRepo": "https://github.com/InternRobotics/InternVLA-M1",
      "ai_summary": "A unified framework for spatial grounding and robot control, InternVLA-M1, enhances instruction-following robots through spatially guided vision-language-action training, achieving significant improvements across various tasks and simulations.",
      "ai_keywords": [
        "spatial grounding",
        "vision-language-action training",
        "spatial reasoning",
        "spatial prompting",
        "SimplerEnv",
        "WidowX",
        "LIBERO Franka",
        "pick-and-place",
        "long-horizon reasoning"
      ],
      "githubStars": 131
    },
    "publishedAt": "2025-10-15T13:30:05.000Z",
    "title": "InternVLA-M1: A Spatially Guided Vision-Language-Action Framework for\n  Generalist Robot Policy",
    "summary": "We introduce InternVLA-M1, a unified framework for spatial grounding and\nrobot control that advances instruction-following robots toward scalable,\ngeneral-purpose intelligence. Its core idea is spatially guided\nvision-language-action training, where spatial grounding serves as the critical\nlink between instructions and robot actions. InternVLA-M1 employs a two-stage\npipeline: (i) spatial grounding pre-training on over 2.3M spatial reasoning\ndata to determine ``where to act'' by aligning instructions with visual,\nembodiment-agnostic positions, and (ii) spatially guided action post-training\nto decide ``how to act'' by generating embodiment-aware actions through\nplug-and-play spatial prompting. This spatially guided training recipe yields\nconsistent gains: InternVLA-M1 outperforms its variant without spatial guidance\nby +14.6% on SimplerEnv Google Robot, +17% on WidowX, and +4.3% on LIBERO\nFranka, while demonstrating stronger spatial reasoning capability in box,\npoint, and trace prediction. To further scale instruction following, we built a\nsimulation engine to collect 244K generalizable pick-and-place episodes,\nenabling a 6.2% average improvement across 200 tasks and 3K+ objects. In\nreal-world clustered pick-and-place, InternVLA-M1 improved by 7.3%, and with\nsynthetic co-training, achieved +20.6% on unseen objects and novel\nconfigurations. Moreover, in long-horizon reasoning-intensive scenarios, it\nsurpassed existing works by over 10%. These results highlight spatially guided\ntraining as a unifying principle for scalable and resilient generalist robots.\nCode and models are available at\nhttps://github.com/InternRobotics/InternVLA-M1.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.13778.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 127
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.13744",
      "authors": [
        {
          "_id": "68f0495d36f8b025381e18b7",
          "name": "Shrey Pandit",
          "hidden": false
        },
        {
          "_id": "68f0495d36f8b025381e18b8",
          "name": "Austin Xu",
          "hidden": false
        },
        {
          "_id": "68f0495d36f8b025381e18b9",
          "name": "Xuan-Phi Nguyen",
          "hidden": false
        },
        {
          "_id": "68f0495d36f8b025381e18ba",
          "name": "Yifei Ming",
          "hidden": false
        },
        {
          "_id": "68f0495d36f8b025381e18bb",
          "name": "Caiming Xiong",
          "hidden": false
        },
        {
          "_id": "68f0495d36f8b025381e18bc",
          "name": "Shafiq Joty",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-15T16:50:54.000Z",
      "submittedOnDailyAt": "2025-10-16T00:20:50.470Z",
      "title": "Hard2Verify: A Step-Level Verification Benchmark for Open-Ended Frontier\n  Math",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Large language model (LLM)-based reasoning systems have recently achieved\ngold medal-level performance in the IMO 2025 competition, writing mathematical\nproofs where, to receive full credit, each step must be not only correct but\nalso sufficiently supported. To train LLM-based reasoners in such challenging,\nopen-ended settings, strong verifiers capable of catching step-level mistakes\nare necessary prerequisites. We introduce Hard2Verify, a human-annotated,\nstep-level verification benchmark produced with over 500 hours of human labor.\nHard2Verify is designed to rigorously assess step-level verifiers at the\nfrontier: Verifiers must provide step-level annotations or identify the first\nerror in responses generated by frontier LLMs for very recent, challenging, and\nopen-ended math questions. We evaluate 29 generative critics and process reward\nmodels, demonstrating that, beyond a few standouts, open-source verifiers lag\nclosed source models. We subsequently analyze what drives poor performance in\nstep-level verification, the impacts of scaling verifier compute, as well as\nfundamental questions such as self-verification and verification-generation\ndynamics.",
      "upvotes": 1,
      "discussionId": "68f0495d36f8b025381e18bd",
      "githubRepo": "https://github.com/SalesforceAIResearch/Hard2Verify",
      "ai_summary": "Hard2Verify, a human-annotated benchmark, evaluates step-level verifiers for LLM-based mathematical reasoning systems, highlighting the challenges and performance gaps between open-source and closed-source models.",
      "ai_keywords": [
        "Large language model",
        "LLM-based reasoning systems",
        "IMO 2025",
        "mathematical proofs",
        "step-level verification",
        "Hard2Verify",
        "generative critics",
        "process reward models",
        "self-verification",
        "verification-generation dynamics"
      ],
      "githubStars": 1,
      "organization": {
        "_id": "5f6d64475e78cc6b0ed31e4c",
        "name": "Salesforce",
        "fullname": "Salesforce",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1602756670970-noauth.jpeg"
      }
    },
    "publishedAt": "2025-10-15T12:50:54.000Z",
    "title": "Hard2Verify: A Step-Level Verification Benchmark for Open-Ended Frontier\n  Math",
    "summary": "Large language model (LLM)-based reasoning systems have recently achieved\ngold medal-level performance in the IMO 2025 competition, writing mathematical\nproofs where, to receive full credit, each step must be not only correct but\nalso sufficiently supported. To train LLM-based reasoners in such challenging,\nopen-ended settings, strong verifiers capable of catching step-level mistakes\nare necessary prerequisites. We introduce Hard2Verify, a human-annotated,\nstep-level verification benchmark produced with over 500 hours of human labor.\nHard2Verify is designed to rigorously assess step-level verifiers at the\nfrontier: Verifiers must provide step-level annotations or identify the first\nerror in responses generated by frontier LLMs for very recent, challenging, and\nopen-ended math questions. We evaluate 29 generative critics and process reward\nmodels, demonstrating that, beyond a few standouts, open-source verifiers lag\nclosed source models. We subsequently analyze what drives poor performance in\nstep-level verification, the impacts of scaling verifier compute, as well as\nfundamental questions such as self-verification and verification-generation\ndynamics.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.13744.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 127
    },
    "organization": {
      "_id": "5f6d64475e78cc6b0ed31e4c",
      "name": "Salesforce",
      "fullname": "Salesforce",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1602756670970-noauth.jpeg"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.13586",
      "authors": [
        {
          "_id": "68f04f9036f8b025381e18ec",
          "name": "Pasin Buakhaw",
          "hidden": false
        },
        {
          "_id": "68f04f9036f8b025381e18ed",
          "name": "Kun Kerdthaisong",
          "hidden": false
        },
        {
          "_id": "68f04f9036f8b025381e18ee",
          "name": "Phuree Phenhiran",
          "hidden": false
        },
        {
          "_id": "68f04f9036f8b025381e18ef",
          "name": "Pitikorn Khlaisamniang",
          "hidden": false
        },
        {
          "_id": "68f04f9036f8b025381e18f0",
          "name": "Supasate Vorathammathorn",
          "hidden": false
        },
        {
          "_id": "68f04f9036f8b025381e18f1",
          "name": "Piyalitt Ittichaiwong",
          "hidden": false
        },
        {
          "_id": "68f04f9036f8b025381e18f2",
          "name": "Nutchanon Yongsatianchot",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-15T14:17:23.000Z",
      "submittedOnDailyAt": "2025-10-16T00:21:39.147Z",
      "title": "Deflanderization for Game Dialogue: Balancing Character Authenticity\n  with Task Execution in LLM-based NPCs",
      "submittedOnDailyBy": {
        "_id": "63a83c5432ed73936eb8363e",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a83c5432ed73936eb8363e/8iJN61whs49R0bccdL7b5.jpeg",
        "isPro": false,
        "fullname": "kun kerdthaisong",
        "user": "augustus2011",
        "type": "user"
      },
      "summary": "The emergence of large language models (LLMs) has opened new opportunities\nfor cre- ating dynamic non-player characters (NPCs) in gaming environments,\nenabling both func- tional task execution and persona-consistent dialogue\ngeneration. In this paper, we (Tu_Character_lab) report our participation in\nthe Commonsense Persona-Grounded Dialogue Challenge (CPDC) 2025 Round 2, which\neval- uates agents across three tracks: task-oriented dialogue, context-aware\ndialogue, and their integration. Our approach combines two complementary\nstrategies: (i) lightweight prompting techniques in the API track, including a\nDeflanderization prompting method to suppress excessive role-play and improve\ntask fidelity, and (ii) fine-tuned large models in the GPU track, leveraging\nQwen3-14B with supervisedfinetuning (SFT) and Low-Rank Adaptation(LoRA). Our\nbest submissions ranked 2nd on Task 1, 2nd on Task 3 (API track), and 4th on\nTask 3 (GPU track).",
      "upvotes": 1,
      "discussionId": "68f04f9036f8b025381e18f3",
      "ai_summary": "Participants in the CPDC 2025 used lightweight prompting and fine-tuned large models to achieve high rankings in task-oriented and context-aware dialogue challenges.",
      "ai_keywords": [
        "large language models",
        "LLMs",
        "non-player characters",
        "NPCs",
        "Commonsense Persona-Grounded Dialogue Challenge",
        "CPDC",
        "task-oriented dialogue",
        "context-aware dialogue",
        "lightweight prompting",
        "Deflanderization",
        "fine-tuned models",
        "Qwen3-14B",
        "supervised fine-tuning",
        "SFT",
        "Low-Rank Adaptation",
        "LoRA"
      ],
      "organization": {
        "_id": "67aaba5d8d478dcb4b2f4281",
        "name": "Character-lab",
        "fullname": "Character-lab"
      }
    },
    "publishedAt": "2025-10-15T10:17:23.000Z",
    "title": "Deflanderization for Game Dialogue: Balancing Character Authenticity\n  with Task Execution in LLM-based NPCs",
    "summary": "The emergence of large language models (LLMs) has opened new opportunities\nfor cre- ating dynamic non-player characters (NPCs) in gaming environments,\nenabling both func- tional task execution and persona-consistent dialogue\ngeneration. In this paper, we (Tu_Character_lab) report our participation in\nthe Commonsense Persona-Grounded Dialogue Challenge (CPDC) 2025 Round 2, which\neval- uates agents across three tracks: task-oriented dialogue, context-aware\ndialogue, and their integration. Our approach combines two complementary\nstrategies: (i) lightweight prompting techniques in the API track, including a\nDeflanderization prompting method to suppress excessive role-play and improve\ntask fidelity, and (ii) fine-tuned large models in the GPU track, leveraging\nQwen3-14B with supervisedfinetuning (SFT) and Low-Rank Adaptation(LoRA). Our\nbest submissions ranked 2nd on Task 1, 2nd on Task 3 (API track), and 4th on\nTask 3 (GPU track).",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.13586.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63a83c5432ed73936eb8363e",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a83c5432ed73936eb8363e/8iJN61whs49R0bccdL7b5.jpeg",
      "fullname": "kun kerdthaisong",
      "name": "augustus2011",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "organization": {
      "_id": "67aaba5d8d478dcb4b2f4281",
      "name": "Character-lab",
      "fullname": "Character-lab"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.11958",
      "authors": [
        {
          "_id": "68f0539136f8b025381e1915",
          "name": "Xuan Luo",
          "hidden": false
        },
        {
          "_id": "68f0539136f8b025381e1916",
          "name": "Weizhi Wang",
          "hidden": false
        },
        {
          "_id": "68f0539136f8b025381e1917",
          "name": "Xifeng Yan",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-13T21:42:37.000Z",
      "submittedOnDailyAt": "2025-10-16T00:39:07.024Z",
      "title": "Direct Multi-Token Decoding",
      "submittedOnDailyBy": {
        "_id": "63d34004b734eaa4d4faeccf",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63d34004b734eaa4d4faeccf/aH7F-xn4PMQjHHcvL-0vs.jpeg",
        "isPro": false,
        "fullname": "Weizhi Wang",
        "user": "weizhiwang",
        "type": "user"
      },
      "summary": "Decoder-only transformers have become the standard architecture for large\nlanguage models (LLMs) due to their strong performance. Recent studies suggest\nthat, in pre-trained LLMs, early, middle, and late layers may serve distinct\nroles: Early layers focus on understanding the input context, middle layers\nhandle task-specific processing, and late layers convert abstract\nrepresentations into output tokens. We hypothesize that once representations\nhave been processed by the early and middle layers, the resulting hidden states\nmay encapsulate sufficient information to support the generation of multiple\ntokens using only the late layers, eliminating the need to repeatedly traverse\nthe early and middle layers. We refer to this inference paradigm as Direct\nMulti-Token Decoding (DMTD). Unlike speculative decoding, our method introduces\nno additional parameters, auxiliary routines, or post-generation verification.\nDespite being trained on a limited dataset, a fine-tuned DMTD Qwen3-4B model\nhas already demonstrated promising results, achieving up to a 2x speedup with\nonly minor performance loss. Moreover, as shown in our scaling analysis, its\nperformance is expected to further improve with larger training datasets.",
      "upvotes": 1,
      "discussionId": "68f0539136f8b025381e1918",
      "ai_summary": "Direct Multi-Token Decoding (DMTD) accelerates large language model inference by using only late layers for token generation, achieving significant speedup with minimal performance loss.",
      "ai_keywords": [
        "decoder-only transformers",
        "large language models (LLMs)",
        "early layers",
        "middle layers",
        "late layers",
        "hidden states",
        "Direct Multi-Token Decoding (DMTD)",
        "speculative decoding",
        "fine-tuned",
        "Qwen3-4B",
        "scaling analysis"
      ]
    },
    "publishedAt": "2025-10-13T17:42:37.000Z",
    "title": "Direct Multi-Token Decoding",
    "summary": "Decoder-only transformers have become the standard architecture for large\nlanguage models (LLMs) due to their strong performance. Recent studies suggest\nthat, in pre-trained LLMs, early, middle, and late layers may serve distinct\nroles: Early layers focus on understanding the input context, middle layers\nhandle task-specific processing, and late layers convert abstract\nrepresentations into output tokens. We hypothesize that once representations\nhave been processed by the early and middle layers, the resulting hidden states\nmay encapsulate sufficient information to support the generation of multiple\ntokens using only the late layers, eliminating the need to repeatedly traverse\nthe early and middle layers. We refer to this inference paradigm as Direct\nMulti-Token Decoding (DMTD). Unlike speculative decoding, our method introduces\nno additional parameters, auxiliary routines, or post-generation verification.\nDespite being trained on a limited dataset, a fine-tuned DMTD Qwen3-4B model\nhas already demonstrated promising results, achieving up to a 2x speedup with\nonly minor performance loss. Moreover, as shown in our scaling analysis, its\nperformance is expected to further improve with larger training datasets.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.11958.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63d34004b734eaa4d4faeccf",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63d34004b734eaa4d4faeccf/aH7F-xn4PMQjHHcvL-0vs.jpeg",
      "fullname": "Weizhi Wang",
      "name": "weizhiwang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 19
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.10611",
      "authors": [
        {
          "_id": "68ef4bbb486b78128f0e35e1",
          "name": "Heng Zhang",
          "hidden": false
        },
        {
          "_id": "68ef4bbb486b78128f0e35e2",
          "user": {
            "_id": "645b0c3ec35da9c7afd95421",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/645b0c3ec35da9c7afd95421/vYBrCDagHsXAo6J2p-uG0.jpeg",
            "isPro": false,
            "fullname": "Yuling",
            "user": "YerbaPage",
            "type": "user"
          },
          "name": "Yuling Shi",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-10-15T15:24:40.752Z",
          "hidden": false
        },
        {
          "_id": "68ef4bbb486b78128f0e35e3",
          "name": "Xiaodong Gu",
          "hidden": false
        },
        {
          "_id": "68ef4bbb486b78128f0e35e4",
          "name": "Zijian Zhang",
          "hidden": false
        },
        {
          "_id": "68ef4bbb486b78128f0e35e5",
          "name": "Haochen You",
          "hidden": false
        },
        {
          "_id": "68ef4bbb486b78128f0e35e6",
          "name": "Lubin Gan",
          "hidden": false
        },
        {
          "_id": "68ef4bbb486b78128f0e35e7",
          "name": "Yilei Yuan",
          "hidden": false
        },
        {
          "_id": "68ef4bbb486b78128f0e35e8",
          "name": "Jin Huang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-12T13:47:42.000Z",
      "submittedOnDailyAt": "2025-10-16T00:46:55.714Z",
      "title": "HyperAgent: Leveraging Hypergraphs for Topology Optimization in\n  Multi-Agent Communication",
      "submittedOnDailyBy": {
        "_id": "645b0c3ec35da9c7afd95421",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/645b0c3ec35da9c7afd95421/vYBrCDagHsXAo6J2p-uG0.jpeg",
        "isPro": false,
        "fullname": "Yuling",
        "user": "YerbaPage",
        "type": "user"
      },
      "summary": "Recent advances in large language model-powered multi-agent systems have\ndemonstrated remarkable collective intelligence through effective\ncommunication. However, existing approaches face two primary challenges: (i)\nIneffective group collaboration modeling, as they rely on pairwise\nedge representations in graph structures, limiting their ability to capture\nrelationships among multiple agents; and (ii) Limited task-adaptiveness\nin communication topology design, leading to excessive communication cost for\nsimple tasks and insufficient coordination for complex scenarios. These issues\nrestrict the scalability and practical deployment of adaptive collaboration\nframeworks. To address these challenges, we propose HyperAgent, a\nhypergraph-based framework that optimizes communication topologies and\neffectively captures group collaboration patterns using direct hyperedge\nrepresentations. Unlike edge-based approaches, HyperAgent uses hyperedges to\nlink multiple agents within the same subtask and employs hypergraph\nconvolutional layers to achieve one-step information aggregation in\ncollaboration groups. Additionally, it incorporates a variational autoencoder\nframework with sparsity regularization to dynamically adjust hypergraph\ntopologies based on task complexity. Experiments highlight the superiority of\nHyperAgent in both performance and efficiency. For instance, on GSM8K,\nHyperAgent achieves 95.07\\% accuracy while reducing token consumption by\n25.33\\%, demonstrating the potential of hypergraph-based optimization for\nmulti-agent communication.",
      "upvotes": 1,
      "discussionId": "68ef4bbc486b78128f0e35e9",
      "ai_summary": "HyperAgent, a hypergraph-based framework, optimizes communication topologies and captures group collaboration patterns, improving performance and efficiency in multi-agent systems.",
      "ai_keywords": [
        "hypergraph-based framework",
        "hyperedges",
        "hypergraph convolutional layers",
        "variational autoencoder",
        "sparsity regularization",
        "GSM8K",
        "token consumption"
      ]
    },
    "publishedAt": "2025-10-12T09:47:42.000Z",
    "title": "HyperAgent: Leveraging Hypergraphs for Topology Optimization in\n  Multi-Agent Communication",
    "summary": "Recent advances in large language model-powered multi-agent systems have\ndemonstrated remarkable collective intelligence through effective\ncommunication. However, existing approaches face two primary challenges: (i)\nIneffective group collaboration modeling, as they rely on pairwise\nedge representations in graph structures, limiting their ability to capture\nrelationships among multiple agents; and (ii) Limited task-adaptiveness\nin communication topology design, leading to excessive communication cost for\nsimple tasks and insufficient coordination for complex scenarios. These issues\nrestrict the scalability and practical deployment of adaptive collaboration\nframeworks. To address these challenges, we propose HyperAgent, a\nhypergraph-based framework that optimizes communication topologies and\neffectively captures group collaboration patterns using direct hyperedge\nrepresentations. Unlike edge-based approaches, HyperAgent uses hyperedges to\nlink multiple agents within the same subtask and employs hypergraph\nconvolutional layers to achieve one-step information aggregation in\ncollaboration groups. Additionally, it incorporates a variational autoencoder\nframework with sparsity regularization to dynamically adjust hypergraph\ntopologies based on task complexity. Experiments highlight the superiority of\nHyperAgent in both performance and efficiency. For instance, on GSM8K,\nHyperAgent achieves 95.07\\% accuracy while reducing token consumption by\n25.33\\%, demonstrating the potential of hypergraph-based optimization for\nmulti-agent communication.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.10611.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "645b0c3ec35da9c7afd95421",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/645b0c3ec35da9c7afd95421/vYBrCDagHsXAo6J2p-uG0.jpeg",
      "fullname": "Yuling",
      "name": "YerbaPage",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 190
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2510.10581",
      "authors": [
        {
          "_id": "68efc102d22df134b7f5ca27",
          "name": "Heng Zhang",
          "hidden": false
        },
        {
          "_id": "68efc102d22df134b7f5ca28",
          "user": {
            "_id": "645b0c3ec35da9c7afd95421",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/645b0c3ec35da9c7afd95421/vYBrCDagHsXAo6J2p-uG0.jpeg",
            "isPro": false,
            "fullname": "Yuling",
            "user": "YerbaPage",
            "type": "user"
          },
          "name": "Yuling Shi",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-10-15T16:55:17.600Z",
          "hidden": false
        },
        {
          "_id": "68efc102d22df134b7f5ca29",
          "name": "Xiaodong Gu",
          "hidden": false
        },
        {
          "_id": "68efc102d22df134b7f5ca2a",
          "name": "Haochen You",
          "hidden": false
        },
        {
          "_id": "68efc102d22df134b7f5ca2b",
          "name": "Zijian Zhang",
          "hidden": false
        },
        {
          "_id": "68efc102d22df134b7f5ca2c",
          "name": "Lubin Gan",
          "hidden": false
        },
        {
          "_id": "68efc102d22df134b7f5ca2d",
          "name": "Yilei Yuan",
          "hidden": false
        },
        {
          "_id": "68efc102d22df134b7f5ca2e",
          "name": "Jin Huang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-12T12:55:42.000Z",
      "submittedOnDailyAt": "2025-10-16T00:47:00.305Z",
      "title": "GraphTracer: Graph-Guided Failure Tracing in LLM Agents for Robust\n  Multi-Turn Deep Search",
      "submittedOnDailyBy": {
        "_id": "645b0c3ec35da9c7afd95421",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/645b0c3ec35da9c7afd95421/vYBrCDagHsXAo6J2p-uG0.jpeg",
        "isPro": false,
        "fullname": "Yuling",
        "user": "YerbaPage",
        "type": "user"
      },
      "summary": "Multi-agent systems powered by Large Language Models excel at complex tasks\nthrough coordinated collaboration, yet they face high failure rates in\nmulti-turn deep search scenarios. Existing temporal attribution methods\nstruggle to accurately diagnose root causes, particularly when errors propagate\nacross multiple agents. Attempts to automate failure attribution by analyzing\naction sequences remain ineffective due to their inability to account for\ninformation dependencies that span agents. This paper identifies two core\nchallenges: (i) distinguishing symptoms from root causes in multi-agent\nerror propagation, and (ii) tracing information dependencies beyond\ntemporal order. To address these issues, we introduce GraphTracer, a\nframework that redefines failure attribution through information flow analysis.\nGraphTracer constructs Information Dependency Graphs (IDGs) to explicitly\ncapture how agents reference and build on prior outputs. It localizes root\ncauses by tracing through these dependency structures instead of relying on\ntemporal sequences. GraphTracer also uses graph-aware synthetic data generation\nto target critical nodes, creating realistic failure scenarios. Evaluations on\nthe Who\\&When benchmark and integration into production systems demonstrate\nthat GraphTracer-8B achieves up to 18.18\\% higher attribution accuracy compared\nto state-of-the-art models and enables 4.8\\% to 14.2\\% performance improvements\nin deployed multi-agent frameworks, establishing a robust solution for\nmulti-agent system debugging.",
      "upvotes": 1,
      "discussionId": "68efc102d22df134b7f5ca2f",
      "ai_summary": "GraphTracer addresses multi-agent system failure attribution by constructing Information Dependency Graphs to trace information flow and improve debugging accuracy.",
      "ai_keywords": [
        "multi-agent systems",
        "Large Language Models",
        "deep search scenarios",
        "temporal attribution",
        "information dependencies",
        "GraphTracer",
        "Information Dependency Graphs",
        "failure attribution",
        "synthetic data generation",
        "Who&When benchmark"
      ]
    },
    "publishedAt": "2025-10-12T08:55:42.000Z",
    "title": "GraphTracer: Graph-Guided Failure Tracing in LLM Agents for Robust\n  Multi-Turn Deep Search",
    "summary": "Multi-agent systems powered by Large Language Models excel at complex tasks\nthrough coordinated collaboration, yet they face high failure rates in\nmulti-turn deep search scenarios. Existing temporal attribution methods\nstruggle to accurately diagnose root causes, particularly when errors propagate\nacross multiple agents. Attempts to automate failure attribution by analyzing\naction sequences remain ineffective due to their inability to account for\ninformation dependencies that span agents. This paper identifies two core\nchallenges: (i) distinguishing symptoms from root causes in multi-agent\nerror propagation, and (ii) tracing information dependencies beyond\ntemporal order. To address these issues, we introduce GraphTracer, a\nframework that redefines failure attribution through information flow analysis.\nGraphTracer constructs Information Dependency Graphs (IDGs) to explicitly\ncapture how agents reference and build on prior outputs. It localizes root\ncauses by tracing through these dependency structures instead of relying on\ntemporal sequences. GraphTracer also uses graph-aware synthetic data generation\nto target critical nodes, creating realistic failure scenarios. Evaluations on\nthe Who\\&When benchmark and integration into production systems demonstrate\nthat GraphTracer-8B achieves up to 18.18\\% higher attribution accuracy compared\nto state-of-the-art models and enables 4.8\\% to 14.2\\% performance improvements\nin deployed multi-agent frameworks, establishing a robust solution for\nmulti-agent system debugging.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.10581.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "645b0c3ec35da9c7afd95421",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/645b0c3ec35da9c7afd95421/vYBrCDagHsXAo6J2p-uG0.jpeg",
      "fullname": "Yuling",
      "name": "YerbaPage",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 190
    },
    "isAuthorParticipating": true
  }
]