[
  {
    "paper": {
      "id": "2602.21534",
      "authors": [
        {
          "_id": "699fbaf1ebfce7fbcca91b4d",
          "name": "Xiaoxuan Wang",
          "hidden": false
        },
        {
          "_id": "699fbaf1ebfce7fbcca91b4e",
          "name": "Han Zhang",
          "hidden": false
        },
        {
          "_id": "699fbaf1ebfce7fbcca91b4f",
          "name": "Haixin Wang",
          "hidden": false
        },
        {
          "_id": "699fbaf1ebfce7fbcca91b50",
          "name": "Yidan Shi",
          "hidden": false
        },
        {
          "_id": "699fbaf1ebfce7fbcca91b51",
          "name": "Ruoyan Li",
          "hidden": false
        },
        {
          "_id": "699fbaf1ebfce7fbcca91b52",
          "name": "Kaiqiao Han",
          "hidden": false
        },
        {
          "_id": "699fbaf1ebfce7fbcca91b53",
          "name": "Chenyi Tong",
          "hidden": false
        },
        {
          "_id": "699fbaf1ebfce7fbcca91b54",
          "name": "Haoran Deng",
          "hidden": false
        },
        {
          "_id": "699fbaf1ebfce7fbcca91b55",
          "name": "Renliang Sun",
          "hidden": false
        },
        {
          "_id": "699fbaf1ebfce7fbcca91b56",
          "name": "Alexander Taylor",
          "hidden": false
        },
        {
          "_id": "699fbaf1ebfce7fbcca91b57",
          "name": "Yanqiao Zhu",
          "hidden": false
        },
        {
          "_id": "699fbaf1ebfce7fbcca91b58",
          "name": "Jason Cong",
          "hidden": false
        },
        {
          "_id": "699fbaf1ebfce7fbcca91b59",
          "name": "Yizhou Sun",
          "hidden": false
        },
        {
          "_id": "699fbaf1ebfce7fbcca91b5a",
          "name": "Wei Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2026-02-25T03:43:34.000Z",
      "submittedOnDailyAt": "2026-02-26T00:47:18.994Z",
      "title": "ARLArena: A Unified Framework for Stable Agentic Reinforcement Learning",
      "submittedOnDailyBy": {
        "_id": "64ba5946c0f19c9025665a3c",
        "avatarUrl": "/avatars/bb148094ce52f1f385d30968dc22e0e6.svg",
        "isPro": false,
        "fullname": "Xiaoxuan Wang",
        "user": "xw27",
        "type": "user"
      },
      "summary": "Agentic reinforcement learning (ARL) has rapidly gained attention as a promising paradigm for training agents to solve complex, multi-step interactive tasks. Despite encouraging early results, ARL remains highly unstable, often leading to training collapse. This instability limits scalability to larger environments and longer interaction horizons, and constrains systematic exploration of algorithmic design choices. In this paper, we first propose ARLArena, a stable training recipe and systematic analysis framework that examines training stability in a controlled and reproducible setting. ARLArena first constructs a clean and standardized testbed. Then, we decompose policy gradient into four core design dimensions and assess the performance and stability of each dimension. Through this fine-grained analysis, we distill a unified perspective on ARL and propose SAMPO, a stable agentic policy optimization method designed to mitigate the dominant sources of instability in ARL. Empirically, SAMPO achieves consistently stable training and strong performance across diverse agentic tasks. Overall, this study provides a unifying policy gradient perspective for ARL and offers practical guidance for building stable and reproducible LLM-based agent training pipelines.",
      "upvotes": 2,
      "discussionId": "699fbaf1ebfce7fbcca91b5b",
      "githubRepo": "https://github.com/WillDreamer/ARL-Arena",
      "githubRepoAddedBy": "user",
      "ai_summary": "ARLArena framework analyzes training stability in agentic reinforcement learning and proposes SAMPO method for stable policy optimization across diverse tasks.",
      "ai_keywords": [
        "agentic reinforcement learning",
        "policy gradient",
        "training stability",
        "policy optimization",
        "ARLArena",
        "SAMPO"
      ],
      "organization": {
        "_id": "67784c39dac147922d8d09f0",
        "name": "UCLA",
        "fullname": "University of California, Los Angeles",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67784bd637dfa531fbce95a2/Nf0seEMEn66sPL3QsJXj4.png"
      }
    },
    "publishedAt": "2026-02-24T22:43:34.000Z",
    "title": "ARLArena: A Unified Framework for Stable Agentic Reinforcement Learning",
    "summary": "Agentic reinforcement learning (ARL) has rapidly gained attention as a promising paradigm for training agents to solve complex, multi-step interactive tasks. Despite encouraging early results, ARL remains highly unstable, often leading to training collapse. This instability limits scalability to larger environments and longer interaction horizons, and constrains systematic exploration of algorithmic design choices. In this paper, we first propose ARLArena, a stable training recipe and systematic analysis framework that examines training stability in a controlled and reproducible setting. ARLArena first constructs a clean and standardized testbed. Then, we decompose policy gradient into four core design dimensions and assess the performance and stability of each dimension. Through this fine-grained analysis, we distill a unified perspective on ARL and propose SAMPO, a stable agentic policy optimization method designed to mitigate the dominant sources of instability in ARL. Empirically, SAMPO achieves consistently stable training and strong performance across diverse agentic tasks. Overall, this study provides a unifying policy gradient perspective for ARL and offers practical guidance for building stable and reproducible LLM-based agent training pipelines.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.21534.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64ba5946c0f19c9025665a3c",
      "avatarUrl": "/avatars/bb148094ce52f1f385d30968dc22e0e6.svg",
      "fullname": "Xiaoxuan Wang",
      "name": "xw27",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "67784c39dac147922d8d09f0",
      "name": "UCLA",
      "fullname": "University of California, Los Angeles",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67784bd637dfa531fbce95a2/Nf0seEMEn66sPL3QsJXj4.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2602.19163",
      "authors": [
        {
          "_id": "699fb334ebfce7fbcca91ad0",
          "name": "Kai Liu",
          "hidden": false
        },
        {
          "_id": "699fb334ebfce7fbcca91ad1",
          "name": "Yanhao Zheng",
          "hidden": false
        },
        {
          "_id": "699fb334ebfce7fbcca91ad2",
          "name": "Kai Wang",
          "hidden": false
        },
        {
          "_id": "699fb334ebfce7fbcca91ad3",
          "name": "Shengqiong Wu",
          "hidden": false
        },
        {
          "_id": "699fb334ebfce7fbcca91ad4",
          "name": "Rongjunchen Zhang",
          "hidden": false
        },
        {
          "_id": "699fb334ebfce7fbcca91ad5",
          "name": "Jiebo Luo",
          "hidden": false
        },
        {
          "_id": "699fb334ebfce7fbcca91ad6",
          "name": "Dimitrios Hatzinakos",
          "hidden": false
        },
        {
          "_id": "699fb334ebfce7fbcca91ad7",
          "name": "Ziwei Liu",
          "hidden": false
        },
        {
          "_id": "699fb334ebfce7fbcca91ad8",
          "name": "Hao Fei",
          "hidden": false
        },
        {
          "_id": "699fb334ebfce7fbcca91ad9",
          "name": "Tat-Seng Chua",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/678bdcbe600666579235a1f3/BdSUgdHoEbTgTu6tHanG1.mp4",
        "https://cdn-uploads.huggingface.co/production/uploads/678bdcbe600666579235a1f3/bhNigK8NVUKgJ33WobH4i.jpeg",
        "https://cdn-uploads.huggingface.co/production/uploads/678bdcbe600666579235a1f3/kggZ01w0SeC0sb3YHCq6m.jpeg",
        "https://cdn-uploads.huggingface.co/production/uploads/678bdcbe600666579235a1f3/fUrz6yQZoxYotNr_YL5E-.jpeg"
      ],
      "publishedAt": "2026-02-22T12:44:28.000Z",
      "submittedOnDailyAt": "2026-02-26T00:22:24.649Z",
      "title": "JavisDiT++: Unified Modeling and Optimization for Joint Audio-Video Generation",
      "submittedOnDailyBy": {
        "_id": "678bdcbe600666579235a1f3",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/FoPxPc-T3xbzzkgQkrIbg.png",
        "isPro": false,
        "fullname": "KAI LIU",
        "user": "kkail8",
        "type": "user"
      },
      "summary": "AIGC has rapidly expanded from text-to-image generation toward high-quality multimodal synthesis across video and audio. Within this context, joint audio-video generation (JAVG) has emerged as a fundamental task that produces synchronized and semantically aligned sound and vision from textual descriptions. However, compared with advanced commercial models such as Veo3, existing open-source methods still suffer from limitations in generation quality, temporal synchrony, and alignment with human preferences. To bridge the gap, this paper presents JavisDiT++, a concise yet powerful framework for unified modeling and optimization of JAVG. First, we introduce a modality-specific mixture-of-experts (MS-MoE) design that enables cross-modal interaction efficacy while enhancing single-modal generation quality. Then, we propose a temporal-aligned RoPE (TA-RoPE) strategy to achieve explicit, frame-level synchronization between audio and video tokens. Besides, we develop an audio-video direct preference optimization (AV-DPO) method to align model outputs with human preference across quality, consistency, and synchrony dimensions. Built upon Wan2.1-1.3B-T2V, our model achieves state-of-the-art performance merely with around 1M public training entries, significantly outperforming prior approaches in both qualitative and quantitative evaluations. Comprehensive ablation studies have been conducted to validate the effectiveness of our proposed modules. All the code, model, and dataset are released at https://JavisVerse.github.io/JavisDiT2-page.",
      "upvotes": 2,
      "discussionId": "699fb335ebfce7fbcca91ada",
      "projectPage": "https://javisverse.github.io/JavisDiT2-page/",
      "githubRepo": "https://github.com/JavisVerse/JavisDiT",
      "githubRepoAddedBy": "user",
      "ai_summary": "JavisDiT++ presents a unified framework for joint audio-video generation using modality-specific mixture-of-experts, temporal-aligned RoPE, and audio-video direct preference optimization to achieve high-quality, synchronized multimedia synthesis.",
      "ai_keywords": [
        "joint audio-video generation",
        "modality-specific mixture-of-experts",
        "MS-MoE",
        "temporal-aligned RoPE",
        "TA-RoPE",
        "audio-video direct preference optimization",
        "AV-DPO",
        "multimodal synthesis",
        "synchronized generation",
        "semantically aligned"
      ],
      "githubStars": 319,
      "organization": {
        "_id": "67adfac46083604e4b664e43",
        "name": "JavisVerse",
        "fullname": "JavisVerse",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/678bdcbe600666579235a1f3/kZQXob4mMAq0CoeZH5ZfG.png"
      }
    },
    "publishedAt": "2026-02-22T07:44:28.000Z",
    "title": "JavisDiT++: Unified Modeling and Optimization for Joint Audio-Video Generation",
    "summary": "AIGC has rapidly expanded from text-to-image generation toward high-quality multimodal synthesis across video and audio. Within this context, joint audio-video generation (JAVG) has emerged as a fundamental task that produces synchronized and semantically aligned sound and vision from textual descriptions. However, compared with advanced commercial models such as Veo3, existing open-source methods still suffer from limitations in generation quality, temporal synchrony, and alignment with human preferences. To bridge the gap, this paper presents JavisDiT++, a concise yet powerful framework for unified modeling and optimization of JAVG. First, we introduce a modality-specific mixture-of-experts (MS-MoE) design that enables cross-modal interaction efficacy while enhancing single-modal generation quality. Then, we propose a temporal-aligned RoPE (TA-RoPE) strategy to achieve explicit, frame-level synchronization between audio and video tokens. Besides, we develop an audio-video direct preference optimization (AV-DPO) method to align model outputs with human preference across quality, consistency, and synchrony dimensions. Built upon Wan2.1-1.3B-T2V, our model achieves state-of-the-art performance merely with around 1M public training entries, significantly outperforming prior approaches in both qualitative and quantitative evaluations. Comprehensive ablation studies have been conducted to validate the effectiveness of our proposed modules. All the code, model, and dataset are released at https://JavisVerse.github.io/JavisDiT2-page.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/678bdcbe600666579235a1f3/BdSUgdHoEbTgTu6tHanG1.mp4",
      "https://cdn-uploads.huggingface.co/production/uploads/678bdcbe600666579235a1f3/bhNigK8NVUKgJ33WobH4i.jpeg",
      "https://cdn-uploads.huggingface.co/production/uploads/678bdcbe600666579235a1f3/kggZ01w0SeC0sb3YHCq6m.jpeg",
      "https://cdn-uploads.huggingface.co/production/uploads/678bdcbe600666579235a1f3/fUrz6yQZoxYotNr_YL5E-.jpeg"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.19163.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "678bdcbe600666579235a1f3",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/FoPxPc-T3xbzzkgQkrIbg.png",
      "fullname": "KAI LIU",
      "name": "kkail8",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "67adfac46083604e4b664e43",
      "name": "JavisVerse",
      "fullname": "JavisVerse",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/678bdcbe600666579235a1f3/kZQXob4mMAq0CoeZH5ZfG.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2602.21818",
      "authors": [
        {
          "_id": "699fba56ebfce7fbcca91b1a",
          "name": "Guibin Chen",
          "hidden": false
        },
        {
          "_id": "699fba56ebfce7fbcca91b1b",
          "name": "Dixuan Lin",
          "hidden": false
        },
        {
          "_id": "699fba56ebfce7fbcca91b1c",
          "name": "Jiangping Yang",
          "hidden": false
        },
        {
          "_id": "699fba56ebfce7fbcca91b1d",
          "name": "Youqiang Zhang",
          "hidden": false
        },
        {
          "_id": "699fba56ebfce7fbcca91b1e",
          "name": "Zhengcong Fei",
          "hidden": false
        },
        {
          "_id": "699fba56ebfce7fbcca91b1f",
          "name": "Debang Li",
          "hidden": false
        },
        {
          "_id": "699fba56ebfce7fbcca91b20",
          "name": "Sheng Chen",
          "hidden": false
        },
        {
          "_id": "699fba56ebfce7fbcca91b21",
          "name": "Chaofeng Ao",
          "hidden": false
        },
        {
          "_id": "699fba56ebfce7fbcca91b22",
          "name": "Nuo Pang",
          "hidden": false
        },
        {
          "_id": "699fba56ebfce7fbcca91b23",
          "name": "Yiming Wang",
          "hidden": false
        },
        {
          "_id": "699fba56ebfce7fbcca91b24",
          "name": "Yikun Dou",
          "hidden": false
        },
        {
          "_id": "699fba56ebfce7fbcca91b25",
          "name": "Zheng Chen",
          "hidden": false
        },
        {
          "_id": "699fba56ebfce7fbcca91b26",
          "name": "Mingyuan Fan",
          "hidden": false
        },
        {
          "_id": "699fba56ebfce7fbcca91b27",
          "name": "Tuanhui Li",
          "hidden": false
        },
        {
          "_id": "699fba56ebfce7fbcca91b28",
          "name": "Mingshan Chang",
          "hidden": false
        },
        {
          "_id": "699fba56ebfce7fbcca91b29",
          "name": "Hao Zhang",
          "hidden": false
        },
        {
          "_id": "699fba56ebfce7fbcca91b2a",
          "name": "Xiaopeng Sun",
          "hidden": false
        },
        {
          "_id": "699fba56ebfce7fbcca91b2b",
          "name": "Jingtao Xu",
          "hidden": false
        },
        {
          "_id": "699fba56ebfce7fbcca91b2c",
          "name": "Yuqiang Xie",
          "hidden": false
        },
        {
          "_id": "699fba56ebfce7fbcca91b2d",
          "name": "Jiahua Wang",
          "hidden": false
        },
        {
          "_id": "699fba56ebfce7fbcca91b2e",
          "name": "Zhiheng Xu",
          "hidden": false
        },
        {
          "_id": "699fba56ebfce7fbcca91b2f",
          "name": "Weiming Xiong",
          "hidden": false
        },
        {
          "_id": "699fba56ebfce7fbcca91b30",
          "name": "Yuzhe Jin",
          "hidden": false
        },
        {
          "_id": "699fba56ebfce7fbcca91b31",
          "name": "Baoxuan Gu",
          "hidden": false
        },
        {
          "_id": "699fba56ebfce7fbcca91b32",
          "name": "Binjie Mao",
          "hidden": false
        },
        {
          "_id": "699fba56ebfce7fbcca91b33",
          "name": "Yunjie Yu",
          "hidden": false
        },
        {
          "_id": "699fba56ebfce7fbcca91b34",
          "name": "Jujie He",
          "hidden": false
        },
        {
          "_id": "699fba56ebfce7fbcca91b35",
          "name": "Yuhao Feng",
          "hidden": false
        },
        {
          "_id": "699fba56ebfce7fbcca91b36",
          "name": "Shiwen Tu",
          "hidden": false
        },
        {
          "_id": "699fba56ebfce7fbcca91b37",
          "name": "Chaojie Wang",
          "hidden": false
        },
        {
          "_id": "699fba56ebfce7fbcca91b38",
          "name": "Rui Yan",
          "hidden": false
        },
        {
          "_id": "699fba56ebfce7fbcca91b39",
          "name": "Wei Shen",
          "hidden": false
        },
        {
          "_id": "699fba56ebfce7fbcca91b3a",
          "name": "Jingchen Wu",
          "hidden": false
        },
        {
          "_id": "699fba56ebfce7fbcca91b3b",
          "name": "Peng Zhao",
          "hidden": false
        },
        {
          "_id": "699fba56ebfce7fbcca91b3c",
          "name": "Xuanyue Zhong",
          "hidden": false
        },
        {
          "_id": "699fba56ebfce7fbcca91b3d",
          "name": "Zhuangzhuang Liu",
          "hidden": false
        },
        {
          "_id": "699fba56ebfce7fbcca91b3e",
          "name": "Kaifei Wang",
          "hidden": false
        },
        {
          "_id": "699fba56ebfce7fbcca91b3f",
          "name": "Fuxiang Zhang",
          "hidden": false
        },
        {
          "_id": "699fba56ebfce7fbcca91b40",
          "name": "Weikai Xu",
          "hidden": false
        },
        {
          "_id": "699fba56ebfce7fbcca91b41",
          "name": "Wenyan Liu",
          "hidden": false
        },
        {
          "_id": "699fba56ebfce7fbcca91b42",
          "name": "Binglu Zhang",
          "hidden": false
        },
        {
          "_id": "699fba56ebfce7fbcca91b43",
          "name": "Yu Shen",
          "hidden": false
        },
        {
          "_id": "699fba56ebfce7fbcca91b44",
          "name": "Tianhui Xiong",
          "hidden": false
        },
        {
          "_id": "699fba56ebfce7fbcca91b45",
          "name": "Bin Peng",
          "hidden": false
        },
        {
          "_id": "699fba56ebfce7fbcca91b46",
          "name": "Liang Zeng",
          "hidden": false
        },
        {
          "_id": "699fba56ebfce7fbcca91b47",
          "name": "Xuchen Song",
          "hidden": false
        },
        {
          "_id": "699fba56ebfce7fbcca91b48",
          "name": "Haoxiang Guo",
          "hidden": false
        },
        {
          "_id": "699fba56ebfce7fbcca91b49",
          "name": "Peiyu Wang",
          "hidden": false
        },
        {
          "_id": "699fba56ebfce7fbcca91b4a",
          "name": "Yahui Zhou",
          "hidden": false
        }
      ],
      "publishedAt": "2026-02-25T11:47:00.000Z",
      "submittedOnDailyAt": "2026-02-26T00:43:38.210Z",
      "title": "SkyReels-V4: Multi-modal Video-Audio Generation, Inpainting and Editing model",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "SkyReels V4 is a unified multi modal video foundation model for joint video audio generation, inpainting, and editing. The model adopts a dual stream Multimodal Diffusion Transformer (MMDiT) architecture, where one branch synthesizes video and the other generates temporally aligned audio, while sharing a powerful text encoder based on the Multimodal Large Language Models (MMLM). SkyReels V4 accepts rich multi modal instructions, including text, images, video clips, masks, and audio references. By combining the MMLMs multi modal instruction following capability with in context learning in the video branch MMDiT, the model can inject fine grained visual guidance under complex conditioning, while the audio branch MMDiT simultaneously leverages audio references to guide sound generation. On the video side, we adopt a channel concatenation formulation that unifies a wide range of inpainting style tasks, such as image to video, video extension, and video editing under a single interface, and naturally extends to vision referenced inpainting and editing via multi modal prompts. SkyReels V4 supports up to 1080p resolution, 32 FPS, and 15 second duration, enabling high fidelity, multi shot, cinema level video generation with synchronized audio. To make such high resolution, long-duration generation computationally feasible, we introduce an efficiency strategy: Joint generation of low resolution full sequences and high-resolution keyframes, followed by dedicated super-resolution and frame interpolation models. To our knowledge, SkyReels V4 is the first video foundation model that simultaneously supports multi-modal input, joint video audio generation, and a unified treatment of generation, inpainting, and editing, while maintaining strong efficiency and quality at cinematic resolutions and durations.",
      "upvotes": 1,
      "discussionId": "699fba56ebfce7fbcca91b4b",
      "ai_summary": "SkyReels V4 is a unified multimodal video foundation model that generates, edits, and inpaints video and audio simultaneously using a dual-stream architecture with shared text encoding and efficient high-resolution processing.",
      "ai_keywords": [
        "Multimodal Diffusion Transformer",
        "MMDiT",
        "Multimodal Large Language Models",
        "MMLM",
        "video audio generation",
        "video inpainting",
        "video editing",
        "channel concatenation formulation",
        "joint generation",
        "super-resolution",
        "frame interpolation"
      ],
      "organization": {
        "_id": "6522615d9334173c627b0efa",
        "name": "Skywork",
        "fullname": "Skywork",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/64535b71bcbd25618f7655da/AvtJ4GuPAyhLxl2-leVt6.jpeg"
      }
    },
    "publishedAt": "2026-02-25T06:47:00.000Z",
    "title": "SkyReels-V4: Multi-modal Video-Audio Generation, Inpainting and Editing model",
    "summary": "SkyReels V4 is a unified multi modal video foundation model for joint video audio generation, inpainting, and editing. The model adopts a dual stream Multimodal Diffusion Transformer (MMDiT) architecture, where one branch synthesizes video and the other generates temporally aligned audio, while sharing a powerful text encoder based on the Multimodal Large Language Models (MMLM). SkyReels V4 accepts rich multi modal instructions, including text, images, video clips, masks, and audio references. By combining the MMLMs multi modal instruction following capability with in context learning in the video branch MMDiT, the model can inject fine grained visual guidance under complex conditioning, while the audio branch MMDiT simultaneously leverages audio references to guide sound generation. On the video side, we adopt a channel concatenation formulation that unifies a wide range of inpainting style tasks, such as image to video, video extension, and video editing under a single interface, and naturally extends to vision referenced inpainting and editing via multi modal prompts. SkyReels V4 supports up to 1080p resolution, 32 FPS, and 15 second duration, enabling high fidelity, multi shot, cinema level video generation with synchronized audio. To make such high resolution, long-duration generation computationally feasible, we introduce an efficiency strategy: Joint generation of low resolution full sequences and high-resolution keyframes, followed by dedicated super-resolution and frame interpolation models. To our knowledge, SkyReels V4 is the first video foundation model that simultaneously supports multi-modal input, joint video audio generation, and a unified treatment of generation, inpainting, and editing, while maintaining strong efficiency and quality at cinematic resolutions and durations.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.21818.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 239,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "6522615d9334173c627b0efa",
      "name": "Skywork",
      "fullname": "Skywork",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/64535b71bcbd25618f7655da/AvtJ4GuPAyhLxl2-leVt6.jpeg"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2602.18527",
      "authors": [
        {
          "_id": "699fbcddebfce7fbcca91b64",
          "name": "Zhan Liu",
          "hidden": false
        },
        {
          "_id": "699fbcddebfce7fbcca91b65",
          "name": "Changli Tang",
          "hidden": false
        },
        {
          "_id": "699fbcddebfce7fbcca91b66",
          "name": "Yuxin Wang",
          "hidden": false
        },
        {
          "_id": "699fbcddebfce7fbcca91b67",
          "name": "Zhiyuan Zhu",
          "hidden": false
        },
        {
          "_id": "699fbcddebfce7fbcca91b68",
          "name": "Youjun Chen",
          "hidden": false
        },
        {
          "_id": "699fbcddebfce7fbcca91b69",
          "name": "Yiwen Shao",
          "hidden": false
        },
        {
          "_id": "699fbcddebfce7fbcca91b6a",
          "name": "Tianzi Wang",
          "hidden": false
        },
        {
          "_id": "699fbcddebfce7fbcca91b6b",
          "name": "Lei Ke",
          "hidden": false
        },
        {
          "_id": "699fbcddebfce7fbcca91b6c",
          "name": "Zengrui Jin",
          "hidden": false
        },
        {
          "_id": "699fbcddebfce7fbcca91b6d",
          "name": "Chao Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2026-02-20T04:06:07.000Z",
      "submittedOnDailyAt": "2026-02-26T00:56:24.242Z",
      "title": "JAEGER: Joint 3D Audio-Visual Grounding and Reasoning in Simulated Physical Environments",
      "submittedOnDailyBy": {
        "_id": "660bb388aeb2c22d9bd3894a",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/660bb388aeb2c22d9bd3894a/zSmagaNp6yXovj8R8-beT.jpeg",
        "isPro": false,
        "fullname": "Jason Liu",
        "user": "liuzhan22",
        "type": "user"
      },
      "summary": "Current audio-visual large language models (AV-LLMs) are predominantly restricted to 2D perception, relying on RGB video and monaural audio. This design choice introduces a fundamental dimensionality mismatch that precludes reliable source localization and spatial reasoning in complex 3D environments. We address this limitation by presenting JAEGER, a framework that extends AV-LLMs to 3D space, to enable joint spatial grounding and reasoning through the integration of RGB-D observations and multi-channel first-order ambisonics. A core contribution of our work is the neural intensity vector (Neural IV), a learned spatial audio representation that encodes robust directional cues to enhance direction-of-arrival estimation, even in adverse acoustic scenarios with overlapping sources. To facilitate large-scale training and systematic evaluation, we propose SpatialSceneQA, a benchmark of 61k instruction-tuning samples curated from simulated physical environments. Extensive experiments demonstrate that our approach consistently surpasses 2D-centric baselines across diverse spatial perception and reasoning tasks, underscoring the necessity of explicit 3D modelling for advancing AI in physical environments. Our source code, pre-trained model checkpoints and datasets will be released upon acceptance.",
      "upvotes": 1,
      "discussionId": "699fbcdeebfce7fbcca91b6e",
      "ai_summary": "JAEGER extends audio-visual large language models to 3D space by integrating RGB-D observations and multi-channel audio to improve spatial reasoning and source localization.",
      "ai_keywords": [
        "audio-visual large language models",
        "3D space",
        "RGB-D observations",
        "multi-channel first-order ambisonics",
        "neural intensity vector",
        "direction-of-arrival estimation",
        "spatial grounding",
        "spatial reasoning",
        "SpatialSceneQA",
        "instruction-tuning"
      ],
      "organization": {
        "_id": "628735cbc83a2d6ab8d14a66",
        "name": "Tsinghua",
        "fullname": "Tsinghua University"
      }
    },
    "publishedAt": "2026-02-19T23:06:07.000Z",
    "title": "JAEGER: Joint 3D Audio-Visual Grounding and Reasoning in Simulated Physical Environments",
    "summary": "Current audio-visual large language models (AV-LLMs) are predominantly restricted to 2D perception, relying on RGB video and monaural audio. This design choice introduces a fundamental dimensionality mismatch that precludes reliable source localization and spatial reasoning in complex 3D environments. We address this limitation by presenting JAEGER, a framework that extends AV-LLMs to 3D space, to enable joint spatial grounding and reasoning through the integration of RGB-D observations and multi-channel first-order ambisonics. A core contribution of our work is the neural intensity vector (Neural IV), a learned spatial audio representation that encodes robust directional cues to enhance direction-of-arrival estimation, even in adverse acoustic scenarios with overlapping sources. To facilitate large-scale training and systematic evaluation, we propose SpatialSceneQA, a benchmark of 61k instruction-tuning samples curated from simulated physical environments. Extensive experiments demonstrate that our approach consistently surpasses 2D-centric baselines across diverse spatial perception and reasoning tasks, underscoring the necessity of explicit 3D modelling for advancing AI in physical environments. Our source code, pre-trained model checkpoints and datasets will be released upon acceptance.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.18527.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "660bb388aeb2c22d9bd3894a",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/660bb388aeb2c22d9bd3894a/zSmagaNp6yXovj8R8-beT.jpeg",
      "fullname": "Jason Liu",
      "name": "liuzhan22",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "628735cbc83a2d6ab8d14a66",
      "name": "Tsinghua",
      "fullname": "Tsinghua University"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2602.14878",
      "authors": [
        {
          "_id": "699e517bdfbcf0b800aeca57",
          "name": "Mohammed Mehedi Hasan",
          "hidden": false
        },
        {
          "_id": "699e517bdfbcf0b800aeca58",
          "user": {
            "_id": "62b4f3b7464e664268bf4e85",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62b4f3b7464e664268bf4e85/atFIscmB37aur0a1zuQ2o.jpeg",
            "isPro": false,
            "fullname": "Leo",
            "user": "hao-li",
            "type": "user"
          },
          "name": "Hao Li",
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-02-25T17:30:08.258Z",
          "hidden": false
        },
        {
          "_id": "699e517bdfbcf0b800aeca59",
          "name": "Gopi Krishnan Rajbahadur",
          "hidden": false
        },
        {
          "_id": "699e517bdfbcf0b800aeca5a",
          "name": "Bram Adams",
          "hidden": false
        },
        {
          "_id": "699e517bdfbcf0b800aeca5b",
          "name": "Ahmed E. Hassan",
          "hidden": false
        }
      ],
      "publishedAt": "2026-02-16T16:10:11.000Z",
      "submittedOnDailyAt": "2026-02-26T00:39:58.966Z",
      "title": "Model Context Protocol (MCP) Tool Descriptions Are Smelly! Towards Improving AI Agent Efficiency with Augmented MCP Tool Descriptions",
      "submittedOnDailyBy": {
        "_id": "62b4f3b7464e664268bf4e85",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62b4f3b7464e664268bf4e85/atFIscmB37aur0a1zuQ2o.jpeg",
        "isPro": false,
        "fullname": "Leo",
        "user": "hao-li",
        "type": "user"
      },
      "summary": "The Model Context Protocol (MCP) introduces a standard specification that defines how Foundation Model (FM)-based agents should interact with external systems by invoking tools. However, to understand a tool's purpose and features, FMs rely on natural-language tool descriptions, making these descriptions a critical component in guiding FMs to select the optimal tool for a given (sub)task and to pass the right arguments to the tool. While defects or smells in these descriptions can misguide FM-based agents, their prevalence and consequences in the MCP ecosystem remain unclear.\n  Hence, we examine 856 tools spread across 103 MCP servers empirically, assess their description quality, and their impact on agent performance. We identify six components of tool descriptions from the literature, develop a scoring rubric utilizing these components, and then formalize tool description smells based on this rubric. By operationalizing this rubric through an FM-based scanner, we find that 97.1% of the analyzed tool descriptions contain at least one smell, with 56% failing to state their purpose clearly. While augmenting these descriptions for all components improves task success rates by a median of 5.85 percentage points and improves partial goal completion by 15.12%, it also increases the number of execution steps by 67.46% and regresses performance in 16.67% of cases. These results indicate that achieving performance gains is not straightforward; while execution cost can act as a trade-off, execution context can also impact. Furthermore, component ablations show that compact variants of different component combinations often preserve behavioral reliability while reducing unnecessary token overhead, enabling more efficient use of the FM context window and lower execution costs.",
      "upvotes": 1,
      "discussionId": "699e517cdfbcf0b800aeca5c",
      "ai_summary": "Foundation model agents rely on natural language tool descriptions for effective interaction with external systems, but poor description quality significantly impacts performance and efficiency.",
      "ai_keywords": [
        "Foundation Model",
        "tool descriptions",
        "agent performance",
        "description quality",
        "execution steps",
        "task success rates",
        "partial goal completion",
        "execution cost",
        "behavioral reliability",
        "token overhead",
        "context window"
      ]
    },
    "publishedAt": "2026-02-16T11:10:11.000Z",
    "title": "Model Context Protocol (MCP) Tool Descriptions Are Smelly! Towards Improving AI Agent Efficiency with Augmented MCP Tool Descriptions",
    "summary": "The Model Context Protocol (MCP) introduces a standard specification that defines how Foundation Model (FM)-based agents should interact with external systems by invoking tools. However, to understand a tool's purpose and features, FMs rely on natural-language tool descriptions, making these descriptions a critical component in guiding FMs to select the optimal tool for a given (sub)task and to pass the right arguments to the tool. While defects or smells in these descriptions can misguide FM-based agents, their prevalence and consequences in the MCP ecosystem remain unclear.\n  Hence, we examine 856 tools spread across 103 MCP servers empirically, assess their description quality, and their impact on agent performance. We identify six components of tool descriptions from the literature, develop a scoring rubric utilizing these components, and then formalize tool description smells based on this rubric. By operationalizing this rubric through an FM-based scanner, we find that 97.1% of the analyzed tool descriptions contain at least one smell, with 56% failing to state their purpose clearly. While augmenting these descriptions for all components improves task success rates by a median of 5.85 percentage points and improves partial goal completion by 15.12%, it also increases the number of execution steps by 67.46% and regresses performance in 16.67% of cases. These results indicate that achieving performance gains is not straightforward; while execution cost can act as a trade-off, execution context can also impact. Furthermore, component ablations show that compact variants of different component combinations often preserve behavioral reliability while reducing unnecessary token overhead, enabling more efficient use of the FM context window and lower execution costs.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.14878.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62b4f3b7464e664268bf4e85",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62b4f3b7464e664268bf4e85/atFIscmB37aur0a1zuQ2o.jpeg",
      "fullname": "Leo",
      "name": "hao-li",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4,
      "isUserFollowing": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2602.22208",
      "authors": [
        {
          "_id": "699fb974ebfce7fbcca91b04",
          "name": "Georgy Savva",
          "hidden": false
        },
        {
          "_id": "699fb974ebfce7fbcca91b05",
          "name": "Oscar Michel",
          "hidden": false
        },
        {
          "_id": "699fb974ebfce7fbcca91b06",
          "name": "Daohan Lu",
          "hidden": false
        },
        {
          "_id": "699fb974ebfce7fbcca91b07",
          "name": "Suppakit Waiwitlikhit",
          "hidden": false
        },
        {
          "_id": "699fb974ebfce7fbcca91b08",
          "name": "Timothy Meehan",
          "hidden": false
        },
        {
          "_id": "699fb974ebfce7fbcca91b09",
          "name": "Dhairya Mishra",
          "hidden": false
        },
        {
          "_id": "699fb974ebfce7fbcca91b0a",
          "name": "Srivats Poddar",
          "hidden": false
        },
        {
          "_id": "699fb974ebfce7fbcca91b0b",
          "name": "Jack Lu",
          "hidden": false
        },
        {
          "_id": "699fb974ebfce7fbcca91b0c",
          "name": "Saining Xie",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/nMpp08pwIVyS2FTX_4zWe.mp4"
      ],
      "publishedAt": "2026-02-25T18:59:01.000Z",
      "submittedOnDailyAt": "2026-02-26T00:40:52.562Z",
      "title": "Solaris: Building a Multiplayer Video World Model in Minecraft",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Existing action-conditioned video generation models (video world models) are limited to single-agent perspectives, failing to capture the multi-agent interactions of real-world environments. We introduce Solaris, a multiplayer video world model that simulates consistent multi-view observations. To enable this, we develop a multiplayer data system designed for robust, continuous, and automated data collection on video games such as Minecraft. Unlike prior platforms built for single-player settings, our system supports coordinated multi-agent interaction and synchronized videos + actions capture. Using this system, we collect 12.64 million multiplayer frames and propose an evaluation framework for multiplayer movement, memory, grounding, building, and view consistency. We train Solaris using a staged pipeline that progressively transitions from single-player to multiplayer modeling, combining bidirectional, causal, and Self Forcing training. In the final stage, we introduce Checkpointed Self Forcing, a memory-efficient Self Forcing variant that enables a longer-horizon teacher. Results show our architecture and training design outperform existing baselines. Through open-sourcing our system and models, we hope to lay the groundwork for a new generation of multi-agent world models.",
      "upvotes": 0,
      "discussionId": "699fb974ebfce7fbcca91b0d",
      "projectPage": "https://solaris-wm.github.io/",
      "githubRepo": "https://github.com/solaris-wm/solaris",
      "githubRepoAddedBy": "user",
      "ai_summary": "Solaris is a multiplayer video world model that simulates consistent multi-view observations through a novel data collection system and staged training approach.",
      "ai_keywords": [
        "video world models",
        "multiplayer",
        "multi-agent interactions",
        "data collection",
        "staged pipeline",
        "bidirectional",
        "causal",
        "Self Forcing",
        "checkpointed Self Forcing"
      ],
      "githubStars": 4
    },
    "publishedAt": "2026-02-25T13:59:01.000Z",
    "title": "Solaris: Building a Multiplayer Video World Model in Minecraft",
    "summary": "Existing action-conditioned video generation models (video world models) are limited to single-agent perspectives, failing to capture the multi-agent interactions of real-world environments. We introduce Solaris, a multiplayer video world model that simulates consistent multi-view observations. To enable this, we develop a multiplayer data system designed for robust, continuous, and automated data collection on video games such as Minecraft. Unlike prior platforms built for single-player settings, our system supports coordinated multi-agent interaction and synchronized videos + actions capture. Using this system, we collect 12.64 million multiplayer frames and propose an evaluation framework for multiplayer movement, memory, grounding, building, and view consistency. We train Solaris using a staged pipeline that progressively transitions from single-player to multiplayer modeling, combining bidirectional, causal, and Self Forcing training. In the final stage, we introduce Checkpointed Self Forcing, a memory-efficient Self Forcing variant that enables a longer-horizon teacher. Results show our architecture and training design outperform existing baselines. Through open-sourcing our system and models, we hope to lay the groundwork for a new generation of multi-agent world models.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/nMpp08pwIVyS2FTX_4zWe.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.22208.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 239,
      "isUserFollowing": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2602.22010",
      "authors": [
        {
          "_id": "699fb776ebfce7fbcca91adc",
          "name": "Yue Su",
          "hidden": false
        },
        {
          "_id": "699fb776ebfce7fbcca91add",
          "name": "Sijin Chen",
          "hidden": false
        },
        {
          "_id": "699fb776ebfce7fbcca91ade",
          "name": "Haixin Shi",
          "hidden": false
        },
        {
          "_id": "699fb776ebfce7fbcca91adf",
          "name": "Mingyu Liu",
          "hidden": false
        },
        {
          "_id": "699fb776ebfce7fbcca91ae0",
          "name": "Zhengshen Zhang",
          "hidden": false
        },
        {
          "_id": "699fb776ebfce7fbcca91ae1",
          "name": "Ningyuan Huang",
          "hidden": false
        },
        {
          "_id": "699fb776ebfce7fbcca91ae2",
          "name": "Weiheng Zhong",
          "hidden": false
        },
        {
          "_id": "699fb776ebfce7fbcca91ae3",
          "name": "Zhengbang Zhu",
          "hidden": false
        },
        {
          "_id": "699fb776ebfce7fbcca91ae4",
          "name": "Yuxiao Liu",
          "hidden": false
        },
        {
          "_id": "699fb776ebfce7fbcca91ae5",
          "name": "Xihui Liu",
          "hidden": false
        }
      ],
      "publishedAt": "2026-02-25T15:27:09.000Z",
      "submittedOnDailyAt": "2026-02-26T00:31:10.968Z",
      "title": "World Guidance: World Modeling in Condition Space for Action Generation",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Leveraging future observation modeling to facilitate action generation presents a promising avenue for enhancing the capabilities of Vision-Language-Action (VLA) models. However, existing approaches struggle to strike a balance between maintaining efficient, predictable future representations and preserving sufficient fine-grained information to guide precise action generation. To address this limitation, we propose WoG (World Guidance), a framework that maps future observations into compact conditions by injecting them into the action inference pipeline. The VLA is then trained to simultaneously predict these compressed conditions alongside future actions, thereby achieving effective world modeling within the condition space for action inference. We demonstrate that modeling and predicting this condition space not only facilitates fine-grained action generation but also exhibits superior generalization capabilities. Moreover, it learns effectively from substantial human manipulation videos. Extensive experiments across both simulation and real-world environments validate that our method significantly outperforms existing methods based on future prediction. Project page is available at: https://selen-suyue.github.io/WoGNet/",
      "upvotes": 0,
      "discussionId": "699fb776ebfce7fbcca91ae6",
      "projectPage": "https://selen-suyue.github.io/WoGNet/",
      "ai_summary": "World Guidance framework enhances Vision-Language-Action models by mapping future observations into compact conditions for improved action generation and generalization.",
      "ai_keywords": [
        "Vision-Language-Action models",
        "future observation modeling",
        "action generation",
        "world modeling",
        "condition space",
        "future prediction",
        "human manipulation videos"
      ],
      "organization": {
        "_id": "67d1140985ea0644e2f14b99",
        "name": "ByteDance-Seed",
        "fullname": "ByteDance Seed",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"
      }
    },
    "publishedAt": "2026-02-25T10:27:09.000Z",
    "title": "World Guidance: World Modeling in Condition Space for Action Generation",
    "summary": "Leveraging future observation modeling to facilitate action generation presents a promising avenue for enhancing the capabilities of Vision-Language-Action (VLA) models. However, existing approaches struggle to strike a balance between maintaining efficient, predictable future representations and preserving sufficient fine-grained information to guide precise action generation. To address this limitation, we propose WoG (World Guidance), a framework that maps future observations into compact conditions by injecting them into the action inference pipeline. The VLA is then trained to simultaneously predict these compressed conditions alongside future actions, thereby achieving effective world modeling within the condition space for action inference. We demonstrate that modeling and predicting this condition space not only facilitates fine-grained action generation but also exhibits superior generalization capabilities. Moreover, it learns effectively from substantial human manipulation videos. Extensive experiments across both simulation and real-world environments validate that our method significantly outperforms existing methods based on future prediction. Project page is available at: https://selen-suyue.github.io/WoGNet/",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.22010.png",
    "numComments": 0,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 239,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "67d1140985ea0644e2f14b99",
      "name": "ByteDance-Seed",
      "fullname": "ByteDance Seed",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2602.21835",
      "authors": [
        {
          "_id": "699fba49ebfce7fbcca91b0f",
          "name": "Jianhui Wei",
          "hidden": false
        },
        {
          "_id": "699fba49ebfce7fbcca91b10",
          "name": "Xiaotian Zhang",
          "hidden": false
        },
        {
          "_id": "699fba49ebfce7fbcca91b11",
          "name": "Yichen Li",
          "hidden": false
        },
        {
          "_id": "699fba49ebfce7fbcca91b12",
          "name": "Yuan Wang",
          "hidden": false
        },
        {
          "_id": "699fba49ebfce7fbcca91b13",
          "name": "Yan Zhang",
          "hidden": false
        },
        {
          "_id": "699fba49ebfce7fbcca91b14",
          "name": "Ziyi Chen",
          "hidden": false
        },
        {
          "_id": "699fba49ebfce7fbcca91b15",
          "name": "Zhihang Tang",
          "hidden": false
        },
        {
          "_id": "699fba49ebfce7fbcca91b16",
          "name": "Wei Xu",
          "hidden": false
        },
        {
          "_id": "699fba49ebfce7fbcca91b17",
          "name": "Zuozhu Liu",
          "hidden": false
        }
      ],
      "publishedAt": "2026-02-25T12:08:53.000Z",
      "submittedOnDailyAt": "2026-02-26T00:45:50.850Z",
      "title": "UniVBench: Towards Unified Evaluation for Video Foundation Models",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Video foundation models aim to integrate video understanding, generation, editing, and instruction following within a single framework, making them a central direction for next-generation multimodal systems. However, existing evaluation benchmarks remain fragmented and limited in scope, as they each target a single task, rely on task-specific metrics, and typically use short or simple video clips. As a result, they do not capture the unified capabilities that these models are designed to deliver. To address this gap, we introduce UniVBench, a benchmark purpose-built for evaluating video foundation models across four core abilities: video understanding, video generation, video editing, and a newly proposed task, video reconstruction, which assesses how faithfully a model can reproduce video content it has encountered. Our benchmark substantially expands the complexity of evaluation by incorporating 200 high-quality, diverse and multi-shot videos, each paired with detailed captions, multi-format editing instructions, and reference images. All videos are human-created and carefully validated, offering richer cinematic information than prior benchmarks. In addition, we develop a unified agentic evaluation system (UniV-Eval) that standardizes prompting, instruction parsing, and scoring across all tasks, enabling fair, scalable, and reproducible comparisons of unified video models. By grounding evaluation in instruction-based multi-shot video tasks, UniVBench provides the first framework for measuring the integrated capabilities that video foundation models aim to achieve. Extensive human annotations ensure our evaluation aligns with human judgment, enabling rigorous assessment and accelerating progress toward robust video intelligence.",
      "upvotes": 0,
      "discussionId": "699fba49ebfce7fbcca91b18",
      "githubRepo": "https://github.com/JianhuiWei7/UniVBench",
      "githubRepoAddedBy": "user",
      "ai_summary": "UniVBench introduces a comprehensive benchmark for evaluating video foundation models across multiple capabilities including understanding, generation, editing, and reconstruction using high-quality, diverse video content and a unified evaluation system.",
      "ai_keywords": [
        "video foundation models",
        "video understanding",
        "video generation",
        "video editing",
        "video reconstruction",
        "UniVBench",
        "UniV-Eval",
        "multimodal systems",
        "instruction following",
        "agentic evaluation system"
      ]
    },
    "publishedAt": "2026-02-25T07:08:53.000Z",
    "title": "UniVBench: Towards Unified Evaluation for Video Foundation Models",
    "summary": "Video foundation models aim to integrate video understanding, generation, editing, and instruction following within a single framework, making them a central direction for next-generation multimodal systems. However, existing evaluation benchmarks remain fragmented and limited in scope, as they each target a single task, rely on task-specific metrics, and typically use short or simple video clips. As a result, they do not capture the unified capabilities that these models are designed to deliver. To address this gap, we introduce UniVBench, a benchmark purpose-built for evaluating video foundation models across four core abilities: video understanding, video generation, video editing, and a newly proposed task, video reconstruction, which assesses how faithfully a model can reproduce video content it has encountered. Our benchmark substantially expands the complexity of evaluation by incorporating 200 high-quality, diverse and multi-shot videos, each paired with detailed captions, multi-format editing instructions, and reference images. All videos are human-created and carefully validated, offering richer cinematic information than prior benchmarks. In addition, we develop a unified agentic evaluation system (UniV-Eval) that standardizes prompting, instruction parsing, and scoring across all tasks, enabling fair, scalable, and reproducible comparisons of unified video models. By grounding evaluation in instruction-based multi-shot video tasks, UniVBench provides the first framework for measuring the integrated capabilities that video foundation models aim to achieve. Extensive human annotations ensure our evaluation aligns with human judgment, enabling rigorous assessment and accelerating progress toward robust video intelligence.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.21835.png",
    "numComments": 0,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 239,
      "isUserFollowing": false
    },
    "isAuthorParticipating": false
  }
]