[
  {
    "paper": {
      "id": "2504.20998",
      "authors": [
        {
          "_id": "6811899ba6198824c5589ed7",
          "name": "Thao Nguyen",
          "hidden": false
        },
        {
          "_id": "6811899ba6198824c5589ed8",
          "name": "Krishna Kumar Singh",
          "hidden": false
        },
        {
          "_id": "6811899ba6198824c5589ed9",
          "name": "Jing Shi",
          "hidden": false
        },
        {
          "_id": "6811899ba6198824c5589eda",
          "name": "Trung Bui",
          "hidden": false
        },
        {
          "_id": "6811899ba6198824c5589edb",
          "name": "Yong Jae Lee",
          "hidden": false
        },
        {
          "_id": "6811899ba6198824c5589edc",
          "name": "Yuheng Li",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-29T17:59:57.000Z",
      "submittedOnDailyAt": "2025-04-30T00:54:06.806Z",
      "title": "YoChameleon: Personalized Vision and Language Generation",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": false,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "Large Multimodal Models (e.g., GPT-4, Gemini, Chameleon) have evolved into\npowerful tools with millions of users. However, they remain generic models and\nlack personalized knowledge of specific user concepts. Previous work has\nexplored personalization for text generation, yet it remains unclear how these\nmethods can be adapted to new modalities, such as image generation. In this\npaper, we introduce Yo'Chameleon, the first attempt to study personalization\nfor large multimodal models. Given 3-5 images of a particular concept,\nYo'Chameleon leverages soft-prompt tuning to embed subject-specific information\nto (i) answer questions about the subject and (ii) recreate pixel-level details\nto produce images of the subject in new contexts. Yo'Chameleon is trained with\n(i) a self-prompting optimization mechanism to balance performance across\nmultiple modalities, and (ii) a ``soft-positive\" image generation approach to\nenhance image quality in a few-shot setting.",
      "upvotes": 0,
      "discussionId": "6811899ca6198824c5589f45"
    },
    "publishedAt": "2025-04-29T13:59:57.000Z",
    "title": "YoChameleon: Personalized Vision and Language Generation",
    "summary": "Large Multimodal Models (e.g., GPT-4, Gemini, Chameleon) have evolved into\npowerful tools with millions of users. However, they remain generic models and\nlack personalized knowledge of specific user concepts. Previous work has\nexplored personalization for text generation, yet it remains unclear how these\nmethods can be adapted to new modalities, such as image generation. In this\npaper, we introduce Yo'Chameleon, the first attempt to study personalization\nfor large multimodal models. Given 3-5 images of a particular concept,\nYo'Chameleon leverages soft-prompt tuning to embed subject-specific information\nto (i) answer questions about the subject and (ii) recreate pixel-level details\nto produce images of the subject in new contexts. Yo'Chameleon is trained with\n(i) a self-prompting optimization mechanism to balance performance across\nmultiple modalities, and (ii) a ``soft-positive\" image generation approach to\nenhance image quality in a few-shot setting.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.20998.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6745
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.20571",
      "authors": [
        {
          "_id": "681187ddda5ce4cbd7556714",
          "name": "Yiping Wang",
          "hidden": false
        },
        {
          "_id": "681187ddda5ce4cbd7556715",
          "name": "Qing Yang",
          "hidden": false
        },
        {
          "_id": "681187ddda5ce4cbd7556716",
          "name": "Zhiyuan Zeng",
          "hidden": false
        },
        {
          "_id": "681187ddda5ce4cbd7556717",
          "name": "Liliang Ren",
          "hidden": false
        },
        {
          "_id": "681187ddda5ce4cbd7556718",
          "name": "Lucas Liu",
          "hidden": false
        },
        {
          "_id": "681187ddda5ce4cbd7556719",
          "name": "Baolin Peng",
          "hidden": false
        },
        {
          "_id": "681187ddda5ce4cbd755671a",
          "name": "Hao Cheng",
          "hidden": false
        },
        {
          "_id": "681187ddda5ce4cbd755671b",
          "name": "Xuehai He",
          "hidden": false
        },
        {
          "_id": "681187ddda5ce4cbd755671c",
          "name": "Kuan Wang",
          "hidden": false
        },
        {
          "_id": "681187ddda5ce4cbd755671d",
          "name": "Jianfeng Gao",
          "hidden": false
        },
        {
          "_id": "681187ddda5ce4cbd755671e",
          "name": "Weizhu Chen",
          "hidden": false
        },
        {
          "_id": "681187ddda5ce4cbd755671f",
          "name": "Shuohang Wang",
          "hidden": false
        },
        {
          "_id": "681187ddda5ce4cbd7556720",
          "name": "Simon Shaolei Du",
          "hidden": false
        },
        {
          "_id": "681187ddda5ce4cbd7556721",
          "name": "Yelong Shen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-29T09:24:30.000Z",
      "submittedOnDailyAt": "2025-04-30T00:46:23.617Z",
      "title": "Reinforcement Learning for Reasoning in Large Language Models with One\n  Training Example",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": false,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "We show that reinforcement learning with verifiable reward using one training\nexample (1-shot RLVR) is effective in incentivizing the math reasoning\ncapabilities of large language models (LLMs). Applying RLVR to the base model\nQwen2.5-Math-1.5B, we identify a single example that elevates model performance\non MATH500 from 36.0% to 73.6%, and improves the average performance across six\ncommon mathematical reasoning benchmarks from 17.6% to 35.7%. This result\nmatches the performance obtained using the 1.2k DeepScaleR subset (MATH500:\n73.6%, average: 35.9%), which includes the aforementioned example. Similar\nsubstantial improvements are observed across various models (Qwen2.5-Math-7B,\nLlama3.2-3B-Instruct, DeepSeek-R1-Distill-Qwen-1.5B), RL algorithms (GRPO and\nPPO), and different math examples (many of which yield approximately 30% or\ngreater improvement on MATH500 when employed as a single training example). In\naddition, we identify some interesting phenomena during 1-shot RLVR, including\ncross-domain generalization, increased frequency of self-reflection, and\nsustained test performance improvement even after the training accuracy has\nsaturated, a phenomenon we term post-saturation generalization. Moreover, we\nverify that the effectiveness of 1-shot RLVR primarily arises from the policy\ngradient loss, distinguishing it from the \"grokking\" phenomenon. We also show\nthe critical role of promoting exploration (e.g., by adding entropy loss with\nan appropriate coefficient) in 1-shot RLVR training. As a bonus, we observe\nthat applying entropy loss alone, without any outcome reward, significantly\nenhances Qwen2.5-Math-1.5B's performance on MATH500 by 27.4%. These findings\ncan inspire future work on RLVR data efficiency and encourage a re-examination\nof both recent progress and the underlying mechanisms in RLVR. Our code, model,\nand data are open source at https://github.com/ypwang61/One-Shot-RLVR",
      "upvotes": 0,
      "discussionId": "681187ddda5ce4cbd7556754"
    },
    "publishedAt": "2025-04-29T05:24:30.000Z",
    "title": "Reinforcement Learning for Reasoning in Large Language Models with One\n  Training Example",
    "summary": "We show that reinforcement learning with verifiable reward using one training\nexample (1-shot RLVR) is effective in incentivizing the math reasoning\ncapabilities of large language models (LLMs). Applying RLVR to the base model\nQwen2.5-Math-1.5B, we identify a single example that elevates model performance\non MATH500 from 36.0% to 73.6%, and improves the average performance across six\ncommon mathematical reasoning benchmarks from 17.6% to 35.7%. This result\nmatches the performance obtained using the 1.2k DeepScaleR subset (MATH500:\n73.6%, average: 35.9%), which includes the aforementioned example. Similar\nsubstantial improvements are observed across various models (Qwen2.5-Math-7B,\nLlama3.2-3B-Instruct, DeepSeek-R1-Distill-Qwen-1.5B), RL algorithms (GRPO and\nPPO), and different math examples (many of which yield approximately 30% or\ngreater improvement on MATH500 when employed as a single training example). In\naddition, we identify some interesting phenomena during 1-shot RLVR, including\ncross-domain generalization, increased frequency of self-reflection, and\nsustained test performance improvement even after the training accuracy has\nsaturated, a phenomenon we term post-saturation generalization. Moreover, we\nverify that the effectiveness of 1-shot RLVR primarily arises from the policy\ngradient loss, distinguishing it from the \"grokking\" phenomenon. We also show\nthe critical role of promoting exploration (e.g., by adding entropy loss with\nan appropriate coefficient) in 1-shot RLVR training. As a bonus, we observe\nthat applying entropy loss alone, without any outcome reward, significantly\nenhances Qwen2.5-Math-1.5B's performance on MATH500 by 27.4%. These findings\ncan inspire future work on RLVR data efficiency and encourage a re-examination\nof both recent progress and the underlying mechanisms in RLVR. Our code, model,\nand data are open source at https://github.com/ypwang61/One-Shot-RLVR",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.20571.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6745
    },
    "isAuthorParticipating": false
  }
]