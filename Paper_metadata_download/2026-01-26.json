[
  {
    "paper": {
      "id": "2601.14133",
      "authors": [
        {
          "_id": "69706e6ea8be625b19c2afac",
          "user": {
            "_id": "63d3b5f1640bb0f77173baea",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674819020331-noauth.jpeg",
            "isPro": false,
            "fullname": "yubin",
            "user": "VLyb",
            "type": "user"
          },
          "name": "Bin Yu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-01-21T09:19:14.460Z",
          "hidden": false
        },
        {
          "_id": "69706e6ea8be625b19c2afad",
          "user": {
            "_id": "65ec01fd770aa0e25d9374dc",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65ec01fd770aa0e25d9374dc/yvLWwBEdAdHb-8EdUHg3n.jpeg",
            "isPro": false,
            "fullname": "Shijie Lian",
            "user": "LiamLian0727",
            "type": "user"
          },
          "name": "Shijie Lian",
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-01-22T08:47:15.246Z",
          "hidden": false
        },
        {
          "_id": "69706e6ea8be625b19c2afae",
          "name": "Xiaopeng Lin",
          "hidden": false
        },
        {
          "_id": "69706e6ea8be625b19c2afaf",
          "name": "Yuliang Wei",
          "hidden": false
        },
        {
          "_id": "69706e6ea8be625b19c2afb0",
          "name": "Zhaolong Shen",
          "hidden": false
        },
        {
          "_id": "69706e6ea8be625b19c2afb1",
          "name": "Changti Wu",
          "hidden": false
        },
        {
          "_id": "69706e6ea8be625b19c2afb2",
          "name": "Yuzhuo Miao",
          "hidden": false
        },
        {
          "_id": "69706e6ea8be625b19c2afb3",
          "name": "Xinming Wang",
          "hidden": false
        },
        {
          "_id": "69706e6ea8be625b19c2afb4",
          "name": "Bailing Wang",
          "hidden": false
        },
        {
          "_id": "69706e6ea8be625b19c2afb5",
          "name": "Cong Huang",
          "hidden": false
        },
        {
          "_id": "69706e6ea8be625b19c2afb6",
          "name": "Kai Chen",
          "hidden": false
        }
      ],
      "publishedAt": "2026-01-20T16:30:07.000Z",
      "submittedOnDailyAt": "2026-01-26T00:14:31.380Z",
      "title": "TwinBrainVLA: Unleashing the Potential of Generalist VLMs for Embodied Tasks via Asymmetric Mixture-of-Transformers",
      "submittedOnDailyBy": {
        "_id": "63d3b5f1640bb0f77173baea",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674819020331-noauth.jpeg",
        "isPro": false,
        "fullname": "yubin",
        "user": "VLyb",
        "type": "user"
      },
      "summary": "Standard Vision-Language-Action (VLA) models typically fine-tune a monolithic Vision-Language Model (VLM) backbone explicitly for robotic control. However, this approach creates a critical tension between maintaining high-level general semantic understanding and learning low-level, fine-grained sensorimotor skills, often leading to \"catastrophic forgetting\" of the model's open-world capabilities. To resolve this conflict, we introduce TwinBrainVLA, a novel architecture that coordinates a generalist VLM retaining universal semantic understanding and a specialist VLM dedicated to embodied proprioception for joint robotic control. TwinBrainVLA synergizes a frozen \"Left Brain\", which retains robust general visual reasoning, with a trainable \"Right Brain\", specialized for embodied perception, via a novel Asymmetric Mixture-of-Transformers (AsyMoT) mechanism. This design allows the Right Brain to dynamically query semantic knowledge from the frozen Left Brain and fuse it with proprioceptive states, providing rich conditioning for a Flow-Matching Action Expert to generate precise continuous controls. Extensive experiments on SimplerEnv and RoboCasa benchmarks demonstrate that TwinBrainVLA achieves superior manipulation performance compared to state-of-the-art baselines while explicitly preserving the comprehensive visual understanding capabilities of the pre-trained VLM, offering a promising direction for building general-purpose robots that simultaneously achieve high-level semantic understanding and low-level physical dexterity.",
      "upvotes": 25,
      "discussionId": "69706e6ea8be625b19c2afb7",
      "githubRepo": "https://github.com/ZGC-EmbodyAI/TwinBrainVLA",
      "githubRepoAddedBy": "user",
      "ai_summary": "TwinBrainVLA addresses the tension between semantic understanding and motor skills in robot control by coordinating a generalist vision-language model with a specialist model through an asymmetric mixture-of-transformers mechanism.",
      "ai_keywords": [
        "Vision-Language-Action models",
        "Vision-Language Models",
        "robotic control",
        "catastrophic forgetting",
        "frozen Left Brain",
        "trainable Right Brain",
        "Asymmetric Mixture-of-Transformers",
        "Flow-Matching Action Expert",
        "embodied perception",
        "proprioception"
      ],
      "githubStars": 8,
      "organization": {
        "_id": "68896d3a716ee5bfb1428441",
        "name": "ZGCA",
        "fullname": "Zhongguancun Academy",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6854c3ab09a3ba7d16243875/aZ3tp3lZk1yQoXDwSklye.png"
      }
    },
    "publishedAt": "2026-01-20T11:30:07.000Z",
    "title": "TwinBrainVLA: Unleashing the Potential of Generalist VLMs for Embodied Tasks via Asymmetric Mixture-of-Transformers",
    "summary": "Standard Vision-Language-Action (VLA) models typically fine-tune a monolithic Vision-Language Model (VLM) backbone explicitly for robotic control. However, this approach creates a critical tension between maintaining high-level general semantic understanding and learning low-level, fine-grained sensorimotor skills, often leading to \"catastrophic forgetting\" of the model's open-world capabilities. To resolve this conflict, we introduce TwinBrainVLA, a novel architecture that coordinates a generalist VLM retaining universal semantic understanding and a specialist VLM dedicated to embodied proprioception for joint robotic control. TwinBrainVLA synergizes a frozen \"Left Brain\", which retains robust general visual reasoning, with a trainable \"Right Brain\", specialized for embodied perception, via a novel Asymmetric Mixture-of-Transformers (AsyMoT) mechanism. This design allows the Right Brain to dynamically query semantic knowledge from the frozen Left Brain and fuse it with proprioceptive states, providing rich conditioning for a Flow-Matching Action Expert to generate precise continuous controls. Extensive experiments on SimplerEnv and RoboCasa benchmarks demonstrate that TwinBrainVLA achieves superior manipulation performance compared to state-of-the-art baselines while explicitly preserving the comprehensive visual understanding capabilities of the pre-trained VLM, offering a promising direction for building general-purpose robots that simultaneously achieve high-level semantic understanding and low-level physical dexterity.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.14133.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63d3b5f1640bb0f77173baea",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674819020331-noauth.jpeg",
      "fullname": "yubin",
      "name": "VLyb",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "68896d3a716ee5bfb1428441",
      "name": "ZGCA",
      "fullname": "Zhongguancun Academy",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6854c3ab09a3ba7d16243875/aZ3tp3lZk1yQoXDwSklye.png"
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2601.16973",
      "authors": [
        {
          "_id": "6976d4695d41524304c13523",
          "name": "Zirui Wang",
          "hidden": false
        },
        {
          "_id": "6976d4695d41524304c13524",
          "name": "Junyi Zhang",
          "hidden": false
        },
        {
          "_id": "6976d4695d41524304c13525",
          "name": "Jiaxin Ge",
          "hidden": false
        },
        {
          "_id": "6976d4695d41524304c13526",
          "name": "Long Lian",
          "hidden": false
        },
        {
          "_id": "6976d4695d41524304c13527",
          "name": "Letian Fu",
          "hidden": false
        },
        {
          "_id": "6976d4695d41524304c13528",
          "name": "Lisa Dunlap",
          "hidden": false
        },
        {
          "_id": "6976d4695d41524304c13529",
          "name": "Ken Goldberg",
          "hidden": false
        },
        {
          "_id": "6976d4695d41524304c1352a",
          "name": "XuDong Wang",
          "hidden": false
        },
        {
          "_id": "6976d4695d41524304c1352b",
          "name": "Ion Stoica",
          "hidden": false
        },
        {
          "_id": "6976d4695d41524304c1352c",
          "name": "David M. Chan",
          "hidden": false
        },
        {
          "_id": "6976d4695d41524304c1352d",
          "name": "Sewon Min",
          "hidden": false
        },
        {
          "_id": "6976d4695d41524304c1352e",
          "name": "Joseph E. Gonzalez",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/641a38fdfb5ffff5ac78ceb0/sC7mJkpqiaPhEB6RZvHAY.mp4"
      ],
      "publishedAt": "2026-01-23T18:43:34.000Z",
      "submittedOnDailyAt": "2026-01-26T00:14:43.910Z",
      "title": "VisGym: Diverse, Customizable, Scalable Environments for Multimodal Agents",
      "submittedOnDailyBy": {
        "_id": "641a38fdfb5ffff5ac78ceb0",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/641a38fdfb5ffff5ac78ceb0/53nGstZ9Ya5WfeGdXAXCr.png",
        "isPro": true,
        "fullname": "Zirui Wang",
        "user": "zwcolin",
        "type": "user"
      },
      "summary": "Modern Vision-Language Models (VLMs) remain poorly characterized in multi-step visual interactions, particularly in how they integrate perception, memory, and action over long horizons. We introduce VisGym, a gymnasium of 17 environments for evaluating and training VLMs. The suite spans symbolic puzzles, real-image understanding, navigation, and manipulation, and provides flexible controls over difficulty, input representation, planning horizon, and feedback. We also provide multi-step solvers that generate structured demonstrations, enabling supervised finetuning. Our evaluations show that all frontier models struggle in interactive settings, achieving low success rates in both the easy (46.6%) and hard (26.0%) configurations. Our experiments reveal notable limitations: models struggle to effectively leverage long context, performing worse with an unbounded history than with truncated windows. Furthermore, we find that several text-based symbolic tasks become substantially harder once rendered visually. However, explicit goal observations, textual feedback, and exploratory demonstrations in partially observable or unknown-dynamics settings for supervised finetuning yield consistent gains, highlighting concrete failure modes and pathways for improving multi-step visual decision-making. Code, data, and models can be found at: https://visgym.github.io/.",
      "upvotes": 7,
      "discussionId": "6976d46a5d41524304c1352f",
      "projectPage": "https://visgym.github.io/",
      "ai_summary": "Modern vision-language models exhibit significant challenges in multi-step visual interaction tasks, particularly in long-horizon perception-memory-action integration, with performance declining when handling unbounded historical contexts.",
      "ai_keywords": [
        "vision-language models",
        "multi-step visual interactions",
        "perception",
        "memory",
        "action",
        "symbolic puzzles",
        "real-image understanding",
        "navigation",
        "manipulation",
        "supervised fine-tuning",
        "goal observations",
        "textual feedback",
        "exploratory demonstrations",
        "partially observable environments",
        "unknown-dynamics settings"
      ],
      "organization": {
        "_id": "66c3b97cd7a9770138e4ce9a",
        "name": "UCB-Sky-Computing-Lab",
        "fullname": "Sky Computing Lab",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66c3740cc55655c7155c47ab/0ZqaCA_kmq5k6ahdnsIJ1.png"
      }
    },
    "publishedAt": "2026-01-23T13:43:34.000Z",
    "title": "VisGym: Diverse, Customizable, Scalable Environments for Multimodal Agents",
    "summary": "Modern Vision-Language Models (VLMs) remain poorly characterized in multi-step visual interactions, particularly in how they integrate perception, memory, and action over long horizons. We introduce VisGym, a gymnasium of 17 environments for evaluating and training VLMs. The suite spans symbolic puzzles, real-image understanding, navigation, and manipulation, and provides flexible controls over difficulty, input representation, planning horizon, and feedback. We also provide multi-step solvers that generate structured demonstrations, enabling supervised finetuning. Our evaluations show that all frontier models struggle in interactive settings, achieving low success rates in both the easy (46.6%) and hard (26.0%) configurations. Our experiments reveal notable limitations: models struggle to effectively leverage long context, performing worse with an unbounded history than with truncated windows. Furthermore, we find that several text-based symbolic tasks become substantially harder once rendered visually. However, explicit goal observations, textual feedback, and exploratory demonstrations in partially observable or unknown-dynamics settings for supervised finetuning yield consistent gains, highlighting concrete failure modes and pathways for improving multi-step visual decision-making. Code, data, and models can be found at: https://visgym.github.io/.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/641a38fdfb5ffff5ac78ceb0/sC7mJkpqiaPhEB6RZvHAY.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.16973.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "641a38fdfb5ffff5ac78ceb0",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/641a38fdfb5ffff5ac78ceb0/53nGstZ9Ya5WfeGdXAXCr.png",
      "fullname": "Zirui Wang",
      "name": "zwcolin",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "66c3b97cd7a9770138e4ce9a",
      "name": "UCB-Sky-Computing-Lab",
      "fullname": "Sky Computing Lab",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66c3740cc55655c7155c47ab/0ZqaCA_kmq5k6ahdnsIJ1.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2601.16725",
      "authors": [
        {
          "_id": "6976d5405d41524304c13537",
          "name": "Meituan LongCat Team",
          "hidden": false
        },
        {
          "_id": "6976d5405d41524304c13538",
          "name": "Anchun Gui",
          "hidden": false
        },
        {
          "_id": "6976d5405d41524304c13539",
          "name": "Bei Li",
          "hidden": false
        },
        {
          "_id": "6976d5405d41524304c1353a",
          "name": "Bingyang Tao",
          "hidden": false
        },
        {
          "_id": "6976d5405d41524304c1353b",
          "name": "Bole Zhou",
          "hidden": false
        },
        {
          "_id": "6976d5405d41524304c1353c",
          "name": "Borun Chen",
          "hidden": false
        },
        {
          "_id": "6976d5405d41524304c1353d",
          "name": "Chao Zhang",
          "hidden": false
        },
        {
          "_id": "6976d5405d41524304c1353e",
          "name": "Chao Zhang",
          "hidden": false
        },
        {
          "_id": "6976d5405d41524304c1353f",
          "name": "Chen Gao",
          "hidden": false
        },
        {
          "_id": "6976d5405d41524304c13540",
          "name": "Chen Zhang",
          "hidden": false
        },
        {
          "_id": "6976d5405d41524304c13541",
          "name": "Chengcheng Han",
          "hidden": false
        },
        {
          "_id": "6976d5405d41524304c13542",
          "name": "Chenhui Yang",
          "hidden": false
        },
        {
          "_id": "6976d5405d41524304c13543",
          "name": "Chuyu Zhang",
          "hidden": false
        },
        {
          "_id": "6976d5405d41524304c13544",
          "name": "Cong Chen",
          "hidden": false
        },
        {
          "_id": "6976d5405d41524304c13545",
          "name": "Cunguang Wang",
          "hidden": false
        },
        {
          "_id": "6976d5405d41524304c13546",
          "name": "Daoru Pan",
          "hidden": false
        },
        {
          "_id": "6976d5405d41524304c13547",
          "name": "Defei Bu",
          "hidden": false
        },
        {
          "_id": "6976d5405d41524304c13548",
          "name": "Dengchang Zhao",
          "hidden": false
        },
        {
          "_id": "6976d5405d41524304c13549",
          "name": "Di Xiu",
          "hidden": false
        },
        {
          "_id": "6976d5405d41524304c1354a",
          "name": "Dishan Liu",
          "hidden": false
        },
        {
          "_id": "6976d5405d41524304c1354b",
          "name": "Dongyu Ru",
          "hidden": false
        },
        {
          "_id": "6976d5405d41524304c1354c",
          "name": "Dunwei Tu",
          "hidden": false
        },
        {
          "_id": "6976d5405d41524304c1354d",
          "name": "Fan Wu",
          "hidden": false
        },
        {
          "_id": "6976d5405d41524304c1354e",
          "name": "Fengcheng Yuan",
          "hidden": false
        },
        {
          "_id": "6976d5405d41524304c1354f",
          "name": "Fengcun Li",
          "hidden": false
        },
        {
          "_id": "6976d5405d41524304c13550",
          "name": "Gang Xu",
          "hidden": false
        },
        {
          "_id": "6976d5405d41524304c13551",
          "name": "Guanyu Wu",
          "hidden": false
        },
        {
          "_id": "6976d5405d41524304c13552",
          "name": "Guoyuan Lin",
          "hidden": false
        },
        {
          "_id": "6976d5405d41524304c13553",
          "name": "Haibin Wang",
          "hidden": false
        },
        {
          "_id": "6976d5405d41524304c13554",
          "name": "Hansi Yang",
          "hidden": false
        },
        {
          "_id": "6976d5405d41524304c13555",
          "name": "Hao Yang",
          "hidden": false
        },
        {
          "_id": "6976d5405d41524304c13556",
          "name": "Haonan Yan",
          "hidden": false
        },
        {
          "_id": "6976d5405d41524304c13557",
          "name": "Haoxiang Ma",
          "hidden": false
        },
        {
          "_id": "6976d5405d41524304c13558",
          "name": "Haoxing Wen",
          "hidden": false
        },
        {
          "_id": "6976d5405d41524304c13559",
          "name": "Hongyan Hao",
          "hidden": false
        },
        {
          "_id": "6976d5405d41524304c1355a",
          "name": "Hongyin Tang",
          "hidden": false
        },
        {
          "_id": "6976d5405d41524304c1355b",
          "name": "Hongyu Zang",
          "hidden": false
        },
        {
          "_id": "6976d5405d41524304c1355c",
          "name": "Hongzhi Ni",
          "hidden": false
        },
        {
          "_id": "6976d5405d41524304c1355d",
          "name": "Hui Su",
          "hidden": false
        },
        {
          "_id": "6976d5405d41524304c1355e",
          "name": "Jiacheng Zhang",
          "hidden": false
        },
        {
          "_id": "6976d5405d41524304c1355f",
          "name": "Jiahong Zhou",
          "hidden": false
        },
        {
          "_id": "6976d5405d41524304c13560",
          "name": "Jiahuan Li",
          "hidden": false
        },
        {
          "_id": "6976d5405d41524304c13561",
          "name": "Jiaming Wang",
          "hidden": false
        },
        {
          "_id": "6976d5405d41524304c13562",
          "name": "Jian Yang",
          "hidden": false
        },
        {
          "_id": "6976d5405d41524304c13563",
          "name": "Jianfei Zhang",
          "hidden": false
        },
        {
          "_id": "6976d5405d41524304c13564",
          "name": "Jianhao Xu",
          "hidden": false
        },
        {
          "_id": "6976d5405d41524304c13565",
          "name": "Jianing Wang",
          "hidden": false
        },
        {
          "_id": "6976d5405d41524304c13566",
          "name": "Jiapeng Zhu",
          "hidden": false
        },
        {
          "_id": "6976d5405d41524304c13567",
          "name": "Jiaqi Sun",
          "hidden": false
        },
        {
          "_id": "6976d5405d41524304c13568",
          "name": "Jiarong Shi",
          "hidden": false
        },
        {
          "_id": "6976d5405d41524304c13569",
          "name": "Jiarui Zhao",
          "hidden": false
        },
        {
          "_id": "6976d5405d41524304c1356a",
          "name": "Jingang Wang",
          "hidden": false
        },
        {
          "_id": "6976d5405d41524304c1356b",
          "name": "Jinluan Yang",
          "hidden": false
        },
        {
          "_id": "6976d5405d41524304c1356c",
          "name": "Jinrui Ding",
          "hidden": false
        },
        {
          "_id": "6976d5405d41524304c1356d",
          "name": "Jinwei Xiao",
          "hidden": false
        },
        {
          "_id": "6976d5405d41524304c1356e",
          "name": "Jiyuan He",
          "hidden": false
        },
        {
          "_id": "6976d5405d41524304c1356f",
          "name": "Juncan Xu",
          "hidden": false
        },
        {
          "_id": "6976d5405d41524304c13570",
          "name": "Kefeng Zhang",
          "hidden": false
        },
        {
          "_id": "6976d5405d41524304c13571",
          "name": "Keheng Wang",
          "hidden": false
        },
        {
          "_id": "6976d5405d41524304c13572",
          "name": "Li Wei",
          "hidden": false
        },
        {
          "_id": "6976d5405d41524304c13573",
          "name": "Lianhui Ma",
          "hidden": false
        },
        {
          "_id": "6976d5405d41524304c13574",
          "name": "Lin Qiu",
          "hidden": false
        },
        {
          "_id": "6976d5405d41524304c13575",
          "name": "Lingbing Kong",
          "hidden": false
        },
        {
          "_id": "6976d5405d41524304c13576",
          "name": "Lingchuan Liu",
          "hidden": false
        },
        {
          "_id": "6976d5405d41524304c13577",
          "name": "Linsen Guo",
          "hidden": false
        },
        {
          "_id": "6976d5405d41524304c13578",
          "name": "Mengshen Zhu",
          "hidden": false
        },
        {
          "_id": "6976d5405d41524304c13579",
          "name": "Mengxia Shen",
          "hidden": false
        },
        {
          "_id": "6976d5405d41524304c1357a",
          "name": "Mingyang Zhu",
          "hidden": false
        },
        {
          "_id": "6976d5405d41524304c1357b",
          "name": "Peiguang Li",
          "hidden": false
        },
        {
          "_id": "6976d5405d41524304c1357c",
          "name": "Peng Pei",
          "hidden": false
        },
        {
          "_id": "6976d5405d41524304c1357d",
          "name": "Pengcheng Jia",
          "hidden": false
        },
        {
          "_id": "6976d5405d41524304c1357e",
          "name": "Pengtao Zhang",
          "hidden": false
        },
        {
          "_id": "6976d5405d41524304c1357f",
          "name": "Peng Zhao",
          "hidden": false
        },
        {
          "_id": "6976d5405d41524304c13580",
          "name": "Qi Gu",
          "hidden": false
        },
        {
          "_id": "6976d5405d41524304c13581",
          "name": "Qiong Huang",
          "hidden": false
        },
        {
          "_id": "6976d5405d41524304c13582",
          "name": "Qiyuan Duan",
          "hidden": false
        },
        {
          "_id": "6976d5405d41524304c13583",
          "name": "Quanchi Weng",
          "hidden": false
        },
        {
          "_id": "6976d5405d41524304c13584",
          "name": "Rongxiang Weng",
          "hidden": false
        },
        {
          "_id": "6976d5405d41524304c13585",
          "name": "Rongzhi Zhang",
          "hidden": false
        },
        {
          "_id": "6976d5405d41524304c13586",
          "name": "Rumei Li",
          "hidden": false
        },
        {
          "_id": "6976d5405d41524304c13587",
          "name": "Shanglin Lei",
          "hidden": false
        },
        {
          "_id": "6976d5405d41524304c13588",
          "name": "Shengnan An",
          "hidden": false
        },
        {
          "_id": "6976d5405d41524304c13589",
          "name": "Shijun Dai",
          "hidden": false
        },
        {
          "_id": "6976d5405d41524304c1358a",
          "name": "Shuaikang Liu",
          "hidden": false
        },
        {
          "_id": "6976d5405d41524304c1358b",
          "name": "Shuang Zhou",
          "hidden": false
        },
        {
          "_id": "6976d5405d41524304c1358c",
          "name": "Shuo Wang",
          "hidden": false
        },
        {
          "_id": "6976d5405d41524304c1358d",
          "name": "Songyuan Zhao",
          "hidden": false
        },
        {
          "_id": "6976d5405d41524304c1358e",
          "name": "Tao Liang",
          "hidden": false
        },
        {
          "_id": "6976d5405d41524304c1358f",
          "name": "Tianhao Hu",
          "hidden": false
        },
        {
          "_id": "6976d5405d41524304c13590",
          "name": "Tianze Chen",
          "hidden": false
        },
        {
          "_id": "6976d5405d41524304c13591",
          "name": "Wei Liu",
          "hidden": false
        },
        {
          "_id": "6976d5405d41524304c13592",
          "name": "Wei Shi",
          "hidden": false
        },
        {
          "_id": "6976d5405d41524304c13593",
          "name": "Wei Wang",
          "hidden": false
        },
        {
          "_id": "6976d5405d41524304c13594",
          "name": "Weifeng Tang",
          "hidden": false
        },
        {
          "_id": "6976d5405d41524304c13595",
          "name": "Wenjie Shi",
          "hidden": false
        },
        {
          "_id": "6976d5405d41524304c13596",
          "name": "Wenlong Zhu",
          "hidden": false
        },
        {
          "_id": "6976d5405d41524304c13597",
          "name": "Wentao Chen",
          "hidden": false
        },
        {
          "_id": "6976d5405d41524304c13598",
          "name": "Wentao Shi",
          "hidden": false
        },
        {
          "_id": "6976d5405d41524304c13599",
          "name": "Xi Su",
          "hidden": false
        },
        {
          "_id": "6976d5405d41524304c1359a",
          "name": "Xiangcheng Liu",
          "hidden": false
        },
        {
          "_id": "6976d5405d41524304c1359b",
          "name": "Xiandi Ma",
          "hidden": false
        },
        {
          "_id": "6976d5405d41524304c1359c",
          "name": "Xiangyu Xi",
          "hidden": false
        },
        {
          "_id": "6976d5405d41524304c1359d",
          "name": "Xiangyuan Liu",
          "hidden": false
        },
        {
          "_id": "6976d5405d41524304c1359e",
          "name": "Xiangzhou Huang",
          "hidden": false
        },
        {
          "_id": "6976d5405d41524304c1359f",
          "name": "Xiao Liu",
          "hidden": false
        },
        {
          "_id": "6976d5405d41524304c135a0",
          "name": "Xiaodong Cai",
          "hidden": false
        },
        {
          "_id": "6976d5405d41524304c135a1",
          "name": "Xiaolong Chen",
          "hidden": false
        },
        {
          "_id": "6976d5405d41524304c135a2",
          "name": "Xiaowei Shi",
          "hidden": false
        },
        {
          "_id": "6976d5405d41524304c135a3",
          "name": "Xiaoyu Li",
          "hidden": false
        },
        {
          "_id": "6976d5405d41524304c135a4",
          "name": "Xin Chen",
          "hidden": false
        },
        {
          "_id": "6976d5405d41524304c135a5",
          "name": "Xingchen Liu",
          "hidden": false
        },
        {
          "_id": "6976d5405d41524304c135a6",
          "name": "Xuan Huang",
          "hidden": false
        },
        {
          "_id": "6976d5405d41524304c135a7",
          "name": "Xuezhi Cao",
          "hidden": false
        },
        {
          "_id": "6976d5405d41524304c135a8",
          "name": "Xunliang Cai",
          "hidden": false
        },
        {
          "_id": "6976d5405d41524304c135a9",
          "name": "Yan Chen",
          "hidden": false
        },
        {
          "_id": "6976d5405d41524304c135aa",
          "name": "Yang Bai",
          "hidden": false
        },
        {
          "_id": "6976d5405d41524304c135ab",
          "name": "Yang Liu",
          "hidden": false
        },
        {
          "_id": "6976d5405d41524304c135ac",
          "name": "Yang Yang",
          "hidden": false
        },
        {
          "_id": "6976d5405d41524304c135ad",
          "name": "Yang Zheng",
          "hidden": false
        },
        {
          "_id": "6976d5405d41524304c135ae",
          "name": "Yaoming Wang",
          "hidden": false
        },
        {
          "_id": "6976d5405d41524304c135af",
          "name": "Yaoming Zhu",
          "hidden": false
        },
        {
          "_id": "6976d5405d41524304c135b0",
          "name": "Yaqi Huo",
          "hidden": false
        },
        {
          "_id": "6976d5405d41524304c135b1",
          "name": "Yanyu Chen",
          "hidden": false
        },
        {
          "_id": "6976d5405d41524304c135b2",
          "name": "Yaorui Shi",
          "hidden": false
        },
        {
          "_id": "6976d5405d41524304c135b3",
          "name": "Yerui Sun",
          "hidden": false
        },
        {
          "_id": "6976d5405d41524304c135b4",
          "name": "Yi Zhang",
          "hidden": false
        },
        {
          "_id": "6976d5405d41524304c135b5",
          "name": "Yihao Chen",
          "hidden": false
        },
        {
          "_id": "6976d5405d41524304c135b6",
          "name": "Yi-Kai Zhang",
          "hidden": false
        },
        {
          "_id": "6976d5405d41524304c135b7",
          "name": "Yifan Lu",
          "hidden": false
        },
        {
          "_id": "6976d5405d41524304c135b8",
          "name": "Yifan Zhao",
          "hidden": false
        },
        {
          "_id": "6976d5405d41524304c135b9",
          "name": "Yitao Zhai",
          "hidden": false
        },
        {
          "_id": "6976d5405d41524304c135ba",
          "name": "Yongjing Yin",
          "hidden": false
        },
        {
          "_id": "6976d5405d41524304c135bb",
          "name": "Yongwei Zhou",
          "hidden": false
        },
        {
          "_id": "6976d5405d41524304c135bc",
          "name": "Youshao Xiao",
          "hidden": false
        },
        {
          "_id": "6976d5405d41524304c135bd",
          "name": "Yuchuan Dai",
          "hidden": false
        },
        {
          "_id": "6976d5405d41524304c135be",
          "name": "Yuchen Xie",
          "hidden": false
        },
        {
          "_id": "6976d5405d41524304c135bf",
          "name": "Yuchen Yu",
          "hidden": false
        },
        {
          "_id": "6976d5405d41524304c135c0",
          "name": "Yufei Zhang",
          "hidden": false
        },
        {
          "_id": "6976d5405d41524304c135c1",
          "name": "Yuhuai Wei",
          "hidden": false
        },
        {
          "_id": "6976d5405d41524304c135c2",
          "name": "Yulei Qian",
          "hidden": false
        },
        {
          "_id": "6976d5405d41524304c135c3",
          "name": "Yunfan Liang",
          "hidden": false
        },
        {
          "_id": "6976d5405d41524304c135c4",
          "name": "Yunke Zhao",
          "hidden": false
        },
        {
          "_id": "6976d5405d41524304c135c5",
          "name": "Yuwei Jiang",
          "hidden": false
        },
        {
          "_id": "6976d5405d41524304c135c6",
          "name": "Yuxin Bian",
          "hidden": false
        },
        {
          "_id": "6976d5405d41524304c135c7",
          "name": "Yuxin Chen",
          "hidden": false
        },
        {
          "_id": "6976d5405d41524304c135c8",
          "name": "Yuxin Liu",
          "hidden": false
        },
        {
          "_id": "6976d5405d41524304c135c9",
          "name": "Yue Xu",
          "hidden": false
        },
        {
          "_id": "6976d5405d41524304c135ca",
          "name": "Yueqing Sun",
          "hidden": false
        },
        {
          "_id": "6976d5405d41524304c135cb",
          "name": "Zeyang Yu",
          "hidden": false
        },
        {
          "_id": "6976d5405d41524304c135cc",
          "name": "Zhao Yang",
          "hidden": false
        },
        {
          "_id": "6976d5405d41524304c135cd",
          "name": "Zhengsheng Huang",
          "hidden": false
        },
        {
          "_id": "6976d5405d41524304c135ce",
          "name": "Zhengyu Chen",
          "hidden": false
        },
        {
          "_id": "6976d5405d41524304c135cf",
          "name": "Zhijian Liu",
          "hidden": false
        },
        {
          "_id": "6976d5405d41524304c135d0",
          "name": "Zhikang Xia",
          "hidden": false
        },
        {
          "_id": "6976d5405d41524304c135d1",
          "name": "Zhimin Lin",
          "hidden": false
        },
        {
          "_id": "6976d5405d41524304c135d2",
          "name": "Zhiyuan Yao",
          "hidden": false
        },
        {
          "_id": "6976d5405d41524304c135d3",
          "name": "Zhuofan Chen",
          "hidden": false
        },
        {
          "_id": "6976d5405d41524304c135d4",
          "name": "Zhuowen Han",
          "hidden": false
        },
        {
          "_id": "6976d5405d41524304c135d5",
          "name": "Zijian Zhang",
          "hidden": false
        },
        {
          "_id": "6976d5405d41524304c135d6",
          "name": "Ziran Li",
          "hidden": false
        },
        {
          "_id": "6976d5405d41524304c135d7",
          "name": "Ziwen Wang",
          "hidden": false
        },
        {
          "_id": "6976d5405d41524304c135d8",
          "name": "Ziyuan Zhuang",
          "hidden": false
        }
      ],
      "publishedAt": "2026-01-23T13:20:09.000Z",
      "submittedOnDailyAt": "2026-01-26T00:15:28.340Z",
      "title": "LongCat-Flash-Thinking-2601 Technical Report",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "We introduce LongCat-Flash-Thinking-2601, a 560-billion-parameter open-source Mixture-of-Experts (MoE) reasoning model with superior agentic reasoning capability. LongCat-Flash-Thinking-2601 achieves state-of-the-art performance among open-source models on a wide range of agentic benchmarks, including agentic search, agentic tool use, and tool-integrated reasoning. Beyond benchmark performance, the model demonstrates strong generalization to complex tool interactions and robust behavior under noisy real-world environments. Its advanced capability stems from a unified training framework that combines domain-parallel expert training with subsequent fusion, together with an end-to-end co-design of data construction, environments, algorithms, and infrastructure spanning from pre-training to post-training. In particular, the model's strong generalization capability in complex tool-use are driven by our in-depth exploration of environment scaling and principled task construction. To optimize long-tailed, skewed generation and multi-turn agentic interactions, and to enable stable training across over 10,000 environments spanning more than 20 domains, we systematically extend our asynchronous reinforcement learning framework, DORA, for stable and efficient large-scale multi-environment training. Furthermore, recognizing that real-world tasks are inherently noisy, we conduct a systematic analysis and decomposition of real-world noise patterns, and design targeted training procedures to explicitly incorporate such imperfections into the training process, resulting in improved robustness for real-world applications. To further enhance performance on complex reasoning tasks, we introduce a Heavy Thinking mode that enables effective test-time scaling by jointly expanding reasoning depth and width through intensive parallel thinking.",
      "upvotes": 4,
      "discussionId": "6976d5405d41524304c135d9",
      "ai_summary": "A 560-billion-parameter Mixture-of-Experts reasoning model achieves state-of-the-art performance on agentic benchmarks through a unified training framework combining domain-parallel expert training with fusion, along with enhancements for complex tool interactions and real-world robustness.",
      "ai_keywords": [
        "Mixture-of-Experts",
        "agentic reasoning",
        "domain-parallel expert training",
        "fusion",
        "asynchronous reinforcement learning",
        "DORA",
        "long-tailed generation",
        "multi-turn agentic interactions",
        "test-time scaling",
        "reasoning depth",
        "reasoning width",
        "parallel thinking"
      ],
      "organization": {
        "_id": "68b28d79a176a9beb30d2049",
        "name": "meituan-longcat",
        "fullname": "LongCat",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/68a2a29ab9d4c5698e02c747/CDCAx7X7rXDt7xjI-DoxG.png"
      }
    },
    "publishedAt": "2026-01-23T08:20:09.000Z",
    "title": "LongCat-Flash-Thinking-2601 Technical Report",
    "summary": "We introduce LongCat-Flash-Thinking-2601, a 560-billion-parameter open-source Mixture-of-Experts (MoE) reasoning model with superior agentic reasoning capability. LongCat-Flash-Thinking-2601 achieves state-of-the-art performance among open-source models on a wide range of agentic benchmarks, including agentic search, agentic tool use, and tool-integrated reasoning. Beyond benchmark performance, the model demonstrates strong generalization to complex tool interactions and robust behavior under noisy real-world environments. Its advanced capability stems from a unified training framework that combines domain-parallel expert training with subsequent fusion, together with an end-to-end co-design of data construction, environments, algorithms, and infrastructure spanning from pre-training to post-training. In particular, the model's strong generalization capability in complex tool-use are driven by our in-depth exploration of environment scaling and principled task construction. To optimize long-tailed, skewed generation and multi-turn agentic interactions, and to enable stable training across over 10,000 environments spanning more than 20 domains, we systematically extend our asynchronous reinforcement learning framework, DORA, for stable and efficient large-scale multi-environment training. Furthermore, recognizing that real-world tasks are inherently noisy, we conduct a systematic analysis and decomposition of real-world noise patterns, and design targeted training procedures to explicitly incorporate such imperfections into the training process, resulting in improved robustness for real-world applications. To further enhance performance on complex reasoning tasks, we introduce a Heavy Thinking mode that enables effective test-time scaling by jointly expanding reasoning depth and width through intensive parallel thinking.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.16725.png",
    "numComments": 0,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 216,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "68b28d79a176a9beb30d2049",
      "name": "meituan-longcat",
      "fullname": "LongCat",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/68a2a29ab9d4c5698e02c747/CDCAx7X7rXDt7xjI-DoxG.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2601.16443",
      "authors": [
        {
          "_id": "6976d4bc5d41524304c13531",
          "name": "Kanishk Gandhi",
          "hidden": false
        },
        {
          "_id": "6976d4bc5d41524304c13532",
          "name": "Shivam Garg",
          "hidden": false
        },
        {
          "_id": "6976d4bc5d41524304c13533",
          "name": "Noah D. Goodman",
          "hidden": false
        },
        {
          "_id": "6976d4bc5d41524304c13534",
          "name": "Dimitris Papailiopoulos",
          "hidden": false
        }
      ],
      "publishedAt": "2026-01-23T04:39:55.000Z",
      "submittedOnDailyAt": "2026-01-26T00:13:12.637Z",
      "title": "Endless Terminals: Scaling RL Environments for Terminal Agents",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Environments are the bottleneck for self-improving agents. Current terminal benchmarks were built for evaluation, not training; reinforcement learning requires a scalable pipeline, not just a dataset. We introduce Endless Terminals, a fully autonomous pipeline that procedurally generates terminal-use tasks without human annotation. The pipeline has four stages: generating diverse task descriptions, building and validating containerized environments, producing completion tests, and filtering for solvability. From this pipeline we obtain 3255 tasks spanning file operations, log management, data processing, scripting, and database operations. We train agents using vanilla PPO with binary episode level rewards and a minimal interaction loop: no retrieval, multi-agent coordination, or specialized tools. Despite this simplicity, models trained on Endless Terminals show substantial gains: on our held-out dev set, Llama-3.2-3B improves from 4.0% to 18.2%, Qwen2.5-7B from 10.7% to 53.3%, and Qwen3-8B-openthinker-sft from 42.6% to 59.0%. These improvements transfer to human-curated benchmarks: models trained on Endless Terminals show substantial gains on held out human curated benchmarks: on TerminalBench 2.0, Llama-3.2-3B improves from 0.0% to 2.2%, Qwen2.5-7B from 2.2% to 3.4%, and Qwen3-8B-openthinker-sft from 1.1% to 6.7%, in each case outperforming alternative approaches including models with more complex agentic scaffolds. These results demonstrate that simple RL succeeds when environments scale.",
      "upvotes": 1,
      "discussionId": "6976d4bc5d41524304c13535",
      "githubRepo": "https://github.com/kanishkg/endless-terminals",
      "githubRepoAddedBy": "user",
      "ai_summary": "Endless Terminals introduces an autonomous pipeline for generating procedural terminal tasks that significantly improves agent performance on both synthetic and human-curated benchmarks through scalable reinforcement learning environments.",
      "ai_keywords": [
        "reinforcement learning",
        "PPO",
        "terminal benchmarks",
        "procedural generation",
        "containerized environments",
        "episode level rewards",
        "agent training",
        "scalability"
      ]
    },
    "publishedAt": "2026-01-22T23:39:55.000Z",
    "title": "Endless Terminals: Scaling RL Environments for Terminal Agents",
    "summary": "Environments are the bottleneck for self-improving agents. Current terminal benchmarks were built for evaluation, not training; reinforcement learning requires a scalable pipeline, not just a dataset. We introduce Endless Terminals, a fully autonomous pipeline that procedurally generates terminal-use tasks without human annotation. The pipeline has four stages: generating diverse task descriptions, building and validating containerized environments, producing completion tests, and filtering for solvability. From this pipeline we obtain 3255 tasks spanning file operations, log management, data processing, scripting, and database operations. We train agents using vanilla PPO with binary episode level rewards and a minimal interaction loop: no retrieval, multi-agent coordination, or specialized tools. Despite this simplicity, models trained on Endless Terminals show substantial gains: on our held-out dev set, Llama-3.2-3B improves from 4.0% to 18.2%, Qwen2.5-7B from 10.7% to 53.3%, and Qwen3-8B-openthinker-sft from 42.6% to 59.0%. These improvements transfer to human-curated benchmarks: models trained on Endless Terminals show substantial gains on held out human curated benchmarks: on TerminalBench 2.0, Llama-3.2-3B improves from 0.0% to 2.2%, Qwen2.5-7B from 2.2% to 3.4%, and Qwen3-8B-openthinker-sft from 1.1% to 6.7%, in each case outperforming alternative approaches including models with more complex agentic scaffolds. These results demonstrate that simple RL succeeds when environments scale.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.16443.png",
    "numComments": 0,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 216,
      "isUserFollowing": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2601.16344",
      "authors": [
        {
          "_id": "6976d5d25d41524304c135db",
          "name": "Fan Nie",
          "hidden": false
        },
        {
          "_id": "6976d5d25d41524304c135dc",
          "name": "Junlin Wang",
          "hidden": false
        },
        {
          "_id": "6976d5d25d41524304c135dd",
          "name": "Harper Hua",
          "hidden": false
        },
        {
          "_id": "6976d5d25d41524304c135de",
          "name": "Federico Bianchi",
          "hidden": false
        },
        {
          "_id": "6976d5d25d41524304c135df",
          "name": "Yongchan Kwon",
          "hidden": false
        },
        {
          "_id": "6976d5d25d41524304c135e0",
          "name": "Zhenting Qi",
          "hidden": false
        },
        {
          "_id": "6976d5d25d41524304c135e1",
          "name": "Owen Queen",
          "hidden": false
        },
        {
          "_id": "6976d5d25d41524304c135e2",
          "name": "Shang Zhu",
          "hidden": false
        },
        {
          "_id": "6976d5d25d41524304c135e3",
          "name": "James Zou",
          "hidden": false
        }
      ],
      "publishedAt": "2026-01-22T22:03:29.000Z",
      "submittedOnDailyAt": "2026-01-26T00:17:54.099Z",
      "title": "DSGym: A Holistic Framework for Evaluating and Training Data Science Agents",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Data science agents promise to accelerate discovery and insight-generation by turning data into executable analyses and findings. Yet existing data science benchmarks fall short due to fragmented evaluation interfaces that make cross-benchmark comparison difficult, narrow task coverage and a lack of rigorous data grounding. In particular, we show that a substantial portion of tasks in current benchmarks can be solved without using the actual data. To address these limitations, we introduce DSGym, a standardized framework for evaluating and training data science agents in self-contained execution environments. Unlike static benchmarks, DSGym provides a modular architecture that makes it easy to add tasks, agent scaffolds, and tools, positioning it as a live, extensible testbed. We curate DSGym-Tasks, a holistic task suite that standardizes and refines existing benchmarks via quality and shortcut solvability filtering. We further expand coverage with (1) DSBio: expert-derived bioinformatics tasks grounded in literature and (2) DSPredict: challenging prediction tasks spanning domains such as computer vision, molecular prediction, and single-cell perturbation. Beyond evaluation, DSGym enables agent training via execution-verified data synthesis pipeline. As a case study, we build a 2,000-example training set and trained a 4B model in DSGym that outperforms GPT-4o on standardized analysis benchmarks. Overall, DSGym enables rigorous end-to-end measurement of whether agents can plan, implement, and validate data analyses in realistic scientific context.",
      "upvotes": 1,
      "discussionId": "6976d5d35d41524304c135e4",
      "ai_summary": "DSGym presents a standardized framework for evaluating data science agents with comprehensive task suites and execution-verified training capabilities.",
      "ai_keywords": [
        "data science agents",
        "DSGym",
        "DSGym-Tasks",
        "DSBio",
        "DSPredict",
        "execution-verified data synthesis",
        "agent training",
        "benchmark evaluation"
      ]
    },
    "publishedAt": "2026-01-22T17:03:29.000Z",
    "title": "DSGym: A Holistic Framework for Evaluating and Training Data Science Agents",
    "summary": "Data science agents promise to accelerate discovery and insight-generation by turning data into executable analyses and findings. Yet existing data science benchmarks fall short due to fragmented evaluation interfaces that make cross-benchmark comparison difficult, narrow task coverage and a lack of rigorous data grounding. In particular, we show that a substantial portion of tasks in current benchmarks can be solved without using the actual data. To address these limitations, we introduce DSGym, a standardized framework for evaluating and training data science agents in self-contained execution environments. Unlike static benchmarks, DSGym provides a modular architecture that makes it easy to add tasks, agent scaffolds, and tools, positioning it as a live, extensible testbed. We curate DSGym-Tasks, a holistic task suite that standardizes and refines existing benchmarks via quality and shortcut solvability filtering. We further expand coverage with (1) DSBio: expert-derived bioinformatics tasks grounded in literature and (2) DSPredict: challenging prediction tasks spanning domains such as computer vision, molecular prediction, and single-cell perturbation. Beyond evaluation, DSGym enables agent training via execution-verified data synthesis pipeline. As a case study, we build a 2,000-example training set and trained a 4B model in DSGym that outperforms GPT-4o on standardized analysis benchmarks. Overall, DSGym enables rigorous end-to-end measurement of whether agents can plan, implement, and validate data analyses in realistic scientific context.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.16344.png",
    "numComments": 0,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 216,
      "isUserFollowing": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2601.16296",
      "authors": [
        {
          "_id": "6976d29a5d41524304c1350a",
          "name": "Dohun Lee",
          "hidden": false
        },
        {
          "_id": "6976d29a5d41524304c1350b",
          "name": "Chun-Hao Paul Huang",
          "hidden": false
        },
        {
          "_id": "6976d29a5d41524304c1350c",
          "name": "Xuelin Chen",
          "hidden": false
        },
        {
          "_id": "6976d29a5d41524304c1350d",
          "name": "Jong Chul Ye",
          "hidden": false
        },
        {
          "_id": "6976d29a5d41524304c1350e",
          "name": "Duygu Ceylan",
          "hidden": false
        },
        {
          "_id": "6976d29a5d41524304c1350f",
          "name": "Hyeonho Jeong",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/jX8O1mKnYcn5zBMk0V0bu.mp4"
      ],
      "publishedAt": "2026-01-22T19:59:17.000Z",
      "submittedOnDailyAt": "2026-01-26T00:06:22.236Z",
      "title": "Memory-V2V: Augmenting Video-to-Video Diffusion Models with Memory",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Recent foundational video-to-video diffusion models have achieved impressive results in editing user provided videos by modifying appearance, motion, or camera movement. However, real-world video editing is often an iterative process, where users refine results across multiple rounds of interaction. In this multi-turn setting, current video editors struggle to maintain cross-consistency across sequential edits. In this work, we tackle, for the first time, the problem of cross-consistency in multi-turn video editing and introduce Memory-V2V, a simple, yet effective framework that augments existing video-to-video models with explicit memory. Given an external cache of previously edited videos, Memory-V2V employs accurate retrieval and dynamic tokenization strategies to condition the current editing step on prior results. To further mitigate redundancy and computational overhead, we propose a learnable token compressor within the DiT backbone that compresses redundant conditioning tokens while preserving essential visual cues, achieving an overall speedup of 30%. We validate Memory-V2V on challenging tasks including video novel view synthesis and text-conditioned long video editing. Extensive experiments show that Memory-V2V produces videos that are significantly more cross-consistent with minimal computational overhead, while maintaining or even improving task-specific performance over state-of-the-art baselines. Project page: https://dohunlee1.github.io/MemoryV2V",
      "upvotes": 1,
      "discussionId": "6976d29b5d41524304c13510",
      "projectPage": "https://dohunlee1.github.io/MemoryV2V",
      "ai_summary": "Memory-V2V enhances multi-turn video editing by maintaining cross-consistency through explicit memory mechanisms and efficient token compression in video-to-video diffusion models.",
      "ai_keywords": [
        "video-to-video diffusion models",
        "cross-consistency",
        "multi-turn video editing",
        "memory augmentation",
        "retrieval",
        "dynamic tokenization",
        "token compressor",
        "DiT backbone",
        "video novel view synthesis",
        "text-conditioned long video editing"
      ],
      "organization": {
        "_id": "61e5d14f77496de0a6d95c6b",
        "name": "adobe",
        "fullname": "Adobe",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1645217431826-61e35e517ac6b6d06cfa8081.png"
      }
    },
    "publishedAt": "2026-01-22T14:59:17.000Z",
    "title": "Memory-V2V: Augmenting Video-to-Video Diffusion Models with Memory",
    "summary": "Recent foundational video-to-video diffusion models have achieved impressive results in editing user provided videos by modifying appearance, motion, or camera movement. However, real-world video editing is often an iterative process, where users refine results across multiple rounds of interaction. In this multi-turn setting, current video editors struggle to maintain cross-consistency across sequential edits. In this work, we tackle, for the first time, the problem of cross-consistency in multi-turn video editing and introduce Memory-V2V, a simple, yet effective framework that augments existing video-to-video models with explicit memory. Given an external cache of previously edited videos, Memory-V2V employs accurate retrieval and dynamic tokenization strategies to condition the current editing step on prior results. To further mitigate redundancy and computational overhead, we propose a learnable token compressor within the DiT backbone that compresses redundant conditioning tokens while preserving essential visual cues, achieving an overall speedup of 30%. We validate Memory-V2V on challenging tasks including video novel view synthesis and text-conditioned long video editing. Extensive experiments show that Memory-V2V produces videos that are significantly more cross-consistent with minimal computational overhead, while maintaining or even improving task-specific performance over state-of-the-art baselines. Project page: https://dohunlee1.github.io/MemoryV2V",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/jX8O1mKnYcn5zBMk0V0bu.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.16296.png",
    "numComments": 0,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 216,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "61e5d14f77496de0a6d95c6b",
      "name": "adobe",
      "fullname": "Adobe",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1645217431826-61e35e517ac6b6d06cfa8081.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2601.16746",
      "authors": [
        {
          "_id": "6976d4105d41524304c13517",
          "name": "Yuhang Wang",
          "hidden": false
        },
        {
          "_id": "6976d4105d41524304c13518",
          "name": "Yuling Shi",
          "hidden": false
        },
        {
          "_id": "6976d4105d41524304c13519",
          "name": "Mo Yang",
          "hidden": false
        },
        {
          "_id": "6976d4105d41524304c1351a",
          "name": "Rongrui Zhang",
          "hidden": false
        },
        {
          "_id": "6976d4105d41524304c1351b",
          "name": "Shilin He",
          "hidden": false
        },
        {
          "_id": "6976d4105d41524304c1351c",
          "name": "Heng Lian",
          "hidden": false
        },
        {
          "_id": "6976d4105d41524304c1351d",
          "name": "Yuting Chen",
          "hidden": false
        },
        {
          "_id": "6976d4105d41524304c1351e",
          "name": "Siyu Ye",
          "hidden": false
        },
        {
          "_id": "6976d4105d41524304c1351f",
          "name": "Kai Cai",
          "hidden": false
        },
        {
          "_id": "6976d4105d41524304c13520",
          "name": "Xiaodong Gu",
          "hidden": false
        }
      ],
      "publishedAt": "2026-01-23T13:51:59.000Z",
      "submittedOnDailyAt": "2026-01-26T00:10:23.451Z",
      "title": "SWE-Pruner: Self-Adaptive Context Pruning for Coding Agents",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "LLM agents have demonstrated remarkable capabilities in software development, but their performance is hampered by long interaction contexts, which incur high API costs and latency. While various context compression approaches such as LongLLMLingua have emerged to tackle this challenge, they typically rely on fixed metrics such as PPL, ignoring the task-specific nature of code understanding. As a result, they frequently disrupt syntactic and logical structure and fail to retain critical implementation details. In this paper, we propose SWE-Pruner, a self-adaptive context pruning framework tailored for coding agents. Drawing inspiration from how human programmers \"selectively skim\" source code during development and debugging, SWE-Pruner performs task-aware adaptive pruning for long contexts. Given the current task, the agent formulates an explicit goal (e.g., \"focus on error handling\") as a hint to guide the pruning targets. A lightweight neural skimmer (0.6B parameters) is trained to dynamically select relevant lines from the surrounding context given the goal. Evaluations across four benchmarks and multiple models validate SWE-Pruner's effectiveness in various scenarios, achieving 23-54% token reduction on agent tasks like SWE-Bench Verified and up to 14.84x compression on single-turn tasks like LongCodeQA with minimal performance impact.",
      "upvotes": 0,
      "discussionId": "6976d4105d41524304c13521",
      "githubRepo": "https://github.com/Ayanami1314/swe-pruner",
      "githubRepoAddedBy": "user",
      "ai_summary": "SWE-Pruner is a self-adaptive context pruning framework for coding agents that uses task-aware pruning to reduce token usage while maintaining performance.",
      "ai_keywords": [
        "context compression",
        "LongLLMLingua",
        "PPL",
        "code understanding",
        "task-aware adaptive pruning",
        "neural skimmer",
        "token reduction",
        "SWE-Bench Verified",
        "LongCodeQA"
      ],
      "organization": {
        "_id": "653b817d32c97d0655575872",
        "name": "ByteDance",
        "fullname": "ByteDance",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/0clr54wj5Ly-RkYU9OXPp.png"
      }
    },
    "publishedAt": "2026-01-23T08:51:59.000Z",
    "title": "SWE-Pruner: Self-Adaptive Context Pruning for Coding Agents",
    "summary": "LLM agents have demonstrated remarkable capabilities in software development, but their performance is hampered by long interaction contexts, which incur high API costs and latency. While various context compression approaches such as LongLLMLingua have emerged to tackle this challenge, they typically rely on fixed metrics such as PPL, ignoring the task-specific nature of code understanding. As a result, they frequently disrupt syntactic and logical structure and fail to retain critical implementation details. In this paper, we propose SWE-Pruner, a self-adaptive context pruning framework tailored for coding agents. Drawing inspiration from how human programmers \"selectively skim\" source code during development and debugging, SWE-Pruner performs task-aware adaptive pruning for long contexts. Given the current task, the agent formulates an explicit goal (e.g., \"focus on error handling\") as a hint to guide the pruning targets. A lightweight neural skimmer (0.6B parameters) is trained to dynamically select relevant lines from the surrounding context given the goal. Evaluations across four benchmarks and multiple models validate SWE-Pruner's effectiveness in various scenarios, achieving 23-54% token reduction on agent tasks like SWE-Bench Verified and up to 14.84x compression on single-turn tasks like LongCodeQA with minimal performance impact.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.16746.png",
    "numComments": 0,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 216,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "653b817d32c97d0655575872",
      "name": "ByteDance",
      "fullname": "ByteDance",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/0clr54wj5Ly-RkYU9OXPp.png"
    },
    "isAuthorParticipating": false
  }
]