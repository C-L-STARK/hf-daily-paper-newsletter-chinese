[
  {
    "paper": {
      "id": "2602.03392",
      "authors": [
        {
          "_id": "69858ae34ad556f294b7ec93",
          "user": {
            "_id": "652f7bf41ad13fee8c407247",
            "avatarUrl": "/avatars/5c7a74a9edf748025bffeeba97a61505.svg",
            "isPro": false,
            "fullname": "Shumin",
            "user": "Mystery",
            "type": "user"
          },
          "name": "Shumin Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-02-06T18:50:45.887Z",
          "hidden": false
        },
        {
          "_id": "69858ae34ad556f294b7ec94",
          "name": "Yuexiang Xie",
          "hidden": false
        },
        {
          "_id": "69858ae34ad556f294b7ec95",
          "name": "Wenhao Zhang",
          "hidden": false
        },
        {
          "_id": "69858ae34ad556f294b7ec96",
          "name": "Yuchang Sun",
          "hidden": false
        },
        {
          "_id": "69858ae34ad556f294b7ec97",
          "name": "Yanxi Chen",
          "hidden": false
        },
        {
          "_id": "69858ae34ad556f294b7ec98",
          "name": "Yaliang Li",
          "hidden": false
        },
        {
          "_id": "69858ae34ad556f294b7ec99",
          "name": "Yanyong Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2026-02-03T11:14:58.000Z",
      "submittedOnDailyAt": "2026-02-09T00:31:59.901Z",
      "title": "On the Entropy Dynamics in Reinforcement Fine-Tuning of Large Language Models",
      "submittedOnDailyBy": {
        "_id": "652f7bf41ad13fee8c407247",
        "avatarUrl": "/avatars/5c7a74a9edf748025bffeeba97a61505.svg",
        "isPro": false,
        "fullname": "Shumin",
        "user": "Mystery",
        "type": "user"
      },
      "summary": "Entropy serves as a critical metric for measuring the diversity of outputs generated by large language models (LLMs), providing valuable insights into their exploration capabilities. While recent studies increasingly focus on monitoring and adjusting entropy to better balance exploration and exploitation in reinforcement fine-tuning (RFT), a principled understanding of entropy dynamics during this process is yet to be thoroughly investigated. In this paper, we establish a theoretical framework for analyzing the entropy dynamics during the RFT process, which begins with a discriminant expression that quantifies entropy change under a single logit update. This foundation enables the derivation of a first-order expression for entropy change, which can be further extended to the update formula of Group Relative Policy Optimization (GRPO). The corollaries and insights drawn from the theoretical analysis inspire the design of entropy control methods, and also offer a unified lens for interpreting various entropy-based methods in existing studies. We provide empirical evidence to support the main conclusions of our analysis and demonstrate the effectiveness of the derived entropy-discriminator clipping methods. This study yields novel insights into RFT training dynamics, providing theoretical support and practical strategies for optimizing the exploration-exploitation balance during LLM fine-tuning.",
      "upvotes": 24,
      "discussionId": "69858ae34ad556f294b7ec9a",
      "githubRepo": "https://github.com/agentscope-ai/Trinity-RFT",
      "githubRepoAddedBy": "user",
      "ai_summary": "The paper establishes a theoretical framework for analyzing entropy dynamics in reinforcement fine-tuning of large language models, deriving expressions for entropy change and proposing entropy control methods based on discriminant analysis.",
      "ai_keywords": [
        "entropy",
        "large language models",
        "reinforcement fine-tuning",
        "RFT",
        "logit update",
        "Group Relative Policy Optimization",
        "GRPO",
        "entropy-discriminator clipping",
        "exploration-exploitation balance"
      ],
      "githubStars": 520
    },
    "publishedAt": "2026-02-03T06:14:58.000Z",
    "title": "On the Entropy Dynamics in Reinforcement Fine-Tuning of Large Language Models",
    "summary": "Entropy serves as a critical metric for measuring the diversity of outputs generated by large language models (LLMs), providing valuable insights into their exploration capabilities. While recent studies increasingly focus on monitoring and adjusting entropy to better balance exploration and exploitation in reinforcement fine-tuning (RFT), a principled understanding of entropy dynamics during this process is yet to be thoroughly investigated. In this paper, we establish a theoretical framework for analyzing the entropy dynamics during the RFT process, which begins with a discriminant expression that quantifies entropy change under a single logit update. This foundation enables the derivation of a first-order expression for entropy change, which can be further extended to the update formula of Group Relative Policy Optimization (GRPO). The corollaries and insights drawn from the theoretical analysis inspire the design of entropy control methods, and also offer a unified lens for interpreting various entropy-based methods in existing studies. We provide empirical evidence to support the main conclusions of our analysis and demonstrate the effectiveness of the derived entropy-discriminator clipping methods. This study yields novel insights into RFT training dynamics, providing theoretical support and practical strategies for optimizing the exploration-exploitation balance during LLM fine-tuning.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.03392.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "652f7bf41ad13fee8c407247",
      "avatarUrl": "/avatars/5c7a74a9edf748025bffeeba97a61505.svg",
      "fullname": "Shumin",
      "name": "Mystery",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "isUserFollowing": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2602.06960",
      "authors": [
        {
          "_id": "69894ae3beecc443208d25b4",
          "name": "Yuchen Yan",
          "hidden": false
        },
        {
          "_id": "69894ae3beecc443208d25b5",
          "name": "Liang Jiang",
          "hidden": false
        },
        {
          "_id": "69894ae3beecc443208d25b6",
          "name": "Jin Jiang",
          "hidden": false
        },
        {
          "_id": "69894ae3beecc443208d25b7",
          "name": "Shuaicheng Li",
          "hidden": false
        },
        {
          "_id": "69894ae3beecc443208d25b8",
          "name": "Zujie Wen",
          "hidden": false
        },
        {
          "_id": "69894ae3beecc443208d25b9",
          "name": "Zhiqiang Zhang",
          "hidden": false
        },
        {
          "_id": "69894ae3beecc443208d25ba",
          "name": "Jun Zhou",
          "hidden": false
        },
        {
          "_id": "69894ae3beecc443208d25bb",
          "name": "Jian Shao",
          "hidden": false
        },
        {
          "_id": "69894ae3beecc443208d25bc",
          "name": "Yueting Zhuang",
          "hidden": false
        },
        {
          "_id": "69894ae3beecc443208d25bd",
          "name": "Yongliang Shen",
          "hidden": false
        }
      ],
      "publishedAt": "2026-02-06T18:59:27.000Z",
      "submittedOnDailyAt": "2026-02-09T00:18:12.522Z",
      "title": "InftyThink+: Effective and Efficient Infinite-Horizon Reasoning via Reinforcement Learning",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Large reasoning models achieve strong performance by scaling inference-time chain-of-thought, but this paradigm suffers from quadratic cost, context length limits, and degraded reasoning due to lost-in-the-middle effects. Iterative reasoning mitigates these issues by periodically summarizing intermediate thoughts, yet existing methods rely on supervised learning or fixed heuristics and fail to optimize when to summarize, what to preserve, and how to resume reasoning. We propose InftyThink+, an end-to-end reinforcement learning framework that optimizes the entire iterative reasoning trajectory, building on model-controlled iteration boundaries and explicit summarization. InftyThink+ adopts a two-stage training scheme with supervised cold-start followed by trajectory-level reinforcement learning, enabling the model to learn strategic summarization and continuation decisions. Experiments on DeepSeek-R1-Distill-Qwen-1.5B show that InftyThink+ improves accuracy by 21% on AIME24 and outperforms conventional long chain-of-thought reinforcement learning by a clear margin, while also generalizing better to out-of-distribution benchmarks. Moreover, InftyThink+ significantly reduces inference latency and accelerates reinforcement learning training, demonstrating improved reasoning efficiency alongside stronger performance.",
      "upvotes": 3,
      "discussionId": "69894ae3beecc443208d25be",
      "projectPage": "https://zju-real.github.io/InftyThink-Plus/",
      "githubRepo": "https://github.com/ZJU-REAL/InftyThink-Plus",
      "githubRepoAddedBy": "user",
      "ai_summary": "InftyThink+ uses reinforcement learning to optimize iterative reasoning processes, improving accuracy and efficiency in large language models.",
      "ai_keywords": [
        "chain-of-thought",
        "iterative reasoning",
        "reinforcement learning",
        "trajectory-level reinforcement learning",
        "summarization",
        "reasoning efficiency",
        "inference latency"
      ]
    },
    "publishedAt": "2026-02-06T13:59:27.000Z",
    "title": "InftyThink+: Effective and Efficient Infinite-Horizon Reasoning via Reinforcement Learning",
    "summary": "Large reasoning models achieve strong performance by scaling inference-time chain-of-thought, but this paradigm suffers from quadratic cost, context length limits, and degraded reasoning due to lost-in-the-middle effects. Iterative reasoning mitigates these issues by periodically summarizing intermediate thoughts, yet existing methods rely on supervised learning or fixed heuristics and fail to optimize when to summarize, what to preserve, and how to resume reasoning. We propose InftyThink+, an end-to-end reinforcement learning framework that optimizes the entire iterative reasoning trajectory, building on model-controlled iteration boundaries and explicit summarization. InftyThink+ adopts a two-stage training scheme with supervised cold-start followed by trajectory-level reinforcement learning, enabling the model to learn strategic summarization and continuation decisions. Experiments on DeepSeek-R1-Distill-Qwen-1.5B show that InftyThink+ improves accuracy by 21% on AIME24 and outperforms conventional long chain-of-thought reinforcement learning by a clear margin, while also generalizing better to out-of-distribution benchmarks. Moreover, InftyThink+ significantly reduces inference latency and accelerates reinforcement learning training, demonstrating improved reasoning efficiency alongside stronger performance.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.06960.png",
    "numComments": 0,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 229,
      "isUserFollowing": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2602.06391",
      "authors": [
        {
          "_id": "69894bc6beecc443208d25c0",
          "name": "Zhongyin Zhao",
          "hidden": false
        },
        {
          "_id": "69894bc6beecc443208d25c1",
          "name": "Yuan Liu",
          "hidden": false
        },
        {
          "_id": "69894bc6beecc443208d25c2",
          "name": "Yikun Liu",
          "hidden": false
        },
        {
          "_id": "69894bc6beecc443208d25c3",
          "name": "Haicheng Wang",
          "hidden": false
        },
        {
          "_id": "69894bc6beecc443208d25c4",
          "name": "Le Tian",
          "hidden": false
        },
        {
          "_id": "69894bc6beecc443208d25c5",
          "name": "Xiao Zhou",
          "hidden": false
        },
        {
          "_id": "69894bc6beecc443208d25c6",
          "name": "Yangxiu You",
          "hidden": false
        },
        {
          "_id": "69894bc6beecc443208d25c7",
          "name": "Zilin Yu",
          "hidden": false
        },
        {
          "_id": "69894bc6beecc443208d25c8",
          "name": "Yang Yu",
          "hidden": false
        },
        {
          "_id": "69894bc6beecc443208d25c9",
          "name": "Jie Zhou",
          "hidden": false
        }
      ],
      "publishedAt": "2026-02-06T05:14:11.000Z",
      "submittedOnDailyAt": "2026-02-09T00:30:58.259Z",
      "title": "POINTS-GUI-G: GUI-Grounding Journey",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "The rapid advancement of vision-language models has catalyzed the emergence of GUI agents, which hold immense potential for automating complex tasks, from online shopping to flight booking, thereby alleviating the burden of repetitive digital workflows. As a foundational capability, GUI grounding is typically established as a prerequisite for end-to-end task execution. It enables models to precisely locate interface elements, such as text and icons, to perform accurate operations like clicking and typing. Unlike prior works that fine-tune models already possessing strong spatial awareness (e.g., Qwen3-VL), we aim to master the full technical pipeline by starting from a base model with minimal grounding ability, such as POINTS-1.5. We introduce POINTS-GUI-G-8B, which achieves state-of-the-art performance with scores of 59.9 on ScreenSpot-Pro, 66.0 on OSWorld-G, 95.7 on ScreenSpot-v2, and 49.9 on UI-Vision. Our model's success is driven by three key factors: (1) Refined Data Engineering, involving the unification of diverse open-source datasets format alongside sophisticated strategies for augmentation, filtering, and difficulty grading; (2) Improved Training Strategies, including continuous fine-tuning of the vision encoder to enhance perceptual accuracy and maintaining resolution consistency between training and inference; and (3) Reinforcement Learning (RL) with Verifiable Rewards. While RL is traditionally used to bolster reasoning, we demonstrate that it significantly improves precision in the perception-intensive GUI grounding task. Furthermore, GUI grounding provides a natural advantage for RL, as rewards are easily verifiable and highly accurate.",
      "upvotes": 3,
      "discussionId": "69894bc6beecc443208d25ca",
      "githubRepo": "https://github.com/Tencent/POINTS-GUI",
      "githubRepoAddedBy": "user",
      "ai_summary": "GUI agents for automated digital tasks rely on vision-language models with enhanced grounding capabilities, achieved through refined data engineering, improved training strategies, and reinforcement learning with verifiable rewards.",
      "ai_keywords": [
        "vision-language models",
        "GUI agents",
        "GUI grounding",
        "ScreenSpot-Pro",
        "OSWorld-G",
        "ScreenSpot-v2",
        "UI-Vision",
        "data engineering",
        "training strategies",
        "reinforcement learning",
        "verifiable rewards"
      ],
      "githubStars": 13
    },
    "publishedAt": "2026-02-06T00:14:11.000Z",
    "title": "POINTS-GUI-G: GUI-Grounding Journey",
    "summary": "The rapid advancement of vision-language models has catalyzed the emergence of GUI agents, which hold immense potential for automating complex tasks, from online shopping to flight booking, thereby alleviating the burden of repetitive digital workflows. As a foundational capability, GUI grounding is typically established as a prerequisite for end-to-end task execution. It enables models to precisely locate interface elements, such as text and icons, to perform accurate operations like clicking and typing. Unlike prior works that fine-tune models already possessing strong spatial awareness (e.g., Qwen3-VL), we aim to master the full technical pipeline by starting from a base model with minimal grounding ability, such as POINTS-1.5. We introduce POINTS-GUI-G-8B, which achieves state-of-the-art performance with scores of 59.9 on ScreenSpot-Pro, 66.0 on OSWorld-G, 95.7 on ScreenSpot-v2, and 49.9 on UI-Vision. Our model's success is driven by three key factors: (1) Refined Data Engineering, involving the unification of diverse open-source datasets format alongside sophisticated strategies for augmentation, filtering, and difficulty grading; (2) Improved Training Strategies, including continuous fine-tuning of the vision encoder to enhance perceptual accuracy and maintaining resolution consistency between training and inference; and (3) Reinforcement Learning (RL) with Verifiable Rewards. While RL is traditionally used to bolster reasoning, we demonstrate that it significantly improves precision in the perception-intensive GUI grounding task. Furthermore, GUI grounding provides a natural advantage for RL, as rewards are easily verifiable and highly accurate.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.06391.png",
    "numComments": 0,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 229,
      "isUserFollowing": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2602.06075",
      "authors": [
        {
          "_id": "698949ccbeecc443208d25a3",
          "name": "Guangyi Liu",
          "hidden": false
        },
        {
          "_id": "698949ccbeecc443208d25a4",
          "name": "Pengxiang Zhao",
          "hidden": false
        },
        {
          "_id": "698949ccbeecc443208d25a5",
          "name": "Yaozhen Liang",
          "hidden": false
        },
        {
          "_id": "698949ccbeecc443208d25a6",
          "name": "Qinyi Luo",
          "hidden": false
        },
        {
          "_id": "698949ccbeecc443208d25a7",
          "name": "Shunye Tang",
          "hidden": false
        },
        {
          "_id": "698949ccbeecc443208d25a8",
          "name": "Yuxiang Chai",
          "hidden": false
        },
        {
          "_id": "698949ccbeecc443208d25a9",
          "name": "Weifeng Lin",
          "hidden": false
        },
        {
          "_id": "698949ccbeecc443208d25aa",
          "name": "Han Xiao",
          "hidden": false
        },
        {
          "_id": "698949ccbeecc443208d25ab",
          "name": "WenHao Wang",
          "hidden": false
        },
        {
          "_id": "698949ccbeecc443208d25ac",
          "name": "Siheng Chen",
          "hidden": false
        },
        {
          "_id": "698949ccbeecc443208d25ad",
          "name": "Zhengxi Lu",
          "hidden": false
        },
        {
          "_id": "698949ccbeecc443208d25ae",
          "name": "Gao Wu",
          "hidden": false
        },
        {
          "_id": "698949ccbeecc443208d25af",
          "name": "Hao Wang",
          "hidden": false
        },
        {
          "_id": "698949ccbeecc443208d25b0",
          "name": "Liang Liu",
          "hidden": false
        },
        {
          "_id": "698949ccbeecc443208d25b1",
          "name": "Yong Liu",
          "hidden": false
        }
      ],
      "publishedAt": "2026-02-03T17:01:59.000Z",
      "submittedOnDailyAt": "2026-02-09T00:13:26.013Z",
      "title": "MemGUI-Bench: Benchmarking Memory of Mobile GUI Agents in Dynamic Environments",
      "submittedOnDailyBy": {
        "_id": "64d761b98ebc40443831f82a",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64d761b98ebc40443831f82a/DHBOtOstiFp2-lDY6b9gb.png",
        "isPro": false,
        "fullname": "Guangyi Liu",
        "user": "lgy0404",
        "type": "user"
      },
      "summary": "Current mobile GUI agent benchmarks systematically fail to assess memory capabilities, with only 5.2-11.8% memory-related tasks and no cross-session learning evaluation. We introduce MemGUI-Bench, a comprehensive memory-centric benchmark with pass@k and staged LLM-as-judge evaluation. Our contributions include: (1) a systematic memory taxonomy analyzing 11 agents across 5 architectures; (2) 128 tasks across 26 applications where 89.8% challenge memory through cross-temporal and cross-spatial retention; (3) MemGUI-Eval, an automated pipeline with Progressive Scrutiny and 7 hierarchical metrics; and (4) RQ-driven assessment of 11 state-of-the-art agents. Our experiments reveal significant memory deficits across all evaluated systems, identify 5 distinct failure modes, and synthesize 5 actionable design implications. All resources including code, benchmark, and evaluation results will be \\textit{fully open-sourced and continuously maintained} at https://lgy0404.github.io/MemGUI-Bench/.",
      "upvotes": 3,
      "discussionId": "698949ccbeecc443208d25b2",
      "projectPage": "https://lgy0404.github.io/MemGUI-Bench/",
      "githubRepo": "https://github.com/lgy0404/MemGUI-Bench",
      "githubRepoAddedBy": "user",
      "ai_summary": "A comprehensive memory-focused benchmark for mobile GUI agents reveals significant memory capability gaps and provides systematic evaluation methods and design insights.",
      "ai_keywords": [
        "memory-centric benchmark",
        "LLM-as-judge evaluation",
        "memory taxonomy",
        "cross-temporal retention",
        "cross-spatial retention",
        "automated pipeline",
        "Progressive Scrutiny",
        "hierarchical metrics",
        "state-of-the-art agents",
        "failure modes",
        "design implications"
      ],
      "githubStars": 5,
      "organization": {
        "_id": "61bac2af530e5c78d7b99667",
        "name": "zju",
        "fullname": "Zhejiang University",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5e1058e9fcf41d740b69966d/7G1xjlxwCdMEmKcxNR0n5.png"
      }
    },
    "publishedAt": "2026-02-03T12:01:59.000Z",
    "title": "MemGUI-Bench: Benchmarking Memory of Mobile GUI Agents in Dynamic Environments",
    "summary": "Current mobile GUI agent benchmarks systematically fail to assess memory capabilities, with only 5.2-11.8% memory-related tasks and no cross-session learning evaluation. We introduce MemGUI-Bench, a comprehensive memory-centric benchmark with pass@k and staged LLM-as-judge evaluation. Our contributions include: (1) a systematic memory taxonomy analyzing 11 agents across 5 architectures; (2) 128 tasks across 26 applications where 89.8% challenge memory through cross-temporal and cross-spatial retention; (3) MemGUI-Eval, an automated pipeline with Progressive Scrutiny and 7 hierarchical metrics; and (4) RQ-driven assessment of 11 state-of-the-art agents. Our experiments reveal significant memory deficits across all evaluated systems, identify 5 distinct failure modes, and synthesize 5 actionable design implications. All resources including code, benchmark, and evaluation results will be \\textit{fully open-sourced and continuously maintained} at https://lgy0404.github.io/MemGUI-Bench/.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.06075.png",
    "numComments": 0,
    "submittedBy": {
      "_id": "64d761b98ebc40443831f82a",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64d761b98ebc40443831f82a/DHBOtOstiFp2-lDY6b9gb.png",
      "fullname": "Guangyi Liu",
      "name": "lgy0404",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "61bac2af530e5c78d7b99667",
      "name": "zju",
      "fullname": "Zhejiang University",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5e1058e9fcf41d740b69966d/7G1xjlxwCdMEmKcxNR0n5.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2602.06949",
      "authors": [
        {
          "_id": "69894c74beecc443208d25db",
          "name": "Shenyuan Gao",
          "hidden": false
        },
        {
          "_id": "69894c74beecc443208d25dc",
          "name": "William Liang",
          "hidden": false
        },
        {
          "_id": "69894c74beecc443208d25dd",
          "name": "Kaiyuan Zheng",
          "hidden": false
        },
        {
          "_id": "69894c74beecc443208d25de",
          "name": "Ayaan Malik",
          "hidden": false
        },
        {
          "_id": "69894c74beecc443208d25df",
          "name": "Seonghyeon Ye",
          "hidden": false
        },
        {
          "_id": "69894c74beecc443208d25e0",
          "name": "Sihyun Yu",
          "hidden": false
        },
        {
          "_id": "69894c74beecc443208d25e1",
          "name": "Wei-Cheng Tseng",
          "hidden": false
        },
        {
          "_id": "69894c74beecc443208d25e2",
          "name": "Yuzhu Dong",
          "hidden": false
        },
        {
          "_id": "69894c74beecc443208d25e3",
          "name": "Kaichun Mo",
          "hidden": false
        },
        {
          "_id": "69894c74beecc443208d25e4",
          "name": "Chen-Hsuan Lin",
          "hidden": false
        },
        {
          "_id": "69894c74beecc443208d25e5",
          "name": "Qianli Ma",
          "hidden": false
        },
        {
          "_id": "69894c74beecc443208d25e6",
          "name": "Seungjun Nah",
          "hidden": false
        },
        {
          "_id": "69894c74beecc443208d25e7",
          "name": "Loic Magne",
          "hidden": false
        },
        {
          "_id": "69894c74beecc443208d25e8",
          "name": "Jiannan Xiang",
          "hidden": false
        },
        {
          "_id": "69894c74beecc443208d25e9",
          "name": "Yuqi Xie",
          "hidden": false
        },
        {
          "_id": "69894c74beecc443208d25ea",
          "name": "Ruijie Zheng",
          "hidden": false
        },
        {
          "_id": "69894c74beecc443208d25eb",
          "name": "Dantong Niu",
          "hidden": false
        },
        {
          "_id": "69894c74beecc443208d25ec",
          "name": "You Liang Tan",
          "hidden": false
        },
        {
          "_id": "69894c74beecc443208d25ed",
          "name": "K. R. Zentner",
          "hidden": false
        },
        {
          "_id": "69894c74beecc443208d25ee",
          "name": "George Kurian",
          "hidden": false
        },
        {
          "_id": "69894c74beecc443208d25ef",
          "name": "Suneel Indupuru",
          "hidden": false
        },
        {
          "_id": "69894c74beecc443208d25f0",
          "name": "Pooya Jannaty",
          "hidden": false
        },
        {
          "_id": "69894c74beecc443208d25f1",
          "name": "Jinwei Gu",
          "hidden": false
        },
        {
          "_id": "69894c74beecc443208d25f2",
          "name": "Jun Zhang",
          "hidden": false
        },
        {
          "_id": "69894c74beecc443208d25f3",
          "name": "Jitendra Malik",
          "hidden": false
        },
        {
          "_id": "69894c74beecc443208d25f4",
          "name": "Pieter Abbeel",
          "hidden": false
        },
        {
          "_id": "69894c74beecc443208d25f5",
          "name": "Ming-Yu Liu",
          "hidden": false
        },
        {
          "_id": "69894c74beecc443208d25f6",
          "name": "Yuke Zhu",
          "hidden": false
        },
        {
          "_id": "69894c74beecc443208d25f7",
          "name": "Joel Jang",
          "hidden": false
        },
        {
          "_id": "69894c74beecc443208d25f8",
          "name": "Linxi \"Jim\" Fan",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/MN-A84kxkw1l1lyftyRTR.mp4"
      ],
      "publishedAt": "2026-02-06T18:49:43.000Z",
      "submittedOnDailyAt": "2026-02-09T00:32:34.350Z",
      "title": "DreamDojo: A Generalist Robot World Model from Large-Scale Human Videos",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Being able to simulate the outcomes of actions in varied environments will revolutionize the development of generalist agents at scale. However, modeling these world dynamics, especially for dexterous robotics tasks, poses significant challenges due to limited data coverage and scarce action labels. As an endeavor towards this end, we introduce DreamDojo, a foundation world model that learns diverse interactions and dexterous controls from 44k hours of egocentric human videos. Our data mixture represents the largest video dataset to date for world model pretraining, spanning a wide range of daily scenarios with diverse objects and skills. To address the scarcity of action labels, we introduce continuous latent actions as unified proxy actions, enhancing interaction knowledge transfer from unlabeled videos. After post-training on small-scale target robot data, DreamDojo demonstrates a strong understanding of physics and precise action controllability. We also devise a distillation pipeline that accelerates DreamDojo to a real-time speed of 10.81 FPS and further improves context consistency. Our work enables several important applications based on generative world models, including live teleoperation, policy evaluation, and model-based planning. Systematic evaluation on multiple challenging out-of-distribution (OOD) benchmarks verifies the significance of our method for simulating open-world, contact-rich tasks, paving the way for general-purpose robot world models.",
      "upvotes": 2,
      "discussionId": "69894c74beecc443208d25f9",
      "projectPage": "https://dreamdojo-world.github.io/",
      "ai_summary": "DreamDojo is a foundation world model trained on 44k hours of egocentric human videos that enables efficient simulation of dexterous robotic tasks through continuous latent actions and real-time distillation.",
      "ai_keywords": [
        "world model",
        "egocentric videos",
        "continuous latent actions",
        "action labels",
        "distillation pipeline",
        "real-time speed",
        "teleoperation",
        "policy evaluation",
        "model-based planning",
        "out-of-distribution benchmarks"
      ],
      "organization": {
        "_id": "60262b67268c201cdc8b7d43",
        "name": "nvidia",
        "fullname": "NVIDIA",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"
      }
    },
    "publishedAt": "2026-02-06T13:49:43.000Z",
    "title": "DreamDojo: A Generalist Robot World Model from Large-Scale Human Videos",
    "summary": "Being able to simulate the outcomes of actions in varied environments will revolutionize the development of generalist agents at scale. However, modeling these world dynamics, especially for dexterous robotics tasks, poses significant challenges due to limited data coverage and scarce action labels. As an endeavor towards this end, we introduce DreamDojo, a foundation world model that learns diverse interactions and dexterous controls from 44k hours of egocentric human videos. Our data mixture represents the largest video dataset to date for world model pretraining, spanning a wide range of daily scenarios with diverse objects and skills. To address the scarcity of action labels, we introduce continuous latent actions as unified proxy actions, enhancing interaction knowledge transfer from unlabeled videos. After post-training on small-scale target robot data, DreamDojo demonstrates a strong understanding of physics and precise action controllability. We also devise a distillation pipeline that accelerates DreamDojo to a real-time speed of 10.81 FPS and further improves context consistency. Our work enables several important applications based on generative world models, including live teleoperation, policy evaluation, and model-based planning. Systematic evaluation on multiple challenging out-of-distribution (OOD) benchmarks verifies the significance of our method for simulating open-world, contact-rich tasks, paving the way for general-purpose robot world models.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/MN-A84kxkw1l1lyftyRTR.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.06949.png",
    "numComments": 0,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 229,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "60262b67268c201cdc8b7d43",
      "name": "nvidia",
      "fullname": "NVIDIA",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2602.06471",
      "authors": [
        {
          "_id": "6989492abeecc443208d259d",
          "name": "Feng-Ting Liao",
          "hidden": false
        },
        {
          "_id": "6989492abeecc443208d259e",
          "name": "Meng-Hsi Chen",
          "hidden": false
        },
        {
          "_id": "6989492abeecc443208d259f",
          "name": "Guan-Ting Yi",
          "hidden": false
        },
        {
          "_id": "6989492abeecc443208d25a0",
          "name": "Da-shan Shiu",
          "hidden": false
        }
      ],
      "publishedAt": "2026-02-06T07:55:30.000Z",
      "submittedOnDailyAt": "2026-02-09T00:11:07.040Z",
      "title": "Revisiting the Shape Convention of Transformer Language Models",
      "submittedOnDailyBy": {
        "_id": "643fb7332397d8eef5b844cd",
        "avatarUrl": "/avatars/e403a19fc13e478d5929c67028230b0e.svg",
        "isPro": false,
        "fullname": "Feng-Ting Liao",
        "user": "FengTing",
        "type": "user"
      },
      "summary": "Dense Transformer language models have largely adhered to one consistent architectural shape: each layer consists of an attention module followed by a feed-forward network (FFN) with a narrow-wide-narrow MLP, allocating most parameters to the MLP at expansion ratios between 2 and 4. Motivated by recent results that residual wide-narrow-wide (hourglass) MLPs offer superior function approximation capabilities, we revisit the long-standing MLP shape convention in Transformer, challenging the necessity of the narrow-wide-narrow design. To study this, we develop a Transformer variant that replaces the conventional FFN with a deeper hourglass-shaped FFN, comprising a stack of hourglass sub-MLPs connected by residual pathways. We posit that a deeper but lighter hourglass FFN can serve as a competitive alternative to the conventional FFN, and that parameters saved by using a lighter hourglass FFN can be more effectively utilized, such as by enlarging model hidden dimensions under fixed budgets. We confirm these through empirical validations across model scales: hourglass FFNs outperform conventional FFNs up to 400M and achieve comparable performance at larger scales to 1B parameters; hourglass FFN variants with reduced FFN and increased attention parameters show consistent improvements over conventional configurations at matched budgets. Together, these findings shed new light on recent work and prompt a rethinking of the narrow-wide-narrow MLP convention and the balance between attention and FFN towards efficient and expressive modern language models.",
      "upvotes": 1,
      "discussionId": "6989492abeecc443208d25a1",
      "ai_summary": "Replacing conventional feed-forward networks with hourglass-shaped MLPs in Transformers improves model efficiency and performance by enabling better parameter utilization and competitive scaling.",
      "ai_keywords": [
        "Transformer",
        "feed-forward network",
        "MLP",
        "attention module",
        "residual pathways",
        "hourglass MLP",
        "narrow-wide-narrow",
        "model scaling",
        "parameter efficiency"
      ],
      "organization": {
        "_id": "6388e08c5a3d2a335624705b",
        "name": "MediaTek-Research",
        "fullname": "MediaTek Research",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1669948083850-6213410828005421265b27d3.jpeg"
      }
    },
    "publishedAt": "2026-02-06T02:55:30.000Z",
    "title": "Revisiting the Shape Convention of Transformer Language Models",
    "summary": "Dense Transformer language models have largely adhered to one consistent architectural shape: each layer consists of an attention module followed by a feed-forward network (FFN) with a narrow-wide-narrow MLP, allocating most parameters to the MLP at expansion ratios between 2 and 4. Motivated by recent results that residual wide-narrow-wide (hourglass) MLPs offer superior function approximation capabilities, we revisit the long-standing MLP shape convention in Transformer, challenging the necessity of the narrow-wide-narrow design. To study this, we develop a Transformer variant that replaces the conventional FFN with a deeper hourglass-shaped FFN, comprising a stack of hourglass sub-MLPs connected by residual pathways. We posit that a deeper but lighter hourglass FFN can serve as a competitive alternative to the conventional FFN, and that parameters saved by using a lighter hourglass FFN can be more effectively utilized, such as by enlarging model hidden dimensions under fixed budgets. We confirm these through empirical validations across model scales: hourglass FFNs outperform conventional FFNs up to 400M and achieve comparable performance at larger scales to 1B parameters; hourglass FFN variants with reduced FFN and increased attention parameters show consistent improvements over conventional configurations at matched budgets. Together, these findings shed new light on recent work and prompt a rethinking of the narrow-wide-narrow MLP convention and the balance between attention and FFN towards efficient and expressive modern language models.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.06471.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "643fb7332397d8eef5b844cd",
      "avatarUrl": "/avatars/e403a19fc13e478d5929c67028230b0e.svg",
      "fullname": "Feng-Ting Liao",
      "name": "FengTing",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "6388e08c5a3d2a335624705b",
      "name": "MediaTek-Research",
      "fullname": "MediaTek Research",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1669948083850-6213410828005421265b27d3.jpeg"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2602.05711",
      "authors": [
        {
          "_id": "69884b90beecc443208d2475",
          "name": "Jingze Shi",
          "hidden": false
        },
        {
          "_id": "69884b90beecc443208d2476",
          "name": "Zhangyang Peng",
          "hidden": false
        },
        {
          "_id": "69884b90beecc443208d2477",
          "name": "Yizhang Zhu",
          "hidden": false
        },
        {
          "_id": "69884b90beecc443208d2478",
          "name": "Yifan Wu",
          "hidden": false
        },
        {
          "_id": "69884b90beecc443208d2479",
          "name": "Guang Liu",
          "hidden": false
        },
        {
          "_id": "69884b90beecc443208d247a",
          "name": "Yuyu Luo",
          "hidden": false
        }
      ],
      "publishedAt": "2026-02-05T14:37:32.000Z",
      "submittedOnDailyAt": "2026-02-09T00:30:00.301Z",
      "title": "OmniMoE: An Efficient MoE by Orchestrating Atomic Experts at Scale",
      "submittedOnDailyBy": {
        "_id": "673ab3647afcea17eb4378fd",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/673ab3647afcea17eb4378fd/YQB6zSH1LPxBMUYayIURi.png",
        "isPro": false,
        "fullname": "Loser Cheems",
        "user": "JingzeShi",
        "type": "user"
      },
      "summary": "Mixture-of-Experts (MoE) architectures are evolving towards finer granularity to improve parameter efficiency. However, existing MoE designs face an inherent trade-off between the granularity of expert specialization and hardware execution efficiency. We propose OmniMoE, a system-algorithm co-designed framework that pushes expert granularity to its logical extreme. OmniMoE introduces vector-level Atomic Experts, enabling scalable routing and execution within a single MoE layer, while retaining a shared dense MLP branch for general-purpose processing. Although this atomic design maximizes capacity, it poses severe challenges for routing complexity and memory access. To address these, OmniMoE adopts a system-algorithm co-design: (i) a Cartesian Product Router that decomposes the massive index space to reduce routing complexity from O(N) to O(sqrt(N)); and (ii) Expert-Centric Scheduling that inverts the execution order to turn scattered, memory-bound lookups into efficient dense matrix operations. Validated on seven benchmarks, OmniMoE (with 1.7B active parameters) achieves 50.9% zero-shot accuracy across seven benchmarks, outperforming coarse-grained (e.g., DeepSeekMoE) and fine-grained (e.g., PEER) baselines. Crucially, OmniMoE reduces inference latency from 73ms to 6.7ms (a 10.9-fold speedup) compared to PEER, demonstrating that massive-scale fine-grained MoE can be fast and accurate. Our code is open-sourced at https://github.com/flash-algo/omni-moe.",
      "upvotes": 1,
      "discussionId": "69884b90beecc443208d247b",
      "githubRepo": "https://github.com/flash-algo/omni-moe",
      "githubRepoAddedBy": "user",
      "ai_summary": "OmniMoE presents a system-algorithm co-designed framework that achieves fine-grained expert specialization in Mixture-of-Experts architectures through vector-level atomic experts and optimized routing and scheduling mechanisms.",
      "ai_keywords": [
        "Mixture-of-Experts",
        "expert specialization",
        "atomic experts",
        "routing complexity",
        "memory access",
        "Cartesian Product Router",
        "Expert-Centric Scheduling",
        "inference latency",
        "zero-shot accuracy"
      ],
      "githubStars": 46,
      "organization": {
        "_id": "61be9739d2f9358e24ca0a4f",
        "name": "BAAI",
        "fullname": "Beijing Academy of Artificial Intelligence",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1664511063789-632c234f42c386ebd2710434.png"
      }
    },
    "publishedAt": "2026-02-05T09:37:32.000Z",
    "title": "OmniMoE: An Efficient MoE by Orchestrating Atomic Experts at Scale",
    "summary": "Mixture-of-Experts (MoE) architectures are evolving towards finer granularity to improve parameter efficiency. However, existing MoE designs face an inherent trade-off between the granularity of expert specialization and hardware execution efficiency. We propose OmniMoE, a system-algorithm co-designed framework that pushes expert granularity to its logical extreme. OmniMoE introduces vector-level Atomic Experts, enabling scalable routing and execution within a single MoE layer, while retaining a shared dense MLP branch for general-purpose processing. Although this atomic design maximizes capacity, it poses severe challenges for routing complexity and memory access. To address these, OmniMoE adopts a system-algorithm co-design: (i) a Cartesian Product Router that decomposes the massive index space to reduce routing complexity from O(N) to O(sqrt(N)); and (ii) Expert-Centric Scheduling that inverts the execution order to turn scattered, memory-bound lookups into efficient dense matrix operations. Validated on seven benchmarks, OmniMoE (with 1.7B active parameters) achieves 50.9% zero-shot accuracy across seven benchmarks, outperforming coarse-grained (e.g., DeepSeekMoE) and fine-grained (e.g., PEER) baselines. Crucially, OmniMoE reduces inference latency from 73ms to 6.7ms (a 10.9-fold speedup) compared to PEER, demonstrating that massive-scale fine-grained MoE can be fast and accurate. Our code is open-sourced at https://github.com/flash-algo/omni-moe.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.05711.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "673ab3647afcea17eb4378fd",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/673ab3647afcea17eb4378fd/YQB6zSH1LPxBMUYayIURi.png",
      "fullname": "Loser Cheems",
      "name": "JingzeShi",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 43,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "61be9739d2f9358e24ca0a4f",
      "name": "BAAI",
      "fullname": "Beijing Academy of Artificial Intelligence",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1664511063789-632c234f42c386ebd2710434.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2602.06139",
      "authors": [
        {
          "_id": "69894d1dbeecc443208d25fb",
          "name": "Ashish Seth",
          "hidden": false
        },
        {
          "_id": "69894d1dbeecc443208d25fc",
          "name": "Xinhao Mei",
          "hidden": false
        },
        {
          "_id": "69894d1dbeecc443208d25fd",
          "name": "Changsheng Zhao",
          "hidden": false
        },
        {
          "_id": "69894d1dbeecc443208d25fe",
          "name": "Varun Nagaraja",
          "hidden": false
        },
        {
          "_id": "69894d1dbeecc443208d25ff",
          "name": "Ernie Chang",
          "hidden": false
        },
        {
          "_id": "69894d1dbeecc443208d2600",
          "name": "Gregory P. Meyer",
          "hidden": false
        },
        {
          "_id": "69894d1dbeecc443208d2601",
          "name": "Gael Le Lan",
          "hidden": false
        },
        {
          "_id": "69894d1dbeecc443208d2602",
          "name": "Yunyang Xiong",
          "hidden": false
        },
        {
          "_id": "69894d1dbeecc443208d2603",
          "name": "Vikas Chandra",
          "hidden": false
        },
        {
          "_id": "69894d1dbeecc443208d2604",
          "name": "Yangyang Shi",
          "hidden": false
        },
        {
          "_id": "69894d1dbeecc443208d2605",
          "name": "Dinesh Manocha",
          "hidden": false
        },
        {
          "_id": "69894d1dbeecc443208d2606",
          "name": "Zhipeng Cai",
          "hidden": false
        }
      ],
      "publishedAt": "2026-02-05T19:16:55.000Z",
      "submittedOnDailyAt": "2026-02-09T00:44:50.275Z",
      "title": "EgoAVU: Egocentric Audio-Visual Understanding",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Understanding egocentric videos plays a vital role for embodied intelligence. Recent multi-modal large language models (MLLMs) can accept both visual and audio inputs. However, due to the challenge of obtaining text labels with coherent joint-modality information, whether MLLMs can jointly understand both modalities in egocentric videos remains under-explored. To address this problem, we introduce EgoAVU, a scalable data engine to automatically generate egocentric audio-visual narrations, questions, and answers. EgoAVU enriches human narrations with multimodal context and generates audio-visual narrations through cross-modal correlation modeling. Token-based video filtering and modular, graph-based curation ensure both data diversity and quality. Leveraging EgoAVU, we construct EgoAVU-Instruct, a large-scale training dataset of 3M samples, and EgoAVU-Bench, a manually verified evaluation split covering diverse tasks. EgoAVU-Bench clearly reveals the limitations of existing MLLMs: they bias heavily toward visual signals, often neglecting audio cues or failing to correspond audio with the visual source. Finetuning MLLMs on EgoAVU-Instruct effectively addresses this issue, enabling up to 113% performance improvement on EgoAVU-Bench. Such benefits also transfer to other benchmarks such as EgoTempo and EgoIllusion, achieving up to 28% relative performance gain. Code will be released to the community.",
      "upvotes": 0,
      "discussionId": "69894d1dbeecc443208d2607",
      "ai_summary": "Multi-modal large language models struggle to jointly understand audio and visual signals in egocentric videos, but a new scalable data engine and dataset significantly improve their performance through targeted fine-tuning.",
      "ai_keywords": [
        "multi-modal large language models",
        "egocentric videos",
        "audio-visual narrations",
        "cross-modal correlation modeling",
        "token-based video filtering",
        "modular curation",
        "graph-based curation",
        "EgoAVU-Instruct",
        "EgoAVU-Bench",
        "EgoTempo",
        "EgoIllusion"
      ]
    },
    "publishedAt": "2026-02-05T14:16:55.000Z",
    "title": "EgoAVU: Egocentric Audio-Visual Understanding",
    "summary": "Understanding egocentric videos plays a vital role for embodied intelligence. Recent multi-modal large language models (MLLMs) can accept both visual and audio inputs. However, due to the challenge of obtaining text labels with coherent joint-modality information, whether MLLMs can jointly understand both modalities in egocentric videos remains under-explored. To address this problem, we introduce EgoAVU, a scalable data engine to automatically generate egocentric audio-visual narrations, questions, and answers. EgoAVU enriches human narrations with multimodal context and generates audio-visual narrations through cross-modal correlation modeling. Token-based video filtering and modular, graph-based curation ensure both data diversity and quality. Leveraging EgoAVU, we construct EgoAVU-Instruct, a large-scale training dataset of 3M samples, and EgoAVU-Bench, a manually verified evaluation split covering diverse tasks. EgoAVU-Bench clearly reveals the limitations of existing MLLMs: they bias heavily toward visual signals, often neglecting audio cues or failing to correspond audio with the visual source. Finetuning MLLMs on EgoAVU-Instruct effectively addresses this issue, enabling up to 113% performance improvement on EgoAVU-Bench. Such benefits also transfer to other benchmarks such as EgoTempo and EgoIllusion, achieving up to 28% relative performance gain. Code will be released to the community.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.06139.png",
    "numComments": 0,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 229,
      "isUserFollowing": false
    },
    "isAuthorParticipating": false
  }
]