[
  {
    "paper": {
      "id": "2505.14302",
      "authors": [
        {
          "_id": "682e887e866a44f6a81409b1",
          "name": "Mengzhao Chen",
          "hidden": false
        },
        {
          "_id": "682e887e866a44f6a81409b2",
          "name": "Chaoyi Zhang",
          "hidden": false
        },
        {
          "_id": "682e887e866a44f6a81409b3",
          "name": "Jing Liu",
          "hidden": false
        },
        {
          "_id": "682e887e866a44f6a81409b4",
          "name": "Yutao Zeng",
          "hidden": false
        },
        {
          "_id": "682e887e866a44f6a81409b5",
          "name": "Zeyue Xue",
          "hidden": false
        },
        {
          "_id": "682e887e866a44f6a81409b6",
          "name": "Zhiheng Liu",
          "hidden": false
        },
        {
          "_id": "682e887e866a44f6a81409b7",
          "name": "Yunshui Li",
          "hidden": false
        },
        {
          "_id": "682e887e866a44f6a81409b8",
          "name": "Jin Ma",
          "hidden": false
        },
        {
          "_id": "682e887e866a44f6a81409b9",
          "name": "Jie Huang",
          "hidden": false
        },
        {
          "_id": "682e887e866a44f6a81409ba",
          "name": "Xun Zhou",
          "hidden": false
        },
        {
          "_id": "682e887e866a44f6a81409bb",
          "name": "Ping Luo",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-20T12:54:43.000Z",
      "submittedOnDailyAt": "2025-05-22T00:44:46.277Z",
      "title": "Scaling Law for Quantization-Aware Training",
      "submittedOnDailyBy": {
        "_id": "64aea082704210bf815e7551",
        "avatarUrl": "/avatars/5c8dc0df57596c526b2bccea21835f53.svg",
        "isPro": false,
        "fullname": "Mengzhao Chen",
        "user": "ChenMnZ",
        "type": "user"
      },
      "summary": "Large language models (LLMs) demand substantial computational and memory\nresources, creating deployment challenges. Quantization-aware training (QAT)\naddresses these challenges by reducing model precision while maintaining\nperformance. However, the scaling behavior of QAT, especially at 4-bit\nprecision (W4A4), is not well understood. Existing QAT scaling laws often\nignore key factors such as the number of training tokens and quantization\ngranularity, which limits their applicability. This paper proposes a unified\nscaling law for QAT that models quantization error as a function of model size,\ntraining data volume, and quantization group size. Through 268 QAT experiments,\nwe show that quantization error decreases as model size increases, but rises\nwith more training tokens and coarser quantization granularity. To identify the\nsources of W4A4 quantization error, we decompose it into weight and activation\ncomponents. Both components follow the overall trend of W4A4 quantization\nerror, but with different sensitivities. Specifically, weight quantization\nerror increases more rapidly with more training tokens. Further analysis shows\nthat the activation quantization error in the FC2 layer, caused by outliers, is\nthe primary bottleneck of W4A4 QAT quantization error. By applying\nmixed-precision quantization to address this bottleneck, we demonstrate that\nweight and activation quantization errors can converge to similar levels.\nAdditionally, with more training data, weight quantization error eventually\nexceeds activation quantization error, suggesting that reducing weight\nquantization error is also important in such scenarios. These findings offer\nkey insights for improving QAT research and development.",
      "upvotes": 19,
      "discussionId": "682e887e866a44f6a81409f0",
      "ai_summary": "A unified scaling law for quantization-aware training (QAT) identifies key factors affecting quantization error, leading to improvements through mixed-precision quantization.",
      "ai_keywords": [
        "quantization-aware training",
        "QAT",
        "quantization error",
        "model size",
        "training tokens",
        "quantization granularity",
        "weight quantization",
        "activation quantization",
        "mixed-precision quantization",
        "FC2 layer"
      ]
    },
    "publishedAt": "2025-05-20T08:54:43.000Z",
    "title": "Scaling Law for Quantization-Aware Training",
    "summary": "Large language models (LLMs) demand substantial computational and memory\nresources, creating deployment challenges. Quantization-aware training (QAT)\naddresses these challenges by reducing model precision while maintaining\nperformance. However, the scaling behavior of QAT, especially at 4-bit\nprecision (W4A4), is not well understood. Existing QAT scaling laws often\nignore key factors such as the number of training tokens and quantization\ngranularity, which limits their applicability. This paper proposes a unified\nscaling law for QAT that models quantization error as a function of model size,\ntraining data volume, and quantization group size. Through 268 QAT experiments,\nwe show that quantization error decreases as model size increases, but rises\nwith more training tokens and coarser quantization granularity. To identify the\nsources of W4A4 quantization error, we decompose it into weight and activation\ncomponents. Both components follow the overall trend of W4A4 quantization\nerror, but with different sensitivities. Specifically, weight quantization\nerror increases more rapidly with more training tokens. Further analysis shows\nthat the activation quantization error in the FC2 layer, caused by outliers, is\nthe primary bottleneck of W4A4 QAT quantization error. By applying\nmixed-precision quantization to address this bottleneck, we demonstrate that\nweight and activation quantization errors can converge to similar levels.\nAdditionally, with more training data, weight quantization error eventually\nexceeds activation quantization error, suggesting that reducing weight\nquantization error is also important in such scenarios. These findings offer\nkey insights for improving QAT research and development.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.14302.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64aea082704210bf815e7551",
      "avatarUrl": "/avatars/5c8dc0df57596c526b2bccea21835f53.svg",
      "fullname": "Mengzhao Chen",
      "name": "ChenMnZ",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 22
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.15277",
      "authors": [
        {
          "_id": "682e854551706f69070aca6b",
          "name": "Hyungjoo Chae",
          "hidden": false
        },
        {
          "_id": "682e854551706f69070aca6c",
          "name": "Sunghwan Kim",
          "hidden": false
        },
        {
          "_id": "682e854551706f69070aca6d",
          "name": "Junhee Cho",
          "hidden": false
        },
        {
          "_id": "682e854551706f69070aca6e",
          "name": "Seungone Kim",
          "hidden": false
        },
        {
          "_id": "682e854551706f69070aca6f",
          "name": "Seungjun Moon",
          "hidden": false
        },
        {
          "_id": "682e854551706f69070aca70",
          "name": "Gyeom Hwangbo",
          "hidden": false
        },
        {
          "_id": "682e854551706f69070aca71",
          "name": "Dongha Lim",
          "hidden": false
        },
        {
          "_id": "682e854551706f69070aca72",
          "name": "Minjin Kim",
          "hidden": false
        },
        {
          "_id": "682e854551706f69070aca73",
          "name": "Yeonjun Hwang",
          "hidden": false
        },
        {
          "_id": "682e854551706f69070aca74",
          "name": "Minju Gwak",
          "hidden": false
        },
        {
          "_id": "682e854551706f69070aca75",
          "name": "Dongwook Choi",
          "hidden": false
        },
        {
          "_id": "682e854551706f69070aca76",
          "name": "Minseok Kang",
          "hidden": false
        },
        {
          "_id": "682e854551706f69070aca77",
          "name": "Gwanhoon Im",
          "hidden": false
        },
        {
          "_id": "682e854551706f69070aca78",
          "name": "ByeongUng Cho",
          "hidden": false
        },
        {
          "_id": "682e854551706f69070aca79",
          "name": "Hyojun Kim",
          "hidden": false
        },
        {
          "_id": "682e854551706f69070aca7a",
          "name": "Jun Hee Han",
          "hidden": false
        },
        {
          "_id": "682e854551706f69070aca7b",
          "name": "Taeyoon Kwon",
          "hidden": false
        },
        {
          "_id": "682e854551706f69070aca7c",
          "name": "Minju Kim",
          "hidden": false
        },
        {
          "_id": "682e854551706f69070aca7d",
          "name": "Beong-woo Kwak",
          "hidden": false
        },
        {
          "_id": "682e854551706f69070aca7e",
          "name": "Dongjin Kang",
          "hidden": false
        },
        {
          "_id": "682e854551706f69070aca7f",
          "name": "Jinyoung Yeo",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/64c8f4cec547ed5243ebd0a8/hXqaaoJTvW35xMW1lPVv0.png"
      ],
      "publishedAt": "2025-05-21T08:56:55.000Z",
      "submittedOnDailyAt": "2025-05-22T00:31:53.858Z",
      "title": "Web-Shepherd: Advancing PRMs for Reinforcing Web Agents",
      "submittedOnDailyBy": {
        "_id": "64c8f4cec547ed5243ebd0a8",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64c8f4cec547ed5243ebd0a8/MiOH5YbMg8Gh9KYlQsLmX.jpeg",
        "isPro": false,
        "fullname": "Hyungjoo Chae",
        "user": "hyungjoochae",
        "type": "user"
      },
      "summary": "Web navigation is a unique domain that can automate many repetitive real-life\ntasks and is challenging as it requires long-horizon sequential decision making\nbeyond typical multimodal large language model (MLLM) tasks. Yet, specialized\nreward models for web navigation that can be utilized during both training and\ntest-time have been absent until now. Despite the importance of speed and\ncost-effectiveness, prior works have utilized MLLMs as reward models, which\nposes significant constraints for real-world deployment. To address this, in\nthis work, we propose the first process reward model (PRM) called Web-Shepherd\nwhich could assess web navigation trajectories in a step-level. To achieve\nthis, we first construct the WebPRM Collection, a large-scale dataset with 40K\nstep-level preference pairs and annotated checklists spanning diverse domains\nand difficulty levels. Next, we also introduce the WebRewardBench, the first\nmeta-evaluation benchmark for evaluating PRMs. In our experiments, we observe\nthat our Web-Shepherd achieves about 30 points better accuracy compared to\nusing GPT-4o on WebRewardBench. Furthermore, when testing on WebArena-lite by\nusing GPT-4o-mini as the policy and Web-Shepherd as the verifier, we achieve\n10.9 points better performance, in 10 less cost compared to using GPT-4o-mini\nas the verifier. Our model, dataset, and code are publicly available at LINK.",
      "upvotes": 16,
      "discussionId": "682e854951706f69070acbf0",
      "githubRepo": "https://github.com/kyle8581/Web-Shepherd",
      "ai_summary": "The paper introduces Web-Shepherd, a process reward model for web navigation, which improves accuracy and cost-effectiveness in step-level trajectory assessment compared to existing multimodal large language models.",
      "ai_keywords": [
        "multimodal large language model",
        "process reward model",
        "web navigation",
        "webPRM collection",
        "webrewardbench",
        "long-horizon sequential decision making",
        "preference pairs",
        "annotated checklists",
        "step-level assessment",
        "webarena-lite",
        "policy",
        "verifier"
      ]
    },
    "publishedAt": "2025-05-21T04:56:55.000Z",
    "title": "Web-Shepherd: Advancing PRMs for Reinforcing Web Agents",
    "summary": "Web navigation is a unique domain that can automate many repetitive real-life\ntasks and is challenging as it requires long-horizon sequential decision making\nbeyond typical multimodal large language model (MLLM) tasks. Yet, specialized\nreward models for web navigation that can be utilized during both training and\ntest-time have been absent until now. Despite the importance of speed and\ncost-effectiveness, prior works have utilized MLLMs as reward models, which\nposes significant constraints for real-world deployment. To address this, in\nthis work, we propose the first process reward model (PRM) called Web-Shepherd\nwhich could assess web navigation trajectories in a step-level. To achieve\nthis, we first construct the WebPRM Collection, a large-scale dataset with 40K\nstep-level preference pairs and annotated checklists spanning diverse domains\nand difficulty levels. Next, we also introduce the WebRewardBench, the first\nmeta-evaluation benchmark for evaluating PRMs. In our experiments, we observe\nthat our Web-Shepherd achieves about 30 points better accuracy compared to\nusing GPT-4o on WebRewardBench. Furthermore, when testing on WebArena-lite by\nusing GPT-4o-mini as the policy and Web-Shepherd as the verifier, we achieve\n10.9 points better performance, in 10 less cost compared to using GPT-4o-mini\nas the verifier. Our model, dataset, and code are publicly available at LINK.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/64c8f4cec547ed5243ebd0a8/hXqaaoJTvW35xMW1lPVv0.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.15277.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64c8f4cec547ed5243ebd0a8",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64c8f4cec547ed5243ebd0a8/MiOH5YbMg8Gh9KYlQsLmX.jpeg",
      "fullname": "Hyungjoo Chae",
      "name": "hyungjoochae",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 12
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.15809",
      "authors": [
        {
          "_id": "682e7e061d7637a25846bf52",
          "name": "Ling Yang",
          "hidden": false
        },
        {
          "_id": "682e7e061d7637a25846bf53",
          "name": "Ye Tian",
          "hidden": false
        },
        {
          "_id": "682e7e061d7637a25846bf54",
          "name": "Bowen Li",
          "hidden": false
        },
        {
          "_id": "682e7e061d7637a25846bf55",
          "name": "Xinchen Zhang",
          "hidden": false
        },
        {
          "_id": "682e7e061d7637a25846bf56",
          "name": "Ke Shen",
          "hidden": false
        },
        {
          "_id": "682e7e061d7637a25846bf57",
          "name": "Yunhai Tong",
          "hidden": false
        },
        {
          "_id": "682e7e061d7637a25846bf58",
          "name": "Mengdi Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-21T17:59:05.000Z",
      "submittedOnDailyAt": "2025-05-22T00:24:15.122Z",
      "title": "MMaDA: Multimodal Large Diffusion Language Models",
      "submittedOnDailyBy": {
        "_id": "64fde4e252e82dd432b74ce9",
        "avatarUrl": "/avatars/061a69d858b86d1600be916122cae7fc.svg",
        "isPro": false,
        "fullname": "Ling Yang",
        "user": "Lingaaaaaaa",
        "type": "user"
      },
      "summary": "We introduce MMaDA, a novel class of multimodal diffusion foundation models\ndesigned to achieve superior performance across diverse domains such as textual\nreasoning, multimodal understanding, and text-to-image generation. The approach\nis distinguished by three key innovations: (i) MMaDA adopts a unified diffusion\narchitecture with a shared probabilistic formulation and a modality-agnostic\ndesign, eliminating the need for modality-specific components. This\narchitecture ensures seamless integration and processing across different data\ntypes. (ii) We implement a mixed long chain-of-thought (CoT) fine-tuning\nstrategy that curates a unified CoT format across modalities. By aligning\nreasoning processes between textual and visual domains, this strategy\nfacilitates cold-start training for the final reinforcement learning (RL)\nstage, thereby enhancing the model's ability to handle complex tasks from the\noutset. (iii) We propose UniGRPO, a unified policy-gradient-based RL algorithm\nspecifically tailored for diffusion foundation models. Utilizing diversified\nreward modeling, UniGRPO unifies post-training across both reasoning and\ngeneration tasks, ensuring consistent performance improvements. Experimental\nresults demonstrate that MMaDA-8B exhibits strong generalization capabilities\nas a unified multimodal foundation model. It surpasses powerful models like\nLLaMA-3-7B and Qwen2-7B in textual reasoning, outperforms Show-o and SEED-X in\nmultimodal understanding, and excels over SDXL and Janus in text-to-image\ngeneration. These achievements highlight MMaDA's effectiveness in bridging the\ngap between pretraining and post-training within unified diffusion\narchitectures, providing a comprehensive framework for future research and\ndevelopment. We open-source our code and trained models at:\nhttps://github.com/Gen-Verse/MMaDA",
      "upvotes": 12,
      "discussionId": "682e7e0a1d7637a25846c03b",
      "projectPage": "https://huggingface.co/spaces/Gen-Verse/MMaDA",
      "githubRepo": "https://github.com/Gen-Verse/MMaDA",
      "ai_summary": "MMaDA, a multimodal diffusion foundation model, achieves superior performance through a unified architecture, mixed long chain-of-thought fine-tuning, and a unified policy-gradient-based RL algorithm.",
      "ai_keywords": [
        "multimodal diffusion foundation models",
        "unified diffusion architecture",
        "modality-agnostic design",
        "mixed long chain-of-thought fine-tuning",
        "cold-start training",
        "reinforcement learning",
        "UniGRPO",
        "policy-gradient-based RL algorithm",
        "diversified reward modeling",
        "generalization capabilities",
        "textual reasoning",
        "multimodal understanding",
        "text-to-image generation"
      ]
    },
    "publishedAt": "2025-05-21T13:59:05.000Z",
    "title": "MMaDA: Multimodal Large Diffusion Language Models",
    "summary": "We introduce MMaDA, a novel class of multimodal diffusion foundation models\ndesigned to achieve superior performance across diverse domains such as textual\nreasoning, multimodal understanding, and text-to-image generation. The approach\nis distinguished by three key innovations: (i) MMaDA adopts a unified diffusion\narchitecture with a shared probabilistic formulation and a modality-agnostic\ndesign, eliminating the need for modality-specific components. This\narchitecture ensures seamless integration and processing across different data\ntypes. (ii) We implement a mixed long chain-of-thought (CoT) fine-tuning\nstrategy that curates a unified CoT format across modalities. By aligning\nreasoning processes between textual and visual domains, this strategy\nfacilitates cold-start training for the final reinforcement learning (RL)\nstage, thereby enhancing the model's ability to handle complex tasks from the\noutset. (iii) We propose UniGRPO, a unified policy-gradient-based RL algorithm\nspecifically tailored for diffusion foundation models. Utilizing diversified\nreward modeling, UniGRPO unifies post-training across both reasoning and\ngeneration tasks, ensuring consistent performance improvements. Experimental\nresults demonstrate that MMaDA-8B exhibits strong generalization capabilities\nas a unified multimodal foundation model. It surpasses powerful models like\nLLaMA-3-7B and Qwen2-7B in textual reasoning, outperforms Show-o and SEED-X in\nmultimodal understanding, and excels over SDXL and Janus in text-to-image\ngeneration. These achievements highlight MMaDA's effectiveness in bridging the\ngap between pretraining and post-training within unified diffusion\narchitectures, providing a comprehensive framework for future research and\ndevelopment. We open-source our code and trained models at:\nhttps://github.com/Gen-Verse/MMaDA",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.15809.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64fde4e252e82dd432b74ce9",
      "avatarUrl": "/avatars/061a69d858b86d1600be916122cae7fc.svg",
      "fullname": "Ling Yang",
      "name": "Lingaaaaaaa",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 8
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.13909",
      "authors": [
        {
          "_id": "682d525994ed89a9b2aec35d",
          "name": "Yanheng He",
          "hidden": false
        },
        {
          "_id": "682d525994ed89a9b2aec35e",
          "name": "Jiahe Jin",
          "hidden": false
        },
        {
          "_id": "682d525994ed89a9b2aec35f",
          "name": "Pengfei Liu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-20T04:20:18.000Z",
      "submittedOnDailyAt": "2025-05-22T00:48:35.836Z",
      "title": "Efficient Agent Training for Computer Use",
      "submittedOnDailyBy": {
        "_id": "663f5e959e6f865ec6d4fb62",
        "avatarUrl": "/avatars/eb0a908562b2c57335ae8bb949220430.svg",
        "isPro": false,
        "fullname": "Jiahe Jin",
        "user": "zizi-0123",
        "type": "user"
      },
      "summary": "Scaling up high-quality trajectory data has long been a critical bottleneck\nfor developing human-like computer use agents. We introduce PC Agent-E, an\nefficient agent training framework that significantly reduces reliance on\nlarge-scale human demonstrations. Starting with just 312 human-annotated\ncomputer use trajectories, we further improved data quality by synthesizing\ndiverse action decisions with Claude 3.7 Sonnet. Trained on these enriched\ntrajectories, our PC Agent-E model achieved a remarkable 141% relative\nimprovement, surpassing the strong Claude 3.7 Sonnet with extended thinking on\nWindowsAgentArena-V2, an improved benchmark we also released. Furthermore, PC\nAgent-E demonstrates strong generalizability to different operating systems on\nOSWorld. Our findings suggest that strong computer use capabilities can be\nstimulated from a small amount of high-quality trajectory data.",
      "upvotes": 11,
      "discussionId": "682d525a94ed89a9b2aec397",
      "githubRepo": "https://github.com/GAIR-NLP/PC-Agent-E",
      "ai_summary": "PC Agent-E framework improves data efficiency and achieves superior performance on human-like computer use tasks through enhanced trajectory synthesis and training.",
      "ai_keywords": [
        "agent training framework",
        "human-annotated trajectories",
        "action decisions",
        "Claude 3.7 Sonnet",
        "WindowsAgentArena-V2",
        "OSWorld",
        "generalizability"
      ]
    },
    "publishedAt": "2025-05-20T00:20:18.000Z",
    "title": "Efficient Agent Training for Computer Use",
    "summary": "Scaling up high-quality trajectory data has long been a critical bottleneck\nfor developing human-like computer use agents. We introduce PC Agent-E, an\nefficient agent training framework that significantly reduces reliance on\nlarge-scale human demonstrations. Starting with just 312 human-annotated\ncomputer use trajectories, we further improved data quality by synthesizing\ndiverse action decisions with Claude 3.7 Sonnet. Trained on these enriched\ntrajectories, our PC Agent-E model achieved a remarkable 141% relative\nimprovement, surpassing the strong Claude 3.7 Sonnet with extended thinking on\nWindowsAgentArena-V2, an improved benchmark we also released. Furthermore, PC\nAgent-E demonstrates strong generalizability to different operating systems on\nOSWorld. Our findings suggest that strong computer use capabilities can be\nstimulated from a small amount of high-quality trajectory data.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.13909.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "663f5e959e6f865ec6d4fb62",
      "avatarUrl": "/avatars/eb0a908562b2c57335ae8bb949220430.svg",
      "fullname": "Jiahe Jin",
      "name": "zizi-0123",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.15210",
      "authors": [
        {
          "_id": "682e84d83291b134b3370184",
          "name": "Jie Ma",
          "hidden": false
        },
        {
          "_id": "682e84d83291b134b3370185",
          "name": "Ning Qu",
          "hidden": false
        },
        {
          "_id": "682e84d83291b134b3370186",
          "name": "Zhitao Gao",
          "hidden": false
        },
        {
          "_id": "682e84d83291b134b3370187",
          "name": "Rui Xing",
          "hidden": false
        },
        {
          "_id": "682e84d83291b134b3370188",
          "name": "Jun Liu",
          "hidden": false
        },
        {
          "_id": "682e84d83291b134b3370189",
          "name": "Hongbin Pei",
          "hidden": false
        },
        {
          "_id": "682e84d83291b134b337018a",
          "name": "Jiang Xie",
          "hidden": false
        },
        {
          "_id": "682e84d83291b134b337018b",
          "name": "Linyun Song",
          "hidden": false
        },
        {
          "_id": "682e84d83291b134b337018c",
          "name": "Pinghui Wang",
          "hidden": false
        },
        {
          "_id": "682e84d83291b134b337018d",
          "name": "Jing Tao",
          "hidden": false
        },
        {
          "_id": "682e84d83291b134b337018e",
          "name": "Zhou Su",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-21T07:38:45.000Z",
      "submittedOnDailyAt": "2025-05-22T00:31:54.508Z",
      "title": "Deliberation on Priors: Trustworthy Reasoning of Large Language Models\n  on Knowledge Graphs",
      "submittedOnDailyBy": {
        "_id": "67a7099286a55d5569acb213",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/rYISkvTtyraUdbgSsNfpC.png",
        "isPro": false,
        "fullname": "JieMa",
        "user": "JamesMile",
        "type": "user"
      },
      "summary": "Knowledge graph-based retrieval-augmented generation seeks to mitigate\nhallucinations in Large Language Models (LLMs) caused by insufficient or\noutdated knowledge. However, existing methods often fail to fully exploit the\nprior knowledge embedded in knowledge graphs (KGs), particularly their\nstructural information and explicit or implicit constraints. The former can\nenhance the faithfulness of LLMs' reasoning, while the latter can improve the\nreliability of response generation. Motivated by these, we propose a\ntrustworthy reasoning framework, termed Deliberation over Priors (DP), which\nsufficiently utilizes the priors contained in KGs. Specifically, DP adopts a\nprogressive knowledge distillation strategy that integrates structural priors\ninto LLMs through a combination of supervised fine-tuning and Kahneman-Tversky\noptimization, thereby improving the faithfulness of relation path generation.\nFurthermore, our framework employs a reasoning-introspection strategy, which\nguides LLMs to perform refined reasoning verification based on extracted\nconstraint priors, ensuring the reliability of response generation. Extensive\nexperiments on three benchmark datasets demonstrate that DP achieves new\nstate-of-the-art performance, especially a Hit@1 improvement of 13% on the\nComplexWebQuestions dataset, and generates highly trustworthy responses. We\nalso conduct various analyses to verify its flexibility and practicality. The\ncode is available at https://github.com/reml-group/Deliberation-on-Priors.",
      "upvotes": 8,
      "discussionId": "682e84d93291b134b33701cb",
      "githubRepo": "https://github.com/reml-group/Deliberation-on-Priors",
      "ai_summary": "The Deliberation over Priors framework enhances the trustworthiness of LLMs by integrating structural and constraint priors from knowledge graphs through knowledge distillation and reasoning introspection.",
      "ai_keywords": [
        "knowledge graph-based retrieval-augmented generation",
        "Large Language Models (LLMs)",
        "knowledge graphs (KGs)",
        "structural information",
        "explicit constraints",
        "implicit constraints",
        "trustworthworthy reasoning framework",
        "Deliberation over Priors (DP)",
        "progressive knowledge distillation",
        "supervised fine-tuning",
        "Kahneman-Tversky optimization",
        "relation path generation",
        "reasoning-introspection strategy",
        "ComplexWebQuestions dataset",
        "Hit@1 improvement"
      ]
    },
    "publishedAt": "2025-05-21T03:38:45.000Z",
    "title": "Deliberation on Priors: Trustworthy Reasoning of Large Language Models\n  on Knowledge Graphs",
    "summary": "Knowledge graph-based retrieval-augmented generation seeks to mitigate\nhallucinations in Large Language Models (LLMs) caused by insufficient or\noutdated knowledge. However, existing methods often fail to fully exploit the\nprior knowledge embedded in knowledge graphs (KGs), particularly their\nstructural information and explicit or implicit constraints. The former can\nenhance the faithfulness of LLMs' reasoning, while the latter can improve the\nreliability of response generation. Motivated by these, we propose a\ntrustworthy reasoning framework, termed Deliberation over Priors (DP), which\nsufficiently utilizes the priors contained in KGs. Specifically, DP adopts a\nprogressive knowledge distillation strategy that integrates structural priors\ninto LLMs through a combination of supervised fine-tuning and Kahneman-Tversky\noptimization, thereby improving the faithfulness of relation path generation.\nFurthermore, our framework employs a reasoning-introspection strategy, which\nguides LLMs to perform refined reasoning verification based on extracted\nconstraint priors, ensuring the reliability of response generation. Extensive\nexperiments on three benchmark datasets demonstrate that DP achieves new\nstate-of-the-art performance, especially a Hit@1 improvement of 13% on the\nComplexWebQuestions dataset, and generates highly trustworthy responses. We\nalso conduct various analyses to verify its flexibility and practicality. The\ncode is available at https://github.com/reml-group/Deliberation-on-Priors.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.15210.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "67a7099286a55d5569acb213",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/rYISkvTtyraUdbgSsNfpC.png",
      "fullname": "JieMa",
      "name": "JamesMile",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.15765",
      "authors": [
        {
          "_id": "682e85aa7b41e70cf11758b6",
          "name": "Kaizhi Zheng",
          "hidden": false
        },
        {
          "_id": "682e85aa7b41e70cf11758b7",
          "name": "Ruijian Zhang",
          "hidden": false
        },
        {
          "_id": "682e85aa7b41e70cf11758b8",
          "name": "Jing Gu",
          "hidden": false
        },
        {
          "_id": "682e85aa7b41e70cf11758b9",
          "name": "Jie Yang",
          "hidden": false
        },
        {
          "_id": "682e85aa7b41e70cf11758ba",
          "name": "Xin Eric Wang",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/64679a226192d39142245e5e/LlIYiX21uaVLjqWu6lAQI.mp4"
      ],
      "publishedAt": "2025-05-21T17:10:47.000Z",
      "submittedOnDailyAt": "2025-05-22T00:36:41.981Z",
      "title": "Constructing a 3D Town from a Single Image",
      "submittedOnDailyBy": {
        "_id": "64679a226192d39142245e5e",
        "avatarUrl": "/avatars/05abee0b6317f100923936ca2099e9eb.svg",
        "isPro": false,
        "fullname": "Xin Eric Wang",
        "user": "xw-eric",
        "type": "user"
      },
      "summary": "Acquiring detailed 3D scenes typically demands costly equipment, multi-view\ndata, or labor-intensive modeling. Therefore, a lightweight alternative,\ngenerating complex 3D scenes from a single top-down image, plays an essential\nrole in real-world applications. While recent 3D generative models have\nachieved remarkable results at the object level, their extension to full-scene\ngeneration often leads to inconsistent geometry, layout hallucinations, and\nlow-quality meshes. In this work, we introduce 3DTown, a training-free\nframework designed to synthesize realistic and coherent 3D scenes from a single\ntop-down view. Our method is grounded in two principles: region-based\ngeneration to improve image-to-3D alignment and resolution, and spatial-aware\n3D inpainting to ensure global scene coherence and high-quality geometry\ngeneration. Specifically, we decompose the input image into overlapping regions\nand generate each using a pretrained 3D object generator, followed by a masked\nrectified flow inpainting process that fills in missing geometry while\nmaintaining structural continuity. This modular design allows us to overcome\nresolution bottlenecks and preserve spatial structure without requiring 3D\nsupervision or fine-tuning. Extensive experiments across diverse scenes show\nthat 3DTown outperforms state-of-the-art baselines, including Trellis,\nHunyuan3D-2, and TripoSG, in terms of geometry quality, spatial coherence, and\ntexture fidelity. Our results demonstrate that high-quality 3D town generation\nis achievable from a single image using a principled, training-free approach.",
      "upvotes": 3,
      "discussionId": "682e85b17b41e70cf1175aa0",
      "ai_summary": "A training-free framework named 3DTown generates realistic 3D scenes from a single top-down image using region-based generation and spatial-aware 3D inpainting techniques.",
      "ai_keywords": [
        "3D generative models",
        "3DTown",
        "region-based generation",
        "spatial-aware 3D inpainting",
        "masked rectified flow",
        "Trellis",
        "Hunyuan3D-2",
        "TripoSG"
      ]
    },
    "publishedAt": "2025-05-21T13:10:47.000Z",
    "title": "Constructing a 3D Town from a Single Image",
    "summary": "Acquiring detailed 3D scenes typically demands costly equipment, multi-view\ndata, or labor-intensive modeling. Therefore, a lightweight alternative,\ngenerating complex 3D scenes from a single top-down image, plays an essential\nrole in real-world applications. While recent 3D generative models have\nachieved remarkable results at the object level, their extension to full-scene\ngeneration often leads to inconsistent geometry, layout hallucinations, and\nlow-quality meshes. In this work, we introduce 3DTown, a training-free\nframework designed to synthesize realistic and coherent 3D scenes from a single\ntop-down view. Our method is grounded in two principles: region-based\ngeneration to improve image-to-3D alignment and resolution, and spatial-aware\n3D inpainting to ensure global scene coherence and high-quality geometry\ngeneration. Specifically, we decompose the input image into overlapping regions\nand generate each using a pretrained 3D object generator, followed by a masked\nrectified flow inpainting process that fills in missing geometry while\nmaintaining structural continuity. This modular design allows us to overcome\nresolution bottlenecks and preserve spatial structure without requiring 3D\nsupervision or fine-tuning. Extensive experiments across diverse scenes show\nthat 3DTown outperforms state-of-the-art baselines, including Trellis,\nHunyuan3D-2, and TripoSG, in terms of geometry quality, spatial coherence, and\ntexture fidelity. Our results demonstrate that high-quality 3D town generation\nis achievable from a single image using a principled, training-free approach.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/64679a226192d39142245e5e/LlIYiX21uaVLjqWu6lAQI.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.15765.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64679a226192d39142245e5e",
      "avatarUrl": "/avatars/05abee0b6317f100923936ca2099e9eb.svg",
      "fullname": "Xin Eric Wang",
      "name": "xw-eric",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.12650",
      "authors": [
        {
          "_id": "682c057b36dd2dbca3931150",
          "name": "Yaotian Yang",
          "hidden": false
        },
        {
          "_id": "682c057b36dd2dbca3931151",
          "name": "Yiwen Tang",
          "hidden": false
        },
        {
          "_id": "682c057b36dd2dbca3931152",
          "name": "Yizhe Chen",
          "hidden": false
        },
        {
          "_id": "682c057b36dd2dbca3931153",
          "name": "Xiao Chen",
          "hidden": false
        },
        {
          "_id": "682c057b36dd2dbca3931154",
          "name": "Jiangjie Qiu",
          "hidden": false
        },
        {
          "_id": "682c057b36dd2dbca3931155",
          "name": "Hao Xiong",
          "hidden": false
        },
        {
          "_id": "682c057b36dd2dbca3931156",
          "name": "Haoyu Yin",
          "hidden": false
        },
        {
          "_id": "682c057b36dd2dbca3931157",
          "name": "Zhiyao Luo",
          "hidden": false
        },
        {
          "_id": "682c057b36dd2dbca3931158",
          "name": "Yifei Zhang",
          "hidden": false
        },
        {
          "_id": "682c057b36dd2dbca3931159",
          "name": "Sijia Tao",
          "hidden": false
        },
        {
          "_id": "682c057b36dd2dbca393115a",
          "name": "Wentao Li",
          "hidden": false
        },
        {
          "_id": "682c057b36dd2dbca393115b",
          "name": "Qinghua Zhang",
          "hidden": false
        },
        {
          "_id": "682c057b36dd2dbca393115c",
          "name": "Yuqiang Li",
          "hidden": false
        },
        {
          "_id": "682c057b36dd2dbca393115d",
          "name": "Wanli Ouyang",
          "hidden": false
        },
        {
          "_id": "682c057b36dd2dbca393115e",
          "name": "Bin Zhao",
          "hidden": false
        },
        {
          "_id": "682c057b36dd2dbca393115f",
          "name": "Xiaonan Wang",
          "hidden": false
        },
        {
          "_id": "682c057b36dd2dbca3931160",
          "name": "Fei Wei",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-19T03:04:50.000Z",
      "submittedOnDailyAt": "2025-05-22T00:29:51.182Z",
      "title": "AutoMat: Enabling Automated Crystal Structure Reconstruction from\n  Microscopy via Agentic Tool Use",
      "submittedOnDailyBy": {
        "_id": "6552f1ad5d55ccb20e9142a0",
        "avatarUrl": "/avatars/0e3e80cba64b5ae0bc5638694ac33dbf.svg",
        "isPro": false,
        "fullname": "Ivan Tang",
        "user": "IvanTang",
        "type": "user"
      },
      "summary": "Machine learning-based interatomic potentials and force fields depend\ncritically on accurate atomic structures, yet such data are scarce due to the\nlimited availability of experimentally resolved crystals. Although\natomic-resolution electron microscopy offers a potential source of structural\ndata, converting these images into simulation-ready formats remains\nlabor-intensive and error-prone, creating a bottleneck for model training and\nvalidation. We introduce AutoMat, an end-to-end, agent-assisted pipeline that\nautomatically transforms scanning transmission electron microscopy (STEM)\nimages into atomic crystal structures and predicts their physical properties.\nAutoMat combines pattern-adaptive denoising, physics-guided template retrieval,\nsymmetry-aware atomic reconstruction, fast relaxation and property prediction\nvia MatterSim, and coordinated orchestration across all stages. We propose the\nfirst dedicated STEM2Mat-Bench for this task and evaluate performance using\nlattice RMSD, formation energy MAE, and structure-matching success rate. By\norchestrating external tool calls, AutoMat enables a text-only LLM to\noutperform vision-language models in this domain, achieving closed-loop\nreasoning throughout the pipeline. In large-scale experiments over 450\nstructure samples, AutoMat substantially outperforms existing multimodal large\nlanguage models and tools. These results validate both AutoMat and\nSTEM2Mat-Bench, marking a key step toward bridging microscopy and atomistic\nsimulation in materials science.The code and dataset are publicly available at\nhttps://github.com/yyt-2378/AutoMat and\nhttps://huggingface.co/datasets/yaotianvector/STEM2Mat.",
      "upvotes": 3,
      "discussionId": "682c057d36dd2dbca3931206",
      "ai_summary": "AutoMat, an agent-assisted pipeline, transforms atomic-resolution STEM images into simulation-ready atomic crystal structures and predicts their properties, overcoming the bottleneck in data availability and processing.",
      "ai_keywords": [
        "end-to-end pipeline",
        "agent-assisted",
        "scanning transmission electron microscopy (STEM)",
        "pattern-adaptive denoising",
        "physics-guided template retrieval",
        "symmetry-aware atomic reconstruction",
        "fast relaxation",
        "property prediction",
        "MatterSim",
        "STEM2Mat-Bench",
        "lattice RMSD",
        "formation energy MAE",
        "structure-matching success rate",
        "multimodal large language models"
      ]
    },
    "publishedAt": "2025-05-18T23:04:50.000Z",
    "title": "AutoMat: Enabling Automated Crystal Structure Reconstruction from\n  Microscopy via Agentic Tool Use",
    "summary": "Machine learning-based interatomic potentials and force fields depend\ncritically on accurate atomic structures, yet such data are scarce due to the\nlimited availability of experimentally resolved crystals. Although\natomic-resolution electron microscopy offers a potential source of structural\ndata, converting these images into simulation-ready formats remains\nlabor-intensive and error-prone, creating a bottleneck for model training and\nvalidation. We introduce AutoMat, an end-to-end, agent-assisted pipeline that\nautomatically transforms scanning transmission electron microscopy (STEM)\nimages into atomic crystal structures and predicts their physical properties.\nAutoMat combines pattern-adaptive denoising, physics-guided template retrieval,\nsymmetry-aware atomic reconstruction, fast relaxation and property prediction\nvia MatterSim, and coordinated orchestration across all stages. We propose the\nfirst dedicated STEM2Mat-Bench for this task and evaluate performance using\nlattice RMSD, formation energy MAE, and structure-matching success rate. By\norchestrating external tool calls, AutoMat enables a text-only LLM to\noutperform vision-language models in this domain, achieving closed-loop\nreasoning throughout the pipeline. In large-scale experiments over 450\nstructure samples, AutoMat substantially outperforms existing multimodal large\nlanguage models and tools. These results validate both AutoMat and\nSTEM2Mat-Bench, marking a key step toward bridging microscopy and atomistic\nsimulation in materials science.The code and dataset are publicly available at\nhttps://github.com/yyt-2378/AutoMat and\nhttps://huggingface.co/datasets/yaotianvector/STEM2Mat.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.12650.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6552f1ad5d55ccb20e9142a0",
      "avatarUrl": "/avatars/0e3e80cba64b5ae0bc5638694ac33dbf.svg",
      "fullname": "Ivan Tang",
      "name": "IvanTang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.15778",
      "authors": [
        {
          "_id": "682e8b59a5b1e59c6978645a",
          "name": "Zhen Zhang",
          "hidden": false
        },
        {
          "_id": "682e8b59a5b1e59c6978645b",
          "name": "Xuehai He",
          "hidden": false
        },
        {
          "_id": "682e8b59a5b1e59c6978645c",
          "name": "Weixiang Yan",
          "hidden": false
        },
        {
          "_id": "682e8b59a5b1e59c6978645d",
          "name": "Ao Shen",
          "hidden": false
        },
        {
          "_id": "682e8b59a5b1e59c6978645e",
          "name": "Chenyang Zhao",
          "hidden": false
        },
        {
          "_id": "682e8b59a5b1e59c6978645f",
          "name": "Shuohang Wang",
          "hidden": false
        },
        {
          "_id": "682e8b59a5b1e59c69786460",
          "name": "Yelong Shen",
          "hidden": false
        },
        {
          "_id": "682e8b59a5b1e59c69786461",
          "name": "Xin Eric Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-21T17:29:15.000Z",
      "submittedOnDailyAt": "2025-05-22T00:56:40.217Z",
      "title": "Soft Thinking: Unlocking the Reasoning Potential of LLMs in Continuous\n  Concept Space",
      "submittedOnDailyBy": {
        "_id": "64679a226192d39142245e5e",
        "avatarUrl": "/avatars/05abee0b6317f100923936ca2099e9eb.svg",
        "isPro": false,
        "fullname": "Xin Eric Wang",
        "user": "xw-eric",
        "type": "user"
      },
      "summary": "Human cognition typically involves thinking through abstract, fluid concepts\nrather than strictly using discrete linguistic tokens. Current reasoning\nmodels, however, are constrained to reasoning within the boundaries of human\nlanguage, processing discrete token embeddings that represent fixed points in\nthe semantic space. This discrete constraint restricts the expressive power and\nupper potential of such reasoning models, often causing incomplete exploration\nof reasoning paths, as standard Chain-of-Thought (CoT) methods rely on sampling\none token per step. In this work, we introduce Soft Thinking, a training-free\nmethod that emulates human-like \"soft\" reasoning by generating soft, abstract\nconcept tokens in a continuous concept space. These concept tokens are created\nby the probability-weighted mixture of token embeddings, which form the\ncontinuous concept space, enabling smooth transitions and richer\nrepresentations that transcend traditional discrete boundaries. In essence,\neach generated concept token encapsulates multiple meanings from related\ndiscrete tokens, implicitly exploring various reasoning paths to converge\neffectively toward the correct answer. Empirical evaluations on diverse\nmathematical and coding benchmarks consistently demonstrate the effectiveness\nand efficiency of Soft Thinking, improving pass@1 accuracy by up to 2.48 points\nwhile simultaneously reducing token usage by up to 22.4% compared to standard\nCoT. Qualitative analysis further reveals that Soft Thinking outputs remain\nhighly interpretable and readable, highlighting the potential of Soft Thinking\nto break the inherent bottleneck of discrete language-based reasoning. Code is\navailable at https://github.com/eric-ai-lab/Soft-Thinking.",
      "upvotes": 2,
      "discussionId": "682e8b5aa5b1e59c697864ce",
      "projectPage": "https://soft-thinking.github.io",
      "githubRepo": "https://github.com/eric-ai-lab/Soft-Thinking",
      "ai_summary": "Soft Thinking, a training-free method, enhances reasoning by generating soft, abstract concept tokens in a continuous space, improving accuracy and efficiency in mathematical and coding benchmarks.",
      "ai_keywords": [
        "Soft Thinking",
        "continuous concept space",
        "token embeddings",
        "Chain-of-Thought",
        "pass@1 accuracy"
      ]
    },
    "publishedAt": "2025-05-21T13:29:15.000Z",
    "title": "Soft Thinking: Unlocking the Reasoning Potential of LLMs in Continuous\n  Concept Space",
    "summary": "Human cognition typically involves thinking through abstract, fluid concepts\nrather than strictly using discrete linguistic tokens. Current reasoning\nmodels, however, are constrained to reasoning within the boundaries of human\nlanguage, processing discrete token embeddings that represent fixed points in\nthe semantic space. This discrete constraint restricts the expressive power and\nupper potential of such reasoning models, often causing incomplete exploration\nof reasoning paths, as standard Chain-of-Thought (CoT) methods rely on sampling\none token per step. In this work, we introduce Soft Thinking, a training-free\nmethod that emulates human-like \"soft\" reasoning by generating soft, abstract\nconcept tokens in a continuous concept space. These concept tokens are created\nby the probability-weighted mixture of token embeddings, which form the\ncontinuous concept space, enabling smooth transitions and richer\nrepresentations that transcend traditional discrete boundaries. In essence,\neach generated concept token encapsulates multiple meanings from related\ndiscrete tokens, implicitly exploring various reasoning paths to converge\neffectively toward the correct answer. Empirical evaluations on diverse\nmathematical and coding benchmarks consistently demonstrate the effectiveness\nand efficiency of Soft Thinking, improving pass@1 accuracy by up to 2.48 points\nwhile simultaneously reducing token usage by up to 22.4% compared to standard\nCoT. Qualitative analysis further reveals that Soft Thinking outputs remain\nhighly interpretable and readable, highlighting the potential of Soft Thinking\nto break the inherent bottleneck of discrete language-based reasoning. Code is\navailable at https://github.com/eric-ai-lab/Soft-Thinking.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.15778.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64679a226192d39142245e5e",
      "avatarUrl": "/avatars/05abee0b6317f100923936ca2099e9eb.svg",
      "fullname": "Xin Eric Wang",
      "name": "xw-eric",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.15656",
      "authors": [
        {
          "_id": "682e8add58a17fe0e9ec02e6",
          "name": "Zhexin Zhang",
          "hidden": false
        },
        {
          "_id": "682e8add58a17fe0e9ec02e7",
          "name": "Yuhao Sun",
          "hidden": false
        },
        {
          "_id": "682e8add58a17fe0e9ec02e8",
          "name": "Junxiao Yang",
          "hidden": false
        },
        {
          "_id": "682e8add58a17fe0e9ec02e9",
          "name": "Shiyao Cui",
          "hidden": false
        },
        {
          "_id": "682e8add58a17fe0e9ec02ea",
          "name": "Hongning Wang",
          "hidden": false
        },
        {
          "_id": "682e8add58a17fe0e9ec02eb",
          "name": "Minlie Huang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-21T15:32:14.000Z",
      "submittedOnDailyAt": "2025-05-22T00:55:08.348Z",
      "title": "Be Careful When Fine-tuning On Open-Source LLMs: Your Fine-tuning Data\n  Could Be Secretly Stolen!",
      "submittedOnDailyBy": {
        "_id": "61b58aa0d65058ce70beb98c",
        "avatarUrl": "/avatars/aefd9271b891abc6dd2ded1a30eebca4.svg",
        "isPro": false,
        "fullname": "Zhexin Zhang",
        "user": "nonstopfor",
        "type": "user"
      },
      "summary": "Fine-tuning on open-source Large Language Models (LLMs) with proprietary data\nis now a standard practice for downstream developers to obtain task-specific\nLLMs. Surprisingly, we reveal a new and concerning risk along with the\npractice: the creator of the open-source LLMs can later extract the private\ndownstream fine-tuning data through simple backdoor training, only requiring\nblack-box access to the fine-tuned downstream model. Our comprehensive\nexperiments, across 4 popularly used open-source models with 3B to 32B\nparameters and 2 downstream datasets, suggest that the extraction performance\ncan be strikingly high: in practical settings, as much as 76.3% downstream\nfine-tuning data (queries) out of a total 5,000 samples can be perfectly\nextracted, and the success rate can increase to 94.9% in more ideal settings.\nWe also explore a detection-based defense strategy but find it can be bypassed\nwith improved attack. Overall, we highlight the emergency of this newly\nidentified data breaching risk in fine-tuning, and we hope that more follow-up\nresearch could push the progress of addressing this concerning risk. The code\nand data used in our experiments are released at\nhttps://github.com/thu-coai/Backdoor-Data-Extraction.",
      "upvotes": 2,
      "discussionId": "682e8ade58a17fe0e9ec0348",
      "ai_summary": "There is a newly identified risk that creators of open-source LLMs can extract fine-tuning data from downstream models through backdoor training, even with black-box access.",
      "ai_keywords": [
        "Large Language Models",
        "fine-tuning",
        "open-source",
        "black-box access",
        "backdoor training",
        "data extraction",
        "data breach"
      ]
    },
    "publishedAt": "2025-05-21T11:32:14.000Z",
    "title": "Be Careful When Fine-tuning On Open-Source LLMs: Your Fine-tuning Data\n  Could Be Secretly Stolen!",
    "summary": "Fine-tuning on open-source Large Language Models (LLMs) with proprietary data\nis now a standard practice for downstream developers to obtain task-specific\nLLMs. Surprisingly, we reveal a new and concerning risk along with the\npractice: the creator of the open-source LLMs can later extract the private\ndownstream fine-tuning data through simple backdoor training, only requiring\nblack-box access to the fine-tuned downstream model. Our comprehensive\nexperiments, across 4 popularly used open-source models with 3B to 32B\nparameters and 2 downstream datasets, suggest that the extraction performance\ncan be strikingly high: in practical settings, as much as 76.3% downstream\nfine-tuning data (queries) out of a total 5,000 samples can be perfectly\nextracted, and the success rate can increase to 94.9% in more ideal settings.\nWe also explore a detection-based defense strategy but find it can be bypassed\nwith improved attack. Overall, we highlight the emergency of this newly\nidentified data breaching risk in fine-tuning, and we hope that more follow-up\nresearch could push the progress of addressing this concerning risk. The code\nand data used in our experiments are released at\nhttps://github.com/thu-coai/Backdoor-Data-Extraction.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.15656.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "61b58aa0d65058ce70beb98c",
      "avatarUrl": "/avatars/aefd9271b891abc6dd2ded1a30eebca4.svg",
      "fullname": "Zhexin Zhang",
      "name": "nonstopfor",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.15404",
      "authors": [
        {
          "_id": "682e8b3de3d1137730d0517b",
          "name": "Zhexin Zhang",
          "hidden": false
        },
        {
          "_id": "682e8b3de3d1137730d0517c",
          "name": "Xian Qi Loye",
          "hidden": false
        },
        {
          "_id": "682e8b3de3d1137730d0517d",
          "name": "Victor Shea-Jay Huang",
          "hidden": false
        },
        {
          "_id": "682e8b3de3d1137730d0517e",
          "name": "Junxiao Yang",
          "hidden": false
        },
        {
          "_id": "682e8b3de3d1137730d0517f",
          "name": "Qi Zhu",
          "hidden": false
        },
        {
          "_id": "682e8b3de3d1137730d05180",
          "name": "Shiyao Cui",
          "hidden": false
        },
        {
          "_id": "682e8b3de3d1137730d05181",
          "name": "Fei Mi",
          "hidden": false
        },
        {
          "_id": "682e8b3de3d1137730d05182",
          "name": "Lifeng Shang",
          "hidden": false
        },
        {
          "_id": "682e8b3de3d1137730d05183",
          "name": "Yingkang Wang",
          "hidden": false
        },
        {
          "_id": "682e8b3de3d1137730d05184",
          "name": "Hongning Wang",
          "hidden": false
        },
        {
          "_id": "682e8b3de3d1137730d05185",
          "name": "Minlie Huang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-21T11:45:29.000Z",
      "submittedOnDailyAt": "2025-05-22T00:57:01.326Z",
      "title": "How Should We Enhance the Safety of Large Reasoning Models: An Empirical\n  Study",
      "submittedOnDailyBy": {
        "_id": "61b58aa0d65058ce70beb98c",
        "avatarUrl": "/avatars/aefd9271b891abc6dd2ded1a30eebca4.svg",
        "isPro": false,
        "fullname": "Zhexin Zhang",
        "user": "nonstopfor",
        "type": "user"
      },
      "summary": "Large Reasoning Models (LRMs) have achieved remarkable success on\nreasoning-intensive tasks such as mathematics and programming. However, their\nenhanced reasoning capabilities do not necessarily translate to improved safety\nperformance-and in some cases, may even degrade it. This raises an important\nresearch question: how can we enhance the safety of LRMs? In this paper, we\npresent a comprehensive empirical study on how to enhance the safety of LRMs\nthrough Supervised Fine-Tuning (SFT). Our investigation begins with an\nunexpected observation: directly distilling safe responses from DeepSeek-R1\nfails to significantly enhance safety. We analyze this phenomenon and identify\nthree key failure patterns that contribute to it. We then demonstrate that\nexplicitly addressing these issues during the data distillation process can\nlead to substantial safety improvements. Next, we explore whether a long and\ncomplex reasoning process is necessary for achieving safety. Interestingly, we\nfind that simply using short or template-based reasoning process can attain\ncomparable safety performance-and are significantly easier for models to learn\nthan more intricate reasoning chains. These findings prompt a deeper reflection\non the role of reasoning in ensuring safety. Finally, we find that mixing math\nreasoning data during safety fine-tuning is helpful to balance safety and\nover-refusal. Overall, we hope our empirical study could provide a more\nholistic picture on enhancing the safety of LRMs. The code and data used in our\nexperiments are released in https://github.com/thu-coai/LRM-Safety-Study.",
      "upvotes": 2,
      "discussionId": "682e8b3fe3d1137730d051f5",
      "ai_summary": "The study investigates methods to enhance the safety of Large Reasoning Models (LRMs) through Supervised Fine-Tuning (SFT), finding that explicit addressing of failure patterns and use of simpler reasoning processes can improve safety without requiring complex reasoning chains or excessive data.",
      "ai_keywords": [
        "Large Reasoning Models",
        "LRMs",
        "Supervised Fine-Tuning",
        "SFT",
        "DeepSeek-R1",
        "data distillation",
        "reasoning process",
        "safety improvements",
        "over-refusal",
        "math reasoning"
      ]
    },
    "publishedAt": "2025-05-21T07:45:29.000Z",
    "title": "How Should We Enhance the Safety of Large Reasoning Models: An Empirical\n  Study",
    "summary": "Large Reasoning Models (LRMs) have achieved remarkable success on\nreasoning-intensive tasks such as mathematics and programming. However, their\nenhanced reasoning capabilities do not necessarily translate to improved safety\nperformance-and in some cases, may even degrade it. This raises an important\nresearch question: how can we enhance the safety of LRMs? In this paper, we\npresent a comprehensive empirical study on how to enhance the safety of LRMs\nthrough Supervised Fine-Tuning (SFT). Our investigation begins with an\nunexpected observation: directly distilling safe responses from DeepSeek-R1\nfails to significantly enhance safety. We analyze this phenomenon and identify\nthree key failure patterns that contribute to it. We then demonstrate that\nexplicitly addressing these issues during the data distillation process can\nlead to substantial safety improvements. Next, we explore whether a long and\ncomplex reasoning process is necessary for achieving safety. Interestingly, we\nfind that simply using short or template-based reasoning process can attain\ncomparable safety performance-and are significantly easier for models to learn\nthan more intricate reasoning chains. These findings prompt a deeper reflection\non the role of reasoning in ensuring safety. Finally, we find that mixing math\nreasoning data during safety fine-tuning is helpful to balance safety and\nover-refusal. Overall, we hope our empirical study could provide a more\nholistic picture on enhancing the safety of LRMs. The code and data used in our\nexperiments are released in https://github.com/thu-coai/LRM-Safety-Study.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.15404.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "61b58aa0d65058ce70beb98c",
      "avatarUrl": "/avatars/aefd9271b891abc6dd2ded1a30eebca4.svg",
      "fullname": "Zhexin Zhang",
      "name": "nonstopfor",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.14357",
      "authors": [
        {
          "_id": "682d2b9ba006b93dbe79a333",
          "user": {
            "_id": "66b06fe4ce2224a4d066cc0a",
            "avatarUrl": "/avatars/3f89007c5c17e57e623a10d82637b5fc.svg",
            "isPro": false,
            "fullname": "knightnemo",
            "user": "knightnemo",
            "type": "user"
          },
          "name": "Siqiao Huang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-21T18:04:10.245Z",
          "hidden": false
        },
        {
          "_id": "682d2b9ba006b93dbe79a334",
          "name": "Jialong Wu",
          "hidden": false
        },
        {
          "_id": "682d2b9ba006b93dbe79a335",
          "name": "Qixing Zhou",
          "hidden": false
        },
        {
          "_id": "682d2b9ba006b93dbe79a336",
          "name": "Shangchen Miao",
          "hidden": false
        },
        {
          "_id": "682d2b9ba006b93dbe79a337",
          "name": "Mingsheng Long",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-20T13:41:45.000Z",
      "submittedOnDailyAt": "2025-05-22T00:17:33.386Z",
      "title": "Vid2World: Crafting Video Diffusion Models to Interactive World Models",
      "submittedOnDailyBy": {
        "_id": "66b06fe4ce2224a4d066cc0a",
        "avatarUrl": "/avatars/3f89007c5c17e57e623a10d82637b5fc.svg",
        "isPro": false,
        "fullname": "knightnemo",
        "user": "knightnemo",
        "type": "user"
      },
      "summary": "World models, which predict transitions based on history observation and\naction sequences, have shown great promise in improving data efficiency for\nsequential decision making. However, existing world models often require\nextensive domain-specific training and still produce low-fidelity, coarse\npredictions, limiting their applicability in complex environments. In contrast,\nvideo diffusion models trained on large, internet-scale datasets have\ndemonstrated impressive capabilities in generating high-quality videos that\ncapture diverse real-world dynamics. In this work, we present Vid2World, a\ngeneral approach for leveraging and transferring pre-trained video diffusion\nmodels into interactive world models. To bridge the gap, Vid2World performs\ncasualization of a pre-trained video diffusion model by crafting its\narchitecture and training objective to enable autoregressive generation.\nFurthermore, it introduces a causal action guidance mechanism to enhance action\ncontrollability in the resulting interactive world model. Extensive experiments\nin robot manipulation and game simulation domains show that our method offers a\nscalable and effective approach for repurposing highly capable video diffusion\nmodels to interactive world models.",
      "upvotes": 2,
      "discussionId": "682d2b9da006b93dbe79a3af",
      "projectPage": "https://knightnemo.github.io/vid2world/",
      "ai_summary": "Vid2World repurposes pre-trained video diffusion models into interactive world models via causalization and action guidance, enhancing action controllability and scalability in complex environments.",
      "ai_keywords": [
        "world models",
        "transitions",
        "history observation",
        "action sequences",
        "data efficiency",
        "sequential decision making",
        "low-fidelity",
        "coarse predictions",
        "video diffusion models",
        "internet-scale datasets",
        "high-quality videos",
        "real-world dynamics",
        "autoregressive generation",
        "causal action guidance",
        "robot manipulation",
        "game simulation"
      ]
    },
    "publishedAt": "2025-05-20T09:41:45.000Z",
    "title": "Vid2World: Crafting Video Diffusion Models to Interactive World Models",
    "summary": "World models, which predict transitions based on history observation and\naction sequences, have shown great promise in improving data efficiency for\nsequential decision making. However, existing world models often require\nextensive domain-specific training and still produce low-fidelity, coarse\npredictions, limiting their applicability in complex environments. In contrast,\nvideo diffusion models trained on large, internet-scale datasets have\ndemonstrated impressive capabilities in generating high-quality videos that\ncapture diverse real-world dynamics. In this work, we present Vid2World, a\ngeneral approach for leveraging and transferring pre-trained video diffusion\nmodels into interactive world models. To bridge the gap, Vid2World performs\ncasualization of a pre-trained video diffusion model by crafting its\narchitecture and training objective to enable autoregressive generation.\nFurthermore, it introduces a causal action guidance mechanism to enhance action\ncontrollability in the resulting interactive world model. Extensive experiments\nin robot manipulation and game simulation domains show that our method offers a\nscalable and effective approach for repurposing highly capable video diffusion\nmodels to interactive world models.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.14357.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66b06fe4ce2224a4d066cc0a",
      "avatarUrl": "/avatars/3f89007c5c17e57e623a10d82637b5fc.svg",
      "fullname": "knightnemo",
      "name": "knightnemo",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.14231",
      "authors": [
        {
          "_id": "682db25d265177367e35d5b1",
          "name": "Sule Bai",
          "hidden": false
        },
        {
          "_id": "682db25d265177367e35d5b2",
          "name": "Mingxing Li",
          "hidden": false
        },
        {
          "_id": "682db25d265177367e35d5b3",
          "name": "Yong Liu",
          "hidden": false
        },
        {
          "_id": "682db25d265177367e35d5b4",
          "name": "Jing Tang",
          "hidden": false
        },
        {
          "_id": "682db25d265177367e35d5b5",
          "name": "Haoji Zhang",
          "hidden": false
        },
        {
          "_id": "682db25d265177367e35d5b6",
          "name": "Lei Sun",
          "hidden": false
        },
        {
          "_id": "682db25d265177367e35d5b7",
          "user": {
            "_id": "66d255e3947594430c723ff6",
            "avatarUrl": "/avatars/c56e4792332a01bf34085a75ee64916e.svg",
            "isPro": false,
            "fullname": "xiaochonglinghu",
            "user": "xiaochonglinghu",
            "type": "user"
          },
          "name": "Xiangxiang Chu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-21T12:27:07.959Z",
          "hidden": false
        },
        {
          "_id": "682db25d265177367e35d5b8",
          "name": "Yansong Tang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-20T11:40:43.000Z",
      "submittedOnDailyAt": "2025-05-22T00:13:45.780Z",
      "title": "UniVG-R1: Reasoning Guided Universal Visual Grounding with Reinforcement\n  Learning",
      "submittedOnDailyBy": {
        "_id": "6513a0f14f1682e4407758a9",
        "avatarUrl": "/avatars/b2a6886114492944cfa235363817565f.svg",
        "isPro": false,
        "fullname": "Mingxing Li",
        "user": "MingxingLi",
        "type": "user"
      },
      "summary": "Traditional visual grounding methods primarily focus on single-image\nscenarios with simple textual references. However, extending these methods to\nreal-world scenarios that involve implicit and complex instructions,\nparticularly in conjunction with multiple images, poses significant challenges,\nwhich is mainly due to the lack of advanced reasoning ability across diverse\nmulti-modal contexts. In this work, we aim to address the more practical\nuniversal grounding task, and propose UniVG-R1, a reasoning guided multimodal\nlarge language model (MLLM) for universal visual grounding, which enhances\nreasoning capabilities through reinforcement learning (RL) combined with\ncold-start data. Specifically, we first construct a high-quality\nChain-of-Thought (CoT) grounding dataset, annotated with detailed reasoning\nchains, to guide the model towards correct reasoning paths via supervised\nfine-tuning. Subsequently, we perform rule-based reinforcement learning to\nencourage the model to identify correct reasoning chains, thereby incentivizing\nits reasoning capabilities. In addition, we identify a difficulty bias arising\nfrom the prevalence of easy samples as RL training progresses, and we propose a\ndifficulty-aware weight adjustment strategy to further strengthen the\nperformance. Experimental results demonstrate the effectiveness of UniVG-R1,\nwhich achieves state-of-the-art performance on MIG-Bench with a 9.1%\nimprovement over the previous method. Furthermore, our model exhibits strong\ngeneralizability, achieving an average improvement of 23.4% in zero-shot\nperformance across four image and video reasoning grounding benchmarks. The\nproject page can be accessed at https://amap-ml.github.io/UniVG-R1-page/.",
      "upvotes": 2,
      "discussionId": "682db25e265177367e35d638",
      "projectPage": "https://amap-ml.github.io/UniVG-R1-page/",
      "ai_summary": "UniVG-R1, a reasoning-guided multimodal large language model, enhances visual grounding by leveraging reinforcement learning and a difficulty-aware strategy, achieving state-of-the-art results and strong generalizability.",
      "ai_keywords": [
        "multimodal large language model",
        "reasoning guided",
        "reinforcement learning",
        "cold-start data",
        "Chain-of-Thought dataset",
        "supervised fine-tuning",
        "rule-based reinforcement learning",
        "difficulty bias",
        "difficulty-aware weight adjustment"
      ]
    },
    "publishedAt": "2025-05-20T07:40:43.000Z",
    "title": "UniVG-R1: Reasoning Guided Universal Visual Grounding with Reinforcement\n  Learning",
    "summary": "Traditional visual grounding methods primarily focus on single-image\nscenarios with simple textual references. However, extending these methods to\nreal-world scenarios that involve implicit and complex instructions,\nparticularly in conjunction with multiple images, poses significant challenges,\nwhich is mainly due to the lack of advanced reasoning ability across diverse\nmulti-modal contexts. In this work, we aim to address the more practical\nuniversal grounding task, and propose UniVG-R1, a reasoning guided multimodal\nlarge language model (MLLM) for universal visual grounding, which enhances\nreasoning capabilities through reinforcement learning (RL) combined with\ncold-start data. Specifically, we first construct a high-quality\nChain-of-Thought (CoT) grounding dataset, annotated with detailed reasoning\nchains, to guide the model towards correct reasoning paths via supervised\nfine-tuning. Subsequently, we perform rule-based reinforcement learning to\nencourage the model to identify correct reasoning chains, thereby incentivizing\nits reasoning capabilities. In addition, we identify a difficulty bias arising\nfrom the prevalence of easy samples as RL training progresses, and we propose a\ndifficulty-aware weight adjustment strategy to further strengthen the\nperformance. Experimental results demonstrate the effectiveness of UniVG-R1,\nwhich achieves state-of-the-art performance on MIG-Bench with a 9.1%\nimprovement over the previous method. Furthermore, our model exhibits strong\ngeneralizability, achieving an average improvement of 23.4% in zero-shot\nperformance across four image and video reasoning grounding benchmarks. The\nproject page can be accessed at https://amap-ml.github.io/UniVG-R1-page/.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.14231.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6513a0f14f1682e4407758a9",
      "avatarUrl": "/avatars/b2a6886114492944cfa235363817565f.svg",
      "fullname": "Mingxing Li",
      "name": "MingxingLi",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.13529",
      "authors": [
        {
          "_id": "682e8c441ffd3af3cb2f27d3",
          "name": "Junxiao Yang",
          "hidden": false
        },
        {
          "_id": "682e8c441ffd3af3cb2f27d4",
          "name": "Jinzhe Tu",
          "hidden": false
        },
        {
          "_id": "682e8c441ffd3af3cb2f27d5",
          "name": "Haoran Liu",
          "hidden": false
        },
        {
          "_id": "682e8c441ffd3af3cb2f27d6",
          "name": "Xiaoce Wang",
          "hidden": false
        },
        {
          "_id": "682e8c441ffd3af3cb2f27d7",
          "name": "Chujie Zheng",
          "hidden": false
        },
        {
          "_id": "682e8c441ffd3af3cb2f27d8",
          "name": "Zhexin Zhang",
          "hidden": false
        },
        {
          "_id": "682e8c441ffd3af3cb2f27d9",
          "name": "Shiyao Cui",
          "hidden": false
        },
        {
          "_id": "682e8c441ffd3af3cb2f27da",
          "name": "Caishun Chen",
          "hidden": false
        },
        {
          "_id": "682e8c441ffd3af3cb2f27db",
          "name": "Tiantian He",
          "hidden": false
        },
        {
          "_id": "682e8c441ffd3af3cb2f27dc",
          "name": "Hongning Wang",
          "hidden": false
        },
        {
          "_id": "682e8c441ffd3af3cb2f27dd",
          "name": "Yew-Soon Ong",
          "hidden": false
        },
        {
          "_id": "682e8c441ffd3af3cb2f27de",
          "name": "Minlie Huang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-18T07:27:34.000Z",
      "submittedOnDailyAt": "2025-05-22T01:02:19.522Z",
      "title": "BARREL: Boundary-Aware Reasoning for Factual and Reliable LRMs",
      "submittedOnDailyBy": {
        "_id": "65d859a3661492b25c46a117",
        "avatarUrl": "/avatars/06a547a00b472f512a25eef4ddd047bf.svg",
        "isPro": false,
        "fullname": "junxiao yang",
        "user": "yangjunxiao2021",
        "type": "user"
      },
      "summary": "Recent advances in Large Reasoning Models (LRMs) have shown impressive\ncapabilities in mathematical and logical reasoning. However, current LRMs\nrarely admit ignorance or respond with \"I don't know\". Instead, they often\nproduce incorrect answers while showing undue confidence, raising concerns\nabout their factual reliability. In this work, we identify two pathological\nreasoning patterns characterized by overthinking that contribute to the\noverconfident and incorrect answers: last-minute guessing and second-thought\nspiraling. To address these issues, we propose BARREL-a novel framework that\npromotes concise and boundary-aware factual reasoning. Our experiments show\nthat BARREL-training increases the reliability of DeepSeek-R1-Distill-Llama-8B\nfrom 39.33% to 61.48%, while still achieving accuracy comparable to models\nfinetuned on reasoning data generated by R1. These results demonstrate that our\npilot study is inspiring to build more reliable and factual System 2 LRMs.",
      "upvotes": 2,
      "discussionId": "682e8c451ffd3af3cb2f281f"
    },
    "publishedAt": "2025-05-18T03:27:34.000Z",
    "title": "BARREL: Boundary-Aware Reasoning for Factual and Reliable LRMs",
    "summary": "Recent advances in Large Reasoning Models (LRMs) have shown impressive\ncapabilities in mathematical and logical reasoning. However, current LRMs\nrarely admit ignorance or respond with \"I don't know\". Instead, they often\nproduce incorrect answers while showing undue confidence, raising concerns\nabout their factual reliability. In this work, we identify two pathological\nreasoning patterns characterized by overthinking that contribute to the\noverconfident and incorrect answers: last-minute guessing and second-thought\nspiraling. To address these issues, we propose BARREL-a novel framework that\npromotes concise and boundary-aware factual reasoning. Our experiments show\nthat BARREL-training increases the reliability of DeepSeek-R1-Distill-Llama-8B\nfrom 39.33% to 61.48%, while still achieving accuracy comparable to models\nfinetuned on reasoning data generated by R1. These results demonstrate that our\npilot study is inspiring to build more reliable and factual System 2 LRMs.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.13529.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65d859a3661492b25c46a117",
      "avatarUrl": "/avatars/06a547a00b472f512a25eef4ddd047bf.svg",
      "fullname": "junxiao yang",
      "name": "yangjunxiao2021",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.15816",
      "authors": [
        {
          "_id": "682e8304e9980508c9a2fc73",
          "name": "Penghao Wu",
          "hidden": false
        },
        {
          "_id": "682e8304e9980508c9a2fc74",
          "name": "Lewei Lu",
          "hidden": false
        },
        {
          "_id": "682e8304e9980508c9a2fc75",
          "name": "Ziwei Liu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-21T17:59:52.000Z",
      "submittedOnDailyAt": "2025-05-22T00:23:40.991Z",
      "title": "Streamline Without Sacrifice - Squeeze out Computation Redundancy in LMM",
      "submittedOnDailyBy": {
        "_id": "64101f81b27543634e377fc1",
        "avatarUrl": "/avatars/557dd9d4707e3b38e0805dfb87c08004.svg",
        "isPro": false,
        "fullname": "Penghao Wu",
        "user": "craigwu",
        "type": "user"
      },
      "summary": "Large multimodal models excel in multimodal tasks but face significant\ncomputational challenges due to excessive computation on visual tokens. Unlike\ntoken reduction methods that focus on token-level redundancy, we identify and\nstudy the computation-level redundancy on vision tokens to ensure no\ninformation loss. Our key insight is that vision tokens from the pretrained\nvision encoder do not necessarily require all the heavy operations (e.g.,\nself-attention, FFNs) in decoder-only LMMs and could be processed more lightly\nwith proper designs. We designed a series of experiments to discover and\nprogressively squeeze out the vision-related computation redundancy. Based on\nour findings, we propose ProxyV, a novel approach that utilizes proxy vision\ntokens to alleviate the computational burden on original vision tokens. ProxyV\nenhances efficiency without compromising performance and can even yield notable\nperformance gains in scenarios with more moderate efficiency improvements.\nFurthermore, the flexibility of ProxyV is demonstrated through its combination\nwith token reduction methods to boost efficiency further. The code will be made\npublic at this https://github.com/penghao-wu/ProxyV URL.",
      "upvotes": 1,
      "discussionId": "682e8305e9980508c9a2fca1",
      "projectPage": "https://penghao-wu.github.io/ProxyV/",
      "githubRepo": "https://github.com/penghao-wu/ProxyV",
      "ai_summary": "ProxyV alleviates computational burdens in large multimodal models by using proxy vision tokens, enhancing efficiency without sacrificing performance.",
      "ai_keywords": [
        "multimodal models",
        "computation-level redundancy",
        "vision tokens",
        "pretrained vision encoder",
        "decoder-only LMMs",
        "self-attention",
        "FFNs",
        "proxy vision tokens",
        "ProxyV"
      ]
    },
    "publishedAt": "2025-05-21T13:59:52.000Z",
    "title": "Streamline Without Sacrifice - Squeeze out Computation Redundancy in LMM",
    "summary": "Large multimodal models excel in multimodal tasks but face significant\ncomputational challenges due to excessive computation on visual tokens. Unlike\ntoken reduction methods that focus on token-level redundancy, we identify and\nstudy the computation-level redundancy on vision tokens to ensure no\ninformation loss. Our key insight is that vision tokens from the pretrained\nvision encoder do not necessarily require all the heavy operations (e.g.,\nself-attention, FFNs) in decoder-only LMMs and could be processed more lightly\nwith proper designs. We designed a series of experiments to discover and\nprogressively squeeze out the vision-related computation redundancy. Based on\nour findings, we propose ProxyV, a novel approach that utilizes proxy vision\ntokens to alleviate the computational burden on original vision tokens. ProxyV\nenhances efficiency without compromising performance and can even yield notable\nperformance gains in scenarios with more moderate efficiency improvements.\nFurthermore, the flexibility of ProxyV is demonstrated through its combination\nwith token reduction methods to boost efficiency further. The code will be made\npublic at this https://github.com/penghao-wu/ProxyV URL.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.15816.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64101f81b27543634e377fc1",
      "avatarUrl": "/avatars/557dd9d4707e3b38e0805dfb87c08004.svg",
      "fullname": "Penghao Wu",
      "name": "craigwu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 15
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.15047",
      "authors": [
        {
          "_id": "682e8a7651706f69070c40d3",
          "name": "Yingming Pu",
          "hidden": false
        },
        {
          "_id": "682e8a7651706f69070c40d4",
          "name": "Tao Lin",
          "hidden": false
        },
        {
          "_id": "682e8a7651706f69070c40d5",
          "name": "Hongyu Chen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-21T03:09:39.000Z",
      "submittedOnDailyAt": "2025-05-22T00:58:05.428Z",
      "title": "PiFlow: Principle-aware Scientific Discovery with Multi-Agent\n  Collaboration",
      "submittedOnDailyBy": {
        "_id": "63d0c7d16b985b0f25d00a22",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63d0c7d16b985b0f25d00a22/6bNSM4WhVsEndIVx4Mb8U.png",
        "isPro": false,
        "fullname": "Mellen Y. Pu",
        "user": "Mellen",
        "type": "user"
      },
      "summary": "Large Language Model (LLM)-based multi-agent systems (MAS) demonstrate\nremarkable potential for scientific discovery. Existing approaches, however,\noften automate scientific discovery using predefined workflows that lack\nrationality constraints. This often leads to aimless hypothesizing and a\nfailure to consistently link hypotheses with evidence, thereby hindering\nsystematic uncertainty reduction. Overcoming these limitations fundamentally\nrequires systematic uncertainty reduction. We introduce PiFlow, an\ninformation-theoretical framework, treating automated scientific discovery as a\nstructured uncertainty reduction problem guided by principles (e.g., scientific\nlaws). In evaluations across three distinct scientific domains -- discovering\nnanomaterial structures, bio-molecules, and superconductor candidates with\ntargeted properties -- our method significantly improves discovery efficiency,\nreflected by a 73.55\\% increase in the Area Under the Curve (AUC) of property\nvalues versus exploration steps, and enhances solution quality by 94.06\\%\ncompared to a vanilla agent system. Overall, PiFlow serves as a\nPlug-and-Play method, establishing a novel paradigm shift in highly efficient\nautomated scientific discovery, paving the way for more robust and accelerated\nAI-driven research. Code is publicly available at our\nhttps://github.com/amair-lab/PiFlow{GitHub}.",
      "upvotes": 1,
      "discussionId": "682e8a7751706f69070c4121",
      "ai_summary": "PiFlow, an information-theoretical framework, improves automated scientific discovery by systematically reducing uncertainty and enhancing solution quality across various scientific domains.",
      "ai_keywords": [
        "Large Language Model",
        "multi-agent systems",
        "automated scientific discovery",
        "predefined workflows",
        "scientific discovery",
        "information-theoretical framework",
        "uncertainty reduction",
        "hypothesis",
        "evidence",
        "Area Under the Curve",
        "exploration steps",
        "solution quality",
        "Plug-and-Play method"
      ]
    },
    "publishedAt": "2025-05-20T23:09:39.000Z",
    "title": "PiFlow: Principle-aware Scientific Discovery with Multi-Agent\n  Collaboration",
    "summary": "Large Language Model (LLM)-based multi-agent systems (MAS) demonstrate\nremarkable potential for scientific discovery. Existing approaches, however,\noften automate scientific discovery using predefined workflows that lack\nrationality constraints. This often leads to aimless hypothesizing and a\nfailure to consistently link hypotheses with evidence, thereby hindering\nsystematic uncertainty reduction. Overcoming these limitations fundamentally\nrequires systematic uncertainty reduction. We introduce PiFlow, an\ninformation-theoretical framework, treating automated scientific discovery as a\nstructured uncertainty reduction problem guided by principles (e.g., scientific\nlaws). In evaluations across three distinct scientific domains -- discovering\nnanomaterial structures, bio-molecules, and superconductor candidates with\ntargeted properties -- our method significantly improves discovery efficiency,\nreflected by a 73.55\\% increase in the Area Under the Curve (AUC) of property\nvalues versus exploration steps, and enhances solution quality by 94.06\\%\ncompared to a vanilla agent system. Overall, PiFlow serves as a\nPlug-and-Play method, establishing a novel paradigm shift in highly efficient\nautomated scientific discovery, paving the way for more robust and accelerated\nAI-driven research. Code is publicly available at our\nhttps://github.com/amair-lab/PiFlow{GitHub}.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.15047.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63d0c7d16b985b0f25d00a22",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63d0c7d16b985b0f25d00a22/6bNSM4WhVsEndIVx4Mb8U.png",
      "fullname": "Mellen Y. Pu",
      "name": "Mellen",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.13934",
      "authors": [
        {
          "_id": "682e8369b16c79c271b4db81",
          "name": "Jialong Wu",
          "hidden": false
        },
        {
          "_id": "682e8369b16c79c271b4db82",
          "name": "Shaofeng Yin",
          "hidden": false
        },
        {
          "_id": "682e8369b16c79c271b4db83",
          "name": "Ningya Feng",
          "hidden": false
        },
        {
          "_id": "682e8369b16c79c271b4db84",
          "name": "Mingsheng Long",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/643b866bff50448bcfc7d1d1/OXSeIgpwt_mjJJqBffXyA.png"
      ],
      "publishedAt": "2025-05-20T05:02:53.000Z",
      "submittedOnDailyAt": "2025-05-22T00:24:21.597Z",
      "title": "RLVR-World: Training World Models with Reinforcement Learning",
      "submittedOnDailyBy": {
        "_id": "643b866bff50448bcfc7d1d1",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/q12v-BatVKimoi8q-coi-.jpeg",
        "isPro": false,
        "fullname": "Jialong Wu",
        "user": "manchery",
        "type": "user"
      },
      "summary": "World models predict state transitions in response to actions and are\nincreasingly developed across diverse modalities. However, standard training\nobjectives such as maximum likelihood estimation (MLE) often misalign with\ntask-specific goals of world models, i.e., transition prediction metrics like\naccuracy or perceptual quality. In this paper, we present RLVR-World, a unified\nframework that leverages reinforcement learning with verifiable rewards (RLVR)\nto directly optimize world models for such metrics. Despite formulating world\nmodeling as autoregressive prediction of tokenized sequences, RLVR-World\nevaluates metrics of decoded predictions as verifiable rewards. We demonstrate\nsubstantial performance gains on both language- and video-based world models\nacross domains, including text games, web navigation, and robot manipulation.\nOur work indicates that, beyond recent advances in reasoning language models,\nRLVR offers a promising post-training paradigm for enhancing the utility of\ngenerative models more broadly.",
      "upvotes": 1,
      "discussionId": "682e836bb16c79c271b4dc78",
      "projectPage": "https://thuml.github.io/RLVR-World/",
      "githubRepo": "https://github.com/thuml/RLVR-World",
      "ai_summary": "RLVR-World uses reinforcement learning with verifiable rewards to optimize world models for task-specific metrics, achieving improved performance across language and video domains.",
      "ai_keywords": [
        "world models",
        "maximum likelihood estimation",
        "transition prediction",
        "reinforcement learning",
        "verifiable rewards",
        "autoregressive prediction",
        "tokenized sequences",
        "text games",
        "web navigation",
        "robot manipulation"
      ]
    },
    "publishedAt": "2025-05-20T01:02:53.000Z",
    "title": "RLVR-World: Training World Models with Reinforcement Learning",
    "summary": "World models predict state transitions in response to actions and are\nincreasingly developed across diverse modalities. However, standard training\nobjectives such as maximum likelihood estimation (MLE) often misalign with\ntask-specific goals of world models, i.e., transition prediction metrics like\naccuracy or perceptual quality. In this paper, we present RLVR-World, a unified\nframework that leverages reinforcement learning with verifiable rewards (RLVR)\nto directly optimize world models for such metrics. Despite formulating world\nmodeling as autoregressive prediction of tokenized sequences, RLVR-World\nevaluates metrics of decoded predictions as verifiable rewards. We demonstrate\nsubstantial performance gains on both language- and video-based world models\nacross domains, including text games, web navigation, and robot manipulation.\nOur work indicates that, beyond recent advances in reasoning language models,\nRLVR offers a promising post-training paradigm for enhancing the utility of\ngenerative models more broadly.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/643b866bff50448bcfc7d1d1/OXSeIgpwt_mjJJqBffXyA.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.13934.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "643b866bff50448bcfc7d1d1",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/q12v-BatVKimoi8q-coi-.jpeg",
      "fullname": "Jialong Wu",
      "name": "manchery",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  }
]