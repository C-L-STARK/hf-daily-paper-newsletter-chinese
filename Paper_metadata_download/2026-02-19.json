[
  {
    "paper": {
      "id": "2602.12675",
      "authors": [
        {
          "_id": "69967cd61268a6b79e0d02d9",
          "name": "Jintao Zhang",
          "hidden": false
        },
        {
          "_id": "69967cd61268a6b79e0d02da",
          "name": "Haoxu Wang",
          "hidden": false
        },
        {
          "_id": "69967cd61268a6b79e0d02db",
          "name": "Kai Jiang",
          "hidden": false
        },
        {
          "_id": "69967cd61268a6b79e0d02dc",
          "name": "Kaiwen Zheng",
          "hidden": false
        },
        {
          "_id": "69967cd61268a6b79e0d02dd",
          "name": "Youhe Jiang",
          "hidden": false
        },
        {
          "_id": "69967cd61268a6b79e0d02de",
          "name": "Ion Stoica",
          "hidden": false
        },
        {
          "_id": "69967cd61268a6b79e0d02df",
          "name": "Jianfei Chen",
          "hidden": false
        },
        {
          "_id": "69967cd61268a6b79e0d02e0",
          "name": "Jun Zhu",
          "hidden": false
        },
        {
          "_id": "69967cd61268a6b79e0d02e1",
          "name": "Joseph E. Gonzalez",
          "hidden": false
        }
      ],
      "publishedAt": "2026-02-13T07:16:02.000Z",
      "submittedOnDailyAt": "2026-02-19T00:32:11.371Z",
      "title": "SLA2: Sparse-Linear Attention with Learnable Routing and QAT",
      "submittedOnDailyBy": {
        "_id": "66c0a08bac74db25de8427ec",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66c0a08bac74db25de8427ec/9D-piDBZqSt6KNkHImmkv.jpeg",
        "isPro": false,
        "fullname": "Jintao Zhang",
        "user": "jt-zhang",
        "type": "user"
      },
      "summary": "Sparse-Linear Attention (SLA) combines sparse and linear attention to accelerate diffusion models and has shown strong performance in video generation. However, (i) SLA relies on a heuristic split that assigns computations to the sparse or linear branch based on attention-weight magnitude, which can be suboptimal. Additionally, (ii) after formally analyzing the attention error in SLA, we identify a mismatch between SLA and a direct decomposition into sparse and linear attention. We propose SLA2, which introduces (I) a learnable router that dynamically selects whether each attention computation should use sparse or linear attention, (II) a more faithful and direct sparse-linear attention formulation that uses a learnable ratio to combine the sparse and linear attention branches, and (III) a sparse + low-bit attention design, where low-bit attention is introduced via quantization-aware fine-tuning to reduce quantization error. Experiments show that on video diffusion models, SLA2 can achieve 97% attention sparsity and deliver an 18.6x attention speedup while preserving generation quality.",
      "upvotes": 13,
      "discussionId": "69967cd61268a6b79e0d02e2",
      "projectPage": "https://github.com/thu-ml/SLA",
      "ai_summary": "SLA2 improves sparse-linear attention in diffusion models by introducing a learnable router, direct attention formulation, and quantization-aware fine-tuning for enhanced efficiency and quality.",
      "ai_keywords": [
        "sparse-linear attention",
        "diffusion models",
        "attention sparsity",
        "learnable router",
        "quantization-aware fine-tuning",
        "attention error",
        "direct decomposition"
      ],
      "organization": {
        "_id": "61f20a9ce108f2cba2dc0730",
        "name": "Berkeley",
        "fullname": "UC Berkeley",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/0FjsTg2txEZZ4dEgmMnQL.png"
      }
    },
    "publishedAt": "2026-02-13T02:16:02.000Z",
    "title": "SLA2: Sparse-Linear Attention with Learnable Routing and QAT",
    "summary": "Sparse-Linear Attention (SLA) combines sparse and linear attention to accelerate diffusion models and has shown strong performance in video generation. However, (i) SLA relies on a heuristic split that assigns computations to the sparse or linear branch based on attention-weight magnitude, which can be suboptimal. Additionally, (ii) after formally analyzing the attention error in SLA, we identify a mismatch between SLA and a direct decomposition into sparse and linear attention. We propose SLA2, which introduces (I) a learnable router that dynamically selects whether each attention computation should use sparse or linear attention, (II) a more faithful and direct sparse-linear attention formulation that uses a learnable ratio to combine the sparse and linear attention branches, and (III) a sparse + low-bit attention design, where low-bit attention is introduced via quantization-aware fine-tuning to reduce quantization error. Experiments show that on video diffusion models, SLA2 can achieve 97% attention sparsity and deliver an 18.6x attention speedup while preserving generation quality.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.12675.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "66c0a08bac74db25de8427ec",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66c0a08bac74db25de8427ec/9D-piDBZqSt6KNkHImmkv.jpeg",
      "fullname": "Jintao Zhang",
      "name": "jt-zhang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 46,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "61f20a9ce108f2cba2dc0730",
      "name": "Berkeley",
      "fullname": "UC Berkeley",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/0FjsTg2txEZZ4dEgmMnQL.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2602.16705",
      "authors": [
        {
          "_id": "69967a291268a6b79e0d02bb",
          "name": "Runpei Dong",
          "hidden": false
        },
        {
          "_id": "69967a291268a6b79e0d02bc",
          "name": "Ziyan Li",
          "hidden": false
        },
        {
          "_id": "69967a291268a6b79e0d02bd",
          "name": "Xialin He",
          "hidden": false
        },
        {
          "_id": "69967a291268a6b79e0d02be",
          "name": "Saurabh Gupta",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6201fc5d91d53938a6432fbf/g69sakZn_StFwG0neUtqz.mp4"
      ],
      "publishedAt": "2026-02-18T18:55:02.000Z",
      "submittedOnDailyAt": "2026-02-19T00:21:31.776Z",
      "title": "Learning Humanoid End-Effector Control for Open-Vocabulary Visual Loco-Manipulation",
      "submittedOnDailyBy": {
        "_id": "6201fc5d91d53938a6432fbf",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6201fc5d91d53938a6432fbf/VLs8ZYaZrop4KBpZn53fH.jpeg",
        "isPro": false,
        "fullname": "Runpei Dong",
        "user": "RunpeiDong",
        "type": "user"
      },
      "summary": "Visual loco-manipulation of arbitrary objects in the wild with humanoid robots requires accurate end-effector (EE) control and a generalizable understanding of the scene via visual inputs (e.g., RGB-D images). Existing approaches are based on real-world imitation learning and exhibit limited generalization due to the difficulty in collecting large-scale training datasets. This paper presents a new paradigm, HERO, for object loco-manipulation with humanoid robots that combines the strong generalization and open-vocabulary understanding of large vision models with strong control performance from simulated training. We achieve this by designing an accurate residual-aware EE tracking policy. This EE tracking policy combines classical robotics with machine learning. It uses a) inverse kinematics to convert residual end-effector targets into reference trajectories, b) a learned neural forward model for accurate forward kinematics, c) goal adjustment, and d) replanning. Together, these innovations help us cut down the end-effector tracking error by 3.2x. We use this accurate end-effector tracker to build a modular system for loco-manipulation, where we use open-vocabulary large vision models for strong visual generalization. Our system is able to operate in diverse real-world environments, from offices to coffee shops, where the robot is able to reliably manipulate various everyday objects (e.g., mugs, apples, toys) on surfaces ranging from 43cm to 92cm in height. Systematic modular and end-to-end tests in simulation and the real world demonstrate the effectiveness of our proposed design. We believe the advances in this paper can open up new ways of training humanoid robots to interact with daily objects.",
      "upvotes": 10,
      "discussionId": "69967a2a1268a6b79e0d02bf",
      "projectPage": "https://hero-humanoid.github.io/",
      "ai_summary": "HERO enables humanoid robots to perform object manipulation in diverse real-world environments by combining accurate end-effector control with open-vocabulary vision models for generalizable scene understanding.",
      "ai_keywords": [
        "end-effector tracking policy",
        "inverse kinematics",
        "neural forward model",
        "goal adjustment",
        "replanning",
        "open-vocabulary large vision models",
        "loco-manipulation",
        "visual generalization"
      ]
    },
    "publishedAt": "2026-02-18T13:55:02.000Z",
    "title": "Learning Humanoid End-Effector Control for Open-Vocabulary Visual Loco-Manipulation",
    "summary": "Visual loco-manipulation of arbitrary objects in the wild with humanoid robots requires accurate end-effector (EE) control and a generalizable understanding of the scene via visual inputs (e.g., RGB-D images). Existing approaches are based on real-world imitation learning and exhibit limited generalization due to the difficulty in collecting large-scale training datasets. This paper presents a new paradigm, HERO, for object loco-manipulation with humanoid robots that combines the strong generalization and open-vocabulary understanding of large vision models with strong control performance from simulated training. We achieve this by designing an accurate residual-aware EE tracking policy. This EE tracking policy combines classical robotics with machine learning. It uses a) inverse kinematics to convert residual end-effector targets into reference trajectories, b) a learned neural forward model for accurate forward kinematics, c) goal adjustment, and d) replanning. Together, these innovations help us cut down the end-effector tracking error by 3.2x. We use this accurate end-effector tracker to build a modular system for loco-manipulation, where we use open-vocabulary large vision models for strong visual generalization. Our system is able to operate in diverse real-world environments, from offices to coffee shops, where the robot is able to reliably manipulate various everyday objects (e.g., mugs, apples, toys) on surfaces ranging from 43cm to 92cm in height. Systematic modular and end-to-end tests in simulation and the real world demonstrate the effectiveness of our proposed design. We believe the advances in this paper can open up new ways of training humanoid robots to interact with daily objects.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6201fc5d91d53938a6432fbf/g69sakZn_StFwG0neUtqz.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.16705.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6201fc5d91d53938a6432fbf",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6201fc5d91d53938a6432fbf/VLs8ZYaZrop4KBpZn53fH.jpeg",
      "fullname": "Runpei Dong",
      "name": "RunpeiDong",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 7,
      "isUserFollowing": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2602.16682",
      "authors": [
        {
          "_id": "69967cbb1268a6b79e0d02cf",
          "name": "Chuhan Li",
          "hidden": false
        },
        {
          "_id": "69967cbb1268a6b79e0d02d0",
          "name": "Ruilin Han",
          "hidden": false
        },
        {
          "_id": "69967cbb1268a6b79e0d02d1",
          "name": "Joy Hsu",
          "hidden": false
        },
        {
          "_id": "69967cbb1268a6b79e0d02d2",
          "name": "Yongyuan Liang",
          "hidden": false
        },
        {
          "_id": "69967cbb1268a6b79e0d02d3",
          "name": "Rajiv Dhawan",
          "hidden": false
        },
        {
          "_id": "69967cbb1268a6b79e0d02d4",
          "name": "Jiajun Wu",
          "hidden": false
        },
        {
          "_id": "69967cbb1268a6b79e0d02d5",
          "name": "Ming-Hsuan Yang",
          "hidden": false
        },
        {
          "_id": "69967cbb1268a6b79e0d02d6",
          "name": "Xin Eric Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2026-02-18T18:22:52.000Z",
      "submittedOnDailyAt": "2026-02-19T00:30:39.390Z",
      "title": "Learning Situated Awareness in the Real World",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "A core aspect of human perception is situated awareness, the ability to relate ourselves to the surrounding physical environment and reason over possible actions in context. However, most existing benchmarks for multimodal foundation models (MFMs) emphasize environment-centric spatial relations (relations among objects in a scene), while largely overlooking observer-centric relationships that require reasoning relative to agent's viewpoint, pose, and motion. To bridge this gap, we introduce SAW-Bench (Situated Awareness in the Real World), a novel benchmark for evaluating egocentric situated awareness using real-world videos. SAW-Bench comprises 786 self-recorded videos captured with Ray-Ban Meta (Gen 2) smart glasses spanning diverse indoor and outdoor environments, and over 2,071 human-annotated question-answer pairs. It probes a model's observer-centric understanding with six different awareness tasks. Our comprehensive evaluation reveals a human-model performance gap of 37.66%, even with the best-performing MFM, Gemini 3 Flash. Beyond this gap, our in-depth analysis uncovers several notable findings; for example, while models can exploit partial geometric cues in egocentric videos, they often fail to infer a coherent camera geometry, leading to systematic spatial reasoning errors. We position SAW-Bench as a benchmark for situated spatial intelligence, moving beyond passive observation to understanding physically grounded, observer-centric dynamics.",
      "upvotes": 2,
      "discussionId": "69967cbc1268a6b79e0d02d7",
      "projectPage": "https://sawbench.github.io/",
      "ai_summary": "SAW-Bench presents a new benchmark for evaluating egocentric situated awareness in multimodal foundation models through real-world video datasets with human-annotated question-answer pairs, focusing on observer-centric spatial reasoning tasks.",
      "ai_keywords": [
        "multimodal foundation models",
        "egocentric videos",
        "observer-centric relationships",
        "situated awareness",
        "spatial reasoning",
        "camera geometry",
        "real-world videos",
        "question-answer pairs"
      ]
    },
    "publishedAt": "2026-02-18T13:22:52.000Z",
    "title": "Learning Situated Awareness in the Real World",
    "summary": "A core aspect of human perception is situated awareness, the ability to relate ourselves to the surrounding physical environment and reason over possible actions in context. However, most existing benchmarks for multimodal foundation models (MFMs) emphasize environment-centric spatial relations (relations among objects in a scene), while largely overlooking observer-centric relationships that require reasoning relative to agent's viewpoint, pose, and motion. To bridge this gap, we introduce SAW-Bench (Situated Awareness in the Real World), a novel benchmark for evaluating egocentric situated awareness using real-world videos. SAW-Bench comprises 786 self-recorded videos captured with Ray-Ban Meta (Gen 2) smart glasses spanning diverse indoor and outdoor environments, and over 2,071 human-annotated question-answer pairs. It probes a model's observer-centric understanding with six different awareness tasks. Our comprehensive evaluation reveals a human-model performance gap of 37.66%, even with the best-performing MFM, Gemini 3 Flash. Beyond this gap, our in-depth analysis uncovers several notable findings; for example, while models can exploit partial geometric cues in egocentric videos, they often fail to infer a coherent camera geometry, leading to systematic spatial reasoning errors. We position SAW-Bench as a benchmark for situated spatial intelligence, moving beyond passive observation to understanding physically grounded, observer-centric dynamics.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.16682.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 235,
      "isUserFollowing": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2602.15989",
      "authors": [
        {
          "_id": "699678401268a6b79e0d0275",
          "name": "Xitong Yang",
          "hidden": false
        },
        {
          "_id": "699678401268a6b79e0d0276",
          "name": "Devansh Kukreja",
          "hidden": false
        },
        {
          "_id": "699678401268a6b79e0d0277",
          "name": "Don Pinkus",
          "hidden": false
        },
        {
          "_id": "699678401268a6b79e0d0278",
          "name": "Anushka Sagar",
          "hidden": false
        },
        {
          "_id": "699678401268a6b79e0d0279",
          "name": "Taosha Fan",
          "hidden": false
        },
        {
          "_id": "699678401268a6b79e0d027a",
          "name": "Jinhyung Park",
          "hidden": false
        },
        {
          "_id": "699678401268a6b79e0d027b",
          "name": "Soyong Shin",
          "hidden": false
        },
        {
          "_id": "699678401268a6b79e0d027c",
          "name": "Jinkun Cao",
          "hidden": false
        },
        {
          "_id": "699678401268a6b79e0d027d",
          "name": "Jiawei Liu",
          "hidden": false
        },
        {
          "_id": "699678401268a6b79e0d027e",
          "name": "Nicolas Ugrinovic",
          "hidden": false
        },
        {
          "_id": "699678401268a6b79e0d027f",
          "name": "Matt Feiszli",
          "hidden": false
        },
        {
          "_id": "699678401268a6b79e0d0280",
          "name": "Jitendra Malik",
          "hidden": false
        },
        {
          "_id": "699678401268a6b79e0d0281",
          "name": "Piotr Dollar",
          "hidden": false
        },
        {
          "_id": "699678401268a6b79e0d0282",
          "name": "Kris Kitani",
          "hidden": false
        }
      ],
      "publishedAt": "2026-02-17T20:26:37.000Z",
      "submittedOnDailyAt": "2026-02-19T00:11:13.148Z",
      "title": "SAM 3D Body: Robust Full-Body Human Mesh Recovery",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "We introduce SAM 3D Body (3DB), a promptable model for single-image full-body 3D human mesh recovery (HMR) that demonstrates state-of-the-art performance, with strong generalization and consistent accuracy in diverse in-the-wild conditions. 3DB estimates the human pose of the body, feet, and hands. It is the first model to use a new parametric mesh representation, Momentum Human Rig (MHR), which decouples skeletal structure and surface shape. 3DB employs an encoder-decoder architecture and supports auxiliary prompts, including 2D keypoints and masks, enabling user-guided inference similar to the SAM family of models. We derive high-quality annotations from a multi-stage annotation pipeline that uses various combinations of manual keypoint annotation, differentiable optimization, multi-view geometry, and dense keypoint detection. Our data engine efficiently selects and processes data to ensure data diversity, collecting unusual poses and rare imaging conditions. We present a new evaluation dataset organized by pose and appearance categories, enabling nuanced analysis of model behavior. Our experiments demonstrate superior generalization and substantial improvements over prior methods in both qualitative user preference studies and traditional quantitative analysis. Both 3DB and MHR are open-source.",
      "upvotes": 1,
      "discussionId": "699678401268a6b79e0d0283",
      "projectPage": "https://ai.meta.com/research/sam3d/",
      "githubRepo": "https://github.com/facebookresearch/sam-3d-body",
      "githubRepoAddedBy": "user",
      "ai_summary": "A promptable 3D human mesh recovery model using a novel parametric representation and encoder-decoder architecture achieves state-of-the-art performance with strong generalization across diverse conditions.",
      "ai_keywords": [
        "3D human mesh recovery",
        "encoder-decoder architecture",
        "parametric mesh representation",
        "Momentum Human Rig",
        "2D keypoints",
        "masks",
        "auxiliary prompts",
        "differentiable optimization",
        "multi-view geometry",
        "dense keypoint detection",
        "data engine",
        "evaluation dataset",
        "qualitative user preference studies",
        "quantitative analysis"
      ],
      "githubStars": 2618
    },
    "publishedAt": "2026-02-17T15:26:37.000Z",
    "title": "SAM 3D Body: Robust Full-Body Human Mesh Recovery",
    "summary": "We introduce SAM 3D Body (3DB), a promptable model for single-image full-body 3D human mesh recovery (HMR) that demonstrates state-of-the-art performance, with strong generalization and consistent accuracy in diverse in-the-wild conditions. 3DB estimates the human pose of the body, feet, and hands. It is the first model to use a new parametric mesh representation, Momentum Human Rig (MHR), which decouples skeletal structure and surface shape. 3DB employs an encoder-decoder architecture and supports auxiliary prompts, including 2D keypoints and masks, enabling user-guided inference similar to the SAM family of models. We derive high-quality annotations from a multi-stage annotation pipeline that uses various combinations of manual keypoint annotation, differentiable optimization, multi-view geometry, and dense keypoint detection. Our data engine efficiently selects and processes data to ensure data diversity, collecting unusual poses and rare imaging conditions. We present a new evaluation dataset organized by pose and appearance categories, enabling nuanced analysis of model behavior. Our experiments demonstrate superior generalization and substantial improvements over prior methods in both qualitative user preference studies and traditional quantitative analysis. Both 3DB and MHR are open-source.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.15989.png",
    "numComments": 0,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 235,
      "isUserFollowing": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2602.15922",
      "authors": [
        {
          "_id": "6996793f1268a6b79e0d028d",
          "name": "Seonghyeon Ye",
          "hidden": false
        },
        {
          "_id": "6996793f1268a6b79e0d028e",
          "name": "Yunhao Ge",
          "hidden": false
        },
        {
          "_id": "6996793f1268a6b79e0d028f",
          "name": "Kaiyuan Zheng",
          "hidden": false
        },
        {
          "_id": "6996793f1268a6b79e0d0290",
          "name": "Shenyuan Gao",
          "hidden": false
        },
        {
          "_id": "6996793f1268a6b79e0d0291",
          "name": "Sihyun Yu",
          "hidden": false
        },
        {
          "_id": "6996793f1268a6b79e0d0292",
          "name": "George Kurian",
          "hidden": false
        },
        {
          "_id": "6996793f1268a6b79e0d0293",
          "name": "Suneel Indupuru",
          "hidden": false
        },
        {
          "_id": "6996793f1268a6b79e0d0294",
          "name": "You Liang Tan",
          "hidden": false
        },
        {
          "_id": "6996793f1268a6b79e0d0295",
          "name": "Chuning Zhu",
          "hidden": false
        },
        {
          "_id": "6996793f1268a6b79e0d0296",
          "name": "Jiannan Xiang",
          "hidden": false
        },
        {
          "_id": "6996793f1268a6b79e0d0297",
          "name": "Ayaan Malik",
          "hidden": false
        },
        {
          "_id": "6996793f1268a6b79e0d0298",
          "name": "Kyungmin Lee",
          "hidden": false
        },
        {
          "_id": "6996793f1268a6b79e0d0299",
          "name": "William Liang",
          "hidden": false
        },
        {
          "_id": "6996793f1268a6b79e0d029a",
          "name": "Nadun Ranawaka",
          "hidden": false
        },
        {
          "_id": "6996793f1268a6b79e0d029b",
          "name": "Jiasheng Gu",
          "hidden": false
        },
        {
          "_id": "6996793f1268a6b79e0d029c",
          "name": "Yinzhen Xu",
          "hidden": false
        },
        {
          "_id": "6996793f1268a6b79e0d029d",
          "name": "Guanzhi Wang",
          "hidden": false
        },
        {
          "_id": "6996793f1268a6b79e0d029e",
          "name": "Fengyuan Hu",
          "hidden": false
        },
        {
          "_id": "6996793f1268a6b79e0d029f",
          "name": "Avnish Narayan",
          "hidden": false
        },
        {
          "_id": "6996793f1268a6b79e0d02a0",
          "name": "Johan Bjorck",
          "hidden": false
        },
        {
          "_id": "6996793f1268a6b79e0d02a1",
          "name": "Jing Wang",
          "hidden": false
        },
        {
          "_id": "6996793f1268a6b79e0d02a2",
          "name": "Gwanghyun Kim",
          "hidden": false
        },
        {
          "_id": "6996793f1268a6b79e0d02a3",
          "name": "Dantong Niu",
          "hidden": false
        },
        {
          "_id": "6996793f1268a6b79e0d02a4",
          "name": "Ruijie Zheng",
          "hidden": false
        },
        {
          "_id": "6996793f1268a6b79e0d02a5",
          "name": "Yuqi Xie",
          "hidden": false
        },
        {
          "_id": "6996793f1268a6b79e0d02a6",
          "name": "Jimmy Wu",
          "hidden": false
        },
        {
          "_id": "6996793f1268a6b79e0d02a7",
          "name": "Qi Wang",
          "hidden": false
        },
        {
          "_id": "6996793f1268a6b79e0d02a8",
          "name": "Ryan Julian",
          "hidden": false
        },
        {
          "_id": "6996793f1268a6b79e0d02a9",
          "name": "Danfei Xu",
          "hidden": false
        },
        {
          "_id": "6996793f1268a6b79e0d02aa",
          "name": "Yilun Du",
          "hidden": false
        },
        {
          "_id": "6996793f1268a6b79e0d02ab",
          "name": "Yevgen Chebotar",
          "hidden": false
        },
        {
          "_id": "6996793f1268a6b79e0d02ac",
          "name": "Scott Reed",
          "hidden": false
        },
        {
          "_id": "6996793f1268a6b79e0d02ad",
          "name": "Jan Kautz",
          "hidden": false
        },
        {
          "_id": "6996793f1268a6b79e0d02ae",
          "name": "Yuke Zhu",
          "hidden": false
        },
        {
          "_id": "6996793f1268a6b79e0d02af",
          "name": "Linxi \"Jim\" Fan",
          "hidden": false
        },
        {
          "_id": "6996793f1268a6b79e0d02b0",
          "name": "Joel Jang",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/aeAgRW6Fq1wOWJzNGPdUm.mp4"
      ],
      "publishedAt": "2026-02-17T15:04:02.000Z",
      "submittedOnDailyAt": "2026-02-19T00:17:09.596Z",
      "title": "World Action Models are Zero-shot Policies",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "State-of-the-art Vision-Language-Action (VLA) models excel at semantic generalization but struggle to generalize to unseen physical motions in novel environments. We introduce DreamZero, a World Action Model (WAM) built upon a pretrained video diffusion backbone. Unlike VLAs, WAMs learn physical dynamics by predicting future world states and actions, using video as a dense representation of how the world evolves. By jointly modeling video and action, DreamZero learns diverse skills effectively from heterogeneous robot data without relying on repetitive demonstrations. This results in over 2x improvement in generalization to new tasks and environments compared to state-of-the-art VLAs in real robot experiments. Crucially, through model and system optimizations, we enable a 14B autoregressive video diffusion model to perform real-time closed-loop control at 7Hz. Finally, we demonstrate two forms of cross-embodiment transfer: video-only demonstrations from other robots or humans yield a relative improvement of over 42% on unseen task performance with just 10-20 minutes of data. More surprisingly, DreamZero enables few-shot embodiment adaptation, transferring to a new embodiment with only 30 minutes of play data while retaining zero-shot generalization.",
      "upvotes": 1,
      "discussionId": "699679401268a6b79e0d02b1",
      "projectPage": "https://dreamzero0.github.io/",
      "githubRepo": "https://github.com/dreamzero0/dreamzero",
      "githubRepoAddedBy": "user",
      "ai_summary": "DreamZero is a World Action Model that leverages video diffusion to enable better generalization of physical motions across novel environments and embodiments compared to vision-language-action models.",
      "ai_keywords": [
        "World Action Model",
        "video diffusion",
        "video backbone",
        "physical dynamics",
        "autoregressive video diffusion model",
        "closed-loop control",
        "cross-embodiment transfer",
        "few-shot embodiment adaptation"
      ],
      "githubStars": 724,
      "organization": {
        "_id": "676eec5d639faf44bc95708b",
        "name": "NVIDIA-DIR",
        "fullname": "NVIDIA Deep Imagination Research"
      }
    },
    "publishedAt": "2026-02-17T10:04:02.000Z",
    "title": "World Action Models are Zero-shot Policies",
    "summary": "State-of-the-art Vision-Language-Action (VLA) models excel at semantic generalization but struggle to generalize to unseen physical motions in novel environments. We introduce DreamZero, a World Action Model (WAM) built upon a pretrained video diffusion backbone. Unlike VLAs, WAMs learn physical dynamics by predicting future world states and actions, using video as a dense representation of how the world evolves. By jointly modeling video and action, DreamZero learns diverse skills effectively from heterogeneous robot data without relying on repetitive demonstrations. This results in over 2x improvement in generalization to new tasks and environments compared to state-of-the-art VLAs in real robot experiments. Crucially, through model and system optimizations, we enable a 14B autoregressive video diffusion model to perform real-time closed-loop control at 7Hz. Finally, we demonstrate two forms of cross-embodiment transfer: video-only demonstrations from other robots or humans yield a relative improvement of over 42% on unseen task performance with just 10-20 minutes of data. More surprisingly, DreamZero enables few-shot embodiment adaptation, transferring to a new embodiment with only 30 minutes of play data while retaining zero-shot generalization.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/aeAgRW6Fq1wOWJzNGPdUm.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.15922.png",
    "numComments": 0,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 235,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "676eec5d639faf44bc95708b",
      "name": "NVIDIA-DIR",
      "fullname": "NVIDIA Deep Imagination Research"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2602.16666",
      "authors": [
        {
          "_id": "69967a061268a6b79e0d02b3",
          "name": "Stephan Rabanser",
          "hidden": false
        },
        {
          "_id": "69967a061268a6b79e0d02b4",
          "name": "Sayash Kapoor",
          "hidden": false
        },
        {
          "_id": "69967a061268a6b79e0d02b5",
          "name": "Peter Kirgis",
          "hidden": false
        },
        {
          "_id": "69967a061268a6b79e0d02b6",
          "name": "Kangheng Liu",
          "hidden": false
        },
        {
          "_id": "69967a061268a6b79e0d02b7",
          "name": "Saiteja Utpala",
          "hidden": false
        },
        {
          "_id": "69967a061268a6b79e0d02b8",
          "name": "Arvind Narayanan",
          "hidden": false
        }
      ],
      "publishedAt": "2026-02-18T18:05:44.000Z",
      "submittedOnDailyAt": "2026-02-19T00:18:48.136Z",
      "title": "Towards a Science of AI Agent Reliability",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "AI agents are increasingly deployed to execute important tasks. While rising accuracy scores on standard benchmarks suggest rapid progress, many agents still continue to fail in practice. This discrepancy highlights a fundamental limitation of current evaluations: compressing agent behavior into a single success metric obscures critical operational flaws. Notably, it ignores whether agents behave consistently across runs, withstand perturbations, fail predictably, or have bounded error severity. Grounded in safety-critical engineering, we provide a holistic performance profile by proposing twelve concrete metrics that decompose agent reliability along four key dimensions: consistency, robustness, predictability, and safety. Evaluating 14 agentic models across two complementary benchmarks, we find that recent capability gains have only yielded small improvements in reliability. By exposing these persistent limitations, our metrics complement traditional evaluations while offering tools for reasoning about how agents perform, degrade, and fail.",
      "upvotes": 0,
      "discussionId": "69967a071268a6b79e0d02b9",
      "projectPage": "https://hal.cs.princeton.edu/reliability",
      "ai_summary": "Traditional benchmark evaluations of AI agents fail to capture critical reliability issues, prompting the development of comprehensive metrics that assess consistency, robustness, predictability, and safety across multiple dimensions.",
      "ai_keywords": [
        "AI agents",
        "reliability",
        "consistency",
        "robustness",
        "predictability",
        "safety",
        "performance profiling",
        "benchmark evaluation"
      ],
      "organization": {
        "_id": "64374111a701a7e744c02b0e",
        "name": "princetonu",
        "fullname": "Princeton University",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/68e396f2b5bb631e9b2fac9a/b3xXusq8Zz3ej8Z6fRTSZ.png"
      }
    },
    "publishedAt": "2026-02-18T13:05:44.000Z",
    "title": "Towards a Science of AI Agent Reliability",
    "summary": "AI agents are increasingly deployed to execute important tasks. While rising accuracy scores on standard benchmarks suggest rapid progress, many agents still continue to fail in practice. This discrepancy highlights a fundamental limitation of current evaluations: compressing agent behavior into a single success metric obscures critical operational flaws. Notably, it ignores whether agents behave consistently across runs, withstand perturbations, fail predictably, or have bounded error severity. Grounded in safety-critical engineering, we provide a holistic performance profile by proposing twelve concrete metrics that decompose agent reliability along four key dimensions: consistency, robustness, predictability, and safety. Evaluating 14 agentic models across two complementary benchmarks, we find that recent capability gains have only yielded small improvements in reliability. By exposing these persistent limitations, our metrics complement traditional evaluations while offering tools for reasoning about how agents perform, degrade, and fail.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.16666.png",
    "numComments": 0,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 235,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "64374111a701a7e744c02b0e",
      "name": "princetonu",
      "fullname": "Princeton University",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/68e396f2b5bb631e9b2fac9a/b3xXusq8Zz3ej8Z6fRTSZ.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2602.16493",
      "authors": [
        {
          "_id": "69967f361268a6b79e0d02e4",
          "name": "Yihao Lu",
          "hidden": false
        },
        {
          "_id": "69967f361268a6b79e0d02e5",
          "name": "Wanru Cheng",
          "hidden": false
        },
        {
          "_id": "69967f361268a6b79e0d02e6",
          "name": "Zeyu Zhang",
          "hidden": false
        },
        {
          "_id": "69967f361268a6b79e0d02e7",
          "name": "Hao Tang",
          "hidden": false
        }
      ],
      "publishedAt": "2026-02-18T14:30:35.000Z",
      "submittedOnDailyAt": "2026-02-19T00:48:16.410Z",
      "title": "MMA: Multimodal Memory Agent",
      "submittedOnDailyBy": {
        "_id": "64ec877bb93654d4ca5c92e9",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ec877bb93654d4ca5c92e9/GvHk_KSdE9Rhnk_o-NaZX.jpeg",
        "isPro": false,
        "fullname": "Zeyu Zhang",
        "user": "SteveZeyuZhang",
        "type": "user"
      },
      "summary": "Long-horizon multimodal agents depend on external memory; however, similarity-based retrieval often surfaces stale, low-credibility, or conflicting items, which can trigger overconfident errors. We propose Multimodal Memory Agent (MMA), which assigns each retrieved memory item a dynamic reliability score by combining source credibility, temporal decay, and conflict-aware network consensus, and uses this signal to reweight evidence and abstain when support is insufficient. We also introduce MMA-Bench, a programmatically generated benchmark for belief dynamics with controlled speaker reliability and structured text-vision contradictions. Using this framework, we uncover the \"Visual Placebo Effect\", revealing how RAG-based agents inherit latent visual biases from foundation models. On FEVER, MMA matches baseline accuracy while reducing variance by 35.2% and improving selective utility; on LoCoMo, a safety-oriented configuration improves actionable accuracy and reduces wrong answers; on MMA-Bench, MMA reaches 41.18% Type-B accuracy in Vision mode, while the baseline collapses to 0.0% under the same protocol. Code: https://github.com/AIGeeksGroup/MMA.",
      "upvotes": 0,
      "discussionId": "69967f371268a6b79e0d02e8",
      "githubRepo": "https://github.com/AIGeeksGroup/MMA",
      "githubRepoAddedBy": "user",
      "ai_summary": "Multimodal Memory Agent (MMA) improves long-horizon agent performance by dynamically scoring memory reliability and handling visual biases in retrieval-augmented systems.",
      "ai_keywords": [
        "multimodal agents",
        "external memory",
        "similarity-based retrieval",
        "reliability scoring",
        "source credibility",
        "temporal decay",
        "conflict-aware network consensus",
        "evidence reweighting",
        "belief dynamics",
        "RAG-based agents",
        "visual placebo effect",
        "retrieval-augmented generation",
        "MMA-Bench",
        "Type-B accuracy"
      ],
      "organization": {
        "_id": "61dcd8e344f59573371b5cb6",
        "name": "PekingUniversity",
        "fullname": "Peking University",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/vavgrBsnkSejriUF4lXDE.png"
      }
    },
    "publishedAt": "2026-02-18T09:30:35.000Z",
    "title": "MMA: Multimodal Memory Agent",
    "summary": "Long-horizon multimodal agents depend on external memory; however, similarity-based retrieval often surfaces stale, low-credibility, or conflicting items, which can trigger overconfident errors. We propose Multimodal Memory Agent (MMA), which assigns each retrieved memory item a dynamic reliability score by combining source credibility, temporal decay, and conflict-aware network consensus, and uses this signal to reweight evidence and abstain when support is insufficient. We also introduce MMA-Bench, a programmatically generated benchmark for belief dynamics with controlled speaker reliability and structured text-vision contradictions. Using this framework, we uncover the \"Visual Placebo Effect\", revealing how RAG-based agents inherit latent visual biases from foundation models. On FEVER, MMA matches baseline accuracy while reducing variance by 35.2% and improving selective utility; on LoCoMo, a safety-oriented configuration improves actionable accuracy and reduces wrong answers; on MMA-Bench, MMA reaches 41.18% Type-B accuracy in Vision mode, while the baseline collapses to 0.0% under the same protocol. Code: https://github.com/AIGeeksGroup/MMA.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.16493.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64ec877bb93654d4ca5c92e9",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ec877bb93654d4ca5c92e9/GvHk_KSdE9Rhnk_o-NaZX.jpeg",
      "fullname": "Zeyu Zhang",
      "name": "SteveZeyuZhang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "61dcd8e344f59573371b5cb6",
      "name": "PekingUniversity",
      "fullname": "Peking University",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/vavgrBsnkSejriUF4lXDE.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2602.16301",
      "authors": [
        {
          "_id": "69967b821268a6b79e0d02c1",
          "name": "Marissa A. Weis",
          "hidden": false
        },
        {
          "_id": "69967b821268a6b79e0d02c2",
          "name": "Maciej Wołczyk",
          "hidden": false
        },
        {
          "_id": "69967b821268a6b79e0d02c3",
          "name": "Rajai Nasser",
          "hidden": false
        },
        {
          "_id": "69967b821268a6b79e0d02c4",
          "name": "Rif A. Saurous",
          "hidden": false
        },
        {
          "_id": "69967b821268a6b79e0d02c5",
          "name": "Blaise Agüera y Arcas",
          "hidden": false
        },
        {
          "_id": "69967b821268a6b79e0d02c6",
          "name": "João Sacramento",
          "hidden": false
        },
        {
          "_id": "69967b821268a6b79e0d02c7",
          "name": "Alexander Meulemans",
          "hidden": false
        }
      ],
      "publishedAt": "2026-02-18T09:31:43.000Z",
      "submittedOnDailyAt": "2026-02-19T00:25:09.660Z",
      "title": "Multi-agent cooperation through in-context co-player inference",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Achieving cooperation among self-interested agents remains a fundamental challenge in multi-agent reinforcement learning. Recent work showed that mutual cooperation can be induced between \"learning-aware\" agents that account for and shape the learning dynamics of their co-players. However, existing approaches typically rely on hardcoded, often inconsistent, assumptions about co-player learning rules or enforce a strict separation between \"naive learners\" updating on fast timescales and \"meta-learners\" observing these updates. Here, we demonstrate that the in-context learning capabilities of sequence models allow for co-player learning awareness without requiring hardcoded assumptions or explicit timescale separation. We show that training sequence model agents against a diverse distribution of co-players naturally induces in-context best-response strategies, effectively functioning as learning algorithms on the fast intra-episode timescale. We find that the cooperative mechanism identified in prior work-where vulnerability to extortion drives mutual shaping-emerges naturally in this setting: in-context adaptation renders agents vulnerable to extortion, and the resulting mutual pressure to shape the opponent's in-context learning dynamics resolves into the learning of cooperative behavior. Our results suggest that standard decentralized reinforcement learning on sequence models combined with co-player diversity provides a scalable path to learning cooperative behaviors.",
      "upvotes": 0,
      "discussionId": "69967b821268a6b79e0d02c8",
      "ai_summary": "Sequence models enable cooperative behavior emergence in multi-agent reinforcement learning through in-context learning without hardcoded assumptions or timescale separation.",
      "ai_keywords": [
        "multi-agent reinforcement learning",
        "sequence models",
        "in-context learning",
        "cooperative behavior",
        "learning-aware agents",
        "learning dynamics",
        "fast timescale",
        "meta-learners",
        "decentralized reinforcement learning",
        "co-player diversity"
      ],
      "organization": {
        "_id": "5e6aca39878b8b2bf9806447",
        "name": "google",
        "fullname": "Google",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/WtA3YYitedOr9n02eHfJe.png"
      }
    },
    "publishedAt": "2026-02-18T04:31:43.000Z",
    "title": "Multi-agent cooperation through in-context co-player inference",
    "summary": "Achieving cooperation among self-interested agents remains a fundamental challenge in multi-agent reinforcement learning. Recent work showed that mutual cooperation can be induced between \"learning-aware\" agents that account for and shape the learning dynamics of their co-players. However, existing approaches typically rely on hardcoded, often inconsistent, assumptions about co-player learning rules or enforce a strict separation between \"naive learners\" updating on fast timescales and \"meta-learners\" observing these updates. Here, we demonstrate that the in-context learning capabilities of sequence models allow for co-player learning awareness without requiring hardcoded assumptions or explicit timescale separation. We show that training sequence model agents against a diverse distribution of co-players naturally induces in-context best-response strategies, effectively functioning as learning algorithms on the fast intra-episode timescale. We find that the cooperative mechanism identified in prior work-where vulnerability to extortion drives mutual shaping-emerges naturally in this setting: in-context adaptation renders agents vulnerable to extortion, and the resulting mutual pressure to shape the opponent's in-context learning dynamics resolves into the learning of cooperative behavior. Our results suggest that standard decentralized reinforcement learning on sequence models combined with co-player diversity provides a scalable path to learning cooperative behaviors.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.16301.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 235,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "5e6aca39878b8b2bf9806447",
      "name": "google",
      "fullname": "Google",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/WtA3YYitedOr9n02eHfJe.png"
    },
    "isAuthorParticipating": false
  }
]