[
  {
    "paper": {
      "id": "2512.24330",
      "authors": [
        {
          "_id": "69560bcd832867f2535256fc",
          "name": "Yong Xien Chng",
          "hidden": false
        },
        {
          "_id": "69560bcd832867f2535256fd",
          "name": "Tao Hu",
          "hidden": false
        },
        {
          "_id": "69560bcd832867f2535256fe",
          "name": "Wenwen Tong",
          "hidden": false
        },
        {
          "_id": "69560bcd832867f2535256ff",
          "name": "Xueheng Li",
          "hidden": false
        },
        {
          "_id": "69560bcd832867f253525700",
          "name": "Jiandong Chen",
          "hidden": false
        },
        {
          "_id": "69560bcd832867f253525701",
          "name": "Haojia Yu",
          "hidden": false
        },
        {
          "_id": "69560bcd832867f253525702",
          "name": "Jiefan Lu",
          "hidden": false
        },
        {
          "_id": "69560bcd832867f253525703",
          "name": "Hewei Guo",
          "hidden": false
        },
        {
          "_id": "69560bcd832867f253525704",
          "name": "Hanming Deng",
          "hidden": false
        },
        {
          "_id": "69560bcd832867f253525705",
          "name": "Chengjun Xie",
          "hidden": false
        },
        {
          "_id": "69560bcd832867f253525706",
          "name": "Gao Huang",
          "hidden": false
        },
        {
          "_id": "69560bcd832867f253525707",
          "name": "Dahua Lin",
          "hidden": false
        },
        {
          "_id": "69560bcd832867f253525708",
          "name": "Lewei Lu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-12-30T16:31:45.000Z",
      "submittedOnDailyAt": "2026-01-05T00:14:32.572Z",
      "title": "SenseNova-MARS: Empowering Multimodal Agentic Reasoning and Search via Reinforcement Learning",
      "submittedOnDailyBy": {
        "_id": "647d4f1236e109abce409c3b",
        "avatarUrl": "/avatars/d166f5f8be666e96b522a0a0effd21c4.svg",
        "isPro": false,
        "fullname": "Wenwen Tong",
        "user": "tongww",
        "type": "user"
      },
      "summary": "While Vision-Language Models (VLMs) can solve complex tasks through agentic reasoning, their capabilities remain largely constrained to text-oriented chain-of-thought or isolated tool invocation. They fail to exhibit the human-like proficiency required to seamlessly interleave dynamic tool manipulation with continuous reasoning, particularly in knowledge-intensive and visually complex scenarios that demand coordinated external tools such as search and image cropping. In this work, we introduce SenseNova-MARS, a novel Multimodal Agentic Reasoning and Search framework that empowers VLMs with interleaved visual reasoning and tool-use capabilities via reinforcement learning (RL). Specifically, SenseNova-MARS dynamically integrates the image search, text search, and image crop tools to tackle fine-grained and knowledge-intensive visual understanding challenges. In the RL stage, we propose the Batch-Normalized Group Sequence Policy Optimization (BN-GSPO) algorithm to improve the training stability and advance the model's ability to invoke tools and reason effectively. To comprehensively evaluate the agentic VLMs on complex visual tasks, we introduce the HR-MMSearch benchmark, the first search-oriented benchmark composed of high-resolution images with knowledge-intensive and search-driven questions. Experiments demonstrate that SenseNova-MARS achieves state-of-the-art performance on open-source search and fine-grained image understanding benchmarks. Specifically, on search-oriented benchmarks, SenseNova-MARS-8B scores 67.84 on MMSearch and 41.64 on HR-MMSearch, surpassing proprietary models such as Gemini-3-Flash and GPT-5. SenseNova-MARS represents a promising step toward agentic VLMs by providing effective and robust tool-use capabilities. To facilitate further research in this field, we will release all code, models, and datasets.",
      "upvotes": 22,
      "discussionId": "69560bcd832867f253525709",
      "githubRepo": "https://github.com/OpenSenseNova/SenseNova-MARS",
      "githubRepoAddedBy": "user",
      "organization": {
        "_id": "64f0405f8a4cf3e5e6b38f9c",
        "name": "sensenova",
        "fullname": "SenseNova",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/652d06833b5997ed71ce5c46/k66xcOMf4NVbMSFulUjHY.png"
      }
    },
    "publishedAt": "2025-12-30T11:31:45.000Z",
    "title": "SenseNova-MARS: Empowering Multimodal Agentic Reasoning and Search via Reinforcement Learning",
    "summary": "While Vision-Language Models (VLMs) can solve complex tasks through agentic reasoning, their capabilities remain largely constrained to text-oriented chain-of-thought or isolated tool invocation. They fail to exhibit the human-like proficiency required to seamlessly interleave dynamic tool manipulation with continuous reasoning, particularly in knowledge-intensive and visually complex scenarios that demand coordinated external tools such as search and image cropping. In this work, we introduce SenseNova-MARS, a novel Multimodal Agentic Reasoning and Search framework that empowers VLMs with interleaved visual reasoning and tool-use capabilities via reinforcement learning (RL). Specifically, SenseNova-MARS dynamically integrates the image search, text search, and image crop tools to tackle fine-grained and knowledge-intensive visual understanding challenges. In the RL stage, we propose the Batch-Normalized Group Sequence Policy Optimization (BN-GSPO) algorithm to improve the training stability and advance the model's ability to invoke tools and reason effectively. To comprehensively evaluate the agentic VLMs on complex visual tasks, we introduce the HR-MMSearch benchmark, the first search-oriented benchmark composed of high-resolution images with knowledge-intensive and search-driven questions. Experiments demonstrate that SenseNova-MARS achieves state-of-the-art performance on open-source search and fine-grained image understanding benchmarks. Specifically, on search-oriented benchmarks, SenseNova-MARS-8B scores 67.84 on MMSearch and 41.64 on HR-MMSearch, surpassing proprietary models such as Gemini-3-Flash and GPT-5. SenseNova-MARS represents a promising step toward agentic VLMs by providing effective and robust tool-use capabilities. To facilitate further research in this field, we will release all code, models, and datasets.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.24330.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "647d4f1236e109abce409c3b",
      "avatarUrl": "/avatars/d166f5f8be666e96b522a0a0effd21c4.svg",
      "fullname": "Wenwen Tong",
      "name": "tongww",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "organization": {
      "_id": "64f0405f8a4cf3e5e6b38f9c",
      "name": "sensenova",
      "fullname": "SenseNova",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/652d06833b5997ed71ce5c46/k66xcOMf4NVbMSFulUjHY.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2512.24615",
      "authors": [
        {
          "_id": "69564d96832867f2535257af",
          "user": {
            "_id": "622b00a776c20fee5d14501b",
            "avatarUrl": "/avatars/e00496dda1e309548e7b5b437839bb65.svg",
            "isPro": false,
            "fullname": "Eason shi",
            "user": "Easonshi",
            "type": "user"
          },
          "name": "Yuchen Shi",
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-01-04T20:09:50.111Z",
          "hidden": false
        },
        {
          "_id": "69564d96832867f2535257b0",
          "name": "Yuzheng Cai",
          "hidden": false
        },
        {
          "_id": "69564d96832867f2535257b1",
          "name": "Siqi Cai",
          "hidden": false
        },
        {
          "_id": "69564d96832867f2535257b2",
          "name": "Zihan Xu",
          "hidden": false
        },
        {
          "_id": "69564d96832867f2535257b3",
          "name": "Lichao Chen",
          "hidden": false
        },
        {
          "_id": "69564d96832867f2535257b4",
          "user": {
            "_id": "6390525c00fb8ec4a424e0ff",
            "avatarUrl": "/avatars/4302571e2ef4a9875491221aa630a329.svg",
            "isPro": false,
            "fullname": "Yulei Qin",
            "user": "yolay",
            "type": "user"
          },
          "name": "Yulei Qin",
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-01-04T20:09:48.064Z",
          "hidden": false
        },
        {
          "_id": "69564d96832867f2535257b5",
          "name": "Zhijian Zhou",
          "hidden": false
        },
        {
          "_id": "69564d96832867f2535257b6",
          "name": "Xiang Fei",
          "hidden": false
        },
        {
          "_id": "69564d96832867f2535257b7",
          "name": "Chaofan Qiu",
          "hidden": false
        },
        {
          "_id": "69564d96832867f2535257b8",
          "name": "Xiaoyu Tan",
          "hidden": false
        },
        {
          "_id": "69564d96832867f2535257b9",
          "name": "Gang Li",
          "hidden": false
        },
        {
          "_id": "69564d96832867f2535257ba",
          "name": "Zongyi Li",
          "hidden": false
        },
        {
          "_id": "69564d96832867f2535257bb",
          "name": "Haojia Lin",
          "hidden": false
        },
        {
          "_id": "69564d96832867f2535257bc",
          "name": "Guocan Cai",
          "hidden": false
        },
        {
          "_id": "69564d96832867f2535257bd",
          "name": "Yong Mao",
          "hidden": false
        },
        {
          "_id": "69564d96832867f2535257be",
          "name": "Yunsheng Wu",
          "hidden": false
        },
        {
          "_id": "69564d96832867f2535257bf",
          "name": "Ke Li",
          "hidden": false
        },
        {
          "_id": "69564d96832867f2535257c0",
          "user": {
            "_id": "647401e50da364bd0d002f2a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/vPuPn7EV092mLBOM2YZXd.png",
            "isPro": false,
            "fullname": "XING SUN",
            "user": "tedsun",
            "type": "user"
          },
          "name": "Xing Sun",
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-01-02T15:38:39.390Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-12-31T04:17:36.000Z",
      "submittedOnDailyAt": "2026-01-05T00:21:56.456Z",
      "title": "Youtu-Agent: Scaling Agent Productivity with Automated Generation and Hybrid Policy Optimization",
      "submittedOnDailyBy": {
        "_id": "63280915eeee4dd858083092",
        "avatarUrl": "/avatars/78347af4af42527d53e88d9969c5c934.svg",
        "isPro": false,
        "fullname": "Ke Li",
        "user": "tristanli",
        "type": "user"
      },
      "summary": "Existing Large Language Model (LLM) agent frameworks face two significant challenges: high configuration costs and static capabilities. Building a high-quality agent often requires extensive manual effort in tool integration and prompt engineering, while deployed agents struggle to adapt to dynamic environments without expensive fine-tuning. To address these issues, we propose Youtu-Agent, a modular framework designed for the automated generation and continuous evolution of LLM agents. Youtu-Agent features a structured configuration system that decouples execution environments, toolkits, and context management, enabling flexible reuse and automated synthesis. We introduce two generation paradigms: a Workflow mode for standard tasks and a Meta-Agent mode for complex, non-standard requirements, capable of automatically generating tool code, prompts, and configurations. Furthermore, Youtu-Agent establishes a hybrid policy optimization system: (1) an Agent Practice module that enables agents to accumulate experience and improve performance through in-context optimization without parameter updates; and (2) an Agent RL module that integrates with distributed training frameworks to enable scalable and stable reinforcement learning of any Youtu-Agents in an end-to-end, large-scale manner. Experiments demonstrate that Youtu-Agent achieves state-of-the-art performance on WebWalkerQA (71.47\\%) and GAIA (72.8\\%) using open-weight models. Our automated generation pipeline achieves over 81\\% tool synthesis success rate, while the Practice module improves performance on AIME 2024/2025 by +2.7\\% and +5.4\\% respectively. Moreover, our Agent RL training achieves 40\\% speedup with steady performance improvement on 7B LLMs, enhancing coding/reasoning and searching capabilities respectively up to 35\\% and 21\\% on Maths and general/multi-hop QA benchmarks.",
      "upvotes": 19,
      "discussionId": "69564d96832867f2535257c1",
      "projectPage": "https://tencentcloudadp.github.io/youtu-agent/",
      "githubRepo": "https://github.com/TencentCloudADP/youtu-agent",
      "githubRepoAddedBy": "user",
      "githubStars": 4056,
      "organization": {
        "_id": "66543b6e420092799d2f625c",
        "name": "tencent",
        "fullname": "Tencent",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png"
      }
    },
    "publishedAt": "2025-12-30T23:17:36.000Z",
    "title": "Youtu-Agent: Scaling Agent Productivity with Automated Generation and Hybrid Policy Optimization",
    "summary": "Existing Large Language Model (LLM) agent frameworks face two significant challenges: high configuration costs and static capabilities. Building a high-quality agent often requires extensive manual effort in tool integration and prompt engineering, while deployed agents struggle to adapt to dynamic environments without expensive fine-tuning. To address these issues, we propose Youtu-Agent, a modular framework designed for the automated generation and continuous evolution of LLM agents. Youtu-Agent features a structured configuration system that decouples execution environments, toolkits, and context management, enabling flexible reuse and automated synthesis. We introduce two generation paradigms: a Workflow mode for standard tasks and a Meta-Agent mode for complex, non-standard requirements, capable of automatically generating tool code, prompts, and configurations. Furthermore, Youtu-Agent establishes a hybrid policy optimization system: (1) an Agent Practice module that enables agents to accumulate experience and improve performance through in-context optimization without parameter updates; and (2) an Agent RL module that integrates with distributed training frameworks to enable scalable and stable reinforcement learning of any Youtu-Agents in an end-to-end, large-scale manner. Experiments demonstrate that Youtu-Agent achieves state-of-the-art performance on WebWalkerQA (71.47\\%) and GAIA (72.8\\%) using open-weight models. Our automated generation pipeline achieves over 81\\% tool synthesis success rate, while the Practice module improves performance on AIME 2024/2025 by +2.7\\% and +5.4\\% respectively. Moreover, our Agent RL training achieves 40\\% speedup with steady performance improvement on 7B LLMs, enhancing coding/reasoning and searching capabilities respectively up to 35\\% and 21\\% on Maths and general/multi-hop QA benchmarks.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.24615.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63280915eeee4dd858083092",
      "avatarUrl": "/avatars/78347af4af42527d53e88d9969c5c934.svg",
      "fullname": "Ke Li",
      "name": "tristanli",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "organization": {
      "_id": "66543b6e420092799d2f625c",
      "name": "tencent",
      "fullname": "Tencent",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2601.00417",
      "authors": [
        {
          "_id": "695b2238832867f253525d62",
          "name": "Yifan Zhang",
          "hidden": false
        },
        {
          "_id": "695b2238832867f253525d63",
          "name": "Yifeng Liu",
          "hidden": false
        },
        {
          "_id": "695b2238832867f253525d64",
          "name": "Mengdi Wang",
          "hidden": false
        },
        {
          "_id": "695b2238832867f253525d65",
          "name": "Quanquan Gu",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/BQO8lEQ-MwHpWvjNsJo6d.png"
      ],
      "publishedAt": "2026-01-01T18:11:38.000Z",
      "submittedOnDailyAt": "2026-01-05T00:00:42.191Z",
      "title": "Deep Delta Learning",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "The efficacy of deep residual networks is fundamentally predicated on the identity shortcut connection. While this mechanism effectively mitigates the vanishing gradient problem, it imposes a strictly additive inductive bias on feature transformations, thereby limiting the network's capacity to model complex state transitions. In this paper, we introduce Deep Delta Learning (DDL), a novel architecture that generalizes the standard residual connection by modulating the identity shortcut with a learnable, data-dependent geometric transformation. This transformation, termed the Delta Operator, constitutes a rank-1 perturbation of the identity matrix, parameterized by a reflection direction vector k(X) and a gating scalar β(X). We provide a spectral analysis of this operator, demonstrating that the gate β(X) enables dynamic interpolation between identity mapping, orthogonal projection, and geometric reflection. Furthermore, we restructure the residual update as a synchronous rank-1 injection, where the gate acts as a dynamic step size governing both the erasure of old information and the writing of new features. This unification empowers the network to explicitly control the spectrum of its layer-wise transition operator, enabling the modeling of complex, non-monotonic dynamics while preserving the stable training characteristics of gated residual architectures.",
      "upvotes": 2,
      "discussionId": "695b2238832867f253525d66",
      "githubRepo": "https://github.com/yifanzhang-pro/deep-delta-learning",
      "githubRepoAddedBy": "user"
    },
    "publishedAt": "2026-01-01T13:11:38.000Z",
    "title": "Deep Delta Learning",
    "summary": "The efficacy of deep residual networks is fundamentally predicated on the identity shortcut connection. While this mechanism effectively mitigates the vanishing gradient problem, it imposes a strictly additive inductive bias on feature transformations, thereby limiting the network's capacity to model complex state transitions. In this paper, we introduce Deep Delta Learning (DDL), a novel architecture that generalizes the standard residual connection by modulating the identity shortcut with a learnable, data-dependent geometric transformation. This transformation, termed the Delta Operator, constitutes a rank-1 perturbation of the identity matrix, parameterized by a reflection direction vector k(X) and a gating scalar β(X). We provide a spectral analysis of this operator, demonstrating that the gate β(X) enables dynamic interpolation between identity mapping, orthogonal projection, and geometric reflection. Furthermore, we restructure the residual update as a synchronous rank-1 injection, where the gate acts as a dynamic step size governing both the erasure of old information and the writing of new features. This unification empowers the network to explicitly control the spectrum of its layer-wise transition operator, enabling the modeling of complex, non-monotonic dynamics while preserving the stable training characteristics of gated residual architectures.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/BQO8lEQ-MwHpWvjNsJo6d.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.00417.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 197
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2601.00664",
      "authors": [
        {
          "_id": "695b237a832867f253525d70",
          "name": "Taekyung Ki",
          "hidden": false
        },
        {
          "_id": "695b237a832867f253525d71",
          "name": "Sangwon Jang",
          "hidden": false
        },
        {
          "_id": "695b237a832867f253525d72",
          "name": "Jaehyeong Jo",
          "hidden": false
        },
        {
          "_id": "695b237a832867f253525d73",
          "name": "Jaehong Yoon",
          "hidden": false
        },
        {
          "_id": "695b237a832867f253525d74",
          "name": "Sung Ju Hwang",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/OjpAmq7fuwGa-ZxL3KbSY.mp4"
      ],
      "publishedAt": "2026-01-02T11:58:48.000Z",
      "submittedOnDailyAt": "2026-01-05T00:05:44.498Z",
      "title": "Avatar Forcing: Real-Time Interactive Head Avatar Generation for Natural Conversation",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Talking head generation creates lifelike avatars from static portraits for virtual communication and content creation. However, current models do not yet convey the feeling of truly interactive communication, often generating one-way responses that lack emotional engagement. We identify two key challenges toward truly interactive avatars: generating motion in real-time under causal constraints and learning expressive, vibrant reactions without additional labeled data. To address these challenges, we propose Avatar Forcing, a new framework for interactive head avatar generation that models real-time user-avatar interactions through diffusion forcing. This design allows the avatar to process real-time multimodal inputs, including the user's audio and motion, with low latency for instant reactions to both verbal and non-verbal cues such as speech, nods, and laughter. Furthermore, we introduce a direct preference optimization method that leverages synthetic losing samples constructed by dropping user conditions, enabling label-free learning of expressive interaction. Experimental results demonstrate that our framework enables real-time interaction with low latency (approximately 500ms), achieving 6.8X speedup compared to the baseline, and produces reactive and expressive avatar motion, which is preferred over 80% against the baseline.",
      "upvotes": 1,
      "discussionId": "695b237a832867f253525d75",
      "projectPage": "https://taekyungki.github.io/AvatarForcing/",
      "githubRepo": "https://github.com/TaekyungKi/AvatarForcing",
      "githubRepoAddedBy": "user",
      "ai_summary": "A framework called Avatar Forcing uses diffusion forcing and direct preference optimization with synthetic losing samples to enable real-time, expressive multimodal interactions in talking head avatars without labeled data.",
      "ai_keywords": [
        "diffusion forcing",
        "direct preference optimization",
        "synthetic losing samples",
        "causal constraints",
        "multimodal inputs",
        "label-free learning"
      ]
    },
    "publishedAt": "2026-01-02T06:58:48.000Z",
    "title": "Avatar Forcing: Real-Time Interactive Head Avatar Generation for Natural Conversation",
    "summary": "Talking head generation creates lifelike avatars from static portraits for virtual communication and content creation. However, current models do not yet convey the feeling of truly interactive communication, often generating one-way responses that lack emotional engagement. We identify two key challenges toward truly interactive avatars: generating motion in real-time under causal constraints and learning expressive, vibrant reactions without additional labeled data. To address these challenges, we propose Avatar Forcing, a new framework for interactive head avatar generation that models real-time user-avatar interactions through diffusion forcing. This design allows the avatar to process real-time multimodal inputs, including the user's audio and motion, with low latency for instant reactions to both verbal and non-verbal cues such as speech, nods, and laughter. Furthermore, we introduce a direct preference optimization method that leverages synthetic losing samples constructed by dropping user conditions, enabling label-free learning of expressive interaction. Experimental results demonstrate that our framework enables real-time interaction with low latency (approximately 500ms), achieving 6.8X speedup compared to the baseline, and produces reactive and expressive avatar motion, which is preferred over 80% against the baseline.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/OjpAmq7fuwGa-ZxL3KbSY.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.00664.png",
    "numComments": 0,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 197
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2601.00671",
      "authors": [
        {
          "_id": "695b23dc832867f253525d77",
          "name": "Tianyu Zhao",
          "hidden": false
        },
        {
          "_id": "695b23dc832867f253525d78",
          "name": "Llion Jones",
          "hidden": false
        }
      ],
      "publishedAt": "2026-01-02T12:37:53.000Z",
      "submittedOnDailyAt": "2026-01-05T00:07:23.841Z",
      "title": "Fast-weight Product Key Memory",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Sequence modeling layers in modern language models typically face a trade-off between storage capacity and computational efficiency. While Softmax attention offers unbounded storage at prohibitive quadratic costs, linear variants provide efficiency but suffer from limited, fixed-size storage. We propose Fast-weight Product Key Memory (FwPKM), a novel architecture that resolves this tension by transforming the sparse Product Key Memory (PKM) from a static module into a dynamic, \"fast-weight\" episodic memory. Unlike PKM, FwPKM updates its parameters dynamically at both training and inference time via local chunk-level gradient descent, allowing the model to rapidly memorize and retrieve new key-value pairs from input sequences. Experiments reveal that FwPKM functions as an effective episodic memory that complements the semantic memory of standard modules, yielding significant perplexity reductions on long-context datasets. Notably, in Needle in a Haystack evaluations, FwPKM generalizes to 128K-token contexts despite being trained on only 4K-token sequences.",
      "upvotes": 0,
      "discussionId": "695b23dc832867f253525d79",
      "ai_summary": "FwPKM introduces a dynamic, fast-weight episodic memory mechanism for sequence modeling that balances storage capacity and efficiency, achieving strong performance on long-context tasks like Needle in a Haystack evaluations.",
      "ai_keywords": [
        "Softmax attention",
        "Product Key Memory (PKM)",
        "Fast-weight Product Key Memory (FwPKM)",
        "fast-weight",
        "local chunk-level gradient descent",
        "episodic memory",
        "Needle in a Haystack evaluations",
        "perplexity"
      ],
      "organization": {
        "_id": "65dfe46e4de6f5d5664ef3af",
        "name": "SakanaAI",
        "fullname": "Sakana AI",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/644b983f0fbe4830f192c4f5/7bSM-11NGnrhHd15hN4W_.jpeg"
      }
    },
    "publishedAt": "2026-01-02T07:37:53.000Z",
    "title": "Fast-weight Product Key Memory",
    "summary": "Sequence modeling layers in modern language models typically face a trade-off between storage capacity and computational efficiency. While Softmax attention offers unbounded storage at prohibitive quadratic costs, linear variants provide efficiency but suffer from limited, fixed-size storage. We propose Fast-weight Product Key Memory (FwPKM), a novel architecture that resolves this tension by transforming the sparse Product Key Memory (PKM) from a static module into a dynamic, \"fast-weight\" episodic memory. Unlike PKM, FwPKM updates its parameters dynamically at both training and inference time via local chunk-level gradient descent, allowing the model to rapidly memorize and retrieve new key-value pairs from input sequences. Experiments reveal that FwPKM functions as an effective episodic memory that complements the semantic memory of standard modules, yielding significant perplexity reductions on long-context datasets. Notably, in Needle in a Haystack evaluations, FwPKM generalizes to 128K-token contexts despite being trained on only 4K-token sequences.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.00671.png",
    "numComments": 0,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 197
    },
    "organization": {
      "_id": "65dfe46e4de6f5d5664ef3af",
      "name": "SakanaAI",
      "fullname": "Sakana AI",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/644b983f0fbe4830f192c4f5/7bSM-11NGnrhHd15hN4W_.jpeg"
    },
    "isAuthorParticipating": false
  }
]