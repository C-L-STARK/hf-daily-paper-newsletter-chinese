[
  {
    "paper": {
      "id": "2510.19363",
      "authors": [
        {
          "_id": "68f9884db9b2e4ae0467374e",
          "name": "Siyuan Wang",
          "hidden": false
        },
        {
          "_id": "68f9884db9b2e4ae0467374f",
          "name": "Gaokai Zhang",
          "hidden": false
        },
        {
          "_id": "68f9884db9b2e4ae04673750",
          "name": "Li Lyna Zhang",
          "hidden": false
        },
        {
          "_id": "68f9884db9b2e4ae04673751",
          "name": "Ning Shang",
          "hidden": false
        },
        {
          "_id": "68f9884db9b2e4ae04673752",
          "name": "Fan Yang",
          "hidden": false
        },
        {
          "_id": "68f9884db9b2e4ae04673753",
          "name": "Dongyao Chen",
          "hidden": false
        },
        {
          "_id": "68f9884db9b2e4ae04673754",
          "name": "Mao Yang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-22T08:35:28.000Z",
      "submittedOnDailyAt": "2025-10-23T00:18:13.806Z",
      "title": "LoongRL:Reinforcement Learning for Advanced Reasoning over Long Contexts",
      "submittedOnDailyBy": {
        "_id": "62b0009c72043b05d29492b2",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62b0009c72043b05d29492b2/NqRkX2YLhlfOLvYysa7dD.png",
        "isPro": false,
        "fullname": "Li Lyna Zhang",
        "user": "lynazhang",
        "type": "user"
      },
      "summary": "Reasoning over long contexts is essential for large language models. While\nreinforcement learning (RL) enhances short-context reasoning by inducing \"Aha\"\nmoments in chain-of-thought, the advanced thinking patterns required for\nlong-context reasoning remain largely unexplored, and high-difficulty RL data\nare scarce. In this paper, we introduce LoongRL, a data-driven RL method for\nadvanced long-context reasoning. Central to LoongRL is KeyChain, a synthesis\napproach that transforms short multi-hop QA into high-difficulty long-context\ntasks by inserting UUID chains that hide the true question among large\ncollections of distracting documents. Solving these tasks requires the model to\ntrace the correct chain step-by-step, identify the true question, retrieve\nrelevant facts and reason over them to answer correctly. RL training on\nKeyChain data induces an emergent plan-retrieve-reason-recheck reasoning\npattern that generalizes far beyond training length. Models trained at 16K\neffectively solve 128K tasks without prohibitive full-length RL rollout costs.\nOn Qwen2.5-7B and 14B, LoongRL substantially improves long-context multi-hop QA\naccuracy by +23.5% and +21.1% absolute gains. The resulting LoongRL-14B reaches\na score of 74.2, rivaling much larger frontier models such as o3-mini (74.5)\nand DeepSeek-R1 (74.9). It also improves long-context retrieval, passes all\n128K needle-in-a-haystack stress tests, and preserves short-context reasoning\ncapabilities.",
      "upvotes": 12,
      "discussionId": "68f9884db9b2e4ae04673755",
      "ai_summary": "LoongRL, a data-driven reinforcement learning method, enhances long-context reasoning by transforming short multi-hop QA into high-difficulty tasks, improving accuracy and generalization in large language models.",
      "ai_keywords": [
        "reinforcement learning",
        "LoongRL",
        "KeyChain",
        "multi-hop QA",
        "long-context reasoning",
        "plan-retrieve-reason-recheck",
        "RL rollout",
        "Qwen2.5-7B",
        "Qwen2.5-14B",
        "o3-mini",
        "DeepSeek-R1",
        "needle-in-a-haystack stress tests"
      ],
      "organization": {
        "_id": "68151d0f51add3813f3f7d1b",
        "name": "MicrosoftResearch",
        "fullname": "Microsoft Research",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6529a4f2f1205983224fa513/PeuVr7jSuJflmDBBGxoDX.png"
      }
    },
    "publishedAt": "2025-10-22T04:35:28.000Z",
    "title": "LoongRL:Reinforcement Learning for Advanced Reasoning over Long Contexts",
    "summary": "Reasoning over long contexts is essential for large language models. While\nreinforcement learning (RL) enhances short-context reasoning by inducing \"Aha\"\nmoments in chain-of-thought, the advanced thinking patterns required for\nlong-context reasoning remain largely unexplored, and high-difficulty RL data\nare scarce. In this paper, we introduce LoongRL, a data-driven RL method for\nadvanced long-context reasoning. Central to LoongRL is KeyChain, a synthesis\napproach that transforms short multi-hop QA into high-difficulty long-context\ntasks by inserting UUID chains that hide the true question among large\ncollections of distracting documents. Solving these tasks requires the model to\ntrace the correct chain step-by-step, identify the true question, retrieve\nrelevant facts and reason over them to answer correctly. RL training on\nKeyChain data induces an emergent plan-retrieve-reason-recheck reasoning\npattern that generalizes far beyond training length. Models trained at 16K\neffectively solve 128K tasks without prohibitive full-length RL rollout costs.\nOn Qwen2.5-7B and 14B, LoongRL substantially improves long-context multi-hop QA\naccuracy by +23.5% and +21.1% absolute gains. The resulting LoongRL-14B reaches\na score of 74.2, rivaling much larger frontier models such as o3-mini (74.5)\nand DeepSeek-R1 (74.9). It also improves long-context retrieval, passes all\n128K needle-in-a-haystack stress tests, and preserves short-context reasoning\ncapabilities.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.19363.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62b0009c72043b05d29492b2",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62b0009c72043b05d29492b2/NqRkX2YLhlfOLvYysa7dD.png",
      "fullname": "Li Lyna Zhang",
      "name": "lynazhang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 35
    },
    "organization": {
      "_id": "68151d0f51add3813f3f7d1b",
      "name": "MicrosoftResearch",
      "fullname": "Microsoft Research",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6529a4f2f1205983224fa513/PeuVr7jSuJflmDBBGxoDX.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.19336",
      "authors": [
        {
          "_id": "68f98a97b9b2e4ae0467377e",
          "name": "Kai Shi",
          "hidden": false
        },
        {
          "_id": "68f98a97b9b2e4ae0467377f",
          "name": "Jun Yang",
          "hidden": false
        },
        {
          "_id": "68f98a97b9b2e4ae04673780",
          "name": "Ni Yang",
          "hidden": false
        },
        {
          "_id": "68f98a97b9b2e4ae04673781",
          "name": "Binqiang Pan",
          "hidden": false
        },
        {
          "_id": "68f98a97b9b2e4ae04673782",
          "name": "Qingsong Xie",
          "hidden": false
        },
        {
          "_id": "68f98a97b9b2e4ae04673783",
          "name": "Chao Zhang",
          "hidden": false
        },
        {
          "_id": "68f98a97b9b2e4ae04673784",
          "name": "Zhenyu Yang",
          "hidden": false
        },
        {
          "_id": "68f98a97b9b2e4ae04673785",
          "name": "Tianhuang Su",
          "hidden": false
        },
        {
          "_id": "68f98a97b9b2e4ae04673786",
          "name": "Haonan Lu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-22T07:57:59.000Z",
      "submittedOnDailyAt": "2025-10-23T00:27:06.757Z",
      "title": "DaMo: Data Mixing Optimizer in Fine-tuning Multimodal LLMs for Mobile\n  Phone Agents",
      "submittedOnDailyBy": {
        "_id": "66e24afddce93c7249b418c0",
        "avatarUrl": "/avatars/14fd5ee60999a7a34a45c8291b2116c6.svg",
        "isPro": false,
        "fullname": "OPPO AI Center",
        "user": "AIGCer-OPPO",
        "type": "user"
      },
      "summary": "Mobile Phone Agents (MPAs) have emerged as a promising research direction due\nto their broad applicability across diverse scenarios. While Multimodal Large\nLanguage Models (MLLMs) serve as the foundation for MPAs, their effectiveness\nin handling multiple mobile phone tasks simultaneously remains limited.\nAlthough multitask supervised fine-tuning (SFT) is widely adopted for multitask\nlearning, existing approaches struggle to determine optimal training data\ncompositions for peak performance. To address this challenge, we propose DaMo\n(Data Mixture Optimizer) - a novel solution employing a trainable network that\npredicts optimal data mixtures by forecasting downstream task performance for\nany given dataset ratio. To support comprehensive evaluation, we introduce\nPhoneAgentBench, the first specialized benchmark to evaluate MLLMs on\nmultimodal mobile phone tasks, comprising 1235 QA pairs spanning diverse\nreal-world industrial mobile application scenarios. Demonstrating strong\npredictive capability (R^2=0.81) in small-scale pilot experiments, DaMo\nefficiently extrapolates optimal data mixing configurations. Our results show\nDaMo achieves a 3.38% performance improvement on PhoneAgentBench compared to\nalternative methods. Furthermore, extensive experiments across established\nbenchmarks including BFCL-v3, MME-Reasoning, MME-Perception, and OCRBench\nreveal DaMo's superior generalization, outperforming other approaches by 2.57%\nin terms of average score. When used solely for MLLM optimization on the\nBFCL-v3 task, DaMo improves the metrics by 12.47% than other methods. Notably,\nDaMo maintains robust scalability, preserving its effectiveness when applied to\nother model architectures. The code and dataset are available at\nhttps://github.com/OPPO-Mente-Lab/DaMo.git",
      "upvotes": 5,
      "discussionId": "68f98a97b9b2e4ae04673787",
      "ai_summary": "DaMo, a trainable network optimizing data mixtures for Multimodal Large Language Models, enhances performance across various mobile phone tasks and benchmarks.",
      "ai_keywords": [
        "Multimodal Large Language Models",
        "multitask supervised fine-tuning",
        "DaMo",
        "Data Mixture Optimizer",
        "PhoneAgentBench",
        "BFCL-v3",
        "MME-Reasoning",
        "MME-Perception",
        "OCRBench"
      ],
      "organization": {
        "_id": "67177eecd0fad5b4ccc09461",
        "name": "OPPOer",
        "fullname": "OPPO",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66e24afddce93c7249b418c0/gQ-XFJehEyAH12zhbeR8Z.png"
      }
    },
    "publishedAt": "2025-10-22T03:57:59.000Z",
    "title": "DaMo: Data Mixing Optimizer in Fine-tuning Multimodal LLMs for Mobile\n  Phone Agents",
    "summary": "Mobile Phone Agents (MPAs) have emerged as a promising research direction due\nto their broad applicability across diverse scenarios. While Multimodal Large\nLanguage Models (MLLMs) serve as the foundation for MPAs, their effectiveness\nin handling multiple mobile phone tasks simultaneously remains limited.\nAlthough multitask supervised fine-tuning (SFT) is widely adopted for multitask\nlearning, existing approaches struggle to determine optimal training data\ncompositions for peak performance. To address this challenge, we propose DaMo\n(Data Mixture Optimizer) - a novel solution employing a trainable network that\npredicts optimal data mixtures by forecasting downstream task performance for\nany given dataset ratio. To support comprehensive evaluation, we introduce\nPhoneAgentBench, the first specialized benchmark to evaluate MLLMs on\nmultimodal mobile phone tasks, comprising 1235 QA pairs spanning diverse\nreal-world industrial mobile application scenarios. Demonstrating strong\npredictive capability (R^2=0.81) in small-scale pilot experiments, DaMo\nefficiently extrapolates optimal data mixing configurations. Our results show\nDaMo achieves a 3.38% performance improvement on PhoneAgentBench compared to\nalternative methods. Furthermore, extensive experiments across established\nbenchmarks including BFCL-v3, MME-Reasoning, MME-Perception, and OCRBench\nreveal DaMo's superior generalization, outperforming other approaches by 2.57%\nin terms of average score. When used solely for MLLM optimization on the\nBFCL-v3 task, DaMo improves the metrics by 12.47% than other methods. Notably,\nDaMo maintains robust scalability, preserving its effectiveness when applied to\nother model architectures. The code and dataset are available at\nhttps://github.com/OPPO-Mente-Lab/DaMo.git",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.19336.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66e24afddce93c7249b418c0",
      "avatarUrl": "/avatars/14fd5ee60999a7a34a45c8291b2116c6.svg",
      "fullname": "OPPO AI Center",
      "name": "AIGCer-OPPO",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 9
    },
    "organization": {
      "_id": "67177eecd0fad5b4ccc09461",
      "name": "OPPOer",
      "fullname": "OPPO",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66e24afddce93c7249b418c0/gQ-XFJehEyAH12zhbeR8Z.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.19817",
      "authors": [
        {
          "_id": "68f98861b9b2e4ae04673757",
          "name": "Jake Poznanski",
          "hidden": false
        },
        {
          "_id": "68f98861b9b2e4ae04673758",
          "name": "Luca Soldaini",
          "hidden": false
        },
        {
          "_id": "68f98861b9b2e4ae04673759",
          "name": "Kyle Lo",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-22T17:53:02.000Z",
      "submittedOnDailyAt": "2025-10-23T00:14:13.036Z",
      "title": "olmOCR 2: Unit Test Rewards for Document OCR",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "We present olmOCR 2, the latest in our family of powerful OCR systems for\nconverting digitized print documents, like PDFs, into clean, naturally ordered\nplain text. olmOCR 2 is powered by olmOCR-2-7B-1025, a specialized, 7B vision\nlanguage model (VLM) trained using reinforcement learning with verifiable\nrewards (RLVR), where our rewards are a diverse set of binary unit tests. To\nscale unit test creation, we develop a pipeline for generating synthetic\ndocuments with diverse and challenging layouts, known ground-truth HTML source\ncode, and extracted test cases. We show that RL training on these test cases\nresults in state-of-the-art performance on olmOCR-Bench, our English-language\nOCR benchmark, with the largest improvements in math formula conversion, table\nparsing, and multi-column layouts compared to previous versions. We release our\nmodel, data and code under permissive open licenses.",
      "upvotes": 2,
      "discussionId": "68f98861b9b2e4ae0467375a",
      "projectPage": "https://olmocr.allen.ai/",
      "ai_summary": "olmOCR 2, a vision language model trained with reinforcement learning and verifiable rewards, achieves state-of-the-art performance in OCR tasks, particularly in math formula conversion, table parsing, and multi-column layouts.",
      "ai_keywords": [
        "vision language model",
        "reinforcement learning",
        "verifiable rewards",
        "synthetic documents",
        "ground-truth HTML",
        "test cases",
        "olmOCR-Bench",
        "math formula conversion",
        "table parsing",
        "multi-column layouts"
      ],
      "organization": {
        "_id": "5e70f3648ce3c604d78fe132",
        "name": "allenai",
        "fullname": "Ai2",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/652db071b62cf1f8463221e2/CxxwFiaomTa1MCX_B7-pT.png"
      }
    },
    "publishedAt": "2025-10-22T13:53:02.000Z",
    "title": "olmOCR 2: Unit Test Rewards for Document OCR",
    "summary": "We present olmOCR 2, the latest in our family of powerful OCR systems for\nconverting digitized print documents, like PDFs, into clean, naturally ordered\nplain text. olmOCR 2 is powered by olmOCR-2-7B-1025, a specialized, 7B vision\nlanguage model (VLM) trained using reinforcement learning with verifiable\nrewards (RLVR), where our rewards are a diverse set of binary unit tests. To\nscale unit test creation, we develop a pipeline for generating synthetic\ndocuments with diverse and challenging layouts, known ground-truth HTML source\ncode, and extracted test cases. We show that RL training on these test cases\nresults in state-of-the-art performance on olmOCR-Bench, our English-language\nOCR benchmark, with the largest improvements in math formula conversion, table\nparsing, and multi-column layouts compared to previous versions. We release our\nmodel, data and code under permissive open licenses.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.19817.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 139
    },
    "organization": {
      "_id": "5e70f3648ce3c604d78fe132",
      "name": "allenai",
      "fullname": "Ai2",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/652db071b62cf1f8463221e2/CxxwFiaomTa1MCX_B7-pT.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.19457",
      "authors": [
        {
          "_id": "68f98c94b9b2e4ae046737a2",
          "name": "Kailin Jiang",
          "hidden": false
        },
        {
          "_id": "68f98c94b9b2e4ae046737a3",
          "name": "Ning Jiang",
          "hidden": false
        },
        {
          "_id": "68f98c94b9b2e4ae046737a4",
          "name": "Yuchen Ren",
          "hidden": false
        },
        {
          "_id": "68f98c94b9b2e4ae046737a5",
          "name": "Yuchen Li",
          "hidden": false
        },
        {
          "_id": "68f98c94b9b2e4ae046737a6",
          "name": "Yifan Gao",
          "hidden": false
        },
        {
          "_id": "68f98c94b9b2e4ae046737a7",
          "name": "Jinhe Bi",
          "hidden": false
        },
        {
          "_id": "68f98c94b9b2e4ae046737a8",
          "name": "Yunpu Ma",
          "hidden": false
        },
        {
          "_id": "68f98c94b9b2e4ae046737a9",
          "name": "Qingqing Liu",
          "hidden": false
        },
        {
          "_id": "68f98c94b9b2e4ae046737aa",
          "name": "Xianhao Wang",
          "hidden": false
        },
        {
          "_id": "68f98c94b9b2e4ae046737ab",
          "name": "Yifan Jia",
          "hidden": false
        },
        {
          "_id": "68f98c94b9b2e4ae046737ac",
          "name": "Hongbo Jiang",
          "hidden": false
        },
        {
          "_id": "68f98c94b9b2e4ae046737ad",
          "name": "Yaocong Hu",
          "hidden": false
        },
        {
          "_id": "68f98c94b9b2e4ae046737ae",
          "name": "Bin Li",
          "hidden": false
        },
        {
          "_id": "68f98c94b9b2e4ae046737af",
          "name": "Lei Liu",
          "hidden": false
        },
        {
          "_id": "68f98c94b9b2e4ae046737b0",
          "name": "Yuntao Du",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-22T10:41:57.000Z",
      "submittedOnDailyAt": "2025-10-23T00:36:11.231Z",
      "title": "MINED: Probing and Updating with Multimodal Time-Sensitive Knowledge for\n  Large Multimodal Models",
      "submittedOnDailyBy": {
        "_id": "65745569839aa08899ea5d27",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/4X8waDwiphbfKZySrYlFy.jpeg",
        "isPro": false,
        "fullname": "kailinjiang",
        "user": "kailinjiang",
        "type": "user"
      },
      "summary": "Large Multimodal Models (LMMs) encode rich factual knowledge via cross-modal\npre-training, yet their static representations struggle to maintain an accurate\nunderstanding of time-sensitive factual knowledge. Existing benchmarks remain\nconstrained by static designs, inadequately evaluating LMMs' ability to\nunderstand time-sensitive knowledge. To address this gap, we propose MINED, a\ncomprehensive benchmark that evaluates temporal awareness along 6 key\ndimensions and 11 challenging tasks: cognition, awareness, trustworthiness,\nunderstanding, reasoning, and robustness. MINED is constructed from Wikipedia\nby two professional annotators, containing 2,104 time-sensitive knowledge\nsamples spanning six knowledge types. Evaluating 15 widely used LMMs on MINED\nshows that Gemini-2.5-Pro achieves the highest average CEM score of 63.07,\nwhile most open-source LMMs still lack time understanding ability. Meanwhile,\nLMMs perform best on organization knowledge, whereas their performance is\nweakest on sport. To address these challenges, we investigate the feasibility\nof updating time-sensitive knowledge in LMMs through knowledge editing methods\nand observe that LMMs can effectively update knowledge via knowledge editing\nmethods in single editing scenarios.",
      "upvotes": 2,
      "discussionId": "68f98c94b9b2e4ae046737b1",
      "projectPage": "https://mined-lmm.github.io/"
    },
    "publishedAt": "2025-10-22T06:41:57.000Z",
    "title": "MINED: Probing and Updating with Multimodal Time-Sensitive Knowledge for\n  Large Multimodal Models",
    "summary": "Large Multimodal Models (LMMs) encode rich factual knowledge via cross-modal\npre-training, yet their static representations struggle to maintain an accurate\nunderstanding of time-sensitive factual knowledge. Existing benchmarks remain\nconstrained by static designs, inadequately evaluating LMMs' ability to\nunderstand time-sensitive knowledge. To address this gap, we propose MINED, a\ncomprehensive benchmark that evaluates temporal awareness along 6 key\ndimensions and 11 challenging tasks: cognition, awareness, trustworthiness,\nunderstanding, reasoning, and robustness. MINED is constructed from Wikipedia\nby two professional annotators, containing 2,104 time-sensitive knowledge\nsamples spanning six knowledge types. Evaluating 15 widely used LMMs on MINED\nshows that Gemini-2.5-Pro achieves the highest average CEM score of 63.07,\nwhile most open-source LMMs still lack time understanding ability. Meanwhile,\nLMMs perform best on organization knowledge, whereas their performance is\nweakest on sport. To address these challenges, we investigate the feasibility\nof updating time-sensitive knowledge in LMMs through knowledge editing methods\nand observe that LMMs can effectively update knowledge via knowledge editing\nmethods in single editing scenarios.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.19457.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65745569839aa08899ea5d27",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/4X8waDwiphbfKZySrYlFy.jpeg",
      "fullname": "kailinjiang",
      "name": "kailinjiang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.19386",
      "authors": [
        {
          "_id": "68f98cceb9b2e4ae046737c3",
          "name": "Ning Li",
          "hidden": false
        },
        {
          "_id": "68f98cceb9b2e4ae046737c4",
          "name": "Qiqiang Lin",
          "hidden": false
        },
        {
          "_id": "68f98cceb9b2e4ae046737c5",
          "name": "Zheng Wu",
          "hidden": false
        },
        {
          "_id": "68f98cceb9b2e4ae046737c6",
          "name": "Xiaoyun Mo",
          "hidden": false
        },
        {
          "_id": "68f98cceb9b2e4ae046737c7",
          "name": "Weiming Zhang",
          "hidden": false
        },
        {
          "_id": "68f98cceb9b2e4ae046737c8",
          "name": "Yin Zhao",
          "hidden": false
        },
        {
          "_id": "68f98cceb9b2e4ae046737c9",
          "name": "Xiangmou Qu",
          "hidden": false
        },
        {
          "_id": "68f98cceb9b2e4ae046737ca",
          "name": "Jiamu Zhou",
          "hidden": false
        },
        {
          "_id": "68f98cceb9b2e4ae046737cb",
          "name": "Jun Wang",
          "hidden": false
        },
        {
          "_id": "68f98cceb9b2e4ae046737cc",
          "name": "Congmin Zheng",
          "hidden": false
        },
        {
          "_id": "68f98cceb9b2e4ae046737cd",
          "name": "Yuanyi Song",
          "hidden": false
        },
        {
          "_id": "68f98cceb9b2e4ae046737ce",
          "name": "Hongjiang Chen",
          "hidden": false
        },
        {
          "_id": "68f98cceb9b2e4ae046737cf",
          "name": "Heyuan Huang",
          "hidden": false
        },
        {
          "_id": "68f98cceb9b2e4ae046737d0",
          "name": "Jihong Wang",
          "hidden": false
        },
        {
          "_id": "68f98cceb9b2e4ae046737d1",
          "name": "Jiaxin Yin",
          "hidden": false
        },
        {
          "_id": "68f98cceb9b2e4ae046737d2",
          "name": "Jingwei Yu",
          "hidden": false
        },
        {
          "_id": "68f98cceb9b2e4ae046737d3",
          "name": "Junwei Liao",
          "hidden": false
        },
        {
          "_id": "68f98cceb9b2e4ae046737d4",
          "name": "Qiuying Peng",
          "hidden": false
        },
        {
          "_id": "68f98cceb9b2e4ae046737d5",
          "name": "Xingyu Lou",
          "hidden": false
        },
        {
          "_id": "68f98cceb9b2e4ae046737d6",
          "name": "Jun Wang",
          "hidden": false
        },
        {
          "_id": "68f98cceb9b2e4ae046737d7",
          "name": "Weiwen Liu",
          "hidden": false
        },
        {
          "_id": "68f98cceb9b2e4ae046737d8",
          "name": "Zhuosheng Zhang",
          "hidden": false
        },
        {
          "_id": "68f98cceb9b2e4ae046737d9",
          "name": "Weinan Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-22T09:02:48.000Z",
      "submittedOnDailyAt": "2025-10-23T00:33:11.605Z",
      "title": "ColorAgent: Building A Robust, Personalized, and Interactive OS Agent",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "With the advancements in hardware, software, and large language model\ntechnologies, the interaction between humans and operating systems has evolved\nfrom the command-line interface to the rapidly emerging AI agent interactions.\nBuilding an operating system (OS) agent capable of executing user instructions\nand faithfully following user desires is becoming a reality. In this technical\nreport, we present ColorAgent, an OS agent designed to engage in long-horizon,\nrobust interactions with the environment while also enabling personalized and\nproactive user interaction. To enable long-horizon interactions with the\nenvironment, we enhance the model's capabilities through step-wise\nreinforcement learning and self-evolving training, while also developing a\ntailored multi-agent framework that ensures generality, consistency, and\nrobustness. In terms of user interaction, we explore personalized user intent\nrecognition and proactive engagement, positioning the OS agent not merely as an\nautomation tool but as a warm, collaborative partner. We evaluate ColorAgent on\nthe AndroidWorld and AndroidLab benchmarks, achieving success rates of 77.2%\nand 50.7%, respectively, establishing a new state of the art. Nonetheless, we\nnote that current benchmarks are insufficient for a comprehensive evaluation of\nOS agents and propose further exploring directions in future work, particularly\nin the areas of evaluation paradigms, agent collaboration, and security. Our\ncode is available at https://github.com/MadeAgents/mobile-use.",
      "upvotes": 2,
      "discussionId": "68f98cceb9b2e4ae046737da",
      "githubRepo": "https://github.com/MadeAgents/mobile-use",
      "ai_summary": "ColorAgent, an OS agent using step-wise reinforcement learning and a multi-agent framework, achieves high success rates in long-horizon interactions and personalized user engagement on Android benchmarks.",
      "ai_keywords": [
        "step-wise reinforcement learning",
        "self-evolving training",
        "multi-agent framework",
        "personalized user intent recognition",
        "proactive engagement",
        "AndroidWorld",
        "AndroidLab"
      ],
      "githubStars": 83
    },
    "publishedAt": "2025-10-22T05:02:48.000Z",
    "title": "ColorAgent: Building A Robust, Personalized, and Interactive OS Agent",
    "summary": "With the advancements in hardware, software, and large language model\ntechnologies, the interaction between humans and operating systems has evolved\nfrom the command-line interface to the rapidly emerging AI agent interactions.\nBuilding an operating system (OS) agent capable of executing user instructions\nand faithfully following user desires is becoming a reality. In this technical\nreport, we present ColorAgent, an OS agent designed to engage in long-horizon,\nrobust interactions with the environment while also enabling personalized and\nproactive user interaction. To enable long-horizon interactions with the\nenvironment, we enhance the model's capabilities through step-wise\nreinforcement learning and self-evolving training, while also developing a\ntailored multi-agent framework that ensures generality, consistency, and\nrobustness. In terms of user interaction, we explore personalized user intent\nrecognition and proactive engagement, positioning the OS agent not merely as an\nautomation tool but as a warm, collaborative partner. We evaluate ColorAgent on\nthe AndroidWorld and AndroidLab benchmarks, achieving success rates of 77.2%\nand 50.7%, respectively, establishing a new state of the art. Nonetheless, we\nnote that current benchmarks are insufficient for a comprehensive evaluation of\nOS agents and propose further exploring directions in future work, particularly\nin the areas of evaluation paradigms, agent collaboration, and security. Our\ncode is available at https://github.com/MadeAgents/mobile-use.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.19386.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 139
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.19338",
      "authors": [
        {
          "_id": "68f98d4fb9b2e4ae046737dc",
          "name": "Ling Team",
          "hidden": false
        },
        {
          "_id": "68f98d4fb9b2e4ae046737dd",
          "name": "Bin Han",
          "hidden": false
        },
        {
          "_id": "68f98d4fb9b2e4ae046737de",
          "name": "Caizhi Tang",
          "hidden": false
        },
        {
          "_id": "68f98d4fb9b2e4ae046737df",
          "name": "Chen Liang",
          "hidden": false
        },
        {
          "_id": "68f98d4fb9b2e4ae046737e0",
          "name": "Donghao Zhang",
          "hidden": false
        },
        {
          "_id": "68f98d4fb9b2e4ae046737e1",
          "name": "Fan Yuan",
          "hidden": false
        },
        {
          "_id": "68f98d4fb9b2e4ae046737e2",
          "name": "Feng Zhu",
          "hidden": false
        },
        {
          "_id": "68f98d4fb9b2e4ae046737e3",
          "name": "Jie Gao",
          "hidden": false
        },
        {
          "_id": "68f98d4fb9b2e4ae046737e4",
          "name": "Jingyu Hu",
          "hidden": false
        },
        {
          "_id": "68f98d4fb9b2e4ae046737e5",
          "name": "Longfei Li",
          "hidden": false
        },
        {
          "_id": "68f98d4fb9b2e4ae046737e6",
          "name": "Meng Li",
          "hidden": false
        },
        {
          "_id": "68f98d4fb9b2e4ae046737e7",
          "name": "Mingyang Zhang",
          "hidden": false
        },
        {
          "_id": "68f98d4fb9b2e4ae046737e8",
          "name": "Peijie Jiang",
          "hidden": false
        },
        {
          "_id": "68f98d4fb9b2e4ae046737e9",
          "name": "Peng Jiao",
          "hidden": false
        },
        {
          "_id": "68f98d4fb9b2e4ae046737ea",
          "name": "Qian Zhao",
          "hidden": false
        },
        {
          "_id": "68f98d4fb9b2e4ae046737eb",
          "name": "Qingyuan Yang",
          "hidden": false
        },
        {
          "_id": "68f98d4fb9b2e4ae046737ec",
          "name": "Wenbo Shen",
          "hidden": false
        },
        {
          "_id": "68f98d4fb9b2e4ae046737ed",
          "name": "Xinxing Yang",
          "hidden": false
        },
        {
          "_id": "68f98d4fb9b2e4ae046737ee",
          "name": "Yalin Zhang",
          "hidden": false
        },
        {
          "_id": "68f98d4fb9b2e4ae046737ef",
          "name": "Yankun Ren",
          "hidden": false
        },
        {
          "_id": "68f98d4fb9b2e4ae046737f0",
          "name": "Yao Zhao",
          "hidden": false
        },
        {
          "_id": "68f98d4fb9b2e4ae046737f1",
          "name": "Yibo Cao",
          "hidden": false
        },
        {
          "_id": "68f98d4fb9b2e4ae046737f2",
          "name": "Yixuan Sun",
          "hidden": false
        },
        {
          "_id": "68f98d4fb9b2e4ae046737f3",
          "name": "Yue Zhang",
          "hidden": false
        },
        {
          "_id": "68f98d4fb9b2e4ae046737f4",
          "name": "Yuchen Fang",
          "hidden": false
        },
        {
          "_id": "68f98d4fb9b2e4ae046737f5",
          "name": "Zibin Lin",
          "hidden": false
        },
        {
          "_id": "68f98d4fb9b2e4ae046737f6",
          "name": "Zixuan Cheng",
          "hidden": false
        },
        {
          "_id": "68f98d4fb9b2e4ae046737f7",
          "name": "Jun Zhou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-22T07:59:38.000Z",
      "submittedOnDailyAt": "2025-10-23T00:35:16.243Z",
      "title": "Every Attention Matters: An Efficient Hybrid Architecture for\n  Long-Context Reasoning",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "In this technical report, we present the Ring-linear model series,\nspecifically including Ring-mini-linear-2.0 and Ring-flash-linear-2.0.\nRing-mini-linear-2.0 comprises 16B parameters and 957M activations, while\nRing-flash-linear-2.0 contains 104B parameters and 6.1B activations. Both\nmodels adopt a hybrid architecture that effectively integrates linear attention\nand softmax attention, significantly reducing I/O and computational overhead in\nlong-context inference scenarios. Compared to a 32 billion parameter dense\nmodel, this series reduces inference cost to 1/10, and compared to the original\nRing series, the cost is also reduced by over 50%. Furthermore, through\nsystematic exploration of the ratio between different attention mechanisms in\nthe hybrid architecture, we have identified the currently optimal model\nstructure. Additionally, by leveraging our self-developed high-performance FP8\noperator library-linghe, overall training efficiency has been improved by 50%.\nBenefiting from the high alignment between the training and inference engine\noperators, the models can undergo long-term, stable, and highly efficient\noptimization during the reinforcement learning phase, consistently maintaining\nSOTA performance across multiple challenging complex reasoning benchmarks.",
      "upvotes": 2,
      "discussionId": "68f98d50b9b2e4ae046737f8",
      "ai_summary": "The Ring-linear model series, including Ring-mini-linear-2.0 and Ring-flash-linear-2.0, uses a hybrid architecture combining linear and softmax attention to reduce inference costs and improve training efficiency.",
      "ai_keywords": [
        "linear attention",
        "softmax attention",
        "hybrid architecture",
        "FP8 operator library",
        "reinforcement learning",
        "SOTA performance"
      ],
      "organization": {
        "_id": "67c1d682826160b28f778510",
        "name": "antgroup",
        "fullname": "Ant Group",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/662e1f9da266499277937d33/7VcPHdLSGlged3ixK1dys.jpeg"
      }
    },
    "publishedAt": "2025-10-22T03:59:38.000Z",
    "title": "Every Attention Matters: An Efficient Hybrid Architecture for\n  Long-Context Reasoning",
    "summary": "In this technical report, we present the Ring-linear model series,\nspecifically including Ring-mini-linear-2.0 and Ring-flash-linear-2.0.\nRing-mini-linear-2.0 comprises 16B parameters and 957M activations, while\nRing-flash-linear-2.0 contains 104B parameters and 6.1B activations. Both\nmodels adopt a hybrid architecture that effectively integrates linear attention\nand softmax attention, significantly reducing I/O and computational overhead in\nlong-context inference scenarios. Compared to a 32 billion parameter dense\nmodel, this series reduces inference cost to 1/10, and compared to the original\nRing series, the cost is also reduced by over 50%. Furthermore, through\nsystematic exploration of the ratio between different attention mechanisms in\nthe hybrid architecture, we have identified the currently optimal model\nstructure. Additionally, by leveraging our self-developed high-performance FP8\noperator library-linghe, overall training efficiency has been improved by 50%.\nBenefiting from the high alignment between the training and inference engine\noperators, the models can undergo long-term, stable, and highly efficient\noptimization during the reinforcement learning phase, consistently maintaining\nSOTA performance across multiple challenging complex reasoning benchmarks.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.19338.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 139
    },
    "organization": {
      "_id": "67c1d682826160b28f778510",
      "name": "antgroup",
      "fullname": "Ant Group",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/662e1f9da266499277937d33/7VcPHdLSGlged3ixK1dys.jpeg"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.19316",
      "authors": [
        {
          "_id": "68f98c87b9b2e4ae04673796",
          "name": "Kailin Jiang",
          "hidden": false
        },
        {
          "_id": "68f98c87b9b2e4ae04673797",
          "name": "Hongbo Jiang",
          "hidden": false
        },
        {
          "_id": "68f98c87b9b2e4ae04673798",
          "name": "Ning Jiang",
          "hidden": false
        },
        {
          "_id": "68f98c87b9b2e4ae04673799",
          "name": "Zhi Gao",
          "hidden": false
        },
        {
          "_id": "68f98c87b9b2e4ae0467379a",
          "name": "Jinhe Bi",
          "hidden": false
        },
        {
          "_id": "68f98c87b9b2e4ae0467379b",
          "name": "Yuchen Ren",
          "hidden": false
        },
        {
          "_id": "68f98c87b9b2e4ae0467379c",
          "name": "Bin Li",
          "hidden": false
        },
        {
          "_id": "68f98c87b9b2e4ae0467379d",
          "name": "Yuntao Du",
          "hidden": false
        },
        {
          "_id": "68f98c87b9b2e4ae0467379e",
          "name": "Lei Liu",
          "hidden": false
        },
        {
          "_id": "68f98c87b9b2e4ae0467379f",
          "name": "Qing Li",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-22T07:26:55.000Z",
      "submittedOnDailyAt": "2025-10-23T00:34:23.319Z",
      "title": "KORE: Enhancing Knowledge Injection for Large Multimodal Models via\n  Knowledge-Oriented Augmentations and Constraints",
      "submittedOnDailyBy": {
        "_id": "65745569839aa08899ea5d27",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/4X8waDwiphbfKZySrYlFy.jpeg",
        "isPro": false,
        "fullname": "kailinjiang",
        "user": "kailinjiang",
        "type": "user"
      },
      "summary": "Large Multimodal Models encode extensive factual knowledge in their\npre-trained weights. However, its knowledge remains static and limited, unable\nto keep pace with real-world developments, which hinders continuous knowledge\nacquisition. Effective knowledge injection thus becomes critical, involving two\ngoals: knowledge adaptation (injecting new knowledge) and knowledge retention\n(preserving old knowledge). Existing methods often struggle to learn new\nknowledge and suffer from catastrophic forgetting. To address this, we propose\nKORE, a synergistic method of KnOwledge-oRientEd augmentations and constraints\nfor injecting new knowledge into large multimodal models while preserving old\nknowledge. Unlike general text or image data augmentation, KORE automatically\nconverts individual knowledge items into structured and comprehensive knowledge\nto ensure that the model accurately learns new knowledge, enabling accurate\nadaptation. Meanwhile, KORE stores previous knowledge in the covariance matrix\nof LMM's linear layer activations and initializes the adapter by projecting the\noriginal weights into the matrix's null space, defining a fine-tuning direction\nthat minimizes interference with previous knowledge, enabling powerful\nretention. Extensive experiments on various LMMs, including LLaVA-v1.5-7B,\nLLaVA-v1.5-13B, and Qwen2.5-VL-7B, show that KORE achieves superior new\nknowledge injection performance and effectively mitigates catastrophic\nforgetting.",
      "upvotes": 2,
      "discussionId": "68f98c87b9b2e4ae046737a0",
      "projectPage": "https://kore-lmm.github.io/",
      "ai_summary": "KORE is a method for injecting new knowledge into large multimodal models while preserving old knowledge, using structured augmentations and covariance matrix constraints to minimize catastrophic forgetting.",
      "ai_keywords": [
        "knowledge adaptation",
        "knowledge retention",
        "catastrophic forgetting",
        "KORE",
        "KnOwledge-oRientEd augmentations",
        "covariance matrix",
        "linear layer activations",
        "adapter",
        "fine-tuning direction",
        "LMMs",
        "LLaVA-v1.5-7B",
        "LLaVA-v1.5-13B",
        "Qwen2.5-VL-7B"
      ]
    },
    "publishedAt": "2025-10-22T03:26:55.000Z",
    "title": "KORE: Enhancing Knowledge Injection for Large Multimodal Models via\n  Knowledge-Oriented Augmentations and Constraints",
    "summary": "Large Multimodal Models encode extensive factual knowledge in their\npre-trained weights. However, its knowledge remains static and limited, unable\nto keep pace with real-world developments, which hinders continuous knowledge\nacquisition. Effective knowledge injection thus becomes critical, involving two\ngoals: knowledge adaptation (injecting new knowledge) and knowledge retention\n(preserving old knowledge). Existing methods often struggle to learn new\nknowledge and suffer from catastrophic forgetting. To address this, we propose\nKORE, a synergistic method of KnOwledge-oRientEd augmentations and constraints\nfor injecting new knowledge into large multimodal models while preserving old\nknowledge. Unlike general text or image data augmentation, KORE automatically\nconverts individual knowledge items into structured and comprehensive knowledge\nto ensure that the model accurately learns new knowledge, enabling accurate\nadaptation. Meanwhile, KORE stores previous knowledge in the covariance matrix\nof LMM's linear layer activations and initializes the adapter by projecting the\noriginal weights into the matrix's null space, defining a fine-tuning direction\nthat minimizes interference with previous knowledge, enabling powerful\nretention. Extensive experiments on various LMMs, including LLaVA-v1.5-7B,\nLLaVA-v1.5-13B, and Qwen2.5-VL-7B, show that KORE achieves superior new\nknowledge injection performance and effectively mitigates catastrophic\nforgetting.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.19316.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65745569839aa08899ea5d27",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/4X8waDwiphbfKZySrYlFy.jpeg",
      "fullname": "kailinjiang",
      "name": "kailinjiang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.18313",
      "authors": [
        {
          "_id": "68f98bbdb9b2e4ae04673789",
          "name": "Bohan Li",
          "hidden": false
        },
        {
          "_id": "68f98bbdb9b2e4ae0467378a",
          "name": "Zhuang Ma",
          "hidden": false
        },
        {
          "_id": "68f98bbdb9b2e4ae0467378b",
          "name": "Dalong Du",
          "hidden": false
        },
        {
          "_id": "68f98bbdb9b2e4ae0467378c",
          "name": "Baorui Peng",
          "hidden": false
        },
        {
          "_id": "68f98bbdb9b2e4ae0467378d",
          "name": "Zhujin Liang",
          "hidden": false
        },
        {
          "_id": "68f98bbdb9b2e4ae0467378e",
          "name": "Zhenqiang Liu",
          "hidden": false
        },
        {
          "_id": "68f98bbdb9b2e4ae0467378f",
          "name": "Chao Ma",
          "hidden": false
        },
        {
          "_id": "68f98bbdb9b2e4ae04673790",
          "name": "Yueming Jin",
          "hidden": false
        },
        {
          "_id": "68f98bbdb9b2e4ae04673791",
          "name": "Hao Zhao",
          "hidden": false
        },
        {
          "_id": "68f98bbdb9b2e4ae04673792",
          "name": "Wenjun Zeng",
          "hidden": false
        },
        {
          "_id": "68f98bbdb9b2e4ae04673793",
          "name": "Xin Jin",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/66480d4cea529a27ecaaee66/CFPOwVCZTaYYRz8UV-xHt.mp4",
        "https://cdn-uploads.huggingface.co/production/uploads/66480d4cea529a27ecaaee66/Br2oqSl1Zmq0Nr6uSbIpl.mp4",
        "https://cdn-uploads.huggingface.co/production/uploads/66480d4cea529a27ecaaee66/tbqN08_U18WLXJtPLQLOE.mp4",
        "https://cdn-uploads.huggingface.co/production/uploads/66480d4cea529a27ecaaee66/YpgOAUqkyroUMo796hCWX.mp4",
        "https://cdn-uploads.huggingface.co/production/uploads/66480d4cea529a27ecaaee66/3ftuJdOh4SK2bAfooEWIw.mp4"
      ],
      "publishedAt": "2025-10-21T05:49:01.000Z",
      "submittedOnDailyAt": "2025-10-23T00:33:35.049Z",
      "title": "OmniNWM: Omniscient Driving Navigation World Models",
      "submittedOnDailyBy": {
        "_id": "66480d4cea529a27ecaaee66",
        "avatarUrl": "/avatars/936f77f0c3c304e1d170d3a5d2737485.svg",
        "isPro": false,
        "fullname": "Bohan Li",
        "user": "Arlolo0",
        "type": "user"
      },
      "summary": "Autonomous driving world models are expected to work effectively across three\ncore dimensions: state, action, and reward. Existing models, however, are\ntypically restricted to limited state modalities, short video sequences,\nimprecise action control, and a lack of reward awareness. In this paper, we\nintroduce OmniNWM, an omniscient panoramic navigation world model that\naddresses all three dimensions within a unified framework. For state, OmniNWM\njointly generates panoramic videos of RGB, semantics, metric depth, and 3D\noccupancy. A flexible forcing strategy enables high-quality long-horizon\nauto-regressive generation. For action, we introduce a normalized panoramic\nPlucker ray-map representation that encodes input trajectories into pixel-level\nsignals, enabling highly precise and generalizable control over panoramic video\ngeneration. Regarding reward, we move beyond learning reward functions with\nexternal image-based models: instead, we leverage the generated 3D occupancy to\ndirectly define rule-based dense rewards for driving compliance and safety.\nExtensive experiments demonstrate that OmniNWM achieves state-of-the-art\nperformance in video generation, control accuracy, and long-horizon stability,\nwhile providing a reliable closed-loop evaluation framework through\noccupancy-grounded rewards. Project page is available at\nhttps://github.com/Arlo0o/OmniNWM.",
      "upvotes": 2,
      "discussionId": "68f98bbdb9b2e4ae04673794",
      "ai_summary": "OmniNWM is a unified world model for autonomous driving that generates panoramic videos, encodes actions using Plucker ray-maps, and defines dense rewards based on 3D occupancy, achieving top performance in video generation, control, and stability.",
      "ai_keywords": [
        "panoramic navigation world model",
        "OmniNWM",
        "panoramic videos",
        "RGB",
        "semantics",
        "metric depth",
        "3D occupancy",
        "auto-regressive generation",
        "normalized panoramic Plucker ray-map",
        "rule-based dense rewards",
        "driving compliance",
        "safety",
        "occupancy-grounded rewards"
      ]
    },
    "publishedAt": "2025-10-21T01:49:01.000Z",
    "title": "OmniNWM: Omniscient Driving Navigation World Models",
    "summary": "Autonomous driving world models are expected to work effectively across three\ncore dimensions: state, action, and reward. Existing models, however, are\ntypically restricted to limited state modalities, short video sequences,\nimprecise action control, and a lack of reward awareness. In this paper, we\nintroduce OmniNWM, an omniscient panoramic navigation world model that\naddresses all three dimensions within a unified framework. For state, OmniNWM\njointly generates panoramic videos of RGB, semantics, metric depth, and 3D\noccupancy. A flexible forcing strategy enables high-quality long-horizon\nauto-regressive generation. For action, we introduce a normalized panoramic\nPlucker ray-map representation that encodes input trajectories into pixel-level\nsignals, enabling highly precise and generalizable control over panoramic video\ngeneration. Regarding reward, we move beyond learning reward functions with\nexternal image-based models: instead, we leverage the generated 3D occupancy to\ndirectly define rule-based dense rewards for driving compliance and safety.\nExtensive experiments demonstrate that OmniNWM achieves state-of-the-art\nperformance in video generation, control accuracy, and long-horizon stability,\nwhile providing a reliable closed-loop evaluation framework through\noccupancy-grounded rewards. Project page is available at\nhttps://github.com/Arlo0o/OmniNWM.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/66480d4cea529a27ecaaee66/CFPOwVCZTaYYRz8UV-xHt.mp4",
      "https://cdn-uploads.huggingface.co/production/uploads/66480d4cea529a27ecaaee66/Br2oqSl1Zmq0Nr6uSbIpl.mp4",
      "https://cdn-uploads.huggingface.co/production/uploads/66480d4cea529a27ecaaee66/tbqN08_U18WLXJtPLQLOE.mp4",
      "https://cdn-uploads.huggingface.co/production/uploads/66480d4cea529a27ecaaee66/YpgOAUqkyroUMo796hCWX.mp4",
      "https://cdn-uploads.huggingface.co/production/uploads/66480d4cea529a27ecaaee66/3ftuJdOh4SK2bAfooEWIw.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.18313.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "66480d4cea529a27ecaaee66",
      "avatarUrl": "/avatars/936f77f0c3c304e1d170d3a5d2737485.svg",
      "fullname": "Bohan Li",
      "name": "Arlolo0",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.19808",
      "authors": [
        {
          "_id": "68f988deb9b2e4ae0467375c",
          "name": "Yusu Qian",
          "hidden": false
        },
        {
          "_id": "68f988deb9b2e4ae0467375d",
          "name": "Eli Bocek-Rivele",
          "hidden": false
        },
        {
          "_id": "68f988deb9b2e4ae0467375e",
          "name": "Liangchen Song",
          "hidden": false
        },
        {
          "_id": "68f988deb9b2e4ae0467375f",
          "name": "Jialing Tong",
          "hidden": false
        },
        {
          "_id": "68f988deb9b2e4ae04673760",
          "name": "Yinfei Yang",
          "hidden": false
        },
        {
          "_id": "68f988deb9b2e4ae04673761",
          "name": "Jiasen Lu",
          "hidden": false
        },
        {
          "_id": "68f988deb9b2e4ae04673762",
          "name": "Wenze Hu",
          "hidden": false
        },
        {
          "_id": "68f988deb9b2e4ae04673763",
          "name": "Zhe Gan",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-22T17:43:15.000Z",
      "submittedOnDailyAt": "2025-10-23T00:16:18.374Z",
      "title": "Pico-Banana-400K: A Large-Scale Dataset for Text-Guided Image Editing",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Recent advances in multimodal models have demonstrated remarkable text-guided\nimage editing capabilities, with systems like GPT-4o and Nano-Banana setting\nnew benchmarks. However, the research community's progress remains constrained\nby the absence of large-scale, high-quality, and openly accessible datasets\nbuilt from real images. We introduce Pico-Banana-400K, a comprehensive\n400K-image dataset for instruction-based image editing. Our dataset is\nconstructed by leveraging Nano-Banana to generate diverse edit pairs from real\nphotographs in the OpenImages collection. What distinguishes Pico-Banana-400K\nfrom previous synthetic datasets is our systematic approach to quality and\ndiversity. We employ a fine-grained image editing taxonomy to ensure\ncomprehensive coverage of edit types while maintaining precise content\npreservation and instruction faithfulness through MLLM-based quality scoring\nand careful curation. Beyond single turn editing, Pico-Banana-400K enables\nresearch into complex editing scenarios. The dataset includes three specialized\nsubsets: (1) a 72K-example multi-turn collection for studying sequential\nediting, reasoning, and planning across consecutive modifications; (2) a\n56K-example preference subset for alignment research and reward model training;\nand (3) paired long-short editing instructions for developing instruction\nrewriting and summarization capabilities. By providing this large-scale,\nhigh-quality, and task-rich resource, Pico-Banana-400K establishes a robust\nfoundation for training and benchmarking the next generation of text-guided\nimage editing models.",
      "upvotes": 1,
      "discussionId": "68f988deb9b2e4ae04673764",
      "ai_summary": "Pico-Banana-400K is a large-scale, high-quality dataset for instruction-based image editing, featuring diverse edit pairs, multi-turn editing, preference subsets, and long-short instruction pairs, enabling comprehensive research and benchmarking.",
      "ai_keywords": [
        "multimodal models",
        "text-guided image editing",
        "GPT-4o",
        "Nano-Banana",
        "OpenImages",
        "fine-grained image editing taxonomy",
        "MLLM-based quality scoring",
        "multi-turn editing",
        "preference subset",
        "instruction rewriting",
        "instruction summarization"
      ],
      "organization": {
        "_id": "628cbd99ef14f971b69948ab",
        "name": "apple",
        "fullname": "Apple",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1653390727490-5dd96eb166059660ed1ee413.jpeg"
      }
    },
    "publishedAt": "2025-10-22T13:43:15.000Z",
    "title": "Pico-Banana-400K: A Large-Scale Dataset for Text-Guided Image Editing",
    "summary": "Recent advances in multimodal models have demonstrated remarkable text-guided\nimage editing capabilities, with systems like GPT-4o and Nano-Banana setting\nnew benchmarks. However, the research community's progress remains constrained\nby the absence of large-scale, high-quality, and openly accessible datasets\nbuilt from real images. We introduce Pico-Banana-400K, a comprehensive\n400K-image dataset for instruction-based image editing. Our dataset is\nconstructed by leveraging Nano-Banana to generate diverse edit pairs from real\nphotographs in the OpenImages collection. What distinguishes Pico-Banana-400K\nfrom previous synthetic datasets is our systematic approach to quality and\ndiversity. We employ a fine-grained image editing taxonomy to ensure\ncomprehensive coverage of edit types while maintaining precise content\npreservation and instruction faithfulness through MLLM-based quality scoring\nand careful curation. Beyond single turn editing, Pico-Banana-400K enables\nresearch into complex editing scenarios. The dataset includes three specialized\nsubsets: (1) a 72K-example multi-turn collection for studying sequential\nediting, reasoning, and planning across consecutive modifications; (2) a\n56K-example preference subset for alignment research and reward model training;\nand (3) paired long-short editing instructions for developing instruction\nrewriting and summarization capabilities. By providing this large-scale,\nhigh-quality, and task-rich resource, Pico-Banana-400K establishes a robust\nfoundation for training and benchmarking the next generation of text-guided\nimage editing models.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.19808.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 139
    },
    "organization": {
      "_id": "628cbd99ef14f971b69948ab",
      "name": "apple",
      "fullname": "Apple",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1653390727490-5dd96eb166059660ed1ee413.jpeg"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.19488",
      "authors": [
        {
          "_id": "68f98a91b9b2e4ae0467376d",
          "name": "Dunjie Lu",
          "hidden": false
        },
        {
          "_id": "68f98a91b9b2e4ae0467376e",
          "name": "Yiheng Xu",
          "hidden": false
        },
        {
          "_id": "68f98a91b9b2e4ae0467376f",
          "name": "Junli Wang",
          "hidden": false
        },
        {
          "_id": "68f98a91b9b2e4ae04673770",
          "name": "Haoyuan Wu",
          "hidden": false
        },
        {
          "_id": "68f98a91b9b2e4ae04673771",
          "name": "Xinyuan Wang",
          "hidden": false
        },
        {
          "_id": "68f98a91b9b2e4ae04673772",
          "name": "Zekun Wang",
          "hidden": false
        },
        {
          "_id": "68f98a91b9b2e4ae04673773",
          "name": "Junlin Yang",
          "hidden": false
        },
        {
          "_id": "68f98a91b9b2e4ae04673774",
          "name": "Hongjin Su",
          "hidden": false
        },
        {
          "_id": "68f98a91b9b2e4ae04673775",
          "name": "Jixuan Chen",
          "hidden": false
        },
        {
          "_id": "68f98a91b9b2e4ae04673776",
          "name": "Junda Chen",
          "hidden": false
        },
        {
          "_id": "68f98a91b9b2e4ae04673777",
          "name": "Yuchen Mao",
          "hidden": false
        },
        {
          "_id": "68f98a91b9b2e4ae04673778",
          "name": "Jingren Zhou",
          "hidden": false
        },
        {
          "_id": "68f98a91b9b2e4ae04673779",
          "name": "Junyang Lin",
          "hidden": false
        },
        {
          "_id": "68f98a91b9b2e4ae0467377a",
          "name": "Binyuan Hui",
          "hidden": false
        },
        {
          "_id": "68f98a91b9b2e4ae0467377b",
          "name": "Tao Yu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-22T11:25:48.000Z",
      "submittedOnDailyAt": "2025-10-23T00:23:34.212Z",
      "title": "VideoAgentTrek: Computer Use Pretraining from Unlabeled Videos",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Training computer-use agents requires massive amounts of GUI interaction\ndata, but manually annotating action trajectories at scale is prohibitively\nexpensive. We present VideoAgentTrek, a scalable pipeline that automatically\nmines training data from publicly available screen-recorded videos at web\nscale, eliminating the need for manual annotation. Our approach addresses a key\nchallenge: raw videos contain implicit demonstrations but lack explicit action\nlabels. To solve this, we develop Video2Action, an inverse dynamics module\n(IDM) with two components: (1) a video grounding model that detects and\nlocalizes GUI actions with precise temporal boundaries and context, and (2) an\naction-content recognizer that extracts structured parameters like click\ncoordinates and typed text with high fidelity. Applied to 39,000 YouTube\ntutorial videos, our pipeline generates 1.52 million interaction steps\nautomatically. We leverage this data through continued pretraining followed by\nsupervised fine-tuning. On OSWorld-Verified, our approach improves task success\nrates from 9.3% (SFT-only baseline) to 15.8%, a 70% relative improvement. On\nAgentNetBench, step accuracy increases from 64.1% to 69.3%. Our results\ndemonstrate that passive internet videos can be transformed into high-quality\nsupervision for computer-use agents, providing a scalable alternative to\nexpensive manual annotation.",
      "upvotes": 1,
      "discussionId": "68f98a91b9b2e4ae0467377c",
      "projectPage": "https://videoagenttrek.github.io/",
      "ai_summary": "VideoAgentTrek automatically extracts GUI interaction data from YouTube videos using Video2Action, an inverse dynamics module, improving task success rates and step accuracy for computer-use agents.",
      "ai_keywords": [
        "inverse dynamics module",
        "video grounding model",
        "action-content recognizer",
        "continued pretraining",
        "supervised fine-tuning",
        "OSWorld-Verified",
        "AgentNetBench"
      ]
    },
    "publishedAt": "2025-10-22T07:25:48.000Z",
    "title": "VideoAgentTrek: Computer Use Pretraining from Unlabeled Videos",
    "summary": "Training computer-use agents requires massive amounts of GUI interaction\ndata, but manually annotating action trajectories at scale is prohibitively\nexpensive. We present VideoAgentTrek, a scalable pipeline that automatically\nmines training data from publicly available screen-recorded videos at web\nscale, eliminating the need for manual annotation. Our approach addresses a key\nchallenge: raw videos contain implicit demonstrations but lack explicit action\nlabels. To solve this, we develop Video2Action, an inverse dynamics module\n(IDM) with two components: (1) a video grounding model that detects and\nlocalizes GUI actions with precise temporal boundaries and context, and (2) an\naction-content recognizer that extracts structured parameters like click\ncoordinates and typed text with high fidelity. Applied to 39,000 YouTube\ntutorial videos, our pipeline generates 1.52 million interaction steps\nautomatically. We leverage this data through continued pretraining followed by\nsupervised fine-tuning. On OSWorld-Verified, our approach improves task success\nrates from 9.3% (SFT-only baseline) to 15.8%, a 70% relative improvement. On\nAgentNetBench, step accuracy increases from 64.1% to 69.3%. Our results\ndemonstrate that passive internet videos can be transformed into high-quality\nsupervision for computer-use agents, providing a scalable alternative to\nexpensive manual annotation.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.19488.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 139
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.19286",
      "authors": [
        {
          "_id": "68f98627b9b2e4ae04673747",
          "name": "Reza Esfandiarpoor",
          "hidden": false
        },
        {
          "_id": "68f98627b9b2e4ae04673748",
          "name": "Vishwas Suryanarayanan",
          "hidden": false
        },
        {
          "_id": "68f98627b9b2e4ae04673749",
          "name": "Stephen H. Bach",
          "hidden": false
        },
        {
          "_id": "68f98627b9b2e4ae0467374a",
          "name": "Vishal Chowdhary",
          "hidden": false
        },
        {
          "_id": "68f98627b9b2e4ae0467374b",
          "name": "Anthony Aue",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-22T06:42:01.000Z",
      "submittedOnDailyAt": "2025-10-23T00:05:22.073Z",
      "title": "TheMCPCompany: Creating General-purpose Agents with Task-specific Tools",
      "submittedOnDailyBy": {
        "_id": "63316bb618711776b465b788",
        "avatarUrl": "/avatars/ac53c8e81f0b09232eab0ca698e1400c.svg",
        "isPro": false,
        "fullname": "Reza Esfandiarpoor",
        "user": "rezaesfandiarpoor",
        "type": "user"
      },
      "summary": "Since the introduction of the Model Context Protocol (MCP), the number of\navailable tools for Large Language Models (LLMs) has increased significantly.\nThese task-specific tool sets offer an alternative to general-purpose tools\nsuch as web browsers, while being easier to develop and maintain than GUIs.\nHowever, current general-purpose agents predominantly rely on web browsers for\ninteracting with the environment. Here, we introduce TheMCPCompany, a benchmark\nfor evaluating tool-calling agents on tasks that involve interacting with\nvarious real-world services. We use the REST APIs of these services to create\nMCP servers, which include over 18,000 tools. We also provide manually\nannotated ground-truth tools for each task. In our experiments, we use the\nground truth tools to show the potential of tool-calling agents for both\nimproving performance and reducing costs assuming perfect tool retrieval. Next,\nwe explore agent performance using tool retrieval to study the real-world\npracticality of tool-based agents. While all models with tool retrieval perform\nsimilarly or better than browser-based agents, smaller models cannot take full\nadvantage of the available tools through retrieval. On the other hand, GPT-5's\nperformance with tool retrieval is very close to its performance with\nground-truth tools. Overall, our work shows that the most advanced reasoning\nmodels are effective at discovering tools in simpler environments, but\nseriously struggle with navigating complex enterprise environments.\nTheMCPCompany reveals that navigating tens of thousands of tools and combining\nthem in non-trivial ways to solve complex problems is still a challenging task\nfor current models and requires both better reasoning and better retrieval\nmodels.",
      "upvotes": 1,
      "discussionId": "68f98628b9b2e4ae0467374c",
      "githubRepo": "https://github.com/Reza-esfandiarpoor/the-mcp-company",
      "ai_summary": "TheMCPCompany evaluates tool-calling agents using REST APIs for interacting with real-world services, showing that advanced models perform well in simpler environments but struggle with complex enterprise environments.",
      "ai_keywords": [
        "Model Context Protocol",
        "Large Language Models",
        "tool-calling agents",
        "REST APIs",
        "ground-truth tools",
        "tool retrieval",
        "GPT-5",
        "reasoning models",
        "retrieval models"
      ],
      "githubStars": 2
    },
    "publishedAt": "2025-10-22T02:42:01.000Z",
    "title": "TheMCPCompany: Creating General-purpose Agents with Task-specific Tools",
    "summary": "Since the introduction of the Model Context Protocol (MCP), the number of\navailable tools for Large Language Models (LLMs) has increased significantly.\nThese task-specific tool sets offer an alternative to general-purpose tools\nsuch as web browsers, while being easier to develop and maintain than GUIs.\nHowever, current general-purpose agents predominantly rely on web browsers for\ninteracting with the environment. Here, we introduce TheMCPCompany, a benchmark\nfor evaluating tool-calling agents on tasks that involve interacting with\nvarious real-world services. We use the REST APIs of these services to create\nMCP servers, which include over 18,000 tools. We also provide manually\nannotated ground-truth tools for each task. In our experiments, we use the\nground truth tools to show the potential of tool-calling agents for both\nimproving performance and reducing costs assuming perfect tool retrieval. Next,\nwe explore agent performance using tool retrieval to study the real-world\npracticality of tool-based agents. While all models with tool retrieval perform\nsimilarly or better than browser-based agents, smaller models cannot take full\nadvantage of the available tools through retrieval. On the other hand, GPT-5's\nperformance with tool retrieval is very close to its performance with\nground-truth tools. Overall, our work shows that the most advanced reasoning\nmodels are effective at discovering tools in simpler environments, but\nseriously struggle with navigating complex enterprise environments.\nTheMCPCompany reveals that navigating tens of thousands of tools and combining\nthem in non-trivial ways to solve complex problems is still a challenging task\nfor current models and requires both better reasoning and better retrieval\nmodels.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.19286.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63316bb618711776b465b788",
      "avatarUrl": "/avatars/ac53c8e81f0b09232eab0ca698e1400c.svg",
      "fullname": "Reza Esfandiarpoor",
      "name": "rezaesfandiarpoor",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.16844",
      "authors": [
        {
          "_id": "68f86f6b7669bcaeecce0e59",
          "name": "Jiajie Jin",
          "hidden": false
        },
        {
          "_id": "68f86f6b7669bcaeecce0e5a",
          "name": "Yuyao Zhang",
          "hidden": false
        },
        {
          "_id": "68f86f6b7669bcaeecce0e5b",
          "name": "Yimeng Xu",
          "hidden": false
        },
        {
          "_id": "68f86f6b7669bcaeecce0e5c",
          "name": "Hongjin Qian",
          "hidden": false
        },
        {
          "_id": "68f86f6b7669bcaeecce0e5d",
          "name": "Yutao Zhu",
          "hidden": false
        },
        {
          "_id": "68f86f6b7669bcaeecce0e5e",
          "name": "Zhicheng Dou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-19T14:05:35.000Z",
      "submittedOnDailyAt": "2025-10-23T00:50:26.504Z",
      "title": "FinSight: Towards Real-World Financial Deep Research",
      "submittedOnDailyBy": {
        "_id": "6695f14df0ffd8e3a379ad61",
        "avatarUrl": "/avatars/5ebb7e55ee9c2d93850b279f440675b0.svg",
        "isPro": false,
        "fullname": "Jiajie Jin",
        "user": "jinjiajie",
        "type": "user"
      },
      "summary": "Generating professional financial reports is a labor-intensive and\nintellectually demanding process that current AI systems struggle to fully\nautomate. To address this challenge, we introduce FinSight (Financial InSight),\na novel multi agent framework for producing high-quality, multimodal financial\nreports. The foundation of FinSight is the Code Agent with Variable Memory\n(CAVM) architecture, which unifies external data, designed tools, and agents\ninto a programmable variable space, enabling flexible data collection, analysis\nand report generation through executable code. To ensure professional-grade\nvisualization, we propose an Iterative Vision-Enhanced Mechanism that\nprogressively refines raw visual outputs into polished financial charts.\nFurthermore, a two stage Writing Framework expands concise Chain-of-Analysis\nsegments into coherent, citation-aware, and multimodal reports, ensuring both\nanalytical depth and structural consistency. Experiments on various company and\nindustry-level tasks demonstrate that FinSight significantly outperforms all\nbaselines, including leading deep research systems in terms of factual\naccuracy, analytical depth, and presentation quality, demonstrating a clear\npath toward generating reports that approach human-expert quality.",
      "upvotes": 1,
      "discussionId": "68f86f6b7669bcaeecce0e5f",
      "ai_summary": "FinSight, a multi-agent framework using CAVM architecture and iterative vision-enhanced mechanism, generates high-quality, multimodal financial reports with superior accuracy and presentation quality compared to existing systems.",
      "ai_keywords": [
        "multi agent framework",
        "Code Agent with Variable Memory (CAVM)",
        "programmable variable space",
        "Iterative Vision-Enhanced Mechanism",
        "Chain-of-Analysis",
        "citation-aware",
        "multimodal reports"
      ]
    },
    "publishedAt": "2025-10-19T10:05:35.000Z",
    "title": "FinSight: Towards Real-World Financial Deep Research",
    "summary": "Generating professional financial reports is a labor-intensive and\nintellectually demanding process that current AI systems struggle to fully\nautomate. To address this challenge, we introduce FinSight (Financial InSight),\na novel multi agent framework for producing high-quality, multimodal financial\nreports. The foundation of FinSight is the Code Agent with Variable Memory\n(CAVM) architecture, which unifies external data, designed tools, and agents\ninto a programmable variable space, enabling flexible data collection, analysis\nand report generation through executable code. To ensure professional-grade\nvisualization, we propose an Iterative Vision-Enhanced Mechanism that\nprogressively refines raw visual outputs into polished financial charts.\nFurthermore, a two stage Writing Framework expands concise Chain-of-Analysis\nsegments into coherent, citation-aware, and multimodal reports, ensuring both\nanalytical depth and structural consistency. Experiments on various company and\nindustry-level tasks demonstrate that FinSight significantly outperforms all\nbaselines, including leading deep research systems in terms of factual\naccuracy, analytical depth, and presentation quality, demonstrating a clear\npath toward generating reports that approach human-expert quality.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.16844.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6695f14df0ffd8e3a379ad61",
      "avatarUrl": "/avatars/5ebb7e55ee9c2d93850b279f440675b0.svg",
      "fullname": "Jiajie Jin",
      "name": "jinjiajie",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 7
    },
    "isAuthorParticipating": false
  }
]