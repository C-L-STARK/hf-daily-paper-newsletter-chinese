[
  {
    "paper": {
      "id": "2508.20722",
      "authors": [
        {
          "_id": "68b10494b19c5400014848cb",
          "name": "Ning Shang",
          "hidden": false
        },
        {
          "_id": "68b10494b19c5400014848cc",
          "name": "Yifei Liu",
          "hidden": false
        },
        {
          "_id": "68b10494b19c5400014848cd",
          "name": "Yi Zhu",
          "hidden": false
        },
        {
          "_id": "68b10494b19c5400014848ce",
          "name": "Li Lyna Zhang",
          "hidden": false
        },
        {
          "_id": "68b10494b19c5400014848cf",
          "name": "Weijiang Xu",
          "hidden": false
        },
        {
          "_id": "68b10494b19c5400014848d0",
          "name": "Xinyu Guan",
          "hidden": false
        },
        {
          "_id": "68b10494b19c5400014848d1",
          "name": "Buze Zhang",
          "hidden": false
        },
        {
          "_id": "68b10494b19c5400014848d2",
          "name": "Bingcheng Dong",
          "hidden": false
        },
        {
          "_id": "68b10494b19c5400014848d3",
          "name": "Xudong Zhou",
          "hidden": false
        },
        {
          "_id": "68b10494b19c5400014848d4",
          "name": "Bowen Zhang",
          "hidden": false
        },
        {
          "_id": "68b10494b19c5400014848d5",
          "name": "Ying Xin",
          "hidden": false
        },
        {
          "_id": "68b10494b19c5400014848d6",
          "name": "Ziming Miao",
          "hidden": false
        },
        {
          "_id": "68b10494b19c5400014848d7",
          "name": "Scarlett Li",
          "hidden": false
        },
        {
          "_id": "68b10494b19c5400014848d8",
          "name": "Fan Yang",
          "hidden": false
        },
        {
          "_id": "68b10494b19c5400014848d9",
          "name": "Mao Yang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-08-28T12:45:25.000Z",
      "submittedOnDailyAt": "2025-08-29T00:25:20.345Z",
      "title": "rStar2-Agent: Agentic Reasoning Technical Report",
      "submittedOnDailyBy": {
        "_id": "62b0009c72043b05d29492b2",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62b0009c72043b05d29492b2/NqRkX2YLhlfOLvYysa7dD.png",
        "isPro": false,
        "fullname": "Li Lyna Zhang",
        "user": "lynazhang",
        "type": "user"
      },
      "summary": "We introduce rStar2-Agent, a 14B math reasoning model trained with agentic\nreinforcement learning to achieve frontier-level performance. Beyond current\nlong CoT, the model demonstrates advanced cognitive behaviors, such as thinking\ncarefully before using Python coding tools and reflecting on code execution\nfeedback to autonomously explore, verify, and refine intermediate steps in\ncomplex problem-solving. This capability is enabled through three key\ninnovations that makes agentic RL effective at scale: (i) an efficient RL\ninfrastructure with a reliable Python code environment that supports\nhigh-throughput execution and mitigates the high rollout costs, enabling\ntraining on limited GPU resources (64 MI300X GPUs); (ii) GRPO-RoC, an agentic\nRL algorithm with a Resample-on-Correct rollout strategy that addresses the\ninherent environment noises from coding tools, allowing the model to reason\nmore effectively in a code environment; (iii) An efficient agent training\nrecipe that starts with non-reasoning SFT and progresses through multi-RL\nstages, yielding advanced cognitive abilities with minimal compute cost. To\nthis end, rStar2-Agent boosts a pre-trained 14B model to state of the art in\nonly 510 RL steps within one week, achieving average pass@1 scores of 80.6% on\nAIME24 and 69.8% on AIME25, surpassing DeepSeek-R1 (671B) with significantly\nshorter responses. Beyond mathematics, rStar2-Agent-14B also demonstrates\nstrong generalization to alignment, scientific reasoning, and agentic tool-use\ntasks. Code and training recipes are available at\nhttps://github.com/microsoft/rStar.",
      "upvotes": 11,
      "discussionId": "68b10495b19c5400014848da",
      "githubRepo": "https://github.com/microsoft/rStar",
      "ai_summary": "rStar2-Agent, a 14B math reasoning model trained with agentic reinforcement learning, achieves state-of-the-art performance by efficiently handling complex problem-solving with advanced cognitive behaviors and minimal computational resources.",
      "ai_keywords": [
        "agentic reinforcement learning",
        "CoT",
        "Python coding tools",
        "GRPO-RoC",
        "Resample-on-Correct",
        "SFT",
        "multi-RL",
        "AIME24",
        "AIME25",
        "DeepSeek-R1",
        "alignment",
        "scientific reasoning",
        "agentic tool-use"
      ],
      "githubStars": 623
    },
    "publishedAt": "2025-08-28T08:45:25.000Z",
    "title": "rStar2-Agent: Agentic Reasoning Technical Report",
    "summary": "We introduce rStar2-Agent, a 14B math reasoning model trained with agentic\nreinforcement learning to achieve frontier-level performance. Beyond current\nlong CoT, the model demonstrates advanced cognitive behaviors, such as thinking\ncarefully before using Python coding tools and reflecting on code execution\nfeedback to autonomously explore, verify, and refine intermediate steps in\ncomplex problem-solving. This capability is enabled through three key\ninnovations that makes agentic RL effective at scale: (i) an efficient RL\ninfrastructure with a reliable Python code environment that supports\nhigh-throughput execution and mitigates the high rollout costs, enabling\ntraining on limited GPU resources (64 MI300X GPUs); (ii) GRPO-RoC, an agentic\nRL algorithm with a Resample-on-Correct rollout strategy that addresses the\ninherent environment noises from coding tools, allowing the model to reason\nmore effectively in a code environment; (iii) An efficient agent training\nrecipe that starts with non-reasoning SFT and progresses through multi-RL\nstages, yielding advanced cognitive abilities with minimal compute cost. To\nthis end, rStar2-Agent boosts a pre-trained 14B model to state of the art in\nonly 510 RL steps within one week, achieving average pass@1 scores of 80.6% on\nAIME24 and 69.8% on AIME25, surpassing DeepSeek-R1 (671B) with significantly\nshorter responses. Beyond mathematics, rStar2-Agent-14B also demonstrates\nstrong generalization to alignment, scientific reasoning, and agentic tool-use\ntasks. Code and training recipes are available at\nhttps://github.com/microsoft/rStar.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.20722.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62b0009c72043b05d29492b2",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62b0009c72043b05d29492b2/NqRkX2YLhlfOLvYysa7dD.png",
      "fullname": "Li Lyna Zhang",
      "name": "lynazhang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 31
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2508.21058",
      "authors": [
        {
          "_id": "68b10915b19c5400014848fb",
          "name": "Shengqu Cai",
          "hidden": false
        },
        {
          "_id": "68b10915b19c5400014848fc",
          "name": "Ceyuan Yang",
          "hidden": false
        },
        {
          "_id": "68b10915b19c5400014848fd",
          "name": "Lvmin Zhang",
          "hidden": false
        },
        {
          "_id": "68b10915b19c5400014848fe",
          "name": "Yuwei Guo",
          "hidden": false
        },
        {
          "_id": "68b10915b19c5400014848ff",
          "name": "Junfei Xiao",
          "hidden": false
        },
        {
          "_id": "68b10915b19c540001484900",
          "name": "Ziyan Yang",
          "hidden": false
        },
        {
          "_id": "68b10915b19c540001484901",
          "name": "Yinghao Xu",
          "hidden": false
        },
        {
          "_id": "68b10915b19c540001484902",
          "name": "Zhenheng Yang",
          "hidden": false
        },
        {
          "_id": "68b10915b19c540001484903",
          "name": "Alan Yuille",
          "hidden": false
        },
        {
          "_id": "68b10915b19c540001484904",
          "name": "Leonidas Guibas",
          "hidden": false
        },
        {
          "_id": "68b10915b19c540001484905",
          "name": "Maneesh Agrawala",
          "hidden": false
        },
        {
          "_id": "68b10915b19c540001484906",
          "name": "Lu Jiang",
          "hidden": false
        },
        {
          "_id": "68b10915b19c540001484907",
          "name": "Gordon Wetzstein",
          "hidden": false
        }
      ],
      "publishedAt": "2025-08-28T17:57:55.000Z",
      "submittedOnDailyAt": "2025-08-29T00:28:28.135Z",
      "title": "Mixture of Contexts for Long Video Generation",
      "submittedOnDailyBy": {
        "_id": "66a1da7cc9e703d2af5ad742",
        "avatarUrl": "/avatars/f9cd9ae3407e249ab4569479200feb1f.svg",
        "isPro": true,
        "fullname": "Shengqu Cai",
        "user": "primecai",
        "type": "user"
      },
      "summary": "Long video generation is fundamentally a long context memory problem: models\nmust retain and retrieve salient events across a long range without collapsing\nor drifting. However, scaling diffusion transformers to generate long-context\nvideos is fundamentally limited by the quadratic cost of self-attention, which\nmakes memory and computation intractable and difficult to optimize for long\nsequences. We recast long-context video generation as an internal information\nretrieval task and propose a simple, learnable sparse attention routing module,\nMixture of Contexts (MoC), as an effective long-term memory retrieval engine.\nIn MoC, each query dynamically selects a few informative chunks plus mandatory\nanchors (caption, local windows) to attend to, with causal routing that\nprevents loop closures. As we scale the data and gradually sparsify the\nrouting, the model allocates compute to salient history, preserving identities,\nactions, and scenes over minutes of content. Efficiency follows as a byproduct\nof retrieval (near-linear scaling), which enables practical training and\nsynthesis, and the emergence of memory and consistency at the scale of minutes.",
      "upvotes": 2,
      "discussionId": "68b10915b19c540001484908",
      "projectPage": "https://primecai.github.io/moc/",
      "ai_summary": "Long video generation is addressed by introducing a sparse attention routing module, Mixture of Contexts, to efficiently manage long-term memory and retrieval in diffusion transformers.",
      "ai_keywords": [
        "diffusion transformers",
        "self-attention",
        "long-context video generation",
        "internal information retrieval",
        "Mixture of Contexts",
        "sparse attention routing",
        "causal routing",
        "salient history",
        "memory",
        "consistency"
      ]
    },
    "publishedAt": "2025-08-28T13:57:55.000Z",
    "title": "Mixture of Contexts for Long Video Generation",
    "summary": "Long video generation is fundamentally a long context memory problem: models\nmust retain and retrieve salient events across a long range without collapsing\nor drifting. However, scaling diffusion transformers to generate long-context\nvideos is fundamentally limited by the quadratic cost of self-attention, which\nmakes memory and computation intractable and difficult to optimize for long\nsequences. We recast long-context video generation as an internal information\nretrieval task and propose a simple, learnable sparse attention routing module,\nMixture of Contexts (MoC), as an effective long-term memory retrieval engine.\nIn MoC, each query dynamically selects a few informative chunks plus mandatory\nanchors (caption, local windows) to attend to, with causal routing that\nprevents loop closures. As we scale the data and gradually sparsify the\nrouting, the model allocates compute to salient history, preserving identities,\nactions, and scenes over minutes of content. Efficiency follows as a byproduct\nof retrieval (near-linear scaling), which enables practical training and\nsynthesis, and the emergence of memory and consistency at the scale of minutes.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.21058.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66a1da7cc9e703d2af5ad742",
      "avatarUrl": "/avatars/f9cd9ae3407e249ab4569479200feb1f.svg",
      "fullname": "Shengqu Cai",
      "name": "primecai",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 13
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2508.20453",
      "authors": [
        {
          "_id": "68b10726b19c5400014848e6",
          "name": "Zhenting Wang",
          "hidden": false
        },
        {
          "_id": "68b10726b19c5400014848e7",
          "name": "Qi Chang",
          "hidden": false
        },
        {
          "_id": "68b10726b19c5400014848e8",
          "name": "Hemani Patel",
          "hidden": false
        },
        {
          "_id": "68b10726b19c5400014848e9",
          "name": "Shashank Biju",
          "hidden": false
        },
        {
          "_id": "68b10726b19c5400014848ea",
          "name": "Cheng-En Wu",
          "hidden": false
        },
        {
          "_id": "68b10726b19c5400014848eb",
          "name": "Quan Liu",
          "hidden": false
        },
        {
          "_id": "68b10726b19c5400014848ec",
          "name": "Aolin Ding",
          "hidden": false
        },
        {
          "_id": "68b10726b19c5400014848ed",
          "name": "Alireza Rezazadeh",
          "hidden": false
        },
        {
          "_id": "68b10726b19c5400014848ee",
          "name": "Ankit Shah",
          "hidden": false
        },
        {
          "_id": "68b10726b19c5400014848ef",
          "name": "Yujia Bao",
          "hidden": false
        },
        {
          "_id": "68b10726b19c5400014848f0",
          "name": "Eugene Siow",
          "hidden": false
        }
      ],
      "publishedAt": "2025-08-28T05:58:57.000Z",
      "submittedOnDailyAt": "2025-08-29T00:22:20.147Z",
      "title": "MCP-Bench: Benchmarking Tool-Using LLM Agents with Complex Real-World\n  Tasks via MCP Servers",
      "submittedOnDailyBy": {
        "_id": "64dfcc62e8b6f3f3baa950e0",
        "avatarUrl": "/avatars/21bbff67d46c08044efe2406575aa77e.svg",
        "isPro": false,
        "fullname": "Zhenting Wang",
        "user": "ztwang",
        "type": "user"
      },
      "summary": "We introduce MCP-Bench, a benchmark for evaluating large language models\n(LLMs) on realistic, multi-step tasks that demand tool use, cross-tool\ncoordination, precise parameter control, and planning/reasoning for solving\ntasks. Built on the Model Context Protocol (MCP), MCP-Bench connects LLMs to 28\nrepresentative live MCP servers spanning 250 tools across domains such as\nfinance, traveling, scientific computing, and academic search. Unlike prior\nAPI-based benchmarks, each MCP server provides a set of complementary tools\ndesigned to work together, enabling the construction of authentic, multi-step\ntasks with rich input-output coupling. Tasks in MCP-Bench test agents' ability\nto retrieve relevant tools from fuzzy instructions without explicit tool names,\nplan multi-hop execution trajectories for complex objectives, ground responses\nin intermediate tool outputs, and orchestrate cross-domain workflows -\ncapabilities not adequately evaluated by existing benchmarks that rely on\nexplicit tool specifications, shallow few-step workflows, and isolated domain\noperations. We propose a multi-faceted evaluation framework covering tool-level\nschema understanding and usage, trajectory-level planning, and task completion.\nExperiments on 20 advanced LLMs reveal persistent challenges in MCP-Bench. Code\nand data: https://github.com/Accenture/mcp-bench.",
      "upvotes": 2,
      "discussionId": "68b10726b19c5400014848f1",
      "ai_summary": "MCP-Bench evaluates large language models on complex, multi-step tasks requiring tool use, coordination, and planning across various domains.",
      "ai_keywords": [
        "large language models",
        "MCP-Bench",
        "Model Context Protocol",
        "MCP servers",
        "multi-step tasks",
        "tool use",
        "cross-tool coordination",
        "parameter control",
        "planning",
        "reasoning",
        "schema understanding",
        "trajectory-level planning",
        "task completion"
      ]
    },
    "publishedAt": "2025-08-28T01:58:57.000Z",
    "title": "MCP-Bench: Benchmarking Tool-Using LLM Agents with Complex Real-World\n  Tasks via MCP Servers",
    "summary": "We introduce MCP-Bench, a benchmark for evaluating large language models\n(LLMs) on realistic, multi-step tasks that demand tool use, cross-tool\ncoordination, precise parameter control, and planning/reasoning for solving\ntasks. Built on the Model Context Protocol (MCP), MCP-Bench connects LLMs to 28\nrepresentative live MCP servers spanning 250 tools across domains such as\nfinance, traveling, scientific computing, and academic search. Unlike prior\nAPI-based benchmarks, each MCP server provides a set of complementary tools\ndesigned to work together, enabling the construction of authentic, multi-step\ntasks with rich input-output coupling. Tasks in MCP-Bench test agents' ability\nto retrieve relevant tools from fuzzy instructions without explicit tool names,\nplan multi-hop execution trajectories for complex objectives, ground responses\nin intermediate tool outputs, and orchestrate cross-domain workflows -\ncapabilities not adequately evaluated by existing benchmarks that rely on\nexplicit tool specifications, shallow few-step workflows, and isolated domain\noperations. We propose a multi-faceted evaluation framework covering tool-level\nschema understanding and usage, trajectory-level planning, and task completion.\nExperiments on 20 advanced LLMs reveal persistent challenges in MCP-Bench. Code\nand data: https://github.com/Accenture/mcp-bench.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.20453.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64dfcc62e8b6f3f3baa950e0",
      "avatarUrl": "/avatars/21bbff67d46c08044efe2406575aa77e.svg",
      "fullname": "Zhenting Wang",
      "name": "ztwang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2508.20751",
      "authors": [
        {
          "_id": "68b10e71b19c54000148491a",
          "name": "Yibin Wang",
          "hidden": false
        },
        {
          "_id": "68b10e71b19c54000148491b",
          "name": "Zhimin Li",
          "hidden": false
        },
        {
          "_id": "68b10e71b19c54000148491c",
          "name": "Yuhang Zang",
          "hidden": false
        },
        {
          "_id": "68b10e71b19c54000148491d",
          "name": "Yujie Zhou",
          "hidden": false
        },
        {
          "_id": "68b10e71b19c54000148491e",
          "name": "Jiazi Bu",
          "hidden": false
        },
        {
          "_id": "68b10e71b19c54000148491f",
          "name": "Chunyu Wang",
          "hidden": false
        },
        {
          "_id": "68b10e71b19c540001484920",
          "name": "Qinglin Lu",
          "hidden": false
        },
        {
          "_id": "68b10e71b19c540001484921",
          "name": "Cheng Jin",
          "hidden": false
        },
        {
          "_id": "68b10e71b19c540001484922",
          "name": "Jiaqi Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-08-28T13:11:24.000Z",
      "submittedOnDailyAt": "2025-08-29T00:53:42.823Z",
      "title": "Pref-GRPO: Pairwise Preference Reward-based GRPO for Stable\n  Text-to-Image Reinforcement Learning",
      "submittedOnDailyBy": {
        "_id": "654c6845bac6e6e49895a5b5",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/QwUOUujUAyiV4tAhB_moO.png",
        "isPro": false,
        "fullname": "SII-Yibin Wang",
        "user": "CodeGoat24",
        "type": "user"
      },
      "summary": "Recent advancements highlight the importance of GRPO-based reinforcement\nlearning methods and benchmarking in enhancing text-to-image (T2I) generation.\nHowever, current methods using pointwise reward models (RM) for scoring\ngenerated images are susceptible to reward hacking. We reveal that this happens\nwhen minimal score differences between images are amplified after\nnormalization, creating illusory advantages that drive the model to\nover-optimize for trivial gains, ultimately destabilizing the image generation\nprocess. To address this, we propose Pref-GRPO, a pairwise preference\nreward-based GRPO method that shifts the optimization objective from score\nmaximization to preference fitting, ensuring more stable training. In\nPref-GRPO, images are pairwise compared within each group using preference RM,\nand the win rate is used as the reward signal. Extensive experiments\ndemonstrate that PREF-GRPO differentiates subtle image quality differences,\nproviding more stable advantages and mitigating reward hacking. Additionally,\nexisting T2I benchmarks are limited by coarse evaluation criteria, hindering\ncomprehensive model assessment. To solve this, we introduce UniGenBench, a\nunified T2I benchmark comprising 600 prompts across 5 main themes and 20\nsubthemes. It evaluates semantic consistency through 10 primary and 27\nsub-criteria, leveraging MLLM for benchmark construction and evaluation. Our\nbenchmarks uncover the strengths and weaknesses of both open and closed-source\nT2I models and validate the effectiveness of Pref-GRPO.",
      "upvotes": 0,
      "discussionId": "68b10e71b19c540001484923",
      "ai_summary": "Pref-GRPO, a pairwise preference reward-based GRPO method, enhances text-to-image generation by mitigating reward hacking and improving stability, while UniGenBench provides a comprehensive benchmark for evaluating T2I models.",
      "ai_keywords": [
        "GRPO",
        "reinforcement learning",
        "text-to-image",
        "pointwise reward models",
        "reward hacking",
        "pairwise preference",
        "preference fitting",
        "win rate",
        "UniGenBench",
        "semantic consistency",
        "MLLM"
      ]
    },
    "publishedAt": "2025-08-28T09:11:24.000Z",
    "title": "Pref-GRPO: Pairwise Preference Reward-based GRPO for Stable\n  Text-to-Image Reinforcement Learning",
    "summary": "Recent advancements highlight the importance of GRPO-based reinforcement\nlearning methods and benchmarking in enhancing text-to-image (T2I) generation.\nHowever, current methods using pointwise reward models (RM) for scoring\ngenerated images are susceptible to reward hacking. We reveal that this happens\nwhen minimal score differences between images are amplified after\nnormalization, creating illusory advantages that drive the model to\nover-optimize for trivial gains, ultimately destabilizing the image generation\nprocess. To address this, we propose Pref-GRPO, a pairwise preference\nreward-based GRPO method that shifts the optimization objective from score\nmaximization to preference fitting, ensuring more stable training. In\nPref-GRPO, images are pairwise compared within each group using preference RM,\nand the win rate is used as the reward signal. Extensive experiments\ndemonstrate that PREF-GRPO differentiates subtle image quality differences,\nproviding more stable advantages and mitigating reward hacking. Additionally,\nexisting T2I benchmarks are limited by coarse evaluation criteria, hindering\ncomprehensive model assessment. To solve this, we introduce UniGenBench, a\nunified T2I benchmark comprising 600 prompts across 5 main themes and 20\nsubthemes. It evaluates semantic consistency through 10 primary and 27\nsub-criteria, leveraging MLLM for benchmark construction and evaluation. Our\nbenchmarks uncover the strengths and weaknesses of both open and closed-source\nT2I models and validate the effectiveness of Pref-GRPO.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.20751.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "654c6845bac6e6e49895a5b5",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/QwUOUujUAyiV4tAhB_moO.png",
      "fullname": "SII-Yibin Wang",
      "name": "CodeGoat24",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 15
    },
    "isAuthorParticipating": false
  }
]