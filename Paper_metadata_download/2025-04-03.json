[
  {
    "paper": {
      "id": "2504.00999",
      "authors": [
        {
          "_id": "67ecc3973d267d266649e075",
          "name": "Siyuan Li",
          "hidden": false
        },
        {
          "_id": "67ecc3973d267d266649e076",
          "name": "Luyuan Zhang",
          "hidden": false
        },
        {
          "_id": "67ecc3973d267d266649e077",
          "name": "Zedong Wang",
          "hidden": false
        },
        {
          "_id": "67ecc3973d267d266649e078",
          "name": "Juanxi Tian",
          "hidden": false
        },
        {
          "_id": "67ecc3973d267d266649e079",
          "name": "Cheng Tan",
          "hidden": false
        },
        {
          "_id": "67ecc3973d267d266649e07a",
          "name": "Zicheng Liu",
          "hidden": false
        },
        {
          "_id": "67ecc3973d267d266649e07b",
          "name": "Chang Yu",
          "hidden": false
        },
        {
          "_id": "67ecc3973d267d266649e07c",
          "name": "Qingsong Xie",
          "hidden": false
        },
        {
          "_id": "67ecc3973d267d266649e07d",
          "name": "Haonan Lu",
          "hidden": false
        },
        {
          "_id": "67ecc3973d267d266649e07e",
          "name": "Haoqian Wang",
          "hidden": false
        },
        {
          "_id": "67ecc3973d267d266649e07f",
          "name": "Zhen Lei",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-01T17:39:19.000Z",
      "submittedOnDailyAt": "2025-04-03T00:15:32.614Z",
      "title": "MergeVQ: A Unified Framework for Visual Generation and Representation\n  with Disentangled Token Merging and Quantization",
      "submittedOnDailyBy": {
        "_id": "670880950e79a8b46f7ff9dd",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/670880950e79a8b46f7ff9dd/hA1TLhwlQblkFsq8wLrkB.jpeg",
        "isPro": false,
        "fullname": "Juanxi Tian",
        "user": "Juanxi",
        "type": "user"
      },
      "summary": "Masked Image Modeling (MIM) with Vector Quantization (VQ) has achieved great\nsuccess in both self-supervised pre-training and image generation. However,\nmost existing methods struggle to address the trade-off in shared latent space\nfor generation quality vs. representation learning and efficiency. To push the\nlimits of this paradigm, we propose MergeVQ, which incorporates token merging\ntechniques into VQ-based generative models to bridge the gap between image\ngeneration and visual representation learning in a unified architecture. During\npre-training, MergeVQ decouples top-k semantics from latent space with the\ntoken merge module after self-attention blocks in the encoder for subsequent\nLook-up Free Quantization (LFQ) and global alignment and recovers their\nfine-grained details through cross-attention in the decoder for reconstruction.\nAs for the second-stage generation, we introduce MergeAR, which performs KV\nCache compression for efficient raster-order prediction. Extensive experiments\non ImageNet verify that MergeVQ as an AR generative model achieves competitive\nperformance in both visual representation learning and image generation tasks\nwhile maintaining favorable token efficiency and inference speed. The code and\nmodel will be available at https://apexgen-x.github.io/MergeVQ.",
      "upvotes": 16,
      "discussionId": "67ecc3993d267d266649e10c",
      "projectPage": "https://apexgen-x.github.io/MergeVQ/",
      "githubRepo": "https://github.com/ApexGen-X/MergeVQ",
      "ai_keywords": [
        "Masked Image Modeling (MIM)",
        "Vector Quantization (VQ)",
        "shared latent space",
        "generation quality",
        "representation learning",
        "token merging",
        "generative models",
        "token merge module",
        "self-attention blocks",
        "encoder",
        "Look-up Free Quantization (LFQ)",
        "global alignment",
        "cross-attention",
        "decoder",
        "reconstruction",
        "MergeAR",
        "KV Cache compression",
        "raster-order prediction",
        "AR generative model",
        "ImageNet",
        "token efficiency",
        "inference speed"
      ]
    },
    "publishedAt": "2025-04-01T13:39:19.000Z",
    "title": "MergeVQ: A Unified Framework for Visual Generation and Representation\n  with Disentangled Token Merging and Quantization",
    "summary": "Masked Image Modeling (MIM) with Vector Quantization (VQ) has achieved great\nsuccess in both self-supervised pre-training and image generation. However,\nmost existing methods struggle to address the trade-off in shared latent space\nfor generation quality vs. representation learning and efficiency. To push the\nlimits of this paradigm, we propose MergeVQ, which incorporates token merging\ntechniques into VQ-based generative models to bridge the gap between image\ngeneration and visual representation learning in a unified architecture. During\npre-training, MergeVQ decouples top-k semantics from latent space with the\ntoken merge module after self-attention blocks in the encoder for subsequent\nLook-up Free Quantization (LFQ) and global alignment and recovers their\nfine-grained details through cross-attention in the decoder for reconstruction.\nAs for the second-stage generation, we introduce MergeAR, which performs KV\nCache compression for efficient raster-order prediction. Extensive experiments\non ImageNet verify that MergeVQ as an AR generative model achieves competitive\nperformance in both visual representation learning and image generation tasks\nwhile maintaining favorable token efficiency and inference speed. The code and\nmodel will be available at https://apexgen-x.github.io/MergeVQ.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.00999.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "670880950e79a8b46f7ff9dd",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/670880950e79a8b46f7ff9dd/hA1TLhwlQblkFsq8wLrkB.jpeg",
      "fullname": "Juanxi Tian",
      "name": "Juanxi",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.00824",
      "authors": [
        {
          "_id": "67ede79d21d7e74ee3e2832a",
          "name": "Yubo Wang",
          "hidden": false
        },
        {
          "_id": "67ede79d21d7e74ee3e2832b",
          "name": "Xueguang Ma",
          "hidden": false
        },
        {
          "_id": "67ede79d21d7e74ee3e2832c",
          "name": "Ping Nie",
          "hidden": false
        },
        {
          "_id": "67ede79d21d7e74ee3e2832d",
          "name": "Huaye Zeng",
          "hidden": false
        },
        {
          "_id": "67ede79d21d7e74ee3e2832e",
          "name": "Zhiheng Lyu",
          "hidden": false
        },
        {
          "_id": "67ede79d21d7e74ee3e2832f",
          "name": "Yuxuan Zhang",
          "hidden": false
        },
        {
          "_id": "67ede79d21d7e74ee3e28330",
          "name": "Benjamin Schneider",
          "hidden": false
        },
        {
          "_id": "67ede79d21d7e74ee3e28331",
          "name": "Yi Lu",
          "hidden": false
        },
        {
          "_id": "67ede79d21d7e74ee3e28332",
          "name": "Xiang Yue",
          "hidden": false
        },
        {
          "_id": "67ede79d21d7e74ee3e28333",
          "name": "Wenhu Chen",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6313a86154e6e5d9f0f94e04/uqsPar9J0O8bRmITeMkzM.png"
      ],
      "publishedAt": "2025-04-01T14:12:14.000Z",
      "submittedOnDailyAt": "2025-04-03T00:13:20.491Z",
      "title": "ScholarCopilot: Training Large Language Models for Academic Writing with\n  Accurate Citations",
      "submittedOnDailyBy": {
        "_id": "6313a86154e6e5d9f0f94e04",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1662232951344-6313a86154e6e5d9f0f94e04.jpeg",
        "isPro": false,
        "fullname": "Wenhu Chen",
        "user": "wenhu",
        "type": "user"
      },
      "summary": "Academic writing requires both coherent text generation and precise citation\nof relevant literature. Although recent Retrieval-Augmented Generation (RAG)\nsystems have significantly improved factual accuracy in general-purpose text\ngeneration, their capacity to adequately support professional academic writing\nremains limited. In this work, we introduce ScholarCopilot, a unified framework\ndesigned to enhance existing large language models for generating professional\nacademic articles with accurate and contextually relevant citations.\nScholarCopilot dynamically determines when to retrieve scholarly references by\ngenerating a retrieval token [RET], and then utilizes its representation to\nlook up relevant citations from a database. The retrieved references are fed\ninto the model to augment the generation process. We jointly optimize both the\ngeneration and citation tasks within a single framework to increase efficiency.\nTrained on 500K papers from arXiv, our model achieves a top-1 retrieval\naccuracy of 40.1% on our evaluation dataset, outperforming baselines such as\nE5-Mistral-7B-Instruct (15.0%) and BM25 (9.8%). On a dataset of 1,000 academic\nwriting samples, ScholarCopilot scores 16.2/25 in generation quality (measured\nacross relevance, coherence, academic rigor, completeness, and innovation),\nsurpassing models with 10x more parameters such as Qwen-2.5-72B-Instruct\n(15.8/25). Human studies also confirm ScholarCopilot's superior performance in\ncitation recall, writing efficiency, and overall user experience, confirming\nthe effectiveness of our approach.",
      "upvotes": 5,
      "discussionId": "67ede79e21d7e74ee3e2838c",
      "githubRepo": "https://github.com/TIGER-AI-Lab/ScholarCopilot",
      "ai_keywords": [
        "Retrieval-Augmented Generation (RAG)",
        "ScholarCopilot",
        "large language models",
        "retrieval token [RET]",
        "scholarly references",
        "top-1 retrieval accuracy",
        "arXiv",
        "generation quality",
        "relevance",
        "coherence",
        "academic rigor",
        "completeness",
        "innovation",
        "citation recall",
        "writing efficiency",
        "user experience"
      ]
    },
    "publishedAt": "2025-04-01T10:12:14.000Z",
    "title": "ScholarCopilot: Training Large Language Models for Academic Writing with\n  Accurate Citations",
    "summary": "Academic writing requires both coherent text generation and precise citation\nof relevant literature. Although recent Retrieval-Augmented Generation (RAG)\nsystems have significantly improved factual accuracy in general-purpose text\ngeneration, their capacity to adequately support professional academic writing\nremains limited. In this work, we introduce ScholarCopilot, a unified framework\ndesigned to enhance existing large language models for generating professional\nacademic articles with accurate and contextually relevant citations.\nScholarCopilot dynamically determines when to retrieve scholarly references by\ngenerating a retrieval token [RET], and then utilizes its representation to\nlook up relevant citations from a database. The retrieved references are fed\ninto the model to augment the generation process. We jointly optimize both the\ngeneration and citation tasks within a single framework to increase efficiency.\nTrained on 500K papers from arXiv, our model achieves a top-1 retrieval\naccuracy of 40.1% on our evaluation dataset, outperforming baselines such as\nE5-Mistral-7B-Instruct (15.0%) and BM25 (9.8%). On a dataset of 1,000 academic\nwriting samples, ScholarCopilot scores 16.2/25 in generation quality (measured\nacross relevance, coherence, academic rigor, completeness, and innovation),\nsurpassing models with 10x more parameters such as Qwen-2.5-72B-Instruct\n(15.8/25). Human studies also confirm ScholarCopilot's superior performance in\ncitation recall, writing efficiency, and overall user experience, confirming\nthe effectiveness of our approach.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6313a86154e6e5d9f0f94e04/uqsPar9J0O8bRmITeMkzM.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.00824.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6313a86154e6e5d9f0f94e04",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1662232951344-6313a86154e6e5d9f0f94e04.jpeg",
      "fullname": "Wenhu Chen",
      "name": "wenhu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 36
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.01308",
      "authors": [
        {
          "_id": "67ede544ed9c94861b82b29f",
          "name": "Jiawei Wang",
          "hidden": false
        },
        {
          "_id": "67ede544ed9c94861b82b2a0",
          "name": "Yushen Zuo",
          "hidden": false
        },
        {
          "_id": "67ede544ed9c94861b82b2a1",
          "name": "Yuanjun Chai",
          "hidden": false
        },
        {
          "_id": "67ede544ed9c94861b82b2a2",
          "name": "Zhendong Liu",
          "hidden": false
        },
        {
          "_id": "67ede544ed9c94861b82b2a3",
          "user": {
            "_id": "6528ce81598467feb33d992d",
            "avatarUrl": "/avatars/e98a7bf16e6fd5118e861d562f93bb9b.svg",
            "isPro": false,
            "fullname": "Yicheng Fu",
            "user": "sofyc",
            "type": "user"
          },
          "name": "Yichen Fu",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-04-03T01:32:57.886Z",
          "hidden": false
        },
        {
          "_id": "67ede544ed9c94861b82b2a4",
          "name": "Yichun Feng",
          "hidden": false
        },
        {
          "_id": "67ede544ed9c94861b82b2a5",
          "name": "Kin-man Lam",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-02T02:35:19.000Z",
      "submittedOnDailyAt": "2025-04-03T00:10:56.307Z",
      "title": "Safeguarding Vision-Language Models: Mitigating Vulnerabilities to\n  Gaussian Noise in Perturbation-based Attacks",
      "submittedOnDailyBy": {
        "_id": "64060b49a577649430bf6974",
        "avatarUrl": "/avatars/74d0d6ed656b593e4c101b09edf18c7a.svg",
        "isPro": false,
        "fullname": "Jiawei Wang",
        "user": "Jarvis1111",
        "type": "user"
      },
      "summary": "Vision-Language Models (VLMs) extend the capabilities of Large Language\nModels (LLMs) by incorporating visual information, yet they remain vulnerable\nto jailbreak attacks, especially when processing noisy or corrupted images.\nAlthough existing VLMs adopt security measures during training to mitigate such\nattacks, vulnerabilities associated with noise-augmented visual inputs are\noverlooked. In this work, we identify that missing noise-augmented training\ncauses critical security gaps: many VLMs are susceptible to even simple\nperturbations such as Gaussian noise. To address this challenge, we propose\nRobust-VLGuard, a multimodal safety dataset with aligned / misaligned\nimage-text pairs, combined with noise-augmented fine-tuning that reduces attack\nsuccess rates while preserving functionality of VLM. For stronger\noptimization-based visual perturbation attacks, we propose DiffPure-VLM,\nleveraging diffusion models to convert adversarial perturbations into\nGaussian-like noise, which can be defended by VLMs with noise-augmented safety\nfine-tuning. Experimental results demonstrate that the distribution-shifting\nproperty of diffusion model aligns well with our fine-tuned VLMs, significantly\nmitigating adversarial perturbations across varying intensities. The dataset\nand code are available at https://github.com/JarvisUSTC/DiffPure-RobustVLM.",
      "upvotes": 4,
      "discussionId": "67ede549ed9c94861b82b433",
      "githubRepo": "https://github.com/JarvisUSTC/DiffPure-RobustVLM",
      "ai_keywords": [
        "Vision-Language Models (VLMs)",
        "Large Language Models (LLMs)",
        "noise-augmented training",
        "Gaussian noise",
        "Robust-VLGuard",
        "multimodal safety dataset",
        "aligned / misaligned image-text pairs",
        "noise-augmented fine-tuning",
        "diffusion models",
        "DiffPure-VLM",
        "diffusion model",
        "distribution-shifting property",
        "adversarial perturbations"
      ]
    },
    "publishedAt": "2025-04-01T22:35:19.000Z",
    "title": "Safeguarding Vision-Language Models: Mitigating Vulnerabilities to\n  Gaussian Noise in Perturbation-based Attacks",
    "summary": "Vision-Language Models (VLMs) extend the capabilities of Large Language\nModels (LLMs) by incorporating visual information, yet they remain vulnerable\nto jailbreak attacks, especially when processing noisy or corrupted images.\nAlthough existing VLMs adopt security measures during training to mitigate such\nattacks, vulnerabilities associated with noise-augmented visual inputs are\noverlooked. In this work, we identify that missing noise-augmented training\ncauses critical security gaps: many VLMs are susceptible to even simple\nperturbations such as Gaussian noise. To address this challenge, we propose\nRobust-VLGuard, a multimodal safety dataset with aligned / misaligned\nimage-text pairs, combined with noise-augmented fine-tuning that reduces attack\nsuccess rates while preserving functionality of VLM. For stronger\noptimization-based visual perturbation attacks, we propose DiffPure-VLM,\nleveraging diffusion models to convert adversarial perturbations into\nGaussian-like noise, which can be defended by VLMs with noise-augmented safety\nfine-tuning. Experimental results demonstrate that the distribution-shifting\nproperty of diffusion model aligns well with our fine-tuned VLMs, significantly\nmitigating adversarial perturbations across varying intensities. The dataset\nand code are available at https://github.com/JarvisUSTC/DiffPure-RobustVLM.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.01308.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64060b49a577649430bf6974",
      "avatarUrl": "/avatars/74d0d6ed656b593e4c101b09edf18c7a.svg",
      "fullname": "Jiawei Wang",
      "name": "Jarvis1111",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.23135",
      "authors": [
        {
          "_id": "67eb3d2110032c28d1ea109f",
          "user": {
            "_id": "628ece6054698ce61d1e7be3",
            "avatarUrl": "/avatars/c6ab33843fd0d8ef003650c1094214c0.svg",
            "isPro": false,
            "fullname": "Ao Wang",
            "user": "jameslahm",
            "type": "user"
          },
          "name": "Ao Wang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-02T16:11:10.373Z",
          "hidden": false
        },
        {
          "_id": "67eb3d2110032c28d1ea10a0",
          "name": "Hui Chen",
          "hidden": false
        },
        {
          "_id": "67eb3d2110032c28d1ea10a1",
          "name": "Zijia Lin",
          "hidden": false
        },
        {
          "_id": "67eb3d2110032c28d1ea10a2",
          "name": "Jungong Han",
          "hidden": false
        },
        {
          "_id": "67eb3d2110032c28d1ea10a3",
          "name": "Guiguang Ding",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/628ece6054698ce61d1e7be3/5E8ul0VN002EtCsoCp3mt.png"
      ],
      "publishedAt": "2025-03-29T16:00:54.000Z",
      "submittedOnDailyAt": "2025-04-03T00:26:03.944Z",
      "title": "LSNet: See Large, Focus Small",
      "submittedOnDailyBy": {
        "_id": "628ece6054698ce61d1e7be3",
        "avatarUrl": "/avatars/c6ab33843fd0d8ef003650c1094214c0.svg",
        "isPro": false,
        "fullname": "Ao Wang",
        "user": "jameslahm",
        "type": "user"
      },
      "summary": "Vision network designs, including Convolutional Neural Networks and Vision\nTransformers, have significantly advanced the field of computer vision. Yet,\ntheir complex computations pose challenges for practical deployments,\nparticularly in real-time applications. To tackle this issue, researchers have\nexplored various lightweight and efficient network designs. However, existing\nlightweight models predominantly leverage self-attention mechanisms and\nconvolutions for token mixing. This dependence brings limitations in\neffectiveness and efficiency in the perception and aggregation processes of\nlightweight networks, hindering the balance between performance and efficiency\nunder limited computational budgets. In this paper, we draw inspiration from\nthe dynamic heteroscale vision ability inherent in the efficient human vision\nsystem and propose a ``See Large, Focus Small'' strategy for lightweight vision\nnetwork design. We introduce LS (Large-Small) convolution,\nwhich combines large-kernel perception and small-kernel aggregation. It can\nefficiently capture a wide range of perceptual information and achieve precise\nfeature aggregation for dynamic and complex visual representations, thus\nenabling proficient processing of visual information. Based on LS convolution,\nwe present LSNet, a new family of lightweight models. Extensive experiments\ndemonstrate that LSNet achieves superior performance and efficiency over\nexisting lightweight networks in various vision tasks. Codes and models are\navailable at https://github.com/jameslahm/lsnet.",
      "upvotes": 2,
      "discussionId": "67eb3d2310032c28d1ea1108",
      "projectPage": "https://github.com/THU-MIG/lsnet",
      "githubRepo": "https://github.com/THU-MIG/lsnet",
      "ai_keywords": [
        "Convolutional Neural Networks",
        "Vision Transformers",
        "lightweight and efficient network designs",
        "self-attention mechanisms",
        "token mixing",
        "small-kernel aggregation",
        "dynamic heteroscale vision ability",
        "human vision system",
        "``See Large, Focus Small'' strategy",
        "LS (\\textbf{L}arge-\\textbf{S}mall) convolution",
        "large-kernel perception",
        "precise feature aggregation",
        "visual representations",
        "efficient processing of visual information",
        "LSNet",
        "superior performance",
        "efficiency"
      ]
    },
    "publishedAt": "2025-03-29T12:00:54.000Z",
    "title": "LSNet: See Large, Focus Small",
    "summary": "Vision network designs, including Convolutional Neural Networks and Vision\nTransformers, have significantly advanced the field of computer vision. Yet,\ntheir complex computations pose challenges for practical deployments,\nparticularly in real-time applications. To tackle this issue, researchers have\nexplored various lightweight and efficient network designs. However, existing\nlightweight models predominantly leverage self-attention mechanisms and\nconvolutions for token mixing. This dependence brings limitations in\neffectiveness and efficiency in the perception and aggregation processes of\nlightweight networks, hindering the balance between performance and efficiency\nunder limited computational budgets. In this paper, we draw inspiration from\nthe dynamic heteroscale vision ability inherent in the efficient human vision\nsystem and propose a ``See Large, Focus Small'' strategy for lightweight vision\nnetwork design. We introduce LS (Large-Small) convolution,\nwhich combines large-kernel perception and small-kernel aggregation. It can\nefficiently capture a wide range of perceptual information and achieve precise\nfeature aggregation for dynamic and complex visual representations, thus\nenabling proficient processing of visual information. Based on LS convolution,\nwe present LSNet, a new family of lightweight models. Extensive experiments\ndemonstrate that LSNet achieves superior performance and efficiency over\nexisting lightweight networks in various vision tasks. Codes and models are\navailable at https://github.com/jameslahm/lsnet.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/628ece6054698ce61d1e7be3/5E8ul0VN002EtCsoCp3mt.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.23135.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "628ece6054698ce61d1e7be3",
      "avatarUrl": "/avatars/c6ab33843fd0d8ef003650c1094214c0.svg",
      "fullname": "Ao Wang",
      "name": "jameslahm",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 26
    },
    "isAuthorParticipating": true
  }
]