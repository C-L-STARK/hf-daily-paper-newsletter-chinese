[
  {
    "paper": {
      "id": "2510.06217",
      "authors": [
        {
          "_id": "68e5bbf7975ac4c405ef200c",
          "name": "Jiaru Zou",
          "hidden": false
        },
        {
          "_id": "68e5bbf7975ac4c405ef200d",
          "name": "Soumya Roy",
          "hidden": false
        },
        {
          "_id": "68e5bbf7975ac4c405ef200e",
          "name": "Vinay Kumar Verma",
          "hidden": false
        },
        {
          "_id": "68e5bbf7975ac4c405ef200f",
          "name": "Ziyi Wang",
          "hidden": false
        },
        {
          "_id": "68e5bbf7975ac4c405ef2010",
          "name": "David Wipf",
          "hidden": false
        },
        {
          "_id": "68e5bbf7975ac4c405ef2011",
          "name": "Pan Lu",
          "hidden": false
        },
        {
          "_id": "68e5bbf7975ac4c405ef2012",
          "name": "Sumit Negi",
          "hidden": false
        },
        {
          "_id": "68e5bbf7975ac4c405ef2013",
          "name": "James Zou",
          "hidden": false
        },
        {
          "_id": "68e5bbf7975ac4c405ef2014",
          "name": "Jingrui He",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-07T17:59:41.000Z",
      "submittedOnDailyAt": "2025-10-08T00:00:51.373Z",
      "title": "TaTToo: Tool-Grounded Thinking PRM for Test-Time Scaling in Tabular\n  Reasoning",
      "submittedOnDailyBy": {
        "_id": "65c288280aa2d53135734a42",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65c288280aa2d53135734a42/5WHmau52EaRS01TOMI3Qg.jpeg",
        "isPro": false,
        "fullname": "Jiaru Zou",
        "user": "jiaruz2",
        "type": "user"
      },
      "summary": "Process Reward Models (PRMs) have recently emerged as a powerful framework\nfor enhancing the reasoning capabilities of large reasoning models (LRMs),\nparticularly in the context of test-time scaling (TTS). However, their\npotential for supervising LRMs on tabular reasoning domains remains\nunderexplored. Through detailed empirical analyses, we identify that existing\nPRMs, though widely adopted for supervising text-only reasoning steps, struggle\nwith table-specific operations such as sub-table retrieval and schema\ninteraction, leading to critical performance bottlenecks. To address this\nlimitation, we propose TaTToo, a novel table-grounded PRM framework that (i)\nreasons explicitly over tabular reasoning steps and (ii) integrates tool-based\nverification to provide precise reward supervision. Concretely, we first design\na scalable data curation pipeline that constructs over 60k high-quality\nstep-level annotations by integrating table verification rationales with\ntool-based executions. Building on the collected data, we train TaTToo with a\ndual-stage paradigm: cold-start supervised fine-tuning to capture tool-use\nreasoning patterns, followed by reinforcement learning with tool-grounded\nreward shaping to align our model with table-based verification. We provide a\ncomprehensive evaluation of the policy improvement induced by our newly\ndesigned PRM. Across 5 challenging tabular reasoning benchmarks covering\nnumerical reasoning, fact-checking, and data analysis, TaTToo improves\ndownstream policy LRMs by 30.9% at inference, surpasses strong PRM baselines\nsuch as Qwen-2.5-Math-PRM-72B with only 8B parameters, and demonstrates strong\ngeneralizability across diverse TTS strategies.",
      "upvotes": 14,
      "discussionId": "68e5bbf8975ac4c405ef2015",
      "ai_summary": "TaTToo, a novel table-grounded Process Reward Model, enhances tabular reasoning by explicitly addressing table-specific operations and integrating tool-based verification, leading to significant performance improvements over existing PRMs.",
      "ai_keywords": [
        "Process Reward Models",
        "large reasoning models",
        "test-time scaling",
        "tabular reasoning",
        "sub-table retrieval",
        "schema interaction",
        "TaTToo",
        "data curation pipeline",
        "step-level annotations",
        "tool-based verification",
        "dual-stage paradigm",
        "cold-start supervised fine-tuning",
        "reinforcement learning",
        "reward shaping",
        "policy improvement",
        "numerical reasoning",
        "fact-checking",
        "data analysis",
        "generalizability"
      ],
      "organization": {
        "_id": "5ffdfbadbba2ae614d771970",
        "name": "amazon",
        "fullname": "Amazon",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66f19ed428ae41c20c470792/8y7msN6A6W82LdQhQd85a.png"
      }
    },
    "publishedAt": "2025-10-07T13:59:41.000Z",
    "title": "TaTToo: Tool-Grounded Thinking PRM for Test-Time Scaling in Tabular\n  Reasoning",
    "summary": "Process Reward Models (PRMs) have recently emerged as a powerful framework\nfor enhancing the reasoning capabilities of large reasoning models (LRMs),\nparticularly in the context of test-time scaling (TTS). However, their\npotential for supervising LRMs on tabular reasoning domains remains\nunderexplored. Through detailed empirical analyses, we identify that existing\nPRMs, though widely adopted for supervising text-only reasoning steps, struggle\nwith table-specific operations such as sub-table retrieval and schema\ninteraction, leading to critical performance bottlenecks. To address this\nlimitation, we propose TaTToo, a novel table-grounded PRM framework that (i)\nreasons explicitly over tabular reasoning steps and (ii) integrates tool-based\nverification to provide precise reward supervision. Concretely, we first design\na scalable data curation pipeline that constructs over 60k high-quality\nstep-level annotations by integrating table verification rationales with\ntool-based executions. Building on the collected data, we train TaTToo with a\ndual-stage paradigm: cold-start supervised fine-tuning to capture tool-use\nreasoning patterns, followed by reinforcement learning with tool-grounded\nreward shaping to align our model with table-based verification. We provide a\ncomprehensive evaluation of the policy improvement induced by our newly\ndesigned PRM. Across 5 challenging tabular reasoning benchmarks covering\nnumerical reasoning, fact-checking, and data analysis, TaTToo improves\ndownstream policy LRMs by 30.9% at inference, surpasses strong PRM baselines\nsuch as Qwen-2.5-Math-PRM-72B with only 8B parameters, and demonstrates strong\ngeneralizability across diverse TTS strategies.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.06217.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "65c288280aa2d53135734a42",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65c288280aa2d53135734a42/5WHmau52EaRS01TOMI3Qg.jpeg",
      "fullname": "Jiaru Zou",
      "name": "jiaruz2",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "organization": {
      "_id": "5ffdfbadbba2ae614d771970",
      "name": "amazon",
      "fullname": "Amazon",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66f19ed428ae41c20c470792/8y7msN6A6W82LdQhQd85a.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2509.26328",
      "authors": [
        {
          "_id": "68e1e6d873e20ab577841e18",
          "name": "Chengyue Wu",
          "hidden": false
        },
        {
          "_id": "68e1e6d873e20ab577841e19",
          "name": "Hao Zhang",
          "hidden": false
        },
        {
          "_id": "68e1e6d873e20ab577841e1a",
          "name": "Shuchen Xue",
          "hidden": false
        },
        {
          "_id": "68e1e6d873e20ab577841e1b",
          "name": "Shizhe Diao",
          "hidden": false
        },
        {
          "_id": "68e1e6d873e20ab577841e1c",
          "name": "Yonggan Fu",
          "hidden": false
        },
        {
          "_id": "68e1e6d873e20ab577841e1d",
          "name": "Zhijian Liu",
          "hidden": false
        },
        {
          "_id": "68e1e6d873e20ab577841e1e",
          "name": "Pavlo Molchanov",
          "hidden": false
        },
        {
          "_id": "68e1e6d873e20ab577841e1f",
          "name": "Ping Luo",
          "hidden": false
        },
        {
          "_id": "68e1e6d873e20ab577841e20",
          "name": "Song Han",
          "hidden": false
        },
        {
          "_id": "68e1e6d873e20ab577841e21",
          "name": "Enze Xie",
          "hidden": false
        }
      ],
      "publishedAt": "2025-09-30T14:40:18.000Z",
      "submittedOnDailyAt": "2025-10-08T00:26:14.372Z",
      "title": "Fast-dLLM v2: Efficient Block-Diffusion LLM",
      "submittedOnDailyBy": {
        "_id": "617526c9de8feb54b0ce45ad",
        "avatarUrl": "/avatars/7faf8c6f71fc318a0113d780d376c381.svg",
        "isPro": false,
        "fullname": "Wu Chengyue",
        "user": "WuChengyue",
        "type": "user"
      },
      "summary": "Autoregressive (AR) large language models (LLMs) have achieved remarkable\nperformance across a wide range of natural language tasks, yet their inherent\nsequential decoding limits inference efficiency. In this work, we propose\nFast-dLLM v2, a carefully designed block diffusion language model (dLLM) that\nefficiently adapts pretrained AR models into dLLMs for parallel text\ngeneration, requiring only approximately 1B tokens of fine-tuning. This\nrepresents a 500x reduction in training data compared to full-attention\ndiffusion LLMs such as Dream (580B tokens), while preserving the original\nmodel's performance. Our approach introduces a novel training recipe that\ncombines a block diffusion mechanism with a complementary attention mask,\nenabling blockwise bidirectional context modeling without sacrificing AR\ntraining objectives. To further accelerate decoding, we design a hierarchical\ncaching mechanism: a block-level cache that stores historical context\nrepresentations across blocks, and a sub-block cache that enables efficient\nparallel generation within partially decoded blocks. Coupled with our parallel\ndecoding pipeline, Fast-dLLM v2 achieves up to 2.5x speedup over standard AR\ndecoding without compromising generation quality. Extensive experiments across\ndiverse benchmarks demonstrate that Fast-dLLM v2 matches or surpasses AR\nbaselines in accuracy, while delivering state-of-the-art efficiency among dLLMs\n- marking a significant step toward the practical deployment of fast and\naccurate LLMs. Code and model will be publicly released.",
      "upvotes": 8,
      "discussionId": "68e1e6d873e20ab577841e22",
      "projectPage": "https://nvlabs.github.io/Fast-dLLM/v2/",
      "githubRepo": "https://github.com/NVlabs/Fast-dLLM",
      "ai_summary": "Fast-dLLM v2, a block diffusion language model, efficiently converts pretrained autoregressive models for parallel text generation, achieving significant speedup without compromising accuracy.",
      "ai_keywords": [
        "autoregressive models",
        "large language models",
        "diffusion language models",
        "block diffusion mechanism",
        "attention mask",
        "blockwise bidirectional context modeling",
        "hierarchical caching",
        "block-level cache",
        "sub-block cache",
        "parallel decoding pipeline"
      ],
      "githubStars": 503,
      "organization": {
        "_id": "60262b67268c201cdc8b7d43",
        "name": "nvidia",
        "fullname": "NVIDIA",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"
      }
    },
    "publishedAt": "2025-09-30T10:40:18.000Z",
    "title": "Fast-dLLM v2: Efficient Block-Diffusion LLM",
    "summary": "Autoregressive (AR) large language models (LLMs) have achieved remarkable\nperformance across a wide range of natural language tasks, yet their inherent\nsequential decoding limits inference efficiency. In this work, we propose\nFast-dLLM v2, a carefully designed block diffusion language model (dLLM) that\nefficiently adapts pretrained AR models into dLLMs for parallel text\ngeneration, requiring only approximately 1B tokens of fine-tuning. This\nrepresents a 500x reduction in training data compared to full-attention\ndiffusion LLMs such as Dream (580B tokens), while preserving the original\nmodel's performance. Our approach introduces a novel training recipe that\ncombines a block diffusion mechanism with a complementary attention mask,\nenabling blockwise bidirectional context modeling without sacrificing AR\ntraining objectives. To further accelerate decoding, we design a hierarchical\ncaching mechanism: a block-level cache that stores historical context\nrepresentations across blocks, and a sub-block cache that enables efficient\nparallel generation within partially decoded blocks. Coupled with our parallel\ndecoding pipeline, Fast-dLLM v2 achieves up to 2.5x speedup over standard AR\ndecoding without compromising generation quality. Extensive experiments across\ndiverse benchmarks demonstrate that Fast-dLLM v2 matches or surpasses AR\nbaselines in accuracy, while delivering state-of-the-art efficiency among dLLMs\n- marking a significant step toward the practical deployment of fast and\naccurate LLMs. Code and model will be publicly released.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.26328.png",
    "numComments": 4,
    "submittedBy": {
      "_id": "617526c9de8feb54b0ce45ad",
      "avatarUrl": "/avatars/7faf8c6f71fc318a0113d780d376c381.svg",
      "fullname": "Wu Chengyue",
      "name": "WuChengyue",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 161
    },
    "organization": {
      "_id": "60262b67268c201cdc8b7d43",
      "name": "nvidia",
      "fullname": "NVIDIA",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.03270",
      "authors": [
        {
          "_id": "68e4787be4e093a7044e4ceb",
          "name": "Haolin Chen",
          "hidden": false
        },
        {
          "_id": "68e4787be4e093a7044e4cec",
          "name": "Shiyu Wang",
          "hidden": false
        },
        {
          "_id": "68e4787be4e093a7044e4ced",
          "name": "Can Qin",
          "hidden": false
        },
        {
          "_id": "68e4787be4e093a7044e4cee",
          "name": "Bo Pang",
          "hidden": false
        },
        {
          "_id": "68e4787be4e093a7044e4cef",
          "name": "Zuxin Liu",
          "hidden": false
        },
        {
          "_id": "68e4787be4e093a7044e4cf0",
          "name": "Jielin Qiu",
          "hidden": false
        },
        {
          "_id": "68e4787be4e093a7044e4cf1",
          "name": "Jianguo Zhang",
          "hidden": false
        },
        {
          "_id": "68e4787be4e093a7044e4cf2",
          "name": "Yingbo Zhou",
          "hidden": false
        },
        {
          "_id": "68e4787be4e093a7044e4cf3",
          "name": "Zeyuan Chen",
          "hidden": false
        },
        {
          "_id": "68e4787be4e093a7044e4cf4",
          "name": "Ran Xu",
          "hidden": false
        },
        {
          "_id": "68e4787be4e093a7044e4cf5",
          "name": "Shelby Heinecke",
          "hidden": false
        },
        {
          "_id": "68e4787be4e093a7044e4cf6",
          "name": "Silvio Savarese",
          "hidden": false
        },
        {
          "_id": "68e4787be4e093a7044e4cf7",
          "name": "Caiming Xiong",
          "hidden": false
        },
        {
          "_id": "68e4787be4e093a7044e4cf8",
          "name": "Huan Wang",
          "hidden": false
        },
        {
          "_id": "68e4787be4e093a7044e4cf9",
          "name": "Weiran Yao",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/632cdea254e2c512c8f95b12/Hce9DQBwDnat1pLoOeZk4.mp4",
        "https://cdn-uploads.huggingface.co/production/uploads/632cdea254e2c512c8f95b12/21y3b_qaKDlIi-Os-vaKD.png"
      ],
      "publishedAt": "2025-09-27T05:41:55.000Z",
      "submittedOnDailyAt": "2025-10-08T00:27:46.813Z",
      "title": "CoDA: Coding LM via Diffusion Adaptation",
      "submittedOnDailyBy": {
        "_id": "632cdea254e2c512c8f95b12",
        "avatarUrl": "/avatars/a6d06cdd75861ae7d589f1343d81a5c5.svg",
        "isPro": false,
        "fullname": "Weiran Yao",
        "user": "weirayao",
        "type": "user"
      },
      "summary": "Diffusion language models promise bidirectional context and infilling\ncapabilities that autoregressive coders lack, yet practical systems remain\nheavyweight. We introduce CoDA, a 1.7B-parameter diffusion coder trained on TPU\nwith a fully open-source training pipeline. CoDA pairs large-scale diffusion\npre-training with code-centric mid-training and instruction tuning, enabling\nconfidence-guided sampling that keeps inference latency competitive. On\nHumaneval, MBPP, and EvalPlus, CoDA-1.7B-Instruct matches or surpasses\ndiffusion models up to 7B parameters. Our release includes model checkpoints,\nevaluation harnesses, and TPU training pipelines to accelerate research on\nlightweight diffusion-based coding assistants.",
      "upvotes": 6,
      "discussionId": "68e4787ce4e093a7044e4cfa",
      "projectPage": "https://huggingface.co/Salesforce/CoDA-v0-Instruct",
      "githubRepo": "https://github.com/SalesforceAIResearch/CoDA/",
      "ai_summary": "CoDA, a 1.7B-parameter diffusion coder, achieves competitive performance with smaller models through confidence-guided sampling and is released with open-source tools.",
      "ai_keywords": [
        "diffusion language models",
        "bidirectional context",
        "infilling capabilities",
        "autoregressive coders",
        "diffusion coder",
        "large-scale diffusion pre-training",
        "code-centric mid-training",
        "instruction tuning",
        "confidence-guided sampling",
        "inference latency",
        "Humaneval",
        "MBPP",
        "EvalPlus",
        "diffusion-based coding assistants"
      ],
      "githubStars": 7,
      "organization": {
        "_id": "5f6d64475e78cc6b0ed31e4c",
        "name": "Salesforce",
        "fullname": "Salesforce",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1602756670970-noauth.jpeg"
      }
    },
    "publishedAt": "2025-09-27T01:41:55.000Z",
    "title": "CoDA: Coding LM via Diffusion Adaptation",
    "summary": "Diffusion language models promise bidirectional context and infilling\ncapabilities that autoregressive coders lack, yet practical systems remain\nheavyweight. We introduce CoDA, a 1.7B-parameter diffusion coder trained on TPU\nwith a fully open-source training pipeline. CoDA pairs large-scale diffusion\npre-training with code-centric mid-training and instruction tuning, enabling\nconfidence-guided sampling that keeps inference latency competitive. On\nHumaneval, MBPP, and EvalPlus, CoDA-1.7B-Instruct matches or surpasses\ndiffusion models up to 7B parameters. Our release includes model checkpoints,\nevaluation harnesses, and TPU training pipelines to accelerate research on\nlightweight diffusion-based coding assistants.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/632cdea254e2c512c8f95b12/Hce9DQBwDnat1pLoOeZk4.mp4",
      "https://cdn-uploads.huggingface.co/production/uploads/632cdea254e2c512c8f95b12/21y3b_qaKDlIi-Os-vaKD.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.03270.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "632cdea254e2c512c8f95b12",
      "avatarUrl": "/avatars/a6d06cdd75861ae7d589f1343d81a5c5.svg",
      "fullname": "Weiran Yao",
      "name": "weirayao",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "organization": {
      "_id": "5f6d64475e78cc6b0ed31e4c",
      "name": "Salesforce",
      "fullname": "Salesforce",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1602756670970-noauth.jpeg"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.06218",
      "authors": [
        {
          "_id": "68e5c2e8975ac4c405ef20bd",
          "name": "Deheng Zhang",
          "hidden": false
        },
        {
          "_id": "68e5c2e8975ac4c405ef20be",
          "name": "Yuqian Fu",
          "hidden": false
        },
        {
          "_id": "68e5c2e8975ac4c405ef20bf",
          "name": "Runyi Yang",
          "hidden": false
        },
        {
          "_id": "68e5c2e8975ac4c405ef20c0",
          "name": "Yang Miao",
          "hidden": false
        },
        {
          "_id": "68e5c2e8975ac4c405ef20c1",
          "name": "Tianwen Qian",
          "hidden": false
        },
        {
          "_id": "68e5c2e8975ac4c405ef20c2",
          "name": "Xu Zheng",
          "hidden": false
        },
        {
          "_id": "68e5c2e8975ac4c405ef20c3",
          "name": "Guolei Sun",
          "hidden": false
        },
        {
          "_id": "68e5c2e8975ac4c405ef20c4",
          "name": "Ajad Chhatkuli",
          "hidden": false
        },
        {
          "_id": "68e5c2e8975ac4c405ef20c5",
          "name": "Xuanjing Huang",
          "hidden": false
        },
        {
          "_id": "68e5c2e8975ac4c405ef20c6",
          "name": "Yu-Gang Jiang",
          "hidden": false
        },
        {
          "_id": "68e5c2e8975ac4c405ef20c7",
          "name": "Luc Van Gool",
          "hidden": false
        },
        {
          "_id": "68e5c2e8975ac4c405ef20c8",
          "name": "Danda Pani Paudel",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-07T17:59:47.000Z",
      "submittedOnDailyAt": "2025-10-08T00:18:35.291Z",
      "title": "EgoNight: Towards Egocentric Vision Understanding at Night with a\n  Challenging Benchmark",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Most existing benchmarks for egocentric vision understanding focus primarily\non daytime scenarios, overlooking the low-light conditions that are inevitable\nin real-world applications. To investigate this gap, we present EgoNight, the\nfirst comprehensive benchmark for nighttime egocentric vision, with visual\nquestion answering (VQA) as the core task. A key feature of EgoNight is the\nintroduction of day-night aligned videos, which enhance night annotation\nquality using the daytime data and reveal clear performance gaps between\nlighting conditions. To achieve this, we collect both synthetic videos rendered\nby Blender and real-world recordings, ensuring that scenes and actions are\nvisually and temporally aligned. Leveraging these paired videos, we construct\nEgoNight-VQA, supported by a novel day-augmented night auto-labeling engine and\nrefinement through extensive human verification. Each QA pair is double-checked\nby annotators for reliability. In total, EgoNight-VQA contains 3658 QA pairs\nacross 90 videos, spanning 12 diverse QA types, with more than 300 hours of\nhuman work. Evaluations of state-of-the-art multimodal large language models\n(MLLMs) reveal substantial performance drops when transferring from day to\nnight, underscoring the challenges of reasoning under low-light conditions.\nBeyond VQA, EgoNight also introduces two auxiliary tasks, day-night\ncorrespondence retrieval and egocentric depth estimation at night, that further\nexplore the boundaries of existing models. We believe EgoNight-VQA provides a\nstrong foundation for advancing application-driven egocentric vision research\nand for developing models that generalize across illumination domains. All the\ndata and code will be made available upon acceptance.",
      "upvotes": 1,
      "discussionId": "68e5c2e8975ac4c405ef20c9",
      "ai_summary": "EgoNight is a comprehensive benchmark for nighttime egocentric vision, focusing on visual question answering and revealing performance gaps between day and night conditions for multimodal large language models.",
      "ai_keywords": [
        "egocentric vision",
        "visual question answering",
        "day-night aligned videos",
        "synthetic videos",
        "real-world recordings",
        "day-augmented night auto-labeling",
        "multimodal large language models",
        "day-night correspondence retrieval",
        "egocentric depth estimation"
      ]
    },
    "publishedAt": "2025-10-07T13:59:47.000Z",
    "title": "EgoNight: Towards Egocentric Vision Understanding at Night with a\n  Challenging Benchmark",
    "summary": "Most existing benchmarks for egocentric vision understanding focus primarily\non daytime scenarios, overlooking the low-light conditions that are inevitable\nin real-world applications. To investigate this gap, we present EgoNight, the\nfirst comprehensive benchmark for nighttime egocentric vision, with visual\nquestion answering (VQA) as the core task. A key feature of EgoNight is the\nintroduction of day-night aligned videos, which enhance night annotation\nquality using the daytime data and reveal clear performance gaps between\nlighting conditions. To achieve this, we collect both synthetic videos rendered\nby Blender and real-world recordings, ensuring that scenes and actions are\nvisually and temporally aligned. Leveraging these paired videos, we construct\nEgoNight-VQA, supported by a novel day-augmented night auto-labeling engine and\nrefinement through extensive human verification. Each QA pair is double-checked\nby annotators for reliability. In total, EgoNight-VQA contains 3658 QA pairs\nacross 90 videos, spanning 12 diverse QA types, with more than 300 hours of\nhuman work. Evaluations of state-of-the-art multimodal large language models\n(MLLMs) reveal substantial performance drops when transferring from day to\nnight, underscoring the challenges of reasoning under low-light conditions.\nBeyond VQA, EgoNight also introduces two auxiliary tasks, day-night\ncorrespondence retrieval and egocentric depth estimation at night, that further\nexplore the boundaries of existing models. We believe EgoNight-VQA provides a\nstrong foundation for advancing application-driven egocentric vision research\nand for developing models that generalize across illumination domains. All the\ndata and code will be made available upon acceptance.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.06218.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 120
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.06139",
      "authors": [
        {
          "_id": "68e5c2b6975ac4c405ef20a8",
          "name": "Zanyi Wang",
          "hidden": false
        },
        {
          "_id": "68e5c2b6975ac4c405ef20a9",
          "name": "Dengyang Jiang",
          "hidden": false
        },
        {
          "_id": "68e5c2b6975ac4c405ef20aa",
          "name": "Liuzhuozheng Li",
          "hidden": false
        },
        {
          "_id": "68e5c2b6975ac4c405ef20ab",
          "name": "Sizhe Dang",
          "hidden": false
        },
        {
          "_id": "68e5c2b6975ac4c405ef20ac",
          "name": "Chengzu Li",
          "hidden": false
        },
        {
          "_id": "68e5c2b6975ac4c405ef20ad",
          "name": "Harry Yang",
          "hidden": false
        },
        {
          "_id": "68e5c2b6975ac4c405ef20ae",
          "name": "Guang Dai",
          "hidden": false
        },
        {
          "_id": "68e5c2b6975ac4c405ef20af",
          "name": "Mengmeng Wang",
          "hidden": false
        },
        {
          "_id": "68e5c2b6975ac4c405ef20b0",
          "name": "Jingdong Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-07T17:14:10.000Z",
      "submittedOnDailyAt": "2025-10-08T00:17:52.028Z",
      "title": "Deforming Videos to Masks: Flow Matching for Referring Video\n  Segmentation",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Referring Video Object Segmentation (RVOS) requires segmenting specific\nobjects in a video guided by a natural language description. The core challenge\nof RVOS is to anchor abstract linguistic concepts onto a specific set of pixels\nand continuously segment them through the complex dynamics of a video. Faced\nwith this difficulty, prior work has often decomposed the task into a pragmatic\n`locate-then-segment' pipeline. However, this cascaded design creates an\ninformation bottleneck by simplifying semantics into coarse geometric prompts\n(e.g, point), and struggles to maintain temporal consistency as the segmenting\nprocess is often decoupled from the initial language grounding. To overcome\nthese fundamental limitations, we propose FlowRVS, a novel framework that\nreconceptualizes RVOS as a conditional continuous flow problem. This allows us\nto harness the inherent strengths of pretrained T2V models, fine-grained pixel\ncontrol, text-video semantic alignment, and temporal coherence. Instead of\nconventional generating from noise to mask or directly predicting mask, we\nreformulate the task by learning a direct, language-guided deformation from a\nvideo's holistic representation to its target mask. Our one-stage, generative\napproach achieves new state-of-the-art results across all major RVOS\nbenchmarks. Specifically, achieving a J&F of 51.1 in\nMeViS (+1.6 over prior SOTA) and 73.3 in the zero shot Ref-DAVIS17 (+2.7),\ndemonstrating the significant potential of modeling video understanding tasks\nas continuous deformation processes.",
      "upvotes": 1,
      "discussionId": "68e5c2b7975ac4c405ef20b1",
      "ai_summary": "FlowRVS addresses the challenges of Referring Video Object Segmentation by reformulating the task as a continuous flow problem, leveraging pretrained T2V models and achieving state-of-the-art results.",
      "ai_keywords": [
        "Referring Video Object Segmentation",
        "RVOS",
        "natural language description",
        "pixel control",
        "text-video semantic alignment",
        "temporal coherence",
        "FlowRVS",
        "T2V models",
        "continuous flow problem",
        "generative approach",
        "$\\mathcal{J}\\&\\mathcal{F}$",
        "MeViS",
        "Ref-DAVIS17"
      ]
    },
    "publishedAt": "2025-10-07T13:14:10.000Z",
    "title": "Deforming Videos to Masks: Flow Matching for Referring Video\n  Segmentation",
    "summary": "Referring Video Object Segmentation (RVOS) requires segmenting specific\nobjects in a video guided by a natural language description. The core challenge\nof RVOS is to anchor abstract linguistic concepts onto a specific set of pixels\nand continuously segment them through the complex dynamics of a video. Faced\nwith this difficulty, prior work has often decomposed the task into a pragmatic\n`locate-then-segment' pipeline. However, this cascaded design creates an\ninformation bottleneck by simplifying semantics into coarse geometric prompts\n(e.g, point), and struggles to maintain temporal consistency as the segmenting\nprocess is often decoupled from the initial language grounding. To overcome\nthese fundamental limitations, we propose FlowRVS, a novel framework that\nreconceptualizes RVOS as a conditional continuous flow problem. This allows us\nto harness the inherent strengths of pretrained T2V models, fine-grained pixel\ncontrol, text-video semantic alignment, and temporal coherence. Instead of\nconventional generating from noise to mask or directly predicting mask, we\nreformulate the task by learning a direct, language-guided deformation from a\nvideo's holistic representation to its target mask. Our one-stage, generative\napproach achieves new state-of-the-art results across all major RVOS\nbenchmarks. Specifically, achieving a J&F of 51.1 in\nMeViS (+1.6 over prior SOTA) and 73.3 in the zero shot Ref-DAVIS17 (+2.7),\ndemonstrating the significant potential of modeling video understanding tasks\nas continuous deformation processes.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.06139.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 120
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.05432",
      "authors": [
        {
          "_id": "68e5c1aa975ac4c405ef20a0",
          "name": "Shambhavi Mishra",
          "hidden": false
        },
        {
          "_id": "68e5c1aa975ac4c405ef20a1",
          "name": "Gaurav Sahu",
          "hidden": false
        },
        {
          "_id": "68e5c1aa975ac4c405ef20a2",
          "name": "Marco Pedersoli",
          "hidden": false
        },
        {
          "_id": "68e5c1aa975ac4c405ef20a3",
          "name": "Laurent Charlin",
          "hidden": false
        },
        {
          "_id": "68e5c1aa975ac4c405ef20a4",
          "name": "Jose Dolz",
          "hidden": false
        },
        {
          "_id": "68e5c1aa975ac4c405ef20a5",
          "name": "Christopher Pal",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-06T22:50:41.000Z",
      "submittedOnDailyAt": "2025-10-08T00:13:51.660Z",
      "title": "AInstein: Assessing the Feasibility of AI-Generated Approaches to\n  Research Problems",
      "submittedOnDailyBy": {
        "_id": "6377ac12f5fe4a39f783b05d",
        "avatarUrl": "/avatars/5f8b6d999cf48dd4703bbd70236c38c8.svg",
        "isPro": false,
        "fullname": "G Sahu",
        "user": "demfier",
        "type": "user"
      },
      "summary": "Large language models (LLMs) demonstrate impressive capabilities across a\nwide range of tasks, yet it remains unclear whether such success reflects\ngenuine reasoning or sophisticated recall. We introduce AInstein, a framework\nfor testing whether LLMs can generate valid solutions to AI research problems\nusing only their pretrained parametric knowledge -- without domain-specific\nfine-tuning, retrieval augmentation, or other external aids. Our approach\nextracts distilled problem statements from high-quality ICLR 2025 submissions,\nthen tasks specialized solver agents with proposing and refining technical\nsolutions through iterative critique loops, mimicking the cycles of proposal,\nreview, and revision central to scientific inquiry. We evaluate AInstein on\n1,214 ICLR papers stratified by acceptance tier (Oral, Spotlight, Poster),\nusing an LLM-as-a-judge paradigm guided by a structured rubric, complemented by\ntargeted manual checks. Performance is assessed with three metrics: Success\nRate (does the solution address the problem?), Rediscovery (does it align with\nhuman-proposed methods?), and Novelty (does it yield valid, original\napproaches?). Our results reveal that while LLMs can rediscover feasible\nsolutions and occasionally propose creative alternatives, their problem-solving\nability remains fragile and highly sensitive to framing. These findings provide\nthe first large-scale evidence on the extent to which LLMs can act as\nautonomous scientific problem-solvers, highlighting both their latent potential\nand their current limitations.",
      "upvotes": 1,
      "discussionId": "68e5c1aa975ac4c405ef20a6",
      "ai_summary": "AInstein evaluates the problem-solving capabilities of large language models by testing their ability to generate valid solutions to AI research problems using only pretrained knowledge, revealing both their potential and limitations.",
      "ai_keywords": [
        "large language models",
        "LLMs",
        "AInstein",
        "pretrained parametric knowledge",
        "domain-specific fine-tuning",
        "retrieval augmentation",
        "ICLR 2025 submissions",
        "solver agents",
        "iterative critique loops",
        "scientific inquiry",
        "LLM-as-a-judge",
        "structured rubric",
        "targeted manual checks",
        "Success Rate",
        "Rediscovery",
        "Novelty"
      ]
    },
    "publishedAt": "2025-10-06T18:50:41.000Z",
    "title": "AInstein: Assessing the Feasibility of AI-Generated Approaches to\n  Research Problems",
    "summary": "Large language models (LLMs) demonstrate impressive capabilities across a\nwide range of tasks, yet it remains unclear whether such success reflects\ngenuine reasoning or sophisticated recall. We introduce AInstein, a framework\nfor testing whether LLMs can generate valid solutions to AI research problems\nusing only their pretrained parametric knowledge -- without domain-specific\nfine-tuning, retrieval augmentation, or other external aids. Our approach\nextracts distilled problem statements from high-quality ICLR 2025 submissions,\nthen tasks specialized solver agents with proposing and refining technical\nsolutions through iterative critique loops, mimicking the cycles of proposal,\nreview, and revision central to scientific inquiry. We evaluate AInstein on\n1,214 ICLR papers stratified by acceptance tier (Oral, Spotlight, Poster),\nusing an LLM-as-a-judge paradigm guided by a structured rubric, complemented by\ntargeted manual checks. Performance is assessed with three metrics: Success\nRate (does the solution address the problem?), Rediscovery (does it align with\nhuman-proposed methods?), and Novelty (does it yield valid, original\napproaches?). Our results reveal that while LLMs can rediscover feasible\nsolutions and occasionally propose creative alternatives, their problem-solving\nability remains fragile and highly sensitive to framing. These findings provide\nthe first large-scale evidence on the extent to which LLMs can act as\nautonomous scientific problem-solvers, highlighting both their latent potential\nand their current limitations.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.05432.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6377ac12f5fe4a39f783b05d",
      "avatarUrl": "/avatars/5f8b6d999cf48dd4703bbd70236c38c8.svg",
      "fullname": "G Sahu",
      "name": "demfier",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.05342",
      "authors": [
        {
          "_id": "68e5b6f9975ac4c405ef2003",
          "name": "Hyung Gyu Rho",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-06T20:09:37.000Z",
      "submittedOnDailyAt": "2025-10-08T00:16:22.419Z",
      "title": "Margin Adaptive DPO: Leveraging Reward Model for Granular Control in\n  Preference Optimization",
      "submittedOnDailyBy": {
        "_id": "6642dafed48363a46ddb69ed",
        "avatarUrl": "/avatars/111fb26ea38fb2e6f8470f7ed513d48d.svg",
        "isPro": false,
        "fullname": "hyung gyu rho",
        "user": "sirano1004",
        "type": "user"
      },
      "summary": "Direct Preference Optimization (DPO) has emerged as a simple and effective\nmethod for aligning large language models. However, its reliance on a fixed\ntemperature parameter leads to suboptimal training on diverse preference data,\ncausing overfitting on easy examples and under-learning from informative ones.\nRecent methods have emerged to counter this. While IPO addresses general\noverfitting, its uniform regularization can be overly conservative. The more\ntargeted approach of beta-DPO suffers from its own limitations: its\nbatch-level adaptation applies a single, compromised temperature to\nmixed-margin pairs, its linear update rule can produce unstable negative\nbeta values, and its filtering mechanism discards potentially useful\ntraining signals. In this work, we introduce Margin-Adaptive Direct Preference\nOptimization (MADPO), a method that provides a stable, data-preserving, and\ninstance-level solution. MADPO employs a practical two-step approach: it first\ntrains a reward model to estimate preference margins and then uses these\nmargins to apply a continuous, adaptive weight to the DPO loss for each\nindividual training sample. This re-weighting scheme creates an effective\ntarget margin that is amplified for hard pairs and dampened for easy pairs,\nallowing for granular control over the learning signal. We provide a\ncomprehensive theoretical analysis, proving that MADPO has a well-behaved\noptimization landscape and is robust to reward model estimation errors. We\nvalidate our theory with experiments on a sentiment generation task, where\nMADPO consistently and significantly outperforms strong baselines across\ndatasets of varying quality. It achieves performance gains of up to +33.3\\% on\nHigh Quality data and +10.5\\% on Low Quality data over the next-best method.\nOur results establish MADPO as a more robust and principled approach to\npreference alignment.",
      "upvotes": 1,
      "discussionId": "68e5b6f9975ac4c405ef2004",
      "githubRepo": "https://github.com/sirano1004/\nMargin-Apative-Direct-Preference-Optimization",
      "ai_summary": "MADPO, a margin-adaptive method, enhances preference alignment in large language models by providing instance-level adaptive weighting to the DPO loss, improving performance across datasets.",
      "ai_keywords": [
        "Direct Preference Optimization",
        "DPO",
        "IPO",
        "Î²-DPO",
        "reward model",
        "preference margins",
        "DPO loss",
        "optimization landscape",
        "reward model estimation errors",
        "sentiment generation task"
      ],
      "githubStars": 0
    },
    "publishedAt": "2025-10-06T16:09:37.000Z",
    "title": "Margin Adaptive DPO: Leveraging Reward Model for Granular Control in\n  Preference Optimization",
    "summary": "Direct Preference Optimization (DPO) has emerged as a simple and effective\nmethod for aligning large language models. However, its reliance on a fixed\ntemperature parameter leads to suboptimal training on diverse preference data,\ncausing overfitting on easy examples and under-learning from informative ones.\nRecent methods have emerged to counter this. While IPO addresses general\noverfitting, its uniform regularization can be overly conservative. The more\ntargeted approach of beta-DPO suffers from its own limitations: its\nbatch-level adaptation applies a single, compromised temperature to\nmixed-margin pairs, its linear update rule can produce unstable negative\nbeta values, and its filtering mechanism discards potentially useful\ntraining signals. In this work, we introduce Margin-Adaptive Direct Preference\nOptimization (MADPO), a method that provides a stable, data-preserving, and\ninstance-level solution. MADPO employs a practical two-step approach: it first\ntrains a reward model to estimate preference margins and then uses these\nmargins to apply a continuous, adaptive weight to the DPO loss for each\nindividual training sample. This re-weighting scheme creates an effective\ntarget margin that is amplified for hard pairs and dampened for easy pairs,\nallowing for granular control over the learning signal. We provide a\ncomprehensive theoretical analysis, proving that MADPO has a well-behaved\noptimization landscape and is robust to reward model estimation errors. We\nvalidate our theory with experiments on a sentiment generation task, where\nMADPO consistently and significantly outperforms strong baselines across\ndatasets of varying quality. It achieves performance gains of up to +33.3\\% on\nHigh Quality data and +10.5\\% on Low Quality data over the next-best method.\nOur results establish MADPO as a more robust and principled approach to\npreference alignment.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.05342.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6642dafed48363a46ddb69ed",
      "avatarUrl": "/avatars/111fb26ea38fb2e6f8470f7ed513d48d.svg",
      "fullname": "hyung gyu rho",
      "name": "sirano1004",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.04087",
      "authors": [
        {
          "_id": "68e4715be4e093a7044e4cce",
          "user": {
            "_id": "6642dafed48363a46ddb69ed",
            "avatarUrl": "/avatars/111fb26ea38fb2e6f8470f7ed513d48d.svg",
            "isPro": false,
            "fullname": "hyung gyu rho",
            "user": "sirano1004",
            "type": "user"
          },
          "name": "Hyung Gyu Rho",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-10-07T12:27:19.479Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-05T08:23:08.000Z",
      "submittedOnDailyAt": "2025-10-08T00:39:48.168Z",
      "title": "A Contextual Quality Reward Model for Reliable and Efficient Best-of-N\n  Sampling",
      "submittedOnDailyBy": {
        "_id": "6642dafed48363a46ddb69ed",
        "avatarUrl": "/avatars/111fb26ea38fb2e6f8470f7ed513d48d.svg",
        "isPro": false,
        "fullname": "hyung gyu rho",
        "user": "sirano1004",
        "type": "user"
      },
      "summary": "Modern preference alignment techniques, such as Best-of-N (BoN) sampling,\nrely on reward models trained with pairwise comparison data. While effective at\nlearning relative preferences, this paradigm fails to capture a signal of\nresponse acceptability, leaving systems vulnerable to selecting the least bad\nof many unacceptable options. This is particularly problematic for hard\nprompts, where the risk of such false acceptances increases with the number of\nsamples. In this paper, we address this critical reliability gap by introducing\na new data collection and modeling framework. By augmenting preference data\nwith an outside option, inspired by discrete choice models, we train a reward\nmodel that can distinguish not just what is better, but what is\ngood enough. We leverage this capability to create an adaptive\ninference strategy, best of mini-N in-loop, which partitions the generation\nbudget into sequential loops with a calibrated, early-exit condition. Our\nexperiments show that when tuned as an alignment guardrail, it reduces\nreliability failures by 70\\%, and when tuned as an inference accelerator, it\nimproves average inference speed by over 22\\% in IMDB-sentiment setting. We\nthus provide a principled and flexible framework for practitioners to\nexplicitly manage the trade-off between reliability and computational\nefficiency.",
      "upvotes": 1,
      "discussionId": "68e4715be4e093a7044e4ccf",
      "ai_summary": "A new framework using an outside option in preference data collection and modeling improves reliability and efficiency in preference alignment techniques.",
      "ai_keywords": [
        "Best-of-N (BoN) sampling",
        "reward models",
        "pairwise comparison data",
        "discrete choice models",
        "adaptive inference strategy",
        "best of mini-N in-loop",
        "reliability failures",
        "inference speed",
        "IMDB-sentiment setting"
      ]
    },
    "publishedAt": "2025-10-05T04:23:08.000Z",
    "title": "A Contextual Quality Reward Model for Reliable and Efficient Best-of-N\n  Sampling",
    "summary": "Modern preference alignment techniques, such as Best-of-N (BoN) sampling,\nrely on reward models trained with pairwise comparison data. While effective at\nlearning relative preferences, this paradigm fails to capture a signal of\nresponse acceptability, leaving systems vulnerable to selecting the least bad\nof many unacceptable options. This is particularly problematic for hard\nprompts, where the risk of such false acceptances increases with the number of\nsamples. In this paper, we address this critical reliability gap by introducing\na new data collection and modeling framework. By augmenting preference data\nwith an outside option, inspired by discrete choice models, we train a reward\nmodel that can distinguish not just what is better, but what is\ngood enough. We leverage this capability to create an adaptive\ninference strategy, best of mini-N in-loop, which partitions the generation\nbudget into sequential loops with a calibrated, early-exit condition. Our\nexperiments show that when tuned as an alignment guardrail, it reduces\nreliability failures by 70\\%, and when tuned as an inference accelerator, it\nimproves average inference speed by over 22\\% in IMDB-sentiment setting. We\nthus provide a principled and flexible framework for practitioners to\nexplicitly manage the trade-off between reliability and computational\nefficiency.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.04087.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6642dafed48363a46ddb69ed",
      "avatarUrl": "/avatars/111fb26ea38fb2e6f8470f7ed513d48d.svg",
      "fullname": "hyung gyu rho",
      "name": "sirano1004",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2510.05137",
      "authors": [
        {
          "_id": "68e5bde7975ac4c405ef2031",
          "name": "Maojia Song",
          "hidden": false
        },
        {
          "_id": "68e5bde7975ac4c405ef2032",
          "name": "Renhang Liu",
          "hidden": false
        },
        {
          "_id": "68e5bde7975ac4c405ef2033",
          "name": "Xinyu Wang",
          "hidden": false
        },
        {
          "_id": "68e5bde7975ac4c405ef2034",
          "name": "Yong Jiang",
          "hidden": false
        },
        {
          "_id": "68e5bde7975ac4c405ef2035",
          "name": "Pengjun Xie",
          "hidden": false
        },
        {
          "_id": "68e5bde7975ac4c405ef2036",
          "name": "Fei Huang",
          "hidden": false
        },
        {
          "_id": "68e5bde7975ac4c405ef2037",
          "name": "Soujanya Poria",
          "hidden": false
        },
        {
          "_id": "68e5bde7975ac4c405ef2038",
          "name": "Jingren Zhou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-01T07:59:03.000Z",
      "submittedOnDailyAt": "2025-10-08T00:02:07.560Z",
      "title": "Demystifying deep search: a holistic evaluation with hint-free multi-hop\n  questions and factorised metrics",
      "submittedOnDailyBy": {
        "_id": "626b626405fe1cb65725aca1",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/626b626405fe1cb65725aca1/ZVSbhynzpQhVGq9kGywW6.png",
        "isPro": false,
        "fullname": "Soujanya Poria",
        "user": "soujanyaporia",
        "type": "user"
      },
      "summary": "RAG (Retrieval-Augmented Generation) systems and web agents are increasingly\nevaluated on multi-hop deep search tasks, yet current practice suffers from two\nmajor limitations. First, most benchmarks leak the reasoning path in the\nquestion text, allowing models to follow surface cues rather than discover\nreasoning chains autonomously. Second, evaluation is typically reduced to a\nsingle pass rate, which collapses diverse behaviours into one score and\nobscures whether failures stem from inadequate search, poor knowledge use, or\ninappropriate refusal. To address these issues, we present WebDetective, a\nbenchmark of hint-free multi-hop questions paired with a controlled Wikipedia\nsandbox that ensures full traceability of model actions, and a holistic\nevaluation framework that separates search sufficiency, knowledge utilisation,\nand refusal behaviour. Our evaluation of 25 state-of-the-art models reveals\nsystematic weaknesses across all architectures: models struggle with knowledge\nutilisation despite having sufficient evidence and demonstrate near-absent\nappropriate refusal when evidence is lacking. These patterns expose a\nfundamental gap: today's systems excel at executing given reasoning paths but\nfail when required to discover them. We develop an agentic workflow,\nEvidenceLoop, that explicitly targets the challenges our benchmark identifies,\nincorporating verification loops and systematic evidence tracking that improve\nboth search and synthesis capabilities. This baseline demonstrates that\nWebDetective's diagnostic framework can guide concrete architectural\nimprovements, establishing our benchmark as a critical tool for developing\ngenuinely autonomous reasoning systems rather than pattern-following agents.",
      "upvotes": 0,
      "discussionId": "68e5bde7975ac4c405ef2039",
      "ai_summary": "WebDetective is a benchmark for evaluating multi-hop reasoning in RAG systems and web agents, addressing issues of reasoning path leakage and single-pass evaluation, and introducing a framework to improve knowledge utilization and refusal behavior.",
      "ai_keywords": [
        "RAG",
        "Retrieval-Augmented Generation",
        "multi-hop deep search",
        "reasoning path",
        "knowledge utilisation",
        "refusal behaviour",
        "Wikipedia sandbox",
        "EvidenceLoop",
        "verification loops",
        "evidence tracking"
      ],
      "organization": {
        "_id": "626ab9dac804c432c1b27a48",
        "name": "declare-lab",
        "fullname": "Deep Cognition and Language Research (DeCLaRe) Lab",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/626b626405fe1cb65725aca1/grq3rj2uj0WRjjPjAtR1I.png"
      }
    },
    "publishedAt": "2025-10-01T03:59:03.000Z",
    "title": "Demystifying deep search: a holistic evaluation with hint-free multi-hop\n  questions and factorised metrics",
    "summary": "RAG (Retrieval-Augmented Generation) systems and web agents are increasingly\nevaluated on multi-hop deep search tasks, yet current practice suffers from two\nmajor limitations. First, most benchmarks leak the reasoning path in the\nquestion text, allowing models to follow surface cues rather than discover\nreasoning chains autonomously. Second, evaluation is typically reduced to a\nsingle pass rate, which collapses diverse behaviours into one score and\nobscures whether failures stem from inadequate search, poor knowledge use, or\ninappropriate refusal. To address these issues, we present WebDetective, a\nbenchmark of hint-free multi-hop questions paired with a controlled Wikipedia\nsandbox that ensures full traceability of model actions, and a holistic\nevaluation framework that separates search sufficiency, knowledge utilisation,\nand refusal behaviour. Our evaluation of 25 state-of-the-art models reveals\nsystematic weaknesses across all architectures: models struggle with knowledge\nutilisation despite having sufficient evidence and demonstrate near-absent\nappropriate refusal when evidence is lacking. These patterns expose a\nfundamental gap: today's systems excel at executing given reasoning paths but\nfail when required to discover them. We develop an agentic workflow,\nEvidenceLoop, that explicitly targets the challenges our benchmark identifies,\nincorporating verification loops and systematic evidence tracking that improve\nboth search and synthesis capabilities. This baseline demonstrates that\nWebDetective's diagnostic framework can guide concrete architectural\nimprovements, establishing our benchmark as a critical tool for developing\ngenuinely autonomous reasoning systems rather than pattern-following agents.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.05137.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "626b626405fe1cb65725aca1",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/626b626405fe1cb65725aca1/ZVSbhynzpQhVGq9kGywW6.png",
      "fullname": "Soujanya Poria",
      "name": "soujanyaporia",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 9
    },
    "organization": {
      "_id": "626ab9dac804c432c1b27a48",
      "name": "declare-lab",
      "fullname": "Deep Cognition and Language Research (DeCLaRe) Lab",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/626b626405fe1cb65725aca1/grq3rj2uj0WRjjPjAtR1I.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.02341",
      "authors": [
        {
          "_id": "68e5c815975ac4c405ef20df",
          "name": "Yifan Wang",
          "hidden": false
        },
        {
          "_id": "68e5c815975ac4c405ef20e0",
          "name": "Bolian Li",
          "hidden": false
        },
        {
          "_id": "68e5c815975ac4c405ef20e1",
          "name": "Junlin Wu",
          "hidden": false
        },
        {
          "_id": "68e5c815975ac4c405ef20e2",
          "name": "Zhaoxuan Tan",
          "hidden": false
        },
        {
          "_id": "68e5c815975ac4c405ef20e3",
          "name": "Zheli Liu",
          "hidden": false
        },
        {
          "_id": "68e5c815975ac4c405ef20e4",
          "name": "Ruqi Zhang",
          "hidden": false
        },
        {
          "_id": "68e5c815975ac4c405ef20e5",
          "name": "Ananth Grama",
          "hidden": false
        },
        {
          "_id": "68e5c815975ac4c405ef20e6",
          "name": "Qingkai Zeng",
          "hidden": false
        }
      ],
      "publishedAt": "2025-09-27T03:06:27.000Z",
      "submittedOnDailyAt": "2025-10-08T00:43:22.431Z",
      "title": "DRIFT: Learning from Abundant User Dissatisfaction in Real-World\n  Preference Learning",
      "submittedOnDailyBy": {
        "_id": "647693afb9c742c51117e1fa",
        "avatarUrl": "/avatars/6a7a3f0f47588b5c38881b289d042b7a.svg",
        "isPro": false,
        "fullname": "Yifan Wang",
        "user": "AmberYifan",
        "type": "user"
      },
      "summary": "Real-world large language model deployments (e.g., conversational AI systems,\ncode generation assistants) naturally generate abundant implicit user\ndissatisfaction (DSAT) signals, as users iterate toward better answers through\nrefinements, corrections, and expressed preferences, while explicit\nsatisfaction (SAT) feedback is scarce. Existing preference learning approaches\nare poorly aligned with this data profile, as they rely on costly human\nannotations or assume plentiful positive responses. In this paper, we introduce\nDRIFT (Dissatisfaction-Refined Iterative\npreFerence Training), which anchors training on real-world\nDSAT signals and samples positives dynamically from the evolving policy.\nEmpirically, DRIFT models trained on real-world WildFeedback datasets\nand synthetic UltraFeedback datasets achieve up to +6.23\\% (7B) /\n+7.61\\% (14B) on WildBench Task Score and up to +8.95\\% (7B) / +12.29\\% (14B)\non AlpacaEval2 win rate over base models, outperforming strong baseline methods\nsuch as iterative DPO and SPIN. At larger scales, the improvements are\nparticularly pronounced: 14B models trained with DRIFT surpass GPT-4o-mini on\nWildBench. Further analysis shows that DRIFT also preserves exploratory\ncapacity, yielding more diverse high-reward solutions rather than collapsing to\nnarrow subsets. Theoretically, we demonstrate that this design preserves\npreference margins and avoids the gradient degeneration. These results show\nthat DRIFT is an effective and scalable recipe for real-world post-training\nthat leverages the most abundant and informative signal. The code and data are\navailable at https://github.com/cacayaya/DRIFT.git.",
      "upvotes": 0,
      "discussionId": "68e5c815975ac4c405ef20e7",
      "ai_summary": "DRIFT, a dissatisfaction-refined iterative preference training method, improves large language models using implicit user dissatisfaction signals, achieving better performance than existing methods on real-world datasets.",
      "ai_keywords": [
        "DRIFT",
        "Dissatisfaction-Refined Iterative Preference Training",
        "WildFeedback",
        "UltraFeedback",
        "WildBench Task Score",
        "AlpacaEval2 win rate",
        "iterative DPO",
        "SPIN",
        "gradient degeneration",
        "preference margins",
        "exploratory capacity"
      ]
    },
    "publishedAt": "2025-09-26T23:06:27.000Z",
    "title": "DRIFT: Learning from Abundant User Dissatisfaction in Real-World\n  Preference Learning",
    "summary": "Real-world large language model deployments (e.g., conversational AI systems,\ncode generation assistants) naturally generate abundant implicit user\ndissatisfaction (DSAT) signals, as users iterate toward better answers through\nrefinements, corrections, and expressed preferences, while explicit\nsatisfaction (SAT) feedback is scarce. Existing preference learning approaches\nare poorly aligned with this data profile, as they rely on costly human\nannotations or assume plentiful positive responses. In this paper, we introduce\nDRIFT (Dissatisfaction-Refined Iterative\npreFerence Training), which anchors training on real-world\nDSAT signals and samples positives dynamically from the evolving policy.\nEmpirically, DRIFT models trained on real-world WildFeedback datasets\nand synthetic UltraFeedback datasets achieve up to +6.23\\% (7B) /\n+7.61\\% (14B) on WildBench Task Score and up to +8.95\\% (7B) / +12.29\\% (14B)\non AlpacaEval2 win rate over base models, outperforming strong baseline methods\nsuch as iterative DPO and SPIN. At larger scales, the improvements are\nparticularly pronounced: 14B models trained with DRIFT surpass GPT-4o-mini on\nWildBench. Further analysis shows that DRIFT also preserves exploratory\ncapacity, yielding more diverse high-reward solutions rather than collapsing to\nnarrow subsets. Theoretically, we demonstrate that this design preserves\npreference margins and avoids the gradient degeneration. These results show\nthat DRIFT is an effective and scalable recipe for real-world post-training\nthat leverages the most abundant and informative signal. The code and data are\navailable at https://github.com/cacayaya/DRIFT.git.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.02341.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "647693afb9c742c51117e1fa",
      "avatarUrl": "/avatars/6a7a3f0f47588b5c38881b289d042b7a.svg",
      "fullname": "Yifan Wang",
      "name": "AmberYifan",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  }
]