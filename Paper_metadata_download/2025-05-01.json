[
  {
    "paper": {
      "id": "2504.21776",
      "authors": [
        {
          "_id": "6812d593060494e99e4835e0",
          "name": "Xiaoxi Li",
          "hidden": false
        },
        {
          "_id": "6812d593060494e99e4835e1",
          "name": "Jiajie Jin",
          "hidden": false
        },
        {
          "_id": "6812d593060494e99e4835e2",
          "name": "Guanting Dong",
          "hidden": false
        },
        {
          "_id": "6812d593060494e99e4835e3",
          "name": "Hongjin Qian",
          "hidden": false
        },
        {
          "_id": "6812d593060494e99e4835e4",
          "name": "Yutao Zhu",
          "hidden": false
        },
        {
          "_id": "6812d593060494e99e4835e5",
          "name": "Yongkang Wu",
          "hidden": false
        },
        {
          "_id": "6812d593060494e99e4835e6",
          "name": "Ji-Rong Wen",
          "hidden": false
        },
        {
          "_id": "6812d593060494e99e4835e7",
          "name": "Zhicheng Dou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-30T16:25:25.000Z",
      "submittedOnDailyAt": "2025-05-01T00:33:55.498Z",
      "title": "WebThinker: Empowering Large Reasoning Models with Deep Research\n  Capability",
      "submittedOnDailyBy": {
        "_id": "61cd4b833dd34ba1985e0753",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/61cd4b833dd34ba1985e0753/BfHfrwotoMESpXZOHiIe4.png",
        "isPro": false,
        "fullname": "KABI",
        "user": "dongguanting",
        "type": "user"
      },
      "summary": "Large reasoning models (LRMs), such as OpenAI-o1 and DeepSeek-R1, demonstrate\nimpressive long-horizon reasoning capabilities. However, their reliance on\nstatic internal knowledge limits their performance on complex,\nknowledge-intensive tasks and hinders their ability to produce comprehensive\nresearch reports requiring synthesis of diverse web information. To address\nthis, we propose WebThinker, a deep research agent that empowers LRMs\nto autonomously search the web, navigate web pages, and draft research reports\nduring the reasoning process. WebThinker integrates a Deep Web\nExplorer module, enabling LRMs to dynamically search, navigate, and extract\ninformation from the web when encountering knowledge gaps. It also employs an\nAutonomous Think-Search-and-Draft strategy, allowing the model to\nseamlessly interleave reasoning, information gathering, and report writing in\nreal time. To further enhance research tool utilization, we introduce an\nRL-based training strategy via iterative online Direct Preference\nOptimization (DPO). Extensive experiments on complex reasoning benchmarks\n(GPQA, GAIA, WebWalkerQA, HLE) and scientific report generation tasks (Glaive)\ndemonstrate that WebThinker significantly outperforms existing methods and\nstrong proprietary systems. Our approach enhances LRM reliability and\napplicability in complex scenarios, paving the way for more capable and\nversatile deep research systems. The code is available at\nhttps://github.com/RUC-NLPIR/WebThinker.",
      "upvotes": 8,
      "discussionId": "6812d594060494e99e48361c",
      "ai_keywords": [
        "Large reasoning models (LRMs)",
        "WebThinker",
        "Deep Web Explorer",
        "Autonomous Think-Search-and-Draft strategy",
        "RL-based training strategy",
        "iterative online Direct Preference Optimization (DPO)",
        "GPQA",
        "GAIA",
        "WebWalkerQA",
        "HLE",
        "Glaive"
      ]
    },
    "publishedAt": "2025-04-30T12:25:25.000Z",
    "title": "WebThinker: Empowering Large Reasoning Models with Deep Research\n  Capability",
    "summary": "Large reasoning models (LRMs), such as OpenAI-o1 and DeepSeek-R1, demonstrate\nimpressive long-horizon reasoning capabilities. However, their reliance on\nstatic internal knowledge limits their performance on complex,\nknowledge-intensive tasks and hinders their ability to produce comprehensive\nresearch reports requiring synthesis of diverse web information. To address\nthis, we propose WebThinker, a deep research agent that empowers LRMs\nto autonomously search the web, navigate web pages, and draft research reports\nduring the reasoning process. WebThinker integrates a Deep Web\nExplorer module, enabling LRMs to dynamically search, navigate, and extract\ninformation from the web when encountering knowledge gaps. It also employs an\nAutonomous Think-Search-and-Draft strategy, allowing the model to\nseamlessly interleave reasoning, information gathering, and report writing in\nreal time. To further enhance research tool utilization, we introduce an\nRL-based training strategy via iterative online Direct Preference\nOptimization (DPO). Extensive experiments on complex reasoning benchmarks\n(GPQA, GAIA, WebWalkerQA, HLE) and scientific report generation tasks (Glaive)\ndemonstrate that WebThinker significantly outperforms existing methods and\nstrong proprietary systems. Our approach enhances LRM reliability and\napplicability in complex scenarios, paving the way for more capable and\nversatile deep research systems. The code is available at\nhttps://github.com/RUC-NLPIR/WebThinker.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.21776.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "61cd4b833dd34ba1985e0753",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/61cd4b833dd34ba1985e0753/BfHfrwotoMESpXZOHiIe4.png",
      "fullname": "KABI",
      "name": "dongguanting",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 18
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.21850",
      "authors": [
        {
          "_id": "6812d4d6ce88881cb51b3b70",
          "name": "Xindi Wu",
          "hidden": false
        },
        {
          "_id": "6812d4d6ce88881cb51b3b71",
          "name": "Hee Seung Hwang",
          "hidden": false
        },
        {
          "_id": "6812d4d6ce88881cb51b3b72",
          "name": "Polina Kirichenko",
          "hidden": false
        },
        {
          "_id": "6812d4d6ce88881cb51b3b73",
          "name": "Olga Russakovsky",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-30T17:57:22.000Z",
      "submittedOnDailyAt": "2025-05-01T00:32:13.807Z",
      "title": "COMPACT: COMPositional Atomic-to-Complex Visual Capability Tuning",
      "submittedOnDailyBy": {
        "_id": "613940c0905b1938233881e3",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/613940c0905b1938233881e3/Vb4kFWKEq6AILUP9KmCZA.png",
        "isPro": false,
        "fullname": "Xindi Wu",
        "user": "xindiw",
        "type": "user"
      },
      "summary": "Multimodal Large Language Models (MLLMs) excel at simple vision-language\ntasks but struggle when faced with complex tasks that require multiple\ncapabilities, such as simultaneously recognizing objects, counting them, and\nunderstanding their spatial relationships. This might be partially the result\nof the fact that Visual Instruction Tuning (VIT), a critical training step for\nMLLMs, has traditionally focused on scaling data volume, but not the\ncompositional complexity of training examples. We propose COMPACT\n(COMPositional Atomic-to-complex visual Capability Tuning), which generates a\ntraining dataset explicitly controlling for the compositional complexity of the\ntraining examples. The data from COMPACT allows MLLMs to train on combinations\nof atomic capabilities to learn complex capabilities more efficiently. Across\nall benchmarks, COMPACT achieves comparable performance to the LLaVA-665k VIT\nwhile using less than 10% of its data budget, and even outperforms it on\nseveral, especially those involving complex multi-capability tasks. For\nexample, COMPACT achieves substantial 83.3% improvement on MMStar and 94.0%\nimprovement on MM-Vet compared to the full-scale VIT on particularly complex\nquestions that require four or more atomic capabilities. COMPACT offers a\nscalable, data-efficient, visual compositional tuning recipe to improve on\ncomplex visual-language tasks.",
      "upvotes": 4,
      "discussionId": "6812d4d7ce88881cb51b3bad",
      "projectPage": "https://princetonvisualai.github.io/compact/",
      "githubRepo": "https://github.com/princetonvisualai/compact",
      "ai_keywords": [
        "Multimodal Large Language Models (MLLMs)",
        "Visual Instruction Tuning (VIT)",
        "compositional complexity",
        "COMPACT (COMPositional Atomic-to-complex visual Capability Tuning)",
        "atomic capabilities",
        "complex capabilities",
        "MMStar",
        "MM-Vet",
        "visual compositional tuning"
      ]
    },
    "publishedAt": "2025-04-30T13:57:22.000Z",
    "title": "COMPACT: COMPositional Atomic-to-Complex Visual Capability Tuning",
    "summary": "Multimodal Large Language Models (MLLMs) excel at simple vision-language\ntasks but struggle when faced with complex tasks that require multiple\ncapabilities, such as simultaneously recognizing objects, counting them, and\nunderstanding their spatial relationships. This might be partially the result\nof the fact that Visual Instruction Tuning (VIT), a critical training step for\nMLLMs, has traditionally focused on scaling data volume, but not the\ncompositional complexity of training examples. We propose COMPACT\n(COMPositional Atomic-to-complex visual Capability Tuning), which generates a\ntraining dataset explicitly controlling for the compositional complexity of the\ntraining examples. The data from COMPACT allows MLLMs to train on combinations\nof atomic capabilities to learn complex capabilities more efficiently. Across\nall benchmarks, COMPACT achieves comparable performance to the LLaVA-665k VIT\nwhile using less than 10% of its data budget, and even outperforms it on\nseveral, especially those involving complex multi-capability tasks. For\nexample, COMPACT achieves substantial 83.3% improvement on MMStar and 94.0%\nimprovement on MM-Vet compared to the full-scale VIT on particularly complex\nquestions that require four or more atomic capabilities. COMPACT offers a\nscalable, data-efficient, visual compositional tuning recipe to improve on\ncomplex visual-language tasks.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.21850.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "613940c0905b1938233881e3",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/613940c0905b1938233881e3/Vb4kFWKEq6AILUP9KmCZA.png",
      "fullname": "Xindi Wu",
      "name": "xindiw",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.21318",
      "authors": [
        {
          "_id": "6812d3c3e74b39182bd0dc82",
          "name": "Marah Abdin",
          "hidden": false
        },
        {
          "_id": "6812d3c3e74b39182bd0dc83",
          "name": "Sahaj Agarwal",
          "hidden": false
        },
        {
          "_id": "6812d3c3e74b39182bd0dc84",
          "name": "Ahmed Awadallah",
          "hidden": false
        },
        {
          "_id": "6812d3c3e74b39182bd0dc85",
          "name": "Vidhisha Balachandran",
          "hidden": false
        },
        {
          "_id": "6812d3c3e74b39182bd0dc86",
          "name": "Harkirat Behl",
          "hidden": false
        },
        {
          "_id": "6812d3c3e74b39182bd0dc87",
          "name": "Lingjiao Chen",
          "hidden": false
        },
        {
          "_id": "6812d3c3e74b39182bd0dc88",
          "name": "Gustavo de Rosa",
          "hidden": false
        },
        {
          "_id": "6812d3c3e74b39182bd0dc89",
          "name": "Suriya Gunasekar",
          "hidden": false
        },
        {
          "_id": "6812d3c3e74b39182bd0dc8a",
          "name": "Mojan Javaheripi",
          "hidden": false
        },
        {
          "_id": "6812d3c3e74b39182bd0dc8b",
          "name": "Neel Joshi",
          "hidden": false
        },
        {
          "_id": "6812d3c3e74b39182bd0dc8c",
          "name": "Piero Kauffmann",
          "hidden": false
        },
        {
          "_id": "6812d3c3e74b39182bd0dc8d",
          "name": "Yash Lara",
          "hidden": false
        },
        {
          "_id": "6812d3c3e74b39182bd0dc8e",
          "name": "Caio César Teodoro Mendes",
          "hidden": false
        },
        {
          "_id": "6812d3c3e74b39182bd0dc8f",
          "name": "Arindam Mitra",
          "hidden": false
        },
        {
          "_id": "6812d3c3e74b39182bd0dc90",
          "name": "Besmira Nushi",
          "hidden": false
        },
        {
          "_id": "6812d3c3e74b39182bd0dc91",
          "name": "Dimitris Papailiopoulos",
          "hidden": false
        },
        {
          "_id": "6812d3c3e74b39182bd0dc92",
          "name": "Olli Saarikivi",
          "hidden": false
        },
        {
          "_id": "6812d3c3e74b39182bd0dc93",
          "name": "Shital Shah",
          "hidden": false
        },
        {
          "_id": "6812d3c3e74b39182bd0dc94",
          "name": "Vaishnavi Shrivastava",
          "hidden": false
        },
        {
          "_id": "6812d3c3e74b39182bd0dc95",
          "name": "Vibhav Vineet",
          "hidden": false
        },
        {
          "_id": "6812d3c3e74b39182bd0dc96",
          "name": "Yue Wu",
          "hidden": false
        },
        {
          "_id": "6812d3c3e74b39182bd0dc97",
          "name": "Safoora Yousefi",
          "hidden": false
        },
        {
          "_id": "6812d3c3e74b39182bd0dc98",
          "name": "Guoqing Zheng",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-30T05:05:09.000Z",
      "submittedOnDailyAt": "2025-05-01T00:22:34.583Z",
      "title": "Phi-4-reasoning Technical Report",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": false,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "We introduce Phi-4-reasoning, a 14-billion parameter reasoning model that\nachieves strong performance on complex reasoning tasks. Trained via supervised\nfine-tuning of Phi-4 on carefully curated set of \"teachable\" prompts-selected\nfor the right level of complexity and diversity-and reasoning demonstrations\ngenerated using o3-mini, Phi-4-reasoning generates detailed reasoning chains\nthat effectively leverage inference-time compute. We further develop\nPhi-4-reasoning-plus, a variant enhanced through a short phase of outcome-based\nreinforcement learning that offers higher performance by generating longer\nreasoning traces. Across a wide range of reasoning tasks, both models\noutperform significantly larger open-weight models such as\nDeepSeek-R1-Distill-Llama-70B model and approach the performance levels of full\nDeepSeek-R1 model. Our comprehensive evaluations span benchmarks in math and\nscientific reasoning, coding, algorithmic problem solving, planning, and\nspatial understanding. Interestingly, we observe a non-trivial transfer of\nimprovements to general-purpose benchmarks as well. In this report, we provide\ninsights into our training data, our training methodologies, and our\nevaluations. We show that the benefit of careful data curation for supervised\nfine-tuning (SFT) extends to reasoning language models, and can be further\namplified by reinforcement learning (RL). Finally, our evaluation points to\nopportunities for improving how we assess the performance and robustness of\nreasoning models.",
      "upvotes": 4,
      "discussionId": "6812d3c4e74b39182bd0dcd1",
      "ai_keywords": [
        "parameter reasoning model",
        "supervised fine-tuning",
        "reasoning demonstrations",
        "inference-time compute",
        "outcome-based reinforcement learning",
        "reasoning chains",
        "reasoning traces",
        "reasoning language models",
        "general-purpose benchmarks",
        "performance assessment",
        "robustness assessment"
      ]
    },
    "publishedAt": "2025-04-30T01:05:09.000Z",
    "title": "Phi-4-reasoning Technical Report",
    "summary": "We introduce Phi-4-reasoning, a 14-billion parameter reasoning model that\nachieves strong performance on complex reasoning tasks. Trained via supervised\nfine-tuning of Phi-4 on carefully curated set of \"teachable\" prompts-selected\nfor the right level of complexity and diversity-and reasoning demonstrations\ngenerated using o3-mini, Phi-4-reasoning generates detailed reasoning chains\nthat effectively leverage inference-time compute. We further develop\nPhi-4-reasoning-plus, a variant enhanced through a short phase of outcome-based\nreinforcement learning that offers higher performance by generating longer\nreasoning traces. Across a wide range of reasoning tasks, both models\noutperform significantly larger open-weight models such as\nDeepSeek-R1-Distill-Llama-70B model and approach the performance levels of full\nDeepSeek-R1 model. Our comprehensive evaluations span benchmarks in math and\nscientific reasoning, coding, algorithmic problem solving, planning, and\nspatial understanding. Interestingly, we observe a non-trivial transfer of\nimprovements to general-purpose benchmarks as well. In this report, we provide\ninsights into our training data, our training methodologies, and our\nevaluations. We show that the benefit of careful data curation for supervised\nfine-tuning (SFT) extends to reasoning language models, and can be further\namplified by reinforcement learning (RL). Finally, our evaluation points to\nopportunities for improving how we assess the performance and robustness of\nreasoning models.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.21318.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6751
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.21233",
      "authors": [
        {
          "_id": "6812d62ae74b39182bd17c9c",
          "name": "Haoran Xu",
          "hidden": false
        },
        {
          "_id": "6812d62ae74b39182bd17c9d",
          "name": "Baolin Peng",
          "hidden": false
        },
        {
          "_id": "6812d62ae74b39182bd17c9e",
          "name": "Hany Awadalla",
          "hidden": false
        },
        {
          "_id": "6812d62ae74b39182bd17c9f",
          "name": "Dongdong Chen",
          "hidden": false
        },
        {
          "_id": "6812d62ae74b39182bd17ca0",
          "name": "Yen-Chun Chen",
          "hidden": false
        },
        {
          "_id": "6812d62ae74b39182bd17ca1",
          "name": "Mei Gao",
          "hidden": false
        },
        {
          "_id": "6812d62ae74b39182bd17ca2",
          "name": "Young Jin Kim",
          "hidden": false
        },
        {
          "_id": "6812d62ae74b39182bd17ca3",
          "name": "Yunsheng Li",
          "hidden": false
        },
        {
          "_id": "6812d62ae74b39182bd17ca4",
          "name": "Liliang Ren",
          "hidden": false
        },
        {
          "_id": "6812d62ae74b39182bd17ca5",
          "name": "Yelong Shen",
          "hidden": false
        },
        {
          "_id": "6812d62ae74b39182bd17ca6",
          "name": "Shuohang Wang",
          "hidden": false
        },
        {
          "_id": "6812d62ae74b39182bd17ca7",
          "name": "Weijian Xu",
          "hidden": false
        },
        {
          "_id": "6812d62ae74b39182bd17ca8",
          "name": "Jianfeng Gao",
          "hidden": false
        },
        {
          "_id": "6812d62ae74b39182bd17ca9",
          "name": "Weizhu Chen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-30T00:04:35.000Z",
      "submittedOnDailyAt": "2025-05-01T00:32:47.316Z",
      "title": "Phi-4-Mini-Reasoning: Exploring the Limits of Small Reasoning Language\n  Models in Math",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": false,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "Chain-of-Thought (CoT) significantly enhances formal reasoning capabilities\nin Large Language Models (LLMs) by training them to explicitly generate\nintermediate reasoning steps. While LLMs readily benefit from such techniques,\nimproving reasoning in Small Language Models (SLMs) remains challenging due to\ntheir limited model capacity. Recent work by Deepseek-R1 demonstrates that\ndistillation from LLM-generated synthetic data can substantially improve the\nreasoning ability of SLM. However, the detailed modeling recipe is not\ndisclosed. In this work, we present a systematic training recipe for SLMs that\nconsists of four steps: (1) large-scale mid-training on diverse distilled\nlong-CoT data, (2) supervised fine-tuning on high-quality long-CoT data, (3)\nRollout DPO leveraging a carefully curated preference dataset, and (4)\nReinforcement Learning (RL) with Verifiable Reward. We apply our method on\nPhi-4-Mini, a compact 3.8B-parameter model. The resulting Phi-4-Mini-Reasoning\nmodel exceeds, on math reasoning tasks, much larger reasoning models, e.g.,\noutperforming DeepSeek-R1-Distill-Qwen-7B by 3.2 points and\nDeepSeek-R1-Distill-Llama-8B by 7.7 points on Math-500. Our results validate\nthat a carefully designed training recipe, with large-scale high-quality CoT\ndata, is effective to unlock strong reasoning capabilities even in\nresource-constrained small models.",
      "upvotes": 4,
      "discussionId": "6812d62be74b39182bd17cdb",
      "ai_keywords": [
        "Chain-of-Thought (CoT)",
        "Large Language Models (LLMs)",
        "Small Language Models (SLMs)",
        "distillation",
        "synthetic data",
        "mid-training",
        "diverse distilled long-CoT data",
        "supervised fine-tuning",
        "high-quality long-CoT data",
        "Rollout DPO",
        "preference dataset",
        "Reinforcement Learning (RL)",
        "Verifiable Reward",
        "Phi-4-Mini",
        "resource-constrained small models"
      ]
    },
    "publishedAt": "2025-04-29T20:04:35.000Z",
    "title": "Phi-4-Mini-Reasoning: Exploring the Limits of Small Reasoning Language\n  Models in Math",
    "summary": "Chain-of-Thought (CoT) significantly enhances formal reasoning capabilities\nin Large Language Models (LLMs) by training them to explicitly generate\nintermediate reasoning steps. While LLMs readily benefit from such techniques,\nimproving reasoning in Small Language Models (SLMs) remains challenging due to\ntheir limited model capacity. Recent work by Deepseek-R1 demonstrates that\ndistillation from LLM-generated synthetic data can substantially improve the\nreasoning ability of SLM. However, the detailed modeling recipe is not\ndisclosed. In this work, we present a systematic training recipe for SLMs that\nconsists of four steps: (1) large-scale mid-training on diverse distilled\nlong-CoT data, (2) supervised fine-tuning on high-quality long-CoT data, (3)\nRollout DPO leveraging a carefully curated preference dataset, and (4)\nReinforcement Learning (RL) with Verifiable Reward. We apply our method on\nPhi-4-Mini, a compact 3.8B-parameter model. The resulting Phi-4-Mini-Reasoning\nmodel exceeds, on math reasoning tasks, much larger reasoning models, e.g.,\noutperforming DeepSeek-R1-Distill-Qwen-7B by 3.2 points and\nDeepSeek-R1-Distill-Llama-8B by 7.7 points on Math-500. Our results validate\nthat a carefully designed training recipe, with large-scale high-quality CoT\ndata, is effective to unlock strong reasoning capabilities even in\nresource-constrained small models.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.21233.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6751
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.18904",
      "authors": [
        {
          "_id": "6812d36790b45f422b0bfdc2",
          "name": "Haoran Geng",
          "hidden": false
        },
        {
          "_id": "6812d36790b45f422b0bfdc3",
          "name": "Feishi Wang",
          "hidden": false
        },
        {
          "_id": "6812d36790b45f422b0bfdc4",
          "name": "Songlin Wei",
          "hidden": false
        },
        {
          "_id": "6812d36790b45f422b0bfdc5",
          "name": "Yuyang Li",
          "hidden": false
        },
        {
          "_id": "6812d36790b45f422b0bfdc6",
          "name": "Bangjun Wang",
          "hidden": false
        },
        {
          "_id": "6812d36790b45f422b0bfdc7",
          "name": "Boshi An",
          "hidden": false
        },
        {
          "_id": "6812d36790b45f422b0bfdc8",
          "name": "Charlie Tianyue Cheng",
          "hidden": false
        },
        {
          "_id": "6812d36790b45f422b0bfdc9",
          "name": "Haozhe Lou",
          "hidden": false
        },
        {
          "_id": "6812d36790b45f422b0bfdca",
          "name": "Peihao Li",
          "hidden": false
        },
        {
          "_id": "6812d36790b45f422b0bfdcb",
          "name": "Yen-Jen Wang",
          "hidden": false
        },
        {
          "_id": "6812d36790b45f422b0bfdcc",
          "name": "Yutong Liang",
          "hidden": false
        },
        {
          "_id": "6812d36790b45f422b0bfdcd",
          "name": "Dylan Goetting",
          "hidden": false
        },
        {
          "_id": "6812d36790b45f422b0bfdce",
          "name": "Chaoyi Xu",
          "hidden": false
        },
        {
          "_id": "6812d36790b45f422b0bfdcf",
          "name": "Haozhe Chen",
          "hidden": false
        },
        {
          "_id": "6812d36790b45f422b0bfdd0",
          "name": "Yuxi Qian",
          "hidden": false
        },
        {
          "_id": "6812d36790b45f422b0bfdd1",
          "name": "Yiran Geng",
          "hidden": false
        },
        {
          "_id": "6812d36790b45f422b0bfdd2",
          "name": "Jiageng Mao",
          "hidden": false
        },
        {
          "_id": "6812d36790b45f422b0bfdd3",
          "name": "Weikang Wan",
          "hidden": false
        },
        {
          "_id": "6812d36790b45f422b0bfdd4",
          "name": "Mingtong Zhang",
          "hidden": false
        },
        {
          "_id": "6812d36790b45f422b0bfdd5",
          "name": "Jiangran Lyu",
          "hidden": false
        },
        {
          "_id": "6812d36790b45f422b0bfdd6",
          "name": "Siheng Zhao",
          "hidden": false
        },
        {
          "_id": "6812d36790b45f422b0bfdd7",
          "name": "Jiazhao Zhang",
          "hidden": false
        },
        {
          "_id": "6812d36790b45f422b0bfdd8",
          "name": "Jialiang Zhang",
          "hidden": false
        },
        {
          "_id": "6812d36790b45f422b0bfdd9",
          "name": "Chengyang Zhao",
          "hidden": false
        },
        {
          "_id": "6812d36790b45f422b0bfdda",
          "name": "Haoran Lu",
          "hidden": false
        },
        {
          "_id": "6812d36790b45f422b0bfddb",
          "name": "Yufei Ding",
          "hidden": false
        },
        {
          "_id": "6812d36790b45f422b0bfddc",
          "name": "Ran Gong",
          "hidden": false
        },
        {
          "_id": "6812d36790b45f422b0bfddd",
          "name": "Yuran Wang",
          "hidden": false
        },
        {
          "_id": "6812d36790b45f422b0bfdde",
          "name": "Yuxuan Kuang",
          "hidden": false
        },
        {
          "_id": "6812d36790b45f422b0bfddf",
          "name": "Ruihai Wu",
          "hidden": false
        },
        {
          "_id": "6812d36790b45f422b0bfde0",
          "name": "Baoxiong Jia",
          "hidden": false
        },
        {
          "_id": "6812d36790b45f422b0bfde1",
          "name": "Carlo Sferrazza",
          "hidden": false
        },
        {
          "_id": "6812d36790b45f422b0bfde2",
          "name": "Hao Dong",
          "hidden": false
        },
        {
          "_id": "6812d36790b45f422b0bfde3",
          "name": "Siyuan Huang",
          "hidden": false
        },
        {
          "_id": "6812d36790b45f422b0bfde4",
          "name": "Yue Wang",
          "hidden": false
        },
        {
          "_id": "6812d36790b45f422b0bfde5",
          "name": "Jitendra Malik",
          "hidden": false
        },
        {
          "_id": "6812d36790b45f422b0bfde6",
          "name": "Pieter Abbeel",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-26T12:31:04.000Z",
      "submittedOnDailyAt": "2025-05-01T00:21:25.704Z",
      "title": "RoboVerse: Towards a Unified Platform, Dataset and Benchmark for\n  Scalable and Generalizable Robot Learning",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": false,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "Data scaling and standardized evaluation benchmarks have driven significant\nadvances in natural language processing and computer vision. However, robotics\nfaces unique challenges in scaling data and establishing evaluation protocols.\nCollecting real-world data is resource-intensive and inefficient, while\nbenchmarking in real-world scenarios remains highly complex. Synthetic data and\nsimulation offer promising alternatives, yet existing efforts often fall short\nin data quality, diversity, and benchmark standardization. To address these\nchallenges, we introduce RoboVerse, a comprehensive framework comprising a\nsimulation platform, a synthetic dataset, and unified benchmarks. Our\nsimulation platform supports multiple simulators and robotic embodiments,\nenabling seamless transitions between different environments. The synthetic\ndataset, featuring high-fidelity physics and photorealistic rendering, is\nconstructed through multiple approaches. Additionally, we propose unified\nbenchmarks for imitation learning and reinforcement learning, enabling\nevaluation across different levels of generalization. At the core of the\nsimulation platform is MetaSim, an infrastructure that abstracts diverse\nsimulation environments into a universal interface. It restructures existing\nsimulation environments into a simulator-agnostic configuration system, as well\nas an API aligning different simulator functionalities, such as launching\nsimulation environments, loading assets with initial states, stepping the\nphysics engine, etc. This abstraction ensures interoperability and\nextensibility. Comprehensive experiments demonstrate that RoboVerse enhances\nthe performance of imitation learning, reinforcement learning, world model\nlearning, and sim-to-real transfer. These results validate the reliability of\nour dataset and benchmarks, establishing RoboVerse as a robust solution for\nadvancing robot learning.",
      "upvotes": 3,
      "discussionId": "6812d36c90b45f422b0bff56",
      "ai_keywords": [
        "simulation platform",
        "synthetic dataset",
        "unified benchmarks",
        "MetaSim",
        "simulator-agnostic configuration system",
        "API",
        "physics engine",
        "imitation learning",
        "reinforcement learning",
        "world model learning",
        "sim-to-real transfer"
      ]
    },
    "publishedAt": "2025-04-26T08:31:04.000Z",
    "title": "RoboVerse: Towards a Unified Platform, Dataset and Benchmark for\n  Scalable and Generalizable Robot Learning",
    "summary": "Data scaling and standardized evaluation benchmarks have driven significant\nadvances in natural language processing and computer vision. However, robotics\nfaces unique challenges in scaling data and establishing evaluation protocols.\nCollecting real-world data is resource-intensive and inefficient, while\nbenchmarking in real-world scenarios remains highly complex. Synthetic data and\nsimulation offer promising alternatives, yet existing efforts often fall short\nin data quality, diversity, and benchmark standardization. To address these\nchallenges, we introduce RoboVerse, a comprehensive framework comprising a\nsimulation platform, a synthetic dataset, and unified benchmarks. Our\nsimulation platform supports multiple simulators and robotic embodiments,\nenabling seamless transitions between different environments. The synthetic\ndataset, featuring high-fidelity physics and photorealistic rendering, is\nconstructed through multiple approaches. Additionally, we propose unified\nbenchmarks for imitation learning and reinforcement learning, enabling\nevaluation across different levels of generalization. At the core of the\nsimulation platform is MetaSim, an infrastructure that abstracts diverse\nsimulation environments into a universal interface. It restructures existing\nsimulation environments into a simulator-agnostic configuration system, as well\nas an API aligning different simulator functionalities, such as launching\nsimulation environments, loading assets with initial states, stepping the\nphysics engine, etc. This abstraction ensures interoperability and\nextensibility. Comprehensive experiments demonstrate that RoboVerse enhances\nthe performance of imitation learning, reinforcement learning, world model\nlearning, and sim-to-real transfer. These results validate the reliability of\nour dataset and benchmarks, establishing RoboVerse as a robust solution for\nadvancing robot learning.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.18904.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6751
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.21855",
      "authors": [
        {
          "_id": "6812db1567abbb1d11fb4e9d",
          "name": "Qihao Liu",
          "hidden": false
        },
        {
          "_id": "6812db1567abbb1d11fb4e9e",
          "name": "Ju He",
          "hidden": false
        },
        {
          "_id": "6812db1567abbb1d11fb4e9f",
          "name": "Qihang Yu",
          "hidden": false
        },
        {
          "_id": "6812db1567abbb1d11fb4ea0",
          "name": "Liang-Chieh Chen",
          "hidden": false
        },
        {
          "_id": "6812db1567abbb1d11fb4ea1",
          "name": "Alan Yuille",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/639f1e519f1f2baab2f00d22/h3IWiXbIAOy1NxTWKDAkO.mp4"
      ],
      "publishedAt": "2025-04-30T17:59:56.000Z",
      "submittedOnDailyAt": "2025-05-01T00:56:11.135Z",
      "title": "ReVision: High-Quality, Low-Cost Video Generation with Explicit 3D\n  Physics Modeling for Complex Motion and Interaction",
      "submittedOnDailyBy": {
        "_id": "639f1e519f1f2baab2f00d22",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/639f1e519f1f2baab2f00d22/pFjd51WZuVZ3A11rItvmk.jpeg",
        "isPro": true,
        "fullname": "Qihao Liu",
        "user": "QHL067",
        "type": "user"
      },
      "summary": "In recent years, video generation has seen significant advancements. However,\nchallenges still persist in generating complex motions and interactions. To\naddress these challenges, we introduce ReVision, a plug-and-play framework that\nexplicitly integrates parameterized 3D physical knowledge into a pretrained\nconditional video generation model, significantly enhancing its ability to\ngenerate high-quality videos with complex motion and interactions.\nSpecifically, ReVision consists of three stages. First, a video diffusion model\nis used to generate a coarse video. Next, we extract a set of 2D and 3D\nfeatures from the coarse video to construct a 3D object-centric representation,\nwhich is then refined by our proposed parameterized physical prior model to\nproduce an accurate 3D motion sequence. Finally, this refined motion sequence\nis fed back into the same video diffusion model as additional conditioning,\nenabling the generation of motion-consistent videos, even in scenarios\ninvolving complex actions and interactions. We validate the effectiveness of\nour approach on Stable Video Diffusion, where ReVision significantly improves\nmotion fidelity and coherence. Remarkably, with only 1.5B parameters, it even\noutperforms a state-of-the-art video generation model with over 13B parameters\non complex video generation by a substantial margin. Our results suggest that,\nby incorporating 3D physical knowledge, even a relatively small video diffusion\nmodel can generate complex motions and interactions with greater realism and\ncontrollability, offering a promising solution for physically plausible video\ngeneration.",
      "upvotes": 1,
      "discussionId": "6812db1767abbb1d11fb4f42",
      "projectPage": "https://revision-video.github.io/"
    },
    "publishedAt": "2025-04-30T13:59:56.000Z",
    "title": "ReVision: High-Quality, Low-Cost Video Generation with Explicit 3D\n  Physics Modeling for Complex Motion and Interaction",
    "summary": "In recent years, video generation has seen significant advancements. However,\nchallenges still persist in generating complex motions and interactions. To\naddress these challenges, we introduce ReVision, a plug-and-play framework that\nexplicitly integrates parameterized 3D physical knowledge into a pretrained\nconditional video generation model, significantly enhancing its ability to\ngenerate high-quality videos with complex motion and interactions.\nSpecifically, ReVision consists of three stages. First, a video diffusion model\nis used to generate a coarse video. Next, we extract a set of 2D and 3D\nfeatures from the coarse video to construct a 3D object-centric representation,\nwhich is then refined by our proposed parameterized physical prior model to\nproduce an accurate 3D motion sequence. Finally, this refined motion sequence\nis fed back into the same video diffusion model as additional conditioning,\nenabling the generation of motion-consistent videos, even in scenarios\ninvolving complex actions and interactions. We validate the effectiveness of\nour approach on Stable Video Diffusion, where ReVision significantly improves\nmotion fidelity and coherence. Remarkably, with only 1.5B parameters, it even\noutperforms a state-of-the-art video generation model with over 13B parameters\non complex video generation by a substantial margin. Our results suggest that,\nby incorporating 3D physical knowledge, even a relatively small video diffusion\nmodel can generate complex motions and interactions with greater realism and\ncontrollability, offering a promising solution for physically plausible video\ngeneration.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/639f1e519f1f2baab2f00d22/h3IWiXbIAOy1NxTWKDAkO.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.21855.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "639f1e519f1f2baab2f00d22",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/639f1e519f1f2baab2f00d22/pFjd51WZuVZ3A11rItvmk.jpeg",
      "fullname": "Qihao Liu",
      "name": "QHL067",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  }
]