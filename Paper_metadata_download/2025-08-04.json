[
  {
    "paper": {
      "id": "2507.23268",
      "authors": [
        {
          "_id": "688c1ec68c434640078cc386",
          "name": "Shuai Wang",
          "hidden": false
        },
        {
          "_id": "688c1ec68c434640078cc387",
          "name": "Ziteng Gao",
          "hidden": false
        },
        {
          "_id": "688c1ec68c434640078cc388",
          "name": "Chenhui Zhu",
          "hidden": false
        },
        {
          "_id": "688c1ec68c434640078cc389",
          "name": "Weilin Huang",
          "hidden": false
        },
        {
          "_id": "688c1ec68c434640078cc38a",
          "name": "Limin Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-31T06:07:20.000Z",
      "submittedOnDailyAt": "2025-08-04T00:45:14.702Z",
      "title": "PixNerd: Pixel Neural Field Diffusion",
      "submittedOnDailyBy": {
        "_id": "66615c855fd9d736e670e0a9",
        "avatarUrl": "/avatars/0ff3127b513552432a7c651e21d7f283.svg",
        "isPro": false,
        "fullname": "wangshuai",
        "user": "wangsssssss",
        "type": "user"
      },
      "summary": "The current success of diffusion transformers heavily depends on the\ncompressed latent space shaped by the pre-trained variational autoencoder(VAE).\nHowever, this two-stage training paradigm inevitably introduces accumulated\nerrors and decoding artifacts. To address the aforementioned problems,\nresearchers return to pixel space at the cost of complicated cascade pipelines\nand increased token complexity. In contrast to their efforts, we propose to\nmodel the patch-wise decoding with neural field and present a single-scale,\nsingle-stage, efficient, end-to-end solution, coined as pixel neural field\ndiffusion~(PixelNerd). Thanks to the efficient neural field representation in\nPixNerd, we directly achieved 2.15 FID on ImageNet 256times256 and 2.84 FID\non ImageNet 512times512 without any complex cascade pipeline or VAE. We also\nextend our PixNerd framework to text-to-image applications. Our PixNerd-XXL/16\nachieved a competitive 0.73 overall score on the GenEval benchmark and 80.9\noverall score on the DPG benchmark.",
      "upvotes": 6,
      "discussionId": "688c1ec68c434640078cc38b",
      "projectPage": "https://huggingface.co/spaces/MCG-NJU/PixNerd",
      "githubRepo": "https://github.com/MCG-NJU/PixNerd",
      "ai_summary": "Pixel Neural Field Diffusion (PixNerd) achieves high-quality image generation in a single-scale, single-stage process without VAEs or complex pipelines, and extends to text-to-image applications with competitive performance.",
      "ai_keywords": [
        "diffusion transformers",
        "compressed latent space",
        "pre-trained variational autoencoder",
        "pixel space",
        "patch-wise decoding",
        "neural field",
        "single-scale",
        "single-stage",
        "end-to-end solution",
        "pixel neural field diffusion",
        "PixNerd",
        "FID",
        "ImageNet",
        "text-to-image",
        "GenEval benchmark",
        "DPG benchmark"
      ]
    },
    "publishedAt": "2025-07-31T02:07:20.000Z",
    "title": "PixNerd: Pixel Neural Field Diffusion",
    "summary": "The current success of diffusion transformers heavily depends on the\ncompressed latent space shaped by the pre-trained variational autoencoder(VAE).\nHowever, this two-stage training paradigm inevitably introduces accumulated\nerrors and decoding artifacts. To address the aforementioned problems,\nresearchers return to pixel space at the cost of complicated cascade pipelines\nand increased token complexity. In contrast to their efforts, we propose to\nmodel the patch-wise decoding with neural field and present a single-scale,\nsingle-stage, efficient, end-to-end solution, coined as pixel neural field\ndiffusion~(PixelNerd). Thanks to the efficient neural field representation in\nPixNerd, we directly achieved 2.15 FID on ImageNet 256times256 and 2.84 FID\non ImageNet 512times512 without any complex cascade pipeline or VAE. We also\nextend our PixNerd framework to text-to-image applications. Our PixNerd-XXL/16\nachieved a competitive 0.73 overall score on the GenEval benchmark and 80.9\noverall score on the DPG benchmark.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.23268.png",
    "numComments": 3,
    "submittedBy": {
      "_id": "66615c855fd9d736e670e0a9",
      "avatarUrl": "/avatars/0ff3127b513552432a7c651e21d7f283.svg",
      "fullname": "wangshuai",
      "name": "wangsssssss",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 8
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2508.00819",
      "authors": [
        {
          "_id": "689020e10a411b3b8d28d67f",
          "name": "Jinsong Li",
          "hidden": false
        },
        {
          "_id": "689020e10a411b3b8d28d680",
          "name": "Xiaoyi Dong",
          "hidden": false
        },
        {
          "_id": "689020e10a411b3b8d28d681",
          "name": "Yuhang Zang",
          "hidden": false
        },
        {
          "_id": "689020e10a411b3b8d28d682",
          "name": "Yuhang Cao",
          "hidden": false
        },
        {
          "_id": "689020e10a411b3b8d28d683",
          "name": "Jiaqi Wang",
          "hidden": false
        },
        {
          "_id": "689020e10a411b3b8d28d684",
          "name": "Dahua Lin",
          "hidden": false
        }
      ],
      "publishedAt": "2025-08-01T17:56:07.000Z",
      "submittedOnDailyAt": "2025-08-04T01:25:12.035Z",
      "title": "Beyond Fixed: Variable-Length Denoising for Diffusion Large Language\n  Models",
      "submittedOnDailyBy": {
        "_id": "64b4eec4faa3181a5eab9c46",
        "avatarUrl": "/avatars/bcc9bf5cbf67546ad2b4c9ec8b96ac96.svg",
        "isPro": true,
        "fullname": "Jiaqi Wang",
        "user": "myownskyW7",
        "type": "user"
      },
      "summary": "Diffusion Large Language Models (DLLMs) are emerging as a powerful\nalternative to the dominant Autoregressive Large Language Models, offering\nefficient parallel generation and capable global context modeling. However, the\npractical application of DLLMs is hindered by a critical architectural\nconstraint: the need for a statically predefined generation length. This static\nlength allocation leads to a problematic trade-off: insufficient lengths\ncripple performance on complex tasks, while excessive lengths incur significant\ncomputational overhead and sometimes result in performance degradation. While\nthe inference framework is rigid, we observe that the model itself possesses\ninternal signals that correlate with the optimal response length for a given\ntask. To bridge this gap, we leverage these latent signals and introduce\nDAEDAL, a novel training-free denoising strategy that enables Dynamic Adaptive\nLength Expansion for Diffusion Large Language Models. DAEDAL operates in two\nphases: 1) Before the denoising process, DAEDAL starts from a short initial\nlength and iteratively expands it to a coarse task-appropriate length, guided\nby a sequence completion metric. 2) During the denoising process, DAEDAL\ndynamically intervenes by pinpointing and expanding insufficient generation\nregions through mask token insertion, ensuring the final output is fully\ndeveloped. Extensive experiments on DLLMs demonstrate that DAEDAL achieves\nperformance comparable, and in some cases superior, to meticulously tuned\nfixed-length baselines, while simultaneously enhancing computational efficiency\nby achieving a higher effective token ratio. By resolving the static length\nconstraint, DAEDAL unlocks new potential for DLLMs, bridging a critical gap\nwith their Autoregressive counterparts and paving the way for more efficient\nand capable generation.",
      "upvotes": 3,
      "discussionId": "689020e20a411b3b8d28d685",
      "ai_summary": "DAEDAL, a novel training-free denoising strategy, enables dynamic length adaptation in Diffusion Large Language Models, improving performance and computational efficiency.",
      "ai_keywords": [
        "Diffusion Large Language Models",
        "DLLMs",
        "Autoregressive Large Language Models",
        "denoising strategy",
        "Dynamic Adaptive Length Expansion",
        "sequence completion metric",
        "mask token insertion",
        "effective token ratio"
      ]
    },
    "publishedAt": "2025-08-01T13:56:07.000Z",
    "title": "Beyond Fixed: Variable-Length Denoising for Diffusion Large Language\n  Models",
    "summary": "Diffusion Large Language Models (DLLMs) are emerging as a powerful\nalternative to the dominant Autoregressive Large Language Models, offering\nefficient parallel generation and capable global context modeling. However, the\npractical application of DLLMs is hindered by a critical architectural\nconstraint: the need for a statically predefined generation length. This static\nlength allocation leads to a problematic trade-off: insufficient lengths\ncripple performance on complex tasks, while excessive lengths incur significant\ncomputational overhead and sometimes result in performance degradation. While\nthe inference framework is rigid, we observe that the model itself possesses\ninternal signals that correlate with the optimal response length for a given\ntask. To bridge this gap, we leverage these latent signals and introduce\nDAEDAL, a novel training-free denoising strategy that enables Dynamic Adaptive\nLength Expansion for Diffusion Large Language Models. DAEDAL operates in two\nphases: 1) Before the denoising process, DAEDAL starts from a short initial\nlength and iteratively expands it to a coarse task-appropriate length, guided\nby a sequence completion metric. 2) During the denoising process, DAEDAL\ndynamically intervenes by pinpointing and expanding insufficient generation\nregions through mask token insertion, ensuring the final output is fully\ndeveloped. Extensive experiments on DLLMs demonstrate that DAEDAL achieves\nperformance comparable, and in some cases superior, to meticulously tuned\nfixed-length baselines, while simultaneously enhancing computational efficiency\nby achieving a higher effective token ratio. By resolving the static length\nconstraint, DAEDAL unlocks new potential for DLLMs, bridging a critical gap\nwith their Autoregressive counterparts and paving the way for more efficient\nand capable generation.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.00819.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64b4eec4faa3181a5eab9c46",
      "avatarUrl": "/avatars/bcc9bf5cbf67546ad2b4c9ec8b96ac96.svg",
      "fullname": "Jiaqi Wang",
      "name": "myownskyW7",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 21
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.23361",
      "authors": [
        {
          "_id": "688c53f38c434640078cc47c",
          "name": "Silin Chen",
          "hidden": false
        },
        {
          "_id": "688c53f38c434640078cc47d",
          "name": "Shaoxin Lin",
          "hidden": false
        },
        {
          "_id": "688c53f38c434640078cc47e",
          "name": "Xiaodong Gu",
          "hidden": false
        },
        {
          "_id": "688c53f38c434640078cc47f",
          "name": "Yuling Shi",
          "hidden": false
        },
        {
          "_id": "688c53f38c434640078cc480",
          "name": "Heng Lian",
          "hidden": false
        },
        {
          "_id": "688c53f38c434640078cc481",
          "name": "Longfei Yun",
          "hidden": false
        },
        {
          "_id": "688c53f38c434640078cc482",
          "name": "Dong Chen",
          "hidden": false
        },
        {
          "_id": "688c53f38c434640078cc483",
          "name": "Weiguo Sun",
          "hidden": false
        },
        {
          "_id": "688c53f38c434640078cc484",
          "name": "Lin Cao",
          "hidden": false
        },
        {
          "_id": "688c53f38c434640078cc485",
          "name": "Qianxiang Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-31T09:13:42.000Z",
      "submittedOnDailyAt": "2025-08-04T01:12:58.702Z",
      "title": "SWE-Exp: Experience-Driven Software Issue Resolution",
      "submittedOnDailyBy": {
        "_id": "645b0c3ec35da9c7afd95421",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/645b0c3ec35da9c7afd95421/vYBrCDagHsXAo6J2p-uG0.jpeg",
        "isPro": false,
        "fullname": "Yuling",
        "user": "YerbaPage",
        "type": "user"
      },
      "summary": "Recent advances in large language model (LLM) agents have shown remarkable\nprogress in software issue resolution, leveraging advanced techniques such as\nmulti-agent collaboration and Monte Carlo Tree Search (MCTS). However, current\nagents act as memoryless explorers - treating each problem separately without\nretaining or reusing knowledge from previous repair experiences. This leads to\nredundant exploration of failed trajectories and missed chances to adapt\nsuccessful issue resolution methods to similar problems. To address this\nproblem, we introduce SWE-Exp, an experience - enhanced approach that distills\nconcise and actionable experience from prior agent trajectories, enabling\ncontinuous learning across issues. Our method introduces a multi-faceted\nexperience bank that captures both successful and failed repair attempts.\nSpecifically, it extracts reusable issue resolution knowledge at different\nlevels - from high-level problem comprehension to specific code changes.\nExperiments show that SWE-Exp achieves state-of-the-art resolution rate (41.6%\nPass@1) on SWE-bench-Verified under open-source agent frameworks. Our approach\nestablishes a new paradigm in which automated software engineering agents\nsystematically accumulate and leverage repair expertise, fundamentally shifting\nfrom trial-and-error exploration to strategic, experience-driven issue\nresolution.",
      "upvotes": 3,
      "discussionId": "688c53f38c434640078cc486",
      "ai_summary": "SWE-Exp enhances software issue resolution by systematically accumulating and leveraging repair expertise from past agent experiences, improving resolution rates.",
      "ai_keywords": [
        "large language model (LLM)",
        "multi-agent collaboration",
        "Monte Carlo Tree Search (MCTS)",
        "experience bank",
        "issue resolution knowledge",
        "SWE-bench-Verified",
        "open-source agent frameworks"
      ]
    },
    "publishedAt": "2025-07-31T05:13:42.000Z",
    "title": "SWE-Exp: Experience-Driven Software Issue Resolution",
    "summary": "Recent advances in large language model (LLM) agents have shown remarkable\nprogress in software issue resolution, leveraging advanced techniques such as\nmulti-agent collaboration and Monte Carlo Tree Search (MCTS). However, current\nagents act as memoryless explorers - treating each problem separately without\nretaining or reusing knowledge from previous repair experiences. This leads to\nredundant exploration of failed trajectories and missed chances to adapt\nsuccessful issue resolution methods to similar problems. To address this\nproblem, we introduce SWE-Exp, an experience - enhanced approach that distills\nconcise and actionable experience from prior agent trajectories, enabling\ncontinuous learning across issues. Our method introduces a multi-faceted\nexperience bank that captures both successful and failed repair attempts.\nSpecifically, it extracts reusable issue resolution knowledge at different\nlevels - from high-level problem comprehension to specific code changes.\nExperiments show that SWE-Exp achieves state-of-the-art resolution rate (41.6%\nPass@1) on SWE-bench-Verified under open-source agent frameworks. Our approach\nestablishes a new paradigm in which automated software engineering agents\nsystematically accumulate and leverage repair expertise, fundamentally shifting\nfrom trial-and-error exploration to strategic, experience-driven issue\nresolution.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.23361.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "645b0c3ec35da9c7afd95421",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/645b0c3ec35da9c7afd95421/vYBrCDagHsXAo6J2p-uG0.jpeg",
      "fullname": "Yuling",
      "name": "YerbaPage",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 264
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.23348",
      "authors": [
        {
          "_id": "688c53d88c434640078cc471",
          "name": "Han Li",
          "hidden": false
        },
        {
          "_id": "688c53d88c434640078cc472",
          "name": "Yuling Shi",
          "hidden": false
        },
        {
          "_id": "688c53d88c434640078cc473",
          "name": "Shaoxin Lin",
          "hidden": false
        },
        {
          "_id": "688c53d88c434640078cc474",
          "name": "Xiaodong Gu",
          "hidden": false
        },
        {
          "_id": "688c53d88c434640078cc475",
          "name": "Heng Lian",
          "hidden": false
        },
        {
          "_id": "688c53d88c434640078cc476",
          "name": "Xin Wang",
          "hidden": false
        },
        {
          "_id": "688c53d88c434640078cc477",
          "name": "Yantao Jia",
          "hidden": false
        },
        {
          "_id": "688c53d88c434640078cc478",
          "name": "Tao Huang",
          "hidden": false
        },
        {
          "_id": "688c53d88c434640078cc479",
          "name": "Qianxiang Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-31T08:54:46.000Z",
      "submittedOnDailyAt": "2025-08-04T01:11:57.584Z",
      "title": "SWE-Debate: Competitive Multi-Agent Debate for Software Issue Resolution",
      "submittedOnDailyBy": {
        "_id": "645b0c3ec35da9c7afd95421",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/645b0c3ec35da9c7afd95421/vYBrCDagHsXAo6J2p-uG0.jpeg",
        "isPro": false,
        "fullname": "Yuling",
        "user": "YerbaPage",
        "type": "user"
      },
      "summary": "Issue resolution has made remarkable progress thanks to the advanced\nreasoning capabilities of large language models (LLMs). Recently, agent-based\nframeworks such as SWE-agent have further advanced this progress by enabling\nautonomous, tool-using agents to tackle complex software engineering tasks.\nWhile existing agent-based issue resolution approaches are primarily based on\nagents' independent explorations, they often get stuck in local solutions and\nfail to identify issue patterns that span across different parts of the\ncodebase. To address this limitation, we propose SWE-Debate, a competitive\nmulti-agent debate framework that encourages diverse reasoning paths and\nachieves more consolidated issue localization. SWE-Debate first creates\nmultiple fault propagation traces as localization proposals by traversing a\ncode dependency graph. Then, it organizes a three-round debate among\nspecialized agents, each embodying distinct reasoning perspectives along the\nfault propagation trace. This structured competition enables agents to\ncollaboratively converge on a consolidated fix plan. Finally, this consolidated\nfix plan is integrated into an MCTS-based code modification agent for patch\ngeneration. Experiments on the SWE-bench benchmark show that SWE-Debate\nachieves new state-of-the-art results in open-source agent frameworks and\noutperforms baselines by a large margin.",
      "upvotes": 3,
      "discussionId": "688c53d98c434640078cc47a",
      "ai_summary": "SWE-Debate, a competitive multi-agent framework, enhances issue resolution in software engineering by promoting diverse reasoning and achieving better issue localization and fix planning.",
      "ai_keywords": [
        "large language models",
        "agent-based frameworks",
        "SWE-agent",
        "autonomous agents",
        "tool-using agents",
        "software engineering tasks",
        "local solutions",
        "issue patterns",
        "codebase",
        "competitive multi-agent debate",
        "fault propagation traces",
        "code dependency graph",
        "specialized agents",
        "reasoning perspectives",
        "structured competition",
        "consolidated fix plan",
        "MCTS-based code modification agent",
        "SWE-bench benchmark"
      ]
    },
    "publishedAt": "2025-07-31T04:54:46.000Z",
    "title": "SWE-Debate: Competitive Multi-Agent Debate for Software Issue Resolution",
    "summary": "Issue resolution has made remarkable progress thanks to the advanced\nreasoning capabilities of large language models (LLMs). Recently, agent-based\nframeworks such as SWE-agent have further advanced this progress by enabling\nautonomous, tool-using agents to tackle complex software engineering tasks.\nWhile existing agent-based issue resolution approaches are primarily based on\nagents' independent explorations, they often get stuck in local solutions and\nfail to identify issue patterns that span across different parts of the\ncodebase. To address this limitation, we propose SWE-Debate, a competitive\nmulti-agent debate framework that encourages diverse reasoning paths and\nachieves more consolidated issue localization. SWE-Debate first creates\nmultiple fault propagation traces as localization proposals by traversing a\ncode dependency graph. Then, it organizes a three-round debate among\nspecialized agents, each embodying distinct reasoning perspectives along the\nfault propagation trace. This structured competition enables agents to\ncollaboratively converge on a consolidated fix plan. Finally, this consolidated\nfix plan is integrated into an MCTS-based code modification agent for patch\ngeneration. Experiments on the SWE-bench benchmark show that SWE-Debate\nachieves new state-of-the-art results in open-source agent frameworks and\noutperforms baselines by a large margin.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.23348.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "645b0c3ec35da9c7afd95421",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/645b0c3ec35da9c7afd95421/vYBrCDagHsXAo6J2p-uG0.jpeg",
      "fullname": "Yuling",
      "name": "YerbaPage",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 264
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2508.00265",
      "authors": [
        {
          "_id": "6890159b0a411b3b8d28d650",
          "name": "Henghui Ding",
          "hidden": false
        },
        {
          "_id": "6890159b0a411b3b8d28d651",
          "name": "Song Tang",
          "hidden": false
        },
        {
          "_id": "6890159b0a411b3b8d28d652",
          "name": "Shuting He",
          "hidden": false
        },
        {
          "_id": "6890159b0a411b3b8d28d653",
          "name": "Chang Liu",
          "hidden": false
        },
        {
          "_id": "6890159b0a411b3b8d28d654",
          "name": "Zuxuan Wu",
          "hidden": false
        },
        {
          "_id": "6890159b0a411b3b8d28d655",
          "name": "Yu-Gang Jiang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-08-01T02:14:00.000Z",
      "submittedOnDailyAt": "2025-08-04T00:37:52.118Z",
      "title": "Multimodal Referring Segmentation: A Survey",
      "submittedOnDailyBy": {
        "_id": "67ff29ecbf6889a333c69c7a",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/fYIJFtF0uciB-1wb0lmTY.png",
        "isPro": false,
        "fullname": "Henghui Ding",
        "user": "HenghuiDing",
        "type": "user"
      },
      "summary": "Multimodal referring segmentation aims to segment target objects in visual\nscenes, such as images, videos, and 3D scenes, based on referring expressions\nin text or audio format. This task plays a crucial role in practical\napplications requiring accurate object perception based on user instructions.\nOver the past decade, it has gained significant attention in the multimodal\ncommunity, driven by advances in convolutional neural networks, transformers,\nand large language models, all of which have substantially improved multimodal\nperception capabilities. This paper provides a comprehensive survey of\nmultimodal referring segmentation. We begin by introducing this field's\nbackground, including problem definitions and commonly used datasets. Next, we\nsummarize a unified meta architecture for referring segmentation and review\nrepresentative methods across three primary visual scenes, including images,\nvideos, and 3D scenes. We further discuss Generalized Referring Expression\n(GREx) methods to address the challenges of real-world complexity, along with\nrelated tasks and practical applications. Extensive performance comparisons on\nstandard benchmarks are also provided. We continually track related works at\nhttps://github.com/henghuiding/Awesome-Multimodal-Referring-Segmentation.",
      "upvotes": 2,
      "discussionId": "6890159b0a411b3b8d28d656",
      "ai_summary": "A survey of multimodal referring segmentation techniques, covering advancements in convolutional neural networks, transformers, and large language models for segmenting objects in images, videos, and 3D scenes based on text or audio instructions.",
      "ai_keywords": [
        "convolutional neural networks",
        "transformers",
        "large language models",
        "multimodal referring segmentation",
        "Generalized Referring Expression (GREx)"
      ]
    },
    "publishedAt": "2025-07-31T22:14:00.000Z",
    "title": "Multimodal Referring Segmentation: A Survey",
    "summary": "Multimodal referring segmentation aims to segment target objects in visual\nscenes, such as images, videos, and 3D scenes, based on referring expressions\nin text or audio format. This task plays a crucial role in practical\napplications requiring accurate object perception based on user instructions.\nOver the past decade, it has gained significant attention in the multimodal\ncommunity, driven by advances in convolutional neural networks, transformers,\nand large language models, all of which have substantially improved multimodal\nperception capabilities. This paper provides a comprehensive survey of\nmultimodal referring segmentation. We begin by introducing this field's\nbackground, including problem definitions and commonly used datasets. Next, we\nsummarize a unified meta architecture for referring segmentation and review\nrepresentative methods across three primary visual scenes, including images,\nvideos, and 3D scenes. We further discuss Generalized Referring Expression\n(GREx) methods to address the challenges of real-world complexity, along with\nrelated tasks and practical applications. Extensive performance comparisons on\nstandard benchmarks are also provided. We continually track related works at\nhttps://github.com/henghuiding/Awesome-Multimodal-Referring-Segmentation.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.00265.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "67ff29ecbf6889a333c69c7a",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/fYIJFtF0uciB-1wb0lmTY.png",
      "fullname": "Henghui Ding",
      "name": "HenghuiDing",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2508.00454",
      "authors": [
        {
          "_id": "689022980a411b3b8d28d69e",
          "name": "Yuqi Tang",
          "hidden": false
        },
        {
          "_id": "689022980a411b3b8d28d69f",
          "name": "Kehua Feng",
          "hidden": false
        },
        {
          "_id": "689022980a411b3b8d28d6a0",
          "name": "Yunfeng Wang",
          "hidden": false
        },
        {
          "_id": "689022980a411b3b8d28d6a1",
          "name": "Zhiwen Chen",
          "hidden": false
        },
        {
          "_id": "689022980a411b3b8d28d6a2",
          "name": "Chengfei Lv",
          "hidden": false
        },
        {
          "_id": "689022980a411b3b8d28d6a3",
          "name": "Gang Yu",
          "hidden": false
        },
        {
          "_id": "689022980a411b3b8d28d6a4",
          "name": "Qiang Zhang",
          "hidden": false
        },
        {
          "_id": "689022980a411b3b8d28d6a5",
          "name": "Keyan Ding",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6479e038a26759caa62ea433/Fz7YpJV6QnC1THsv7jjSt.png"
      ],
      "publishedAt": "2025-08-01T09:26:01.000Z",
      "submittedOnDailyAt": "2025-08-04T01:39:47.788Z",
      "title": "Learning an Efficient Multi-Turn Dialogue Evaluator from Multiple Judges",
      "submittedOnDailyBy": {
        "_id": "6479e038a26759caa62ea433",
        "avatarUrl": "/avatars/7a9a023b1fae802eedbe90ec40c791d1.svg",
        "isPro": false,
        "fullname": "Feng",
        "user": "Kehua",
        "type": "user"
      },
      "summary": "Evaluating the conversational abilities of large language models (LLMs)\nremains a challenging task. Current mainstream approaches primarily rely on the\n``LLM-as-a-judge\" paradigm, where an LLM is prompted to serve as an evaluator\nto assess dialogue quality. However, such methods often suffer from various\nbiases, which undermine the reliability and consistency of the evaluation\nresults. To mitigate these biases, recent methods employ multiple LLMs as\njudges and aggregate their judgments to select the optimal assessment. Although\neffective, this multi-judge approach incurs significant computational overhead\nduring inference. In this paper, we propose an efficient multi-turn dialogue\nevaluator that captures the collective wisdom of multiple LLM judges by\naggregating their preference knowledge into a single model. Our approach\npreserves the advantages of diverse multi-judge feedback while drastically\nreducing the evaluation cost, enabling fast and flexible dialogue quality\nassessment. Extensive experiments on seven single rating and pairwise\ncomparison dialogue evaluation benchmarks demonstrate that our method\noutperforms existing baselines across diverse scenarios, showcasing its\nefficiency and robustness.",
      "upvotes": 0,
      "discussionId": "689022980a411b3b8d28d6a6",
      "ai_summary": "An efficient multi-turn dialogue evaluator aggregates multiple LLM judgments into a single model to assess dialogue quality with reduced computational cost.",
      "ai_keywords": [
        "LLM-as-a-judge",
        "multi-judge approach",
        "preference knowledge",
        "multi-turn dialogue evaluator",
        "dialogue evaluation benchmarks"
      ]
    },
    "publishedAt": "2025-08-01T05:26:01.000Z",
    "title": "Learning an Efficient Multi-Turn Dialogue Evaluator from Multiple Judges",
    "summary": "Evaluating the conversational abilities of large language models (LLMs)\nremains a challenging task. Current mainstream approaches primarily rely on the\n``LLM-as-a-judge\" paradigm, where an LLM is prompted to serve as an evaluator\nto assess dialogue quality. However, such methods often suffer from various\nbiases, which undermine the reliability and consistency of the evaluation\nresults. To mitigate these biases, recent methods employ multiple LLMs as\njudges and aggregate their judgments to select the optimal assessment. Although\neffective, this multi-judge approach incurs significant computational overhead\nduring inference. In this paper, we propose an efficient multi-turn dialogue\nevaluator that captures the collective wisdom of multiple LLM judges by\naggregating their preference knowledge into a single model. Our approach\npreserves the advantages of diverse multi-judge feedback while drastically\nreducing the evaluation cost, enabling fast and flexible dialogue quality\nassessment. Extensive experiments on seven single rating and pairwise\ncomparison dialogue evaluation benchmarks demonstrate that our method\noutperforms existing baselines across diverse scenarios, showcasing its\nefficiency and robustness.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6479e038a26759caa62ea433/Fz7YpJV6QnC1THsv7jjSt.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.00454.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6479e038a26759caa62ea433",
      "avatarUrl": "/avatars/7a9a023b1fae802eedbe90ec40c791d1.svg",
      "fullname": "Feng",
      "name": "Kehua",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  }
]