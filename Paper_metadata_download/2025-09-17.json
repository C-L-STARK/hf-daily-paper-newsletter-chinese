[
  {
    "paper": {
      "id": "2509.13232",
      "authors": [
        {
          "_id": "68ca17a96e0073c09bd1de5f",
          "name": "Zhongwen Xu",
          "hidden": false
        },
        {
          "_id": "68ca17a96e0073c09bd1de60",
          "name": "Zihan Ding",
          "hidden": false
        }
      ],
      "publishedAt": "2025-09-16T16:39:11.000Z",
      "submittedOnDailyAt": "2025-09-17T00:38:24.543Z",
      "title": "Single-stream Policy Optimization",
      "submittedOnDailyBy": {
        "_id": "672db76fa34d64e774fc42c9",
        "avatarUrl": "/avatars/be529f4bfcc40514697facbe8e874735.svg",
        "isPro": false,
        "fullname": "Zhongwen Xu",
        "user": "zhongwenxu",
        "type": "user"
      },
      "summary": "We revisit policy-gradient optimization for Large Language Models (LLMs) from\na single-stream perspective. Prevailing group-based methods like GRPO reduce\nvariance with on-the-fly baselines but suffer from critical flaws: frequent\ndegenerate groups erase learning signals, and synchronization barriers hinder\nscalability. We introduce Single-stream Policy Optimization (SPO), which\neliminates these issues by design. SPO replaces per-group baselines with a\npersistent, KL-adaptive value tracker and normalizes advantages globally across\nthe batch, providing a stable, low-variance learning signal for every sample.\nBeing group-free, SPO enables higher throughput and scales effectively in\nlong-horizon or tool-integrated settings where generation times vary.\nFurthermore, the persistent value tracker naturally enables an adaptive\ncurriculum via prioritized sampling. Experiments using Qwen3-8B show that SPO\nconverges more smoothly and attains higher accuracy than GRPO, while\neliminating computation wasted on degenerate groups. Ablation studies confirm\nthat SPO's gains stem from its principled approach to baseline estimation and\nadvantage normalization, offering a more robust and efficient path for LLM\nreasoning. Across five hard math benchmarks with Qwen3 8B, SPO improves the\naverage maj@32 by +3.4 percentage points (pp) over GRPO, driven by substantial\nabsolute point gains on challenging datasets, including +7.3 pp on BRUMO 25,\n+4.4 pp on AIME 25, +3.3 pp on HMMT 25, and achieves consistent relative gain\nin pass@k across the evaluated k values. SPO's success challenges the\nprevailing trend of adding incidental complexity to RL algorithms, highlighting\na path where fundamental principles, not architectural workarounds, drive the\nnext wave of progress in LLM reasoning.",
      "upvotes": 7,
      "discussionId": "68ca17a96e0073c09bd1de61",
      "ai_summary": "Single-stream Policy Optimization (SPO) improves policy-gradient training for Large Language Models by eliminating group-based issues and providing a stable, low-variance learning signal, leading to better performance and efficiency.",
      "ai_keywords": [
        "policy-gradient optimization",
        "Large Language Models (LLMs)",
        "GRPO",
        "variance reduction",
        "on-the-fly baselines",
        "degenerate groups",
        "synchronization barriers",
        "Single-stream Policy Optimization (SPO)",
        "KL-adaptive value tracker",
        "advantage normalization",
        "adaptive curriculum",
        "prioritized sampling",
        "Qwen3-8B",
        "hard math benchmarks",
        "maj@32",
        "BRUMO 25",
        "AIME 25",
        "HMMT 25",
        "pass@$k$"
      ]
    },
    "publishedAt": "2025-09-16T12:39:11.000Z",
    "title": "Single-stream Policy Optimization",
    "summary": "We revisit policy-gradient optimization for Large Language Models (LLMs) from\na single-stream perspective. Prevailing group-based methods like GRPO reduce\nvariance with on-the-fly baselines but suffer from critical flaws: frequent\ndegenerate groups erase learning signals, and synchronization barriers hinder\nscalability. We introduce Single-stream Policy Optimization (SPO), which\neliminates these issues by design. SPO replaces per-group baselines with a\npersistent, KL-adaptive value tracker and normalizes advantages globally across\nthe batch, providing a stable, low-variance learning signal for every sample.\nBeing group-free, SPO enables higher throughput and scales effectively in\nlong-horizon or tool-integrated settings where generation times vary.\nFurthermore, the persistent value tracker naturally enables an adaptive\ncurriculum via prioritized sampling. Experiments using Qwen3-8B show that SPO\nconverges more smoothly and attains higher accuracy than GRPO, while\neliminating computation wasted on degenerate groups. Ablation studies confirm\nthat SPO's gains stem from its principled approach to baseline estimation and\nadvantage normalization, offering a more robust and efficient path for LLM\nreasoning. Across five hard math benchmarks with Qwen3 8B, SPO improves the\naverage maj@32 by +3.4 percentage points (pp) over GRPO, driven by substantial\nabsolute point gains on challenging datasets, including +7.3 pp on BRUMO 25,\n+4.4 pp on AIME 25, +3.3 pp on HMMT 25, and achieves consistent relative gain\nin pass@k across the evaluated k values. SPO's success challenges the\nprevailing trend of adding incidental complexity to RL algorithms, highlighting\na path where fundamental principles, not architectural workarounds, drive the\nnext wave of progress in LLM reasoning.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.13232.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "672db76fa34d64e774fc42c9",
      "avatarUrl": "/avatars/be529f4bfcc40514697facbe8e874735.svg",
      "fullname": "Zhongwen Xu",
      "name": "zhongwenxu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2509.13317",
      "authors": [
        {
          "_id": "68ca17616e0073c09bd1de4b",
          "name": "An-Chieh Cheng",
          "hidden": false
        },
        {
          "_id": "68ca17616e0073c09bd1de4c",
          "name": "Yang Fu",
          "hidden": false
        },
        {
          "_id": "68ca17616e0073c09bd1de4d",
          "name": "Yukang Chen",
          "hidden": false
        },
        {
          "_id": "68ca17616e0073c09bd1de4e",
          "name": "Zhijian Liu",
          "hidden": false
        },
        {
          "_id": "68ca17616e0073c09bd1de4f",
          "name": "Xiaolong Li",
          "hidden": false
        },
        {
          "_id": "68ca17616e0073c09bd1de50",
          "name": "Subhashree Radhakrishnan",
          "hidden": false
        },
        {
          "_id": "68ca17616e0073c09bd1de51",
          "name": "Song Han",
          "hidden": false
        },
        {
          "_id": "68ca17616e0073c09bd1de52",
          "name": "Yao Lu",
          "hidden": false
        },
        {
          "_id": "68ca17616e0073c09bd1de53",
          "name": "Jan Kautz",
          "hidden": false
        },
        {
          "_id": "68ca17616e0073c09bd1de54",
          "name": "Pavlo Molchanov",
          "hidden": false
        },
        {
          "_id": "68ca17616e0073c09bd1de55",
          "name": "Hongxu Yin",
          "hidden": false
        },
        {
          "_id": "68ca17616e0073c09bd1de56",
          "name": "Xiaolong Wang",
          "hidden": false
        },
        {
          "_id": "68ca17616e0073c09bd1de57",
          "name": "Sifei Liu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-09-16T17:59:06.000Z",
      "submittedOnDailyAt": "2025-09-17T00:35:36.289Z",
      "title": "3D Aware Region Prompted Vision Language Model",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "We present Spatial Region 3D (SR-3D) aware vision-language model that\nconnects single-view 2D images and multi-view 3D data through a shared visual\ntoken space. SR-3D supports flexible region prompting, allowing users to\nannotate regions with bounding boxes, segmentation masks on any frame, or\ndirectly in 3D, without the need for exhaustive multi-frame labeling. We\nachieve this by enriching 2D visual features with 3D positional embeddings,\nwhich allows the 3D model to draw upon strong 2D priors for more accurate\nspatial reasoning across frames, even when objects of interest do not co-occur\nwithin the same view. Extensive experiments on both general 2D vision language\nand specialized 3D spatial benchmarks demonstrate that SR-3D achieves\nstate-of-the-art performance, underscoring its effectiveness for unifying 2D\nand 3D representation space on scene understanding. Moreover, we observe\napplicability to in-the-wild videos without sensory 3D inputs or ground-truth\n3D annotations, where SR-3D accurately infers spatial relationships and metric\nmeasurements.",
      "upvotes": 1,
      "discussionId": "68ca17616e0073c09bd1de58",
      "projectPage": "https://www.anjiecheng.me/sr3d",
      "ai_summary": "A Spatial Region 3D (SR-3D) vision-language model unifies 2D and 3D representations by enriching 2D features with 3D positional embeddings, enabling flexible region prompting and accurate spatial reasoning across frames.",
      "ai_keywords": [
        "Spatial Region 3D",
        "SR-3D",
        "vision-language model",
        "visual token space",
        "region prompting",
        "bounding boxes",
        "segmentation masks",
        "3D positional embeddings",
        "spatial reasoning",
        "scene understanding",
        "in-the-wild videos",
        "spatial relationships",
        "metric measurements"
      ]
    },
    "publishedAt": "2025-09-16T13:59:06.000Z",
    "title": "3D Aware Region Prompted Vision Language Model",
    "summary": "We present Spatial Region 3D (SR-3D) aware vision-language model that\nconnects single-view 2D images and multi-view 3D data through a shared visual\ntoken space. SR-3D supports flexible region prompting, allowing users to\nannotate regions with bounding boxes, segmentation masks on any frame, or\ndirectly in 3D, without the need for exhaustive multi-frame labeling. We\nachieve this by enriching 2D visual features with 3D positional embeddings,\nwhich allows the 3D model to draw upon strong 2D priors for more accurate\nspatial reasoning across frames, even when objects of interest do not co-occur\nwithin the same view. Extensive experiments on both general 2D vision language\nand specialized 3D spatial benchmarks demonstrate that SR-3D achieves\nstate-of-the-art performance, underscoring its effectiveness for unifying 2D\nand 3D representation space on scene understanding. Moreover, we observe\napplicability to in-the-wild videos without sensory 3D inputs or ground-truth\n3D annotations, where SR-3D accurately infers spatial relationships and metric\nmeasurements.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.13317.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 106
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2509.13312",
      "authors": [
        {
          "_id": "68ca16756e0073c09bd1de0f",
          "name": "Zijian Li",
          "hidden": false
        },
        {
          "_id": "68ca16756e0073c09bd1de10",
          "name": "Xin Guan",
          "hidden": false
        },
        {
          "_id": "68ca16756e0073c09bd1de11",
          "name": "Bo Zhang",
          "hidden": false
        },
        {
          "_id": "68ca16756e0073c09bd1de12",
          "name": "Shen Huang",
          "hidden": false
        },
        {
          "_id": "68ca16756e0073c09bd1de13",
          "name": "Houquan Zhou",
          "hidden": false
        },
        {
          "_id": "68ca16756e0073c09bd1de14",
          "name": "Shaopeng Lai",
          "hidden": false
        },
        {
          "_id": "68ca16756e0073c09bd1de15",
          "name": "Ming Yan",
          "hidden": false
        },
        {
          "_id": "68ca16756e0073c09bd1de16",
          "name": "Yong Jiang",
          "hidden": false
        },
        {
          "_id": "68ca16756e0073c09bd1de17",
          "name": "Pengjun Xie",
          "hidden": false
        },
        {
          "_id": "68ca16756e0073c09bd1de18",
          "name": "Fei Huang",
          "hidden": false
        },
        {
          "_id": "68ca16756e0073c09bd1de19",
          "name": "Jun Zhang",
          "hidden": false
        },
        {
          "_id": "68ca16756e0073c09bd1de1a",
          "name": "Jingren Zhou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-09-16T17:57:21.000Z",
      "submittedOnDailyAt": "2025-09-17T00:31:34.945Z",
      "title": "WebWeaver: Structuring Web-Scale Evidence with Dynamic Outlines for\n  Open-Ended Deep Research",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "This paper tackles open-ended deep research (OEDR), a complex challenge where\nAI agents must synthesize vast web-scale information into insightful reports.\nCurrent approaches are plagued by dual-fold limitations: static research\npipelines that decouple planning from evidence acquisition and one-shot\ngeneration paradigms that easily suffer from long-context failure issues like\n\"loss in the middle\" and hallucinations. To address these challenges, we\nintroduce WebWeaver, a novel dual-agent framework that emulates the human\nresearch process. The planner operates in a dynamic cycle, iteratively\ninterleaving evidence acquisition with outline optimization to produce a\ncomprehensive, source-grounded outline linking to a memory bank of evidence.\nThe writer then executes a hierarchical retrieval and writing process,\ncomposing the report section by section. By performing targeted retrieval of\nonly the necessary evidence from the memory bank for each part, it effectively\nmitigates long-context issues. Our framework establishes a new state-of-the-art\nacross major OEDR benchmarks, including DeepResearch Bench, DeepConsult, and\nDeepResearchGym. These results validate our human-centric, iterative\nmethodology, demonstrating that adaptive planning and focused synthesis are\ncrucial for producing high-quality, reliable, and well-structured reports.",
      "upvotes": 1,
      "discussionId": "68ca16756e0073c09bd1de1b",
      "projectPage": "https://tongyi-agent.github.io/blog/",
      "githubRepo": "https://github.com/Alibaba-NLP/DeepResearch",
      "ai_summary": "WebWeaver, a dual-agent framework, addresses open-ended deep research challenges by integrating adaptive planning and focused synthesis to produce high-quality, reliable reports.",
      "ai_keywords": [
        "open-ended deep research",
        "AI agents",
        "static research pipelines",
        "one-shot generation",
        "long-context failure",
        "loss in the middle",
        "hallucinations",
        "dual-agent framework",
        "human research process",
        "planner",
        "evidence acquisition",
        "outline optimization",
        "memory bank",
        "writer",
        "hierarchical retrieval",
        "writing process",
        "DeepResearch Bench",
        "DeepConsult",
        "DeepResearchGym"
      ],
      "githubStars": 6737
    },
    "publishedAt": "2025-09-16T13:57:21.000Z",
    "title": "WebWeaver: Structuring Web-Scale Evidence with Dynamic Outlines for\n  Open-Ended Deep Research",
    "summary": "This paper tackles open-ended deep research (OEDR), a complex challenge where\nAI agents must synthesize vast web-scale information into insightful reports.\nCurrent approaches are plagued by dual-fold limitations: static research\npipelines that decouple planning from evidence acquisition and one-shot\ngeneration paradigms that easily suffer from long-context failure issues like\n\"loss in the middle\" and hallucinations. To address these challenges, we\nintroduce WebWeaver, a novel dual-agent framework that emulates the human\nresearch process. The planner operates in a dynamic cycle, iteratively\ninterleaving evidence acquisition with outline optimization to produce a\ncomprehensive, source-grounded outline linking to a memory bank of evidence.\nThe writer then executes a hierarchical retrieval and writing process,\ncomposing the report section by section. By performing targeted retrieval of\nonly the necessary evidence from the memory bank for each part, it effectively\nmitigates long-context issues. Our framework establishes a new state-of-the-art\nacross major OEDR benchmarks, including DeepResearch Bench, DeepConsult, and\nDeepResearchGym. These results validate our human-centric, iterative\nmethodology, demonstrating that adaptive planning and focused synthesis are\ncrucial for producing high-quality, reliable, and well-structured reports.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.13312.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 106
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2509.13311",
      "authors": [
        {
          "_id": "68ca16c06e0073c09bd1de1d",
          "name": "Runnan Fang",
          "hidden": false
        },
        {
          "_id": "68ca16c06e0073c09bd1de1e",
          "name": "Shihao Cai",
          "hidden": false
        },
        {
          "_id": "68ca16c06e0073c09bd1de1f",
          "name": "Baixuan Li",
          "hidden": false
        },
        {
          "_id": "68ca16c06e0073c09bd1de20",
          "name": "Jialong Wu",
          "hidden": false
        },
        {
          "_id": "68ca16c06e0073c09bd1de21",
          "name": "Guangyu Li",
          "hidden": false
        },
        {
          "_id": "68ca16c06e0073c09bd1de22",
          "name": "Wenbiao Yin",
          "hidden": false
        },
        {
          "_id": "68ca16c06e0073c09bd1de23",
          "name": "Xinyu Wang",
          "hidden": false
        },
        {
          "_id": "68ca16c06e0073c09bd1de24",
          "name": "Xiaobin Wang",
          "hidden": false
        },
        {
          "_id": "68ca16c06e0073c09bd1de25",
          "name": "Liangcai Su",
          "hidden": false
        },
        {
          "_id": "68ca16c06e0073c09bd1de26",
          "name": "Zhen Zhang",
          "hidden": false
        },
        {
          "_id": "68ca16c06e0073c09bd1de27",
          "name": "Shibin Wu",
          "hidden": false
        },
        {
          "_id": "68ca16c06e0073c09bd1de28",
          "name": "Zhengwei Tao",
          "hidden": false
        },
        {
          "_id": "68ca16c06e0073c09bd1de29",
          "name": "Yong Jiang",
          "hidden": false
        },
        {
          "_id": "68ca16c06e0073c09bd1de2a",
          "name": "Pengjun Xie",
          "hidden": false
        },
        {
          "_id": "68ca16c06e0073c09bd1de2b",
          "name": "Fei Huang",
          "hidden": false
        },
        {
          "_id": "68ca16c06e0073c09bd1de2c",
          "name": "Jingren Zhou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-09-16T17:57:20.000Z",
      "submittedOnDailyAt": "2025-09-17T00:32:53.838Z",
      "title": "Towards General Agentic Intelligence via Environment Scaling",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Advanced agentic intelligence is a prerequisite for deploying Large Language\nModels in practical, real-world applications. Diverse real-world APIs demand\nprecise, robust function-calling intelligence, which needs agents to develop\nthese capabilities through interaction in varied environments. The breadth of\nfunction-calling competence is closely tied to the diversity of environments in\nwhich agents are trained. In this work, we scale up environments as a step\ntowards advancing general agentic intelligence. This gives rise to two central\nchallenges: (i) how to scale environments in a principled manner, and (ii) how\nto effectively train agentic capabilities from experiences derived through\ninteractions with these environments. To address these, we design a scalable\nframework that automatically constructs heterogeneous environments that are\nfully simulated, systematically broadening the space of function-calling\nscenarios. We further adapt a two-phase agent fine-tuning strategy: first\nendowing agents with fundamental agentic capabilities, then specializing them\nfor domain-specific contexts. Extensive experiments on agentic benchmarks,\ntau-bench, tau2-Bench, and ACEBench, demonstrate that our trained model,\nAgentScaler, significantly enhances the function-calling capability of models.",
      "upvotes": 1,
      "discussionId": "68ca16c16e0073c09bd1de2d",
      "ai_summary": "A scalable framework and two-phase fine-tuning strategy enhance function-calling capabilities of agents in diverse environments, improving performance on agentic benchmarks.",
      "ai_keywords": [
        "agentic intelligence",
        "Large Language Models",
        "function-calling intelligence",
        "heterogeneous environments",
        "two-phase agent fine-tuning",
        "tau-bench",
        "tau2-Bench",
        "ACEBench",
        "AgentScaler"
      ]
    },
    "publishedAt": "2025-09-16T13:57:20.000Z",
    "title": "Towards General Agentic Intelligence via Environment Scaling",
    "summary": "Advanced agentic intelligence is a prerequisite for deploying Large Language\nModels in practical, real-world applications. Diverse real-world APIs demand\nprecise, robust function-calling intelligence, which needs agents to develop\nthese capabilities through interaction in varied environments. The breadth of\nfunction-calling competence is closely tied to the diversity of environments in\nwhich agents are trained. In this work, we scale up environments as a step\ntowards advancing general agentic intelligence. This gives rise to two central\nchallenges: (i) how to scale environments in a principled manner, and (ii) how\nto effectively train agentic capabilities from experiences derived through\ninteractions with these environments. To address these, we design a scalable\nframework that automatically constructs heterogeneous environments that are\nfully simulated, systematically broadening the space of function-calling\nscenarios. We further adapt a two-phase agent fine-tuning strategy: first\nendowing agents with fundamental agentic capabilities, then specializing them\nfor domain-specific contexts. Extensive experiments on agentic benchmarks,\ntau-bench, tau2-Bench, and ACEBench, demonstrate that our trained model,\nAgentScaler, significantly enhances the function-calling capability of models.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.13311.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 106
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2509.13309",
      "authors": [
        {
          "_id": "68ca171d6e0073c09bd1de39",
          "name": "Zile Qiao",
          "hidden": false
        },
        {
          "_id": "68ca171d6e0073c09bd1de3a",
          "name": "Guoxin Chen",
          "hidden": false
        },
        {
          "_id": "68ca171d6e0073c09bd1de3b",
          "name": "Xuanzhong Chen",
          "hidden": false
        },
        {
          "_id": "68ca171d6e0073c09bd1de3c",
          "name": "Donglei Yu",
          "hidden": false
        },
        {
          "_id": "68ca171d6e0073c09bd1de3d",
          "name": "Wenbiao Yin",
          "hidden": false
        },
        {
          "_id": "68ca171d6e0073c09bd1de3e",
          "name": "Xinyu Wang",
          "hidden": false
        },
        {
          "_id": "68ca171d6e0073c09bd1de3f",
          "name": "Zhen Zhang",
          "hidden": false
        },
        {
          "_id": "68ca171d6e0073c09bd1de40",
          "name": "Baixuan Li",
          "hidden": false
        },
        {
          "_id": "68ca171d6e0073c09bd1de41",
          "name": "Huifeng Yin",
          "hidden": false
        },
        {
          "_id": "68ca171d6e0073c09bd1de42",
          "name": "Kuan Li",
          "hidden": false
        },
        {
          "_id": "68ca171d6e0073c09bd1de43",
          "name": "Rui Min",
          "hidden": false
        },
        {
          "_id": "68ca171d6e0073c09bd1de44",
          "name": "Minpeng Liao",
          "hidden": false
        },
        {
          "_id": "68ca171d6e0073c09bd1de45",
          "name": "Yong Jiang",
          "hidden": false
        },
        {
          "_id": "68ca171d6e0073c09bd1de46",
          "name": "Pengjun Xie",
          "hidden": false
        },
        {
          "_id": "68ca171d6e0073c09bd1de47",
          "name": "Fei Huang",
          "hidden": false
        },
        {
          "_id": "68ca171d6e0073c09bd1de48",
          "name": "Jingren Zhou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-09-16T17:57:17.000Z",
      "submittedOnDailyAt": "2025-09-17T00:34:22.753Z",
      "title": "WebResearcher: Unleashing unbounded reasoning capability in Long-Horizon\n  Agents",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Recent advances in deep-research systems have demonstrated the potential for\nAI agents to autonomously discover and synthesize knowledge from external\nsources. In this paper, we introduce WebResearcher, a novel framework for\nbuilding such agents through two key components: (1) WebResearcher, an\niterative deep-research paradigm that reformulates deep research as a Markov\nDecision Process, where agents periodically consolidate findings into evolving\nreports while maintaining focused workspaces, overcoming the context\nsuffocation and noise contamination that plague existing mono-contextual\napproaches; and (2) WebFrontier, a scalable data synthesis engine that\ngenerates high-quality training data through tool-augmented complexity\nescalation, enabling systematic creation of research tasks that bridge the gap\nbetween passive knowledge recall and active knowledge construction. Notably, we\nfind that the training data from our paradigm significantly enhances tool-use\ncapabilities even for traditional mono-contextual methods. Furthermore, our\nparadigm naturally scales through parallel thinking, enabling concurrent\nmulti-agent exploration for more comprehensive conclusions. Extensive\nexperiments across 6 challenging benchmarks demonstrate that WebResearcher\nachieves state-of-the-art performance, even surpassing frontier proprietary\nsystems.",
      "upvotes": 1,
      "discussionId": "68ca171d6e0073c09bd1de49",
      "ai_summary": "WebResearcher, a deep-research framework, enhances AI agents' knowledge synthesis by reformulating research as a Markov Decision Process and using a scalable data synthesis engine, achieving superior performance across benchmarks.",
      "ai_keywords": [
        "deep-research systems",
        "AI agents",
        "Markov Decision Process",
        "WebResearcher",
        "WebFrontier",
        "data synthesis engine",
        "tool-augmented complexity escalation",
        "parallel thinking",
        "multi-agent exploration"
      ]
    },
    "publishedAt": "2025-09-16T13:57:17.000Z",
    "title": "WebResearcher: Unleashing unbounded reasoning capability in Long-Horizon\n  Agents",
    "summary": "Recent advances in deep-research systems have demonstrated the potential for\nAI agents to autonomously discover and synthesize knowledge from external\nsources. In this paper, we introduce WebResearcher, a novel framework for\nbuilding such agents through two key components: (1) WebResearcher, an\niterative deep-research paradigm that reformulates deep research as a Markov\nDecision Process, where agents periodically consolidate findings into evolving\nreports while maintaining focused workspaces, overcoming the context\nsuffocation and noise contamination that plague existing mono-contextual\napproaches; and (2) WebFrontier, a scalable data synthesis engine that\ngenerates high-quality training data through tool-augmented complexity\nescalation, enabling systematic creation of research tasks that bridge the gap\nbetween passive knowledge recall and active knowledge construction. Notably, we\nfind that the training data from our paradigm significantly enhances tool-use\ncapabilities even for traditional mono-contextual methods. Furthermore, our\nparadigm naturally scales through parallel thinking, enabling concurrent\nmulti-agent exploration for more comprehensive conclusions. Extensive\nexperiments across 6 challenging benchmarks demonstrate that WebResearcher\nachieves state-of-the-art performance, even surpassing frontier proprietary\nsystems.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.13309.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 106
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2509.12815",
      "authors": [
        {
          "_id": "68ca18456e0073c09bd1de63",
          "name": "Biwen Lei",
          "hidden": false
        },
        {
          "_id": "68ca18456e0073c09bd1de64",
          "name": "Yang Li",
          "hidden": false
        },
        {
          "_id": "68ca18456e0073c09bd1de65",
          "name": "Xinhai Liu",
          "hidden": false
        },
        {
          "_id": "68ca18456e0073c09bd1de66",
          "name": "Shuhui Yang",
          "hidden": false
        },
        {
          "_id": "68ca18456e0073c09bd1de67",
          "name": "Lixin Xu",
          "hidden": false
        },
        {
          "_id": "68ca18456e0073c09bd1de68",
          "name": "Jingwei Huang",
          "hidden": false
        },
        {
          "_id": "68ca18456e0073c09bd1de69",
          "name": "Ruining Tang",
          "hidden": false
        },
        {
          "_id": "68ca18456e0073c09bd1de6a",
          "name": "Haohan Weng",
          "hidden": false
        },
        {
          "_id": "68ca18456e0073c09bd1de6b",
          "name": "Jian Liu",
          "hidden": false
        },
        {
          "_id": "68ca18456e0073c09bd1de6c",
          "name": "Jing Xu",
          "hidden": false
        },
        {
          "_id": "68ca18456e0073c09bd1de6d",
          "name": "Zhen Zhou",
          "hidden": false
        },
        {
          "_id": "68ca18456e0073c09bd1de6e",
          "name": "Yiling Zhu",
          "hidden": false
        },
        {
          "_id": "68ca18456e0073c09bd1de6f",
          "name": "Jiankai Xing",
          "hidden": false
        },
        {
          "_id": "68ca18456e0073c09bd1de70",
          "name": "Jiachen Xu",
          "hidden": false
        },
        {
          "_id": "68ca18456e0073c09bd1de71",
          "name": "Changfeng Ma",
          "hidden": false
        },
        {
          "_id": "68ca18456e0073c09bd1de72",
          "name": "Xinhao Yan",
          "hidden": false
        },
        {
          "_id": "68ca18456e0073c09bd1de73",
          "name": "Yunhan Yang",
          "hidden": false
        },
        {
          "_id": "68ca18456e0073c09bd1de74",
          "name": "Chunshi Wang",
          "hidden": false
        },
        {
          "_id": "68ca18456e0073c09bd1de75",
          "name": "Duoteng Xu",
          "hidden": false
        },
        {
          "_id": "68ca18456e0073c09bd1de76",
          "name": "Xueqi Ma",
          "hidden": false
        },
        {
          "_id": "68ca18456e0073c09bd1de77",
          "name": "Yuguang Chen",
          "hidden": false
        },
        {
          "_id": "68ca18456e0073c09bd1de78",
          "name": "Jing Li",
          "hidden": false
        },
        {
          "_id": "68ca18456e0073c09bd1de79",
          "name": "Mingxin Yang",
          "hidden": false
        },
        {
          "_id": "68ca18456e0073c09bd1de7a",
          "name": "Sheng Zhang",
          "hidden": false
        },
        {
          "_id": "68ca18456e0073c09bd1de7b",
          "name": "Yifei Feng",
          "hidden": false
        },
        {
          "_id": "68ca18456e0073c09bd1de7c",
          "name": "Xin Huang",
          "hidden": false
        },
        {
          "_id": "68ca18456e0073c09bd1de7d",
          "name": "Di Luo",
          "hidden": false
        },
        {
          "_id": "68ca18456e0073c09bd1de7e",
          "name": "Zebin He",
          "hidden": false
        },
        {
          "_id": "68ca18456e0073c09bd1de7f",
          "name": "Puhua Jiang",
          "hidden": false
        },
        {
          "_id": "68ca18456e0073c09bd1de80",
          "name": "Changrong Hu",
          "hidden": false
        },
        {
          "_id": "68ca18456e0073c09bd1de81",
          "name": "Zihan Qin",
          "hidden": false
        },
        {
          "_id": "68ca18456e0073c09bd1de82",
          "name": "Shiwei Miao",
          "hidden": false
        },
        {
          "_id": "68ca18456e0073c09bd1de83",
          "name": "Haolin Liu",
          "hidden": false
        },
        {
          "_id": "68ca18456e0073c09bd1de84",
          "name": "Yunfei Zhao",
          "hidden": false
        },
        {
          "_id": "68ca18456e0073c09bd1de85",
          "name": "Zeqiang Lai",
          "hidden": false
        },
        {
          "_id": "68ca18456e0073c09bd1de86",
          "name": "Qingxiang Lin",
          "hidden": false
        },
        {
          "_id": "68ca18456e0073c09bd1de87",
          "name": "Zibo Zhao",
          "hidden": false
        },
        {
          "_id": "68ca18456e0073c09bd1de88",
          "name": "Kunhong Li",
          "hidden": false
        },
        {
          "_id": "68ca18456e0073c09bd1de89",
          "name": "Xianghui Yang",
          "hidden": false
        },
        {
          "_id": "68ca18456e0073c09bd1de8a",
          "name": "Huiwen Shi",
          "hidden": false
        },
        {
          "_id": "68ca18456e0073c09bd1de8b",
          "name": "Xin Yang",
          "hidden": false
        },
        {
          "_id": "68ca18456e0073c09bd1de8c",
          "name": "Yuxuan Wang",
          "hidden": false
        },
        {
          "_id": "68ca18456e0073c09bd1de8d",
          "name": "Zebin Yao",
          "hidden": false
        },
        {
          "_id": "68ca18456e0073c09bd1de8e",
          "name": "Yihang Lian",
          "hidden": false
        },
        {
          "_id": "68ca18456e0073c09bd1de8f",
          "name": "Sicong Liu",
          "hidden": false
        },
        {
          "_id": "68ca18456e0073c09bd1de90",
          "name": "Xintong Han",
          "hidden": false
        },
        {
          "_id": "68ca18456e0073c09bd1de91",
          "name": "Wangchen Qin",
          "hidden": false
        },
        {
          "_id": "68ca18456e0073c09bd1de92",
          "name": "Caisheng Ouyang",
          "hidden": false
        },
        {
          "_id": "68ca18456e0073c09bd1de93",
          "name": "Jianyin Liu",
          "hidden": false
        },
        {
          "_id": "68ca18456e0073c09bd1de94",
          "name": "Tianwen Yuan",
          "hidden": false
        },
        {
          "_id": "68ca18456e0073c09bd1de95",
          "name": "Shuai Jiang",
          "hidden": false
        },
        {
          "_id": "68ca18456e0073c09bd1de96",
          "name": "Hong Duan",
          "hidden": false
        },
        {
          "_id": "68ca18456e0073c09bd1de97",
          "name": "Yanqi Niu",
          "hidden": false
        },
        {
          "_id": "68ca18456e0073c09bd1de98",
          "name": "Wencong Lin",
          "hidden": false
        },
        {
          "_id": "68ca18456e0073c09bd1de99",
          "name": "Yifu Sun",
          "hidden": false
        },
        {
          "_id": "68ca18456e0073c09bd1de9a",
          "name": "Shirui Huang",
          "hidden": false
        },
        {
          "_id": "68ca18456e0073c09bd1de9b",
          "name": "Lin Niu",
          "hidden": false
        },
        {
          "_id": "68ca18456e0073c09bd1de9c",
          "name": "Gu Gong",
          "hidden": false
        },
        {
          "_id": "68ca18456e0073c09bd1de9d",
          "name": "Guojian Xiao",
          "hidden": false
        },
        {
          "_id": "68ca18456e0073c09bd1de9e",
          "name": "Bojian Zheng",
          "hidden": false
        },
        {
          "_id": "68ca18456e0073c09bd1de9f",
          "name": "Xiang Yuan",
          "hidden": false
        },
        {
          "_id": "68ca18456e0073c09bd1dea0",
          "name": "Qi Chen",
          "hidden": false
        },
        {
          "_id": "68ca18456e0073c09bd1dea1",
          "name": "Jie Xiao",
          "hidden": false
        },
        {
          "_id": "68ca18456e0073c09bd1dea2",
          "name": "Dongyang Zheng",
          "hidden": false
        },
        {
          "_id": "68ca18456e0073c09bd1dea3",
          "name": "Xiaofeng Yang",
          "hidden": false
        },
        {
          "_id": "68ca18456e0073c09bd1dea4",
          "name": "Kai Liu",
          "hidden": false
        },
        {
          "_id": "68ca18456e0073c09bd1dea5",
          "name": "Jianchen Zhu",
          "hidden": false
        },
        {
          "_id": "68ca18456e0073c09bd1dea6",
          "name": "Lifu Wang",
          "hidden": false
        },
        {
          "_id": "68ca18456e0073c09bd1dea7",
          "name": "Qinglin Lu",
          "hidden": false
        },
        {
          "_id": "68ca18456e0073c09bd1dea8",
          "name": "Jie Liu",
          "hidden": false
        },
        {
          "_id": "68ca18456e0073c09bd1dea9",
          "name": "Liang Dong",
          "hidden": false
        },
        {
          "_id": "68ca18456e0073c09bd1deaa",
          "name": "Fan Jiang",
          "hidden": false
        },
        {
          "_id": "68ca18456e0073c09bd1deab",
          "name": "Ruibin Chen",
          "hidden": false
        },
        {
          "_id": "68ca18456e0073c09bd1deac",
          "name": "Lei Wang",
          "hidden": false
        },
        {
          "_id": "68ca18456e0073c09bd1dead",
          "name": "Chao Zhang",
          "hidden": false
        },
        {
          "_id": "68ca18456e0073c09bd1deae",
          "name": "Jiaxin Lin",
          "hidden": false
        },
        {
          "_id": "68ca18456e0073c09bd1deaf",
          "name": "Hao Zhang",
          "hidden": false
        },
        {
          "_id": "68ca18456e0073c09bd1deb0",
          "name": "Zheng Ye",
          "hidden": false
        },
        {
          "_id": "68ca18456e0073c09bd1deb1",
          "name": "Peng He",
          "hidden": false
        },
        {
          "_id": "68ca18456e0073c09bd1deb2",
          "name": "Runzhou Wu",
          "hidden": false
        },
        {
          "_id": "68ca18456e0073c09bd1deb3",
          "name": "Yinhe Wu",
          "hidden": false
        },
        {
          "_id": "68ca18456e0073c09bd1deb4",
          "name": "Jiayao Du",
          "hidden": false
        },
        {
          "_id": "68ca18456e0073c09bd1deb5",
          "name": "Jupeng Chen",
          "hidden": false
        },
        {
          "_id": "68ca18456e0073c09bd1deb6",
          "name": "Xinyue Mao",
          "hidden": false
        },
        {
          "_id": "68ca18456e0073c09bd1deb7",
          "name": "Dongyuan Guo",
          "hidden": false
        },
        {
          "_id": "68ca18456e0073c09bd1deb8",
          "name": "Yixuan Tang",
          "hidden": false
        },
        {
          "_id": "68ca18456e0073c09bd1deb9",
          "name": "Yulin Tsai",
          "hidden": false
        },
        {
          "_id": "68ca18456e0073c09bd1deba",
          "name": "Yonghao Tan",
          "hidden": false
        },
        {
          "_id": "68ca18456e0073c09bd1debb",
          "name": "Jiaao Yu",
          "hidden": false
        },
        {
          "_id": "68ca18456e0073c09bd1debc",
          "name": "Junlin Yu",
          "hidden": false
        },
        {
          "_id": "68ca18456e0073c09bd1debd",
          "name": "Keren Zhang",
          "hidden": false
        },
        {
          "_id": "68ca18456e0073c09bd1debe",
          "name": "Yifan Li",
          "hidden": false
        },
        {
          "_id": "68ca18456e0073c09bd1debf",
          "name": "Peng Chen",
          "hidden": false
        },
        {
          "_id": "68ca18456e0073c09bd1dec0",
          "name": "Tian Liu",
          "hidden": false
        },
        {
          "_id": "68ca18456e0073c09bd1dec1",
          "name": "Di Wang",
          "hidden": false
        },
        {
          "_id": "68ca18456e0073c09bd1dec2",
          "name": "Yuhong Liu",
          "hidden": false
        },
        {
          "_id": "68ca18456e0073c09bd1dec3",
          "name": "Linus",
          "hidden": false
        },
        {
          "_id": "68ca18456e0073c09bd1dec4",
          "name": "Jie Jiang",
          "hidden": false
        },
        {
          "_id": "68ca18456e0073c09bd1dec5",
          "name": "Zhuo Chen",
          "hidden": false
        },
        {
          "_id": "68ca18456e0073c09bd1dec6",
          "name": "Chunchao Guo",
          "hidden": false
        }
      ],
      "publishedAt": "2025-09-16T08:33:03.000Z",
      "submittedOnDailyAt": "2025-09-17T00:39:17.325Z",
      "title": "Hunyuan3D Studio: End-to-End AI Pipeline for Game-Ready 3D Asset\n  Generation",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "The creation of high-quality 3D assets, a cornerstone of modern game\ndevelopment, has long been characterized by labor-intensive and specialized\nworkflows. This paper presents Hunyuan3D Studio, an end-to-end AI-powered\ncontent creation platform designed to revolutionize the game production\npipeline by automating and streamlining the generation of game-ready 3D assets.\nAt its core, Hunyuan3D Studio integrates a suite of advanced neural modules\n(such as Part-level 3D Generation, Polygon Generation, Semantic UV, etc.) into\na cohesive and user-friendly system. This unified framework allows for the\nrapid transformation of a single concept image or textual description into a\nfully-realized, production-quality 3D model complete with optimized geometry\nand high-fidelity PBR textures. We demonstrate that assets generated by\nHunyuan3D Studio are not only visually compelling but also adhere to the\nstringent technical requirements of contemporary game engines, significantly\nreducing iteration time and lowering the barrier to entry for 3D content\ncreation. By providing a seamless bridge from creative intent to technical\nasset, Hunyuan3D Studio represents a significant leap forward for AI-assisted\nworkflows in game development and interactive media.",
      "upvotes": 1,
      "discussionId": "68ca18456e0073c09bd1dec7",
      "ai_summary": "Hunyuan3D Studio automates 3D asset creation using AI, integrating neural modules to transform concept images or text into high-quality, game-ready 3D models with optimized geometry and PBR textures.",
      "ai_keywords": [
        "Part-level 3D Generation",
        "Polygon Generation",
        "Semantic UV",
        "PBR textures"
      ]
    },
    "publishedAt": "2025-09-16T04:33:03.000Z",
    "title": "Hunyuan3D Studio: End-to-End AI Pipeline for Game-Ready 3D Asset\n  Generation",
    "summary": "The creation of high-quality 3D assets, a cornerstone of modern game\ndevelopment, has long been characterized by labor-intensive and specialized\nworkflows. This paper presents Hunyuan3D Studio, an end-to-end AI-powered\ncontent creation platform designed to revolutionize the game production\npipeline by automating and streamlining the generation of game-ready 3D assets.\nAt its core, Hunyuan3D Studio integrates a suite of advanced neural modules\n(such as Part-level 3D Generation, Polygon Generation, Semantic UV, etc.) into\na cohesive and user-friendly system. This unified framework allows for the\nrapid transformation of a single concept image or textual description into a\nfully-realized, production-quality 3D model complete with optimized geometry\nand high-fidelity PBR textures. We demonstrate that assets generated by\nHunyuan3D Studio are not only visually compelling but also adhere to the\nstringent technical requirements of contemporary game engines, significantly\nreducing iteration time and lowering the barrier to entry for 3D content\ncreation. By providing a seamless bridge from creative intent to technical\nasset, Hunyuan3D Studio represents a significant leap forward for AI-assisted\nworkflows in game development and interactive media.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.12815.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 106
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2509.11526",
      "authors": [
        {
          "_id": "68ca148b6e0073c09bd1de00",
          "name": "Wenhao Tang",
          "hidden": false
        },
        {
          "_id": "68ca148b6e0073c09bd1de01",
          "name": "Sheng Huang",
          "hidden": false
        },
        {
          "_id": "68ca148b6e0073c09bd1de02",
          "name": "Heng Fang",
          "hidden": false
        },
        {
          "_id": "68ca148b6e0073c09bd1de03",
          "name": "Fengtao Zhou",
          "hidden": false
        },
        {
          "_id": "68ca148b6e0073c09bd1de04",
          "name": "Bo Liu",
          "hidden": false
        },
        {
          "_id": "68ca148b6e0073c09bd1de05",
          "name": "Qingshan Liu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-09-15T02:31:33.000Z",
      "submittedOnDailyAt": "2025-09-17T00:25:06.245Z",
      "title": "Multiple Instance Learning Framework with Masked Hard Instance Mining\n  for Gigapixel Histopathology Image Analysis",
      "submittedOnDailyBy": {
        "_id": "649010619852e20da842e84c",
        "avatarUrl": "/avatars/5016c566d804d1a00fccbba69059e2ef.svg",
        "isPro": false,
        "fullname": "Wenhao",
        "user": "Dearcat",
        "type": "user"
      },
      "summary": "Digitizing pathological images into gigapixel Whole Slide Images (WSIs) has\nopened new avenues for Computational Pathology (CPath). As positive tissue\ncomprises only a small fraction of gigapixel WSIs, existing Multiple Instance\nLearning (MIL) methods typically focus on identifying salient instances via\nattention mechanisms. However, this leads to a bias towards easy-to-classify\ninstances while neglecting challenging ones. Recent studies have shown that\nhard examples are crucial for accurately modeling discriminative boundaries.\nApplying such an idea at the instance level, we elaborate a novel MIL framework\nwith masked hard instance mining (MHIM-MIL), which utilizes a Siamese structure\nwith a consistency constraint to explore the hard instances. Using a\nclass-aware instance probability, MHIM-MIL employs a momentum teacher to mask\nsalient instances and implicitly mine hard instances for training the student\nmodel. To obtain diverse, non-redundant hard instances, we adopt large-scale\nrandom masking while utilizing a global recycle network to mitigate the risk of\nlosing key features. Furthermore, the student updates the teacher using an\nexponential moving average, which identifies new hard instances for subsequent\ntraining iterations and stabilizes optimization. Experimental results on cancer\ndiagnosis, subtyping, survival analysis tasks, and 12 benchmarks demonstrate\nthat MHIM-MIL outperforms the latest methods in both performance and\nefficiency. The code is available at: https://github.com/DearCaat/MHIM-MIL.",
      "upvotes": 1,
      "discussionId": "68ca148b6e0073c09bd1de06",
      "githubRepo": "https://github.com/DearCaat/MHIM-MIL",
      "ai_summary": "A novel MIL framework, MHIM-MIL, uses masked hard instance mining with a Siamese structure and momentum teacher to improve cancer diagnosis and subtyping accuracy.",
      "ai_keywords": [
        "Multiple Instance Learning",
        "MIL",
        "masked hard instance mining",
        "MHIM-MIL",
        "Siamese structure",
        "consistency constraint",
        "class-aware instance probability",
        "momentum teacher",
        "large-scale random masking",
        "global recycle network",
        "exponential moving average"
      ],
      "githubStars": 82
    },
    "publishedAt": "2025-09-14T22:31:33.000Z",
    "title": "Multiple Instance Learning Framework with Masked Hard Instance Mining\n  for Gigapixel Histopathology Image Analysis",
    "summary": "Digitizing pathological images into gigapixel Whole Slide Images (WSIs) has\nopened new avenues for Computational Pathology (CPath). As positive tissue\ncomprises only a small fraction of gigapixel WSIs, existing Multiple Instance\nLearning (MIL) methods typically focus on identifying salient instances via\nattention mechanisms. However, this leads to a bias towards easy-to-classify\ninstances while neglecting challenging ones. Recent studies have shown that\nhard examples are crucial for accurately modeling discriminative boundaries.\nApplying such an idea at the instance level, we elaborate a novel MIL framework\nwith masked hard instance mining (MHIM-MIL), which utilizes a Siamese structure\nwith a consistency constraint to explore the hard instances. Using a\nclass-aware instance probability, MHIM-MIL employs a momentum teacher to mask\nsalient instances and implicitly mine hard instances for training the student\nmodel. To obtain diverse, non-redundant hard instances, we adopt large-scale\nrandom masking while utilizing a global recycle network to mitigate the risk of\nlosing key features. Furthermore, the student updates the teacher using an\nexponential moving average, which identifies new hard instances for subsequent\ntraining iterations and stabilizes optimization. Experimental results on cancer\ndiagnosis, subtyping, survival analysis tasks, and 12 benchmarks demonstrate\nthat MHIM-MIL outperforms the latest methods in both performance and\nefficiency. The code is available at: https://github.com/DearCaat/MHIM-MIL.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.11526.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "649010619852e20da842e84c",
      "avatarUrl": "/avatars/5016c566d804d1a00fccbba69059e2ef.svg",
      "fullname": "Wenhao",
      "name": "Dearcat",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  }
]