[
  {
    "paper": {
      "id": "2505.10527",
      "authors": [
        {
          "_id": "682699da19c4a596dbcea4f5",
          "name": "Binghai Wang",
          "hidden": false
        },
        {
          "_id": "682699da19c4a596dbcea4f6",
          "name": "Runji Lin",
          "hidden": false
        },
        {
          "_id": "682699da19c4a596dbcea4f7",
          "name": "Keming Lu",
          "hidden": false
        },
        {
          "_id": "682699da19c4a596dbcea4f8",
          "name": "Le Yu",
          "hidden": false
        },
        {
          "_id": "682699da19c4a596dbcea4f9",
          "name": "Zhenru Zhang",
          "hidden": false
        },
        {
          "_id": "682699da19c4a596dbcea4fa",
          "name": "Fei Huang",
          "hidden": false
        },
        {
          "_id": "682699da19c4a596dbcea4fb",
          "name": "Chujie Zheng",
          "hidden": false
        },
        {
          "_id": "682699da19c4a596dbcea4fc",
          "name": "Kai Dang",
          "hidden": false
        },
        {
          "_id": "682699da19c4a596dbcea4fd",
          "name": "Yang Fan",
          "hidden": false
        },
        {
          "_id": "682699da19c4a596dbcea4fe",
          "name": "Xingzhang Ren",
          "hidden": false
        },
        {
          "_id": "682699da19c4a596dbcea4ff",
          "name": "An Yang",
          "hidden": false
        },
        {
          "_id": "682699da19c4a596dbcea500",
          "name": "Binyuan Hui",
          "hidden": false
        },
        {
          "_id": "682699da19c4a596dbcea501",
          "name": "Dayiheng Liu",
          "hidden": false
        },
        {
          "_id": "682699da19c4a596dbcea502",
          "name": "Tao Gui",
          "hidden": false
        },
        {
          "_id": "682699da19c4a596dbcea503",
          "name": "Qi Zhang",
          "hidden": false
        },
        {
          "_id": "682699da19c4a596dbcea504",
          "name": "Xuanjing Huang",
          "hidden": false
        },
        {
          "_id": "682699da19c4a596dbcea505",
          "name": "Yu-Gang Jiang",
          "hidden": false
        },
        {
          "_id": "682699da19c4a596dbcea506",
          "name": "Bowen Yu",
          "hidden": false
        },
        {
          "_id": "682699da19c4a596dbcea507",
          "name": "Jingren Zhou",
          "hidden": false
        },
        {
          "_id": "682699da19c4a596dbcea508",
          "name": "Junyang Lin",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-15T17:38:37.000Z",
      "submittedOnDailyAt": "2025-05-16T00:22:55.977Z",
      "title": "WorldPM: Scaling Human Preference Modeling",
      "submittedOnDailyBy": {
        "_id": "63d9d68c1cae35c27bf7a6a7",
        "avatarUrl": "/avatars/b5ad98cf269ae5f1fe90861fb4170fae.svg",
        "isPro": false,
        "fullname": "Bowen Yu",
        "user": "Tigerph",
        "type": "user"
      },
      "summary": "Motivated by scaling laws in language modeling that demonstrate how test loss\nscales as a power law with model and dataset sizes, we find that similar laws\nexist in preference modeling. We propose World Preference Modeling$ (WorldPM)\nto emphasize this scaling potential, where World Preference embodies a unified\nrepresentation of human preferences. In this paper, we collect preference data\nfrom public forums covering diverse user communities, and conduct extensive\ntraining using 15M-scale data across models ranging from 1.5B to 72B\nparameters. We observe distinct patterns across different evaluation metrics:\n(1) Adversarial metrics (ability to identify deceptive features) consistently\nscale up with increased training data and base model size; (2) Objective\nmetrics (objective knowledge with well-defined answers) show emergent behavior\nin larger language models, highlighting WorldPM's scalability potential; (3)\nSubjective metrics (subjective preferences from a limited number of humans or\nAI) do not demonstrate scaling trends. Further experiments validate the\neffectiveness of WorldPM as a foundation for preference fine-tuning. Through\nevaluations on 7 benchmarks with 20 subtasks, we find that WorldPM broadly\nimproves the generalization performance across human preference datasets of\nvarying sizes (7K, 100K and 800K samples), with performance gains exceeding 5%\non many key subtasks. Integrating WorldPM into our internal RLHF pipeline, we\nobserve significant improvements on both in-house and public evaluation sets,\nwith notable gains of 4% to 8% in our in-house evaluations.",
      "upvotes": 3,
      "discussionId": "682699dd19c4a596dbcea604",
      "ai_keywords": [
        "preference modeling",
        "World Preference Modeling (WorldPM)",
        "World Preference",
        "unified representation",
        "human preferences",
        "preference data",
        "public forums",
        "user communities",
        "extensive training",
        "15M-scale data",
        "parameter-efficient fine-tuning",
        "Adversarial metrics",
        "deceptive features",
        "Objective metrics",
        "Subjective metrics",
        "preference fine-tuning",
        "generalization performance",
        "human preference datasets",
        "RLHF (Reinforcement Learning from Human Feedback)",
        "in-house evaluations",
        "public evaluation sets"
      ]
    },
    "publishedAt": "2025-05-15T13:38:37.000Z",
    "title": "WorldPM: Scaling Human Preference Modeling",
    "summary": "Motivated by scaling laws in language modeling that demonstrate how test loss\nscales as a power law with model and dataset sizes, we find that similar laws\nexist in preference modeling. We propose World Preference Modeling$ (WorldPM)\nto emphasize this scaling potential, where World Preference embodies a unified\nrepresentation of human preferences. In this paper, we collect preference data\nfrom public forums covering diverse user communities, and conduct extensive\ntraining using 15M-scale data across models ranging from 1.5B to 72B\nparameters. We observe distinct patterns across different evaluation metrics:\n(1) Adversarial metrics (ability to identify deceptive features) consistently\nscale up with increased training data and base model size; (2) Objective\nmetrics (objective knowledge with well-defined answers) show emergent behavior\nin larger language models, highlighting WorldPM's scalability potential; (3)\nSubjective metrics (subjective preferences from a limited number of humans or\nAI) do not demonstrate scaling trends. Further experiments validate the\neffectiveness of WorldPM as a foundation for preference fine-tuning. Through\nevaluations on 7 benchmarks with 20 subtasks, we find that WorldPM broadly\nimproves the generalization performance across human preference datasets of\nvarying sizes (7K, 100K and 800K samples), with performance gains exceeding 5%\non many key subtasks. Integrating WorldPM into our internal RLHF pipeline, we\nobserve significant improvements on both in-house and public evaluation sets,\nwith notable gains of 4% to 8% in our in-house evaluations.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.10527.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63d9d68c1cae35c27bf7a6a7",
      "avatarUrl": "/avatars/b5ad98cf269ae5f1fe90861fb4170fae.svg",
      "fullname": "Bowen Yu",
      "name": "Tigerph",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 7
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.07782",
      "authors": [
        {
          "_id": "6822b3c8c10ac9c466c63e01",
          "user": {
            "_id": "6466e31a14e059dde8bbe4be",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6466e31a14e059dde8bbe4be/D_DWGWEkhOnFdbbdCNP3N.jpeg",
            "isPro": true,
            "fullname": "Rushi Qiang",
            "user": "Jerrycool",
            "type": "user"
          },
          "name": "Rushi Qiang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-14T07:35:59.704Z",
          "hidden": false
        },
        {
          "_id": "6822b3c8c10ac9c466c63e02",
          "name": "Yuchen Zhuang",
          "hidden": false
        },
        {
          "_id": "6822b3c8c10ac9c466c63e03",
          "name": "Yinghao Li",
          "hidden": false
        },
        {
          "_id": "6822b3c8c10ac9c466c63e04",
          "name": "Dingu Sagar V K",
          "hidden": false
        },
        {
          "_id": "6822b3c8c10ac9c466c63e05",
          "name": "Rongzhi Zhang",
          "hidden": false
        },
        {
          "_id": "6822b3c8c10ac9c466c63e06",
          "name": "Changhao Li",
          "hidden": false
        },
        {
          "_id": "6822b3c8c10ac9c466c63e07",
          "name": "Ian Shu-Hei Wong",
          "hidden": false
        },
        {
          "_id": "6822b3c8c10ac9c466c63e08",
          "name": "Sherry Yang",
          "hidden": false
        },
        {
          "_id": "6822b3c8c10ac9c466c63e09",
          "name": "Percy Liang",
          "hidden": false
        },
        {
          "_id": "6822b3c8c10ac9c466c63e0a",
          "name": "Chao Zhang",
          "hidden": false
        },
        {
          "_id": "6822b3c8c10ac9c466c63e0b",
          "name": "Bo Dai",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-12T17:35:43.000Z",
      "submittedOnDailyAt": "2025-05-16T00:26:20.796Z",
      "title": "MLE-Dojo: Interactive Environments for Empowering LLM Agents in Machine\n  Learning Engineering",
      "submittedOnDailyBy": {
        "_id": "6466e31a14e059dde8bbe4be",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6466e31a14e059dde8bbe4be/D_DWGWEkhOnFdbbdCNP3N.jpeg",
        "isPro": true,
        "fullname": "Rushi Qiang",
        "user": "Jerrycool",
        "type": "user"
      },
      "summary": "We introduce MLE-Dojo, a Gym-style framework for systematically reinforcement\nlearning, evaluating, and improving autonomous large language model (LLM)\nagents in iterative machine learning engineering (MLE) workflows. Unlike\nexisting benchmarks that primarily rely on static datasets or single-attempt\nevaluations, MLE-Dojo provides an interactive environment enabling agents to\niteratively experiment, debug, and refine solutions through structured feedback\nloops. Built upon 200+ real-world Kaggle challenges, MLE-Dojo covers diverse,\nopen-ended MLE tasks carefully curated to reflect realistic engineering\nscenarios such as data processing, architecture search, hyperparameter tuning,\nand code debugging. Its fully executable environment supports comprehensive\nagent training via both supervised fine-tuning and reinforcement learning,\nfacilitating iterative experimentation, realistic data sampling, and real-time\noutcome verification. Extensive evaluations of eight frontier LLMs reveal that\nwhile current models achieve meaningful iterative improvements, they still\nexhibit significant limitations in autonomously generating long-horizon\nsolutions and efficiently resolving complex errors. Furthermore, MLE-Dojo's\nflexible and extensible architecture seamlessly integrates diverse data\nsources, tools, and evaluation protocols, uniquely enabling model-based agent\ntuning and promoting interoperability, scalability, and reproducibility. We\nopen-source our framework and benchmarks to foster community-driven innovation\ntowards next-generation MLE agents.",
      "upvotes": 3,
      "discussionId": "6822b3c9c10ac9c466c63e8a",
      "ai_keywords": [
        "reinforcement learning",
        "autonomous large language model (LLM) agents",
        "iterative machine learning engineering (MLE) workflows",
        "Kaggle challenges",
        "MLE tasks",
        "data processing",
        "architecture search",
        "hyperparameter tuning",
        "code debugging",
        "supervised fine-tuning",
        "model-based agent tuning"
      ]
    },
    "publishedAt": "2025-05-12T13:35:43.000Z",
    "title": "MLE-Dojo: Interactive Environments for Empowering LLM Agents in Machine\n  Learning Engineering",
    "summary": "We introduce MLE-Dojo, a Gym-style framework for systematically reinforcement\nlearning, evaluating, and improving autonomous large language model (LLM)\nagents in iterative machine learning engineering (MLE) workflows. Unlike\nexisting benchmarks that primarily rely on static datasets or single-attempt\nevaluations, MLE-Dojo provides an interactive environment enabling agents to\niteratively experiment, debug, and refine solutions through structured feedback\nloops. Built upon 200+ real-world Kaggle challenges, MLE-Dojo covers diverse,\nopen-ended MLE tasks carefully curated to reflect realistic engineering\nscenarios such as data processing, architecture search, hyperparameter tuning,\nand code debugging. Its fully executable environment supports comprehensive\nagent training via both supervised fine-tuning and reinforcement learning,\nfacilitating iterative experimentation, realistic data sampling, and real-time\noutcome verification. Extensive evaluations of eight frontier LLMs reveal that\nwhile current models achieve meaningful iterative improvements, they still\nexhibit significant limitations in autonomously generating long-horizon\nsolutions and efficiently resolving complex errors. Furthermore, MLE-Dojo's\nflexible and extensible architecture seamlessly integrates diverse data\nsources, tools, and evaluation protocols, uniquely enabling model-based agent\ntuning and promoting interoperability, scalability, and reproducibility. We\nopen-source our framework and benchmarks to foster community-driven innovation\ntowards next-generation MLE agents.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.07782.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6466e31a14e059dde8bbe4be",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6466e31a14e059dde8bbe4be/D_DWGWEkhOnFdbbdCNP3N.jpeg",
      "fullname": "Rushi Qiang",
      "name": "Jerrycool",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.10554",
      "authors": [
        {
          "_id": "6826a569ea77771e3880f793",
          "name": "Zhiyuan Hu",
          "hidden": false
        },
        {
          "_id": "6826a569ea77771e3880f794",
          "name": "Yibo Wang",
          "hidden": false
        },
        {
          "_id": "6826a569ea77771e3880f795",
          "name": "Hanze Dong",
          "hidden": false
        },
        {
          "_id": "6826a569ea77771e3880f796",
          "name": "Yuhui Xu",
          "hidden": false
        },
        {
          "_id": "6826a569ea77771e3880f797",
          "name": "Amrita Saha",
          "hidden": false
        },
        {
          "_id": "6826a569ea77771e3880f798",
          "name": "Caiming Xiong",
          "hidden": false
        },
        {
          "_id": "6826a569ea77771e3880f799",
          "name": "Bryan Hooi",
          "hidden": false
        },
        {
          "_id": "6826a569ea77771e3880f79a",
          "name": "Junnan Li",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-15T17:58:33.000Z",
      "submittedOnDailyAt": "2025-05-16T01:09:52.437Z",
      "title": "Beyond 'Aha!': Toward Systematic Meta-Abilities Alignment in Large\n  Reasoning Models",
      "submittedOnDailyBy": {
        "_id": "64351475901c5734bcb64248",
        "avatarUrl": "/avatars/12346d4301c1bfb00ce0ea128a93cc15.svg",
        "isPro": false,
        "fullname": "Zhiyuan Hu",
        "user": "zhiyuanhucs",
        "type": "user"
      },
      "summary": "Large reasoning models (LRMs) already possess a latent capacity for long\nchain-of-thought reasoning. Prior work has shown that outcome-based\nreinforcement learning (RL) can incidentally elicit advanced reasoning\nbehaviors such as self-correction, backtracking, and verification phenomena\noften referred to as the model's \"aha moment\". However, the timing and\nconsistency of these emergent behaviors remain unpredictable and\nuncontrollable, limiting the scalability and reliability of LRMs' reasoning\ncapabilities. To address these limitations, we move beyond reliance on prompts\nand coincidental \"aha moments\". Instead, we explicitly align models with three\nmeta-abilities: deduction, induction, and abduction, using automatically\ngenerated, self-verifiable tasks. Our three stage-pipeline individual\nalignment, parameter-space merging, and domain-specific reinforcement learning,\nboosting performance by over 10\\% relative to instruction-tuned baselines.\nFurthermore, domain-specific RL from the aligned checkpoint yields an\nadditional 2\\% average gain in the performance ceiling across math, coding, and\nscience benchmarks, demonstrating that explicit meta-ability alignment offers a\nscalable and dependable foundation for reasoning. Code is available at:\nhttps://github.com/zhiyuanhubj/Meta-Ability-Alignment",
      "upvotes": 1,
      "discussionId": "6826a56aea77771e3880f7c8"
    },
    "publishedAt": "2025-05-15T13:58:33.000Z",
    "title": "Beyond 'Aha!': Toward Systematic Meta-Abilities Alignment in Large\n  Reasoning Models",
    "summary": "Large reasoning models (LRMs) already possess a latent capacity for long\nchain-of-thought reasoning. Prior work has shown that outcome-based\nreinforcement learning (RL) can incidentally elicit advanced reasoning\nbehaviors such as self-correction, backtracking, and verification phenomena\noften referred to as the model's \"aha moment\". However, the timing and\nconsistency of these emergent behaviors remain unpredictable and\nuncontrollable, limiting the scalability and reliability of LRMs' reasoning\ncapabilities. To address these limitations, we move beyond reliance on prompts\nand coincidental \"aha moments\". Instead, we explicitly align models with three\nmeta-abilities: deduction, induction, and abduction, using automatically\ngenerated, self-verifiable tasks. Our three stage-pipeline individual\nalignment, parameter-space merging, and domain-specific reinforcement learning,\nboosting performance by over 10\\% relative to instruction-tuned baselines.\nFurthermore, domain-specific RL from the aligned checkpoint yields an\nadditional 2\\% average gain in the performance ceiling across math, coding, and\nscience benchmarks, demonstrating that explicit meta-ability alignment offers a\nscalable and dependable foundation for reasoning. Code is available at:\nhttps://github.com/zhiyuanhubj/Meta-Ability-Alignment",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.10554.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64351475901c5734bcb64248",
      "avatarUrl": "/avatars/12346d4301c1bfb00ce0ea128a93cc15.svg",
      "fullname": "Zhiyuan Hu",
      "name": "zhiyuanhucs",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.10320",
      "authors": [
        {
          "_id": "6826a180cad9000ebc70f038",
          "name": "Chenxi Whitehouse",
          "hidden": false
        },
        {
          "_id": "6826a180cad9000ebc70f039",
          "name": "Tianlu Wang",
          "hidden": false
        },
        {
          "_id": "6826a180cad9000ebc70f03a",
          "name": "Ping Yu",
          "hidden": false
        },
        {
          "_id": "6826a180cad9000ebc70f03b",
          "name": "Xian Li",
          "hidden": false
        },
        {
          "_id": "6826a180cad9000ebc70f03c",
          "name": "Jason Weston",
          "hidden": false
        },
        {
          "_id": "6826a180cad9000ebc70f03d",
          "name": "Ilia Kulikov",
          "hidden": false
        },
        {
          "_id": "6826a180cad9000ebc70f03e",
          "user": {
            "_id": "64b75f4b037d6452a30f71aa",
            "avatarUrl": "/avatars/5a0322e7ecda05164e45526d605e3619.svg",
            "isPro": false,
            "fullname": "Swarnadeep Saha",
            "user": "swarna92",
            "type": "user"
          },
          "name": "Swarnadeep Saha",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-05-16T02:22:57.318Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-15T14:05:15.000Z",
      "submittedOnDailyAt": "2025-05-16T00:58:47.493Z",
      "title": "J1: Incentivizing Thinking in LLM-as-a-Judge via Reinforcement Learning",
      "submittedOnDailyBy": {
        "_id": "64b6feee17681d64b19b112b",
        "avatarUrl": "/avatars/afe708392d16af5b55d3bc4b42a585e8.svg",
        "isPro": false,
        "fullname": "Swarnadeep Saha",
        "user": "swarnaNLP",
        "type": "user"
      },
      "summary": "The progress of AI is bottlenecked by the quality of evaluation, and powerful\nLLM-as-a-Judge models have proved to be a core solution. Improved judgment\nability is enabled by stronger chain-of-thought reasoning, motivating the need\nto find the best recipes for training such models to think. In this work we\nintroduce J1, a reinforcement learning approach to training such models. Our\nmethod converts both verifiable and non-verifiable prompts to judgment tasks\nwith verifiable rewards that incentivize thinking and mitigate judgment bias.\nIn particular, our approach outperforms all other existing 8B or 70B models\nwhen trained at those sizes, including models distilled from DeepSeek-R1. J1\nalso outperforms o1-mini, and even R1 on some benchmarks, despite training a\nsmaller model. We provide analysis and ablations comparing Pairwise-J1 vs\nPointwise-J1 models, offline vs online training recipes, reward strategies,\nseed prompts, and variations in thought length and content. We find that our\nmodels make better judgments by learning to outline evaluation criteria,\ncomparing against self-generated reference answers, and re-evaluating the\ncorrectness of model responses.",
      "upvotes": 1,
      "discussionId": "6826a181cad9000ebc70f0a3"
    },
    "publishedAt": "2025-05-15T10:05:15.000Z",
    "title": "J1: Incentivizing Thinking in LLM-as-a-Judge via Reinforcement Learning",
    "summary": "The progress of AI is bottlenecked by the quality of evaluation, and powerful\nLLM-as-a-Judge models have proved to be a core solution. Improved judgment\nability is enabled by stronger chain-of-thought reasoning, motivating the need\nto find the best recipes for training such models to think. In this work we\nintroduce J1, a reinforcement learning approach to training such models. Our\nmethod converts both verifiable and non-verifiable prompts to judgment tasks\nwith verifiable rewards that incentivize thinking and mitigate judgment bias.\nIn particular, our approach outperforms all other existing 8B or 70B models\nwhen trained at those sizes, including models distilled from DeepSeek-R1. J1\nalso outperforms o1-mini, and even R1 on some benchmarks, despite training a\nsmaller model. We provide analysis and ablations comparing Pairwise-J1 vs\nPointwise-J1 models, offline vs online training recipes, reward strategies,\nseed prompts, and variations in thought length and content. We find that our\nmodels make better judgments by learning to outline evaluation criteria,\ncomparing against self-generated reference answers, and re-evaluating the\ncorrectness of model responses.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.10320.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64b6feee17681d64b19b112b",
      "avatarUrl": "/avatars/afe708392d16af5b55d3bc4b42a585e8.svg",
      "fullname": "Swarnadeep Saha",
      "name": "swarnaNLP",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.10185",
      "authors": [
        {
          "_id": "68269f67a47cb2b87646b98c",
          "name": "Seongyun Lee",
          "hidden": false
        },
        {
          "_id": "68269f67a47cb2b87646b98d",
          "name": "Seungone Kim",
          "hidden": false
        },
        {
          "_id": "68269f67a47cb2b87646b98e",
          "name": "Minju Seo",
          "hidden": false
        },
        {
          "_id": "68269f67a47cb2b87646b98f",
          "name": "Yongrae Jo",
          "hidden": false
        },
        {
          "_id": "68269f67a47cb2b87646b990",
          "name": "Dongyoung Go",
          "hidden": false
        },
        {
          "_id": "68269f67a47cb2b87646b991",
          "name": "Hyeonbin Hwang",
          "hidden": false
        },
        {
          "_id": "68269f67a47cb2b87646b992",
          "name": "Jinho Park",
          "hidden": false
        },
        {
          "_id": "68269f67a47cb2b87646b993",
          "name": "Xiang Yue",
          "hidden": false
        },
        {
          "_id": "68269f67a47cb2b87646b994",
          "name": "Sean Welleck",
          "hidden": false
        },
        {
          "_id": "68269f67a47cb2b87646b995",
          "name": "Graham Neubig",
          "hidden": false
        },
        {
          "_id": "68269f67a47cb2b87646b996",
          "name": "Moontae Lee",
          "hidden": false
        },
        {
          "_id": "68269f67a47cb2b87646b997",
          "name": "Minjoon Seo",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-15T11:31:02.000Z",
      "submittedOnDailyAt": "2025-05-16T00:44:19.223Z",
      "title": "The CoT Encyclopedia: Analyzing, Predicting, and Controlling how a\n  Reasoning Model will Think",
      "submittedOnDailyBy": {
        "_id": "6550c4f27bbfce1878f5f280",
        "avatarUrl": "/avatars/0ecedbcd8a55b2c4abd1da9e741a6652.svg",
        "isPro": false,
        "fullname": "seongyun_lee",
        "user": "Seongyun",
        "type": "user"
      },
      "summary": "Long chain-of-thought (CoT) is an essential ingredient in effective usage of\nmodern large language models, but our understanding of the reasoning strategies\nunderlying these capabilities remains limited. While some prior works have\nattempted to categorize CoTs using predefined strategy types, such approaches\nare constrained by human intuition and fail to capture the full diversity of\nmodel behaviors. In this work, we introduce the CoT Encyclopedia, a bottom-up\nframework for analyzing and steering model reasoning. Our method automatically\nextracts diverse reasoning criteria from model-generated CoTs, embeds them into\na semantic space, clusters them into representative categories, and derives\ncontrastive rubrics to interpret reasoning behavior. Human evaluations show\nthat this framework produces more interpretable and comprehensive analyses than\nexisting methods. Moreover, we demonstrate that this understanding enables\nperformance gains: we can predict which strategy a model is likely to use and\nguide it toward more effective alternatives. Finally, we provide practical\ninsights, such as that training data format (e.g., free-form vs.\nmultiple-choice) has a far greater impact on reasoning behavior than data\ndomain, underscoring the importance of format-aware model design.",
      "upvotes": 1,
      "discussionId": "68269f68a47cb2b87646b9ed"
    },
    "publishedAt": "2025-05-15T07:31:02.000Z",
    "title": "The CoT Encyclopedia: Analyzing, Predicting, and Controlling how a\n  Reasoning Model will Think",
    "summary": "Long chain-of-thought (CoT) is an essential ingredient in effective usage of\nmodern large language models, but our understanding of the reasoning strategies\nunderlying these capabilities remains limited. While some prior works have\nattempted to categorize CoTs using predefined strategy types, such approaches\nare constrained by human intuition and fail to capture the full diversity of\nmodel behaviors. In this work, we introduce the CoT Encyclopedia, a bottom-up\nframework for analyzing and steering model reasoning. Our method automatically\nextracts diverse reasoning criteria from model-generated CoTs, embeds them into\na semantic space, clusters them into representative categories, and derives\ncontrastive rubrics to interpret reasoning behavior. Human evaluations show\nthat this framework produces more interpretable and comprehensive analyses than\nexisting methods. Moreover, we demonstrate that this understanding enables\nperformance gains: we can predict which strategy a model is likely to use and\nguide it toward more effective alternatives. Finally, we provide practical\ninsights, such as that training data format (e.g., free-form vs.\nmultiple-choice) has a far greater impact on reasoning behavior than data\ndomain, underscoring the importance of format-aware model design.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.10185.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6550c4f27bbfce1878f5f280",
      "avatarUrl": "/avatars/0ecedbcd8a55b2c4abd1da9e741a6652.svg",
      "fullname": "seongyun_lee",
      "name": "Seongyun",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 7
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.09926",
      "authors": [
        {
          "_id": "68268c3408f7cb26defd82fc",
          "name": "Bin-Bin Gao",
          "hidden": false
        },
        {
          "_id": "68268c3408f7cb26defd82fd",
          "name": "Yue Zhu",
          "hidden": false
        },
        {
          "_id": "68268c3408f7cb26defd82fe",
          "name": "Jiangtao Yan",
          "hidden": false
        },
        {
          "_id": "68268c3408f7cb26defd82ff",
          "name": "Yuezhi Cai",
          "hidden": false
        },
        {
          "_id": "68268c3408f7cb26defd8300",
          "name": "Weixi Zhang",
          "hidden": false
        },
        {
          "_id": "68268c3408f7cb26defd8301",
          "name": "Meng Wang",
          "hidden": false
        },
        {
          "_id": "68268c3408f7cb26defd8302",
          "name": "Jun Liu",
          "hidden": false
        },
        {
          "_id": "68268c3408f7cb26defd8303",
          "name": "Yong Liu",
          "hidden": false
        },
        {
          "_id": "68268c3408f7cb26defd8304",
          "name": "Lei Wang",
          "hidden": false
        },
        {
          "_id": "68268c3408f7cb26defd8305",
          "name": "Chengjie Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-15T03:24:28.000Z",
      "submittedOnDailyAt": "2025-05-16T00:07:29.541Z",
      "title": "AdaptCLIP: Adapting CLIP for Universal Visual Anomaly Detection",
      "submittedOnDailyBy": {
        "_id": "648972ff99f6c45ff6bbd295",
        "avatarUrl": "/avatars/d1fe9d631a786462d55cfe43c565c88e.svg",
        "isPro": false,
        "fullname": "Bin-Bin Gao",
        "user": "csgaobb",
        "type": "user"
      },
      "summary": "Universal visual anomaly detection aims to identify anomalies from novel or\nunseen vision domains without additional fine-tuning, which is critical in open\nscenarios. Recent studies have demonstrated that pre-trained vision-language\nmodels like CLIP exhibit strong generalization with just zero or a few normal\nimages. However, existing methods struggle with designing prompt templates,\ncomplex token interactions, or requiring additional fine-tuning, resulting in\nlimited flexibility. In this work, we present a simple yet effective method\ncalled AdaptCLIP based on two key insights. First, adaptive visual and textual\nrepresentations should be learned alternately rather than jointly. Second,\ncomparative learning between query and normal image prompt should incorporate\nboth contextual and aligned residual features, rather than relying solely on\nresidual features. AdaptCLIP treats CLIP models as a foundational service,\nadding only three simple adapters, visual adapter, textual adapter, and\nprompt-query adapter, at its input or output ends. AdaptCLIP supports\nzero-/few-shot generalization across domains and possesses a training-free\nmanner on target domains once trained on a base dataset. AdaptCLIP achieves\nstate-of-the-art performance on 12 anomaly detection benchmarks from industrial\nand medical domains, significantly outperforming existing competitive methods.\nWe will make the code and model of AdaptCLIP available at\nhttps://github.com/gaobb/AdaptCLIP.",
      "upvotes": 1,
      "discussionId": "68268c3808f7cb26defd83bf",
      "ai_keywords": [
        "CLIP",
        "pre-trained vision-language models",
        "prompt templates",
        "token interactions",
        "fine-tuning",
        "adaptive visual and textual representations",
        "comparative learning",
        "query and normal image prompt",
        "contextual and aligned residual features",
        "residual features",
        "visual adapter",
        "textual adapter",
        "prompt-query adapter",
        "zero-/few-shot generalization",
        "training-free",
        "anomaly detection benchmarks",
        "industrial domains",
        "medical domains"
      ]
    },
    "publishedAt": "2025-05-14T23:24:28.000Z",
    "title": "AdaptCLIP: Adapting CLIP for Universal Visual Anomaly Detection",
    "summary": "Universal visual anomaly detection aims to identify anomalies from novel or\nunseen vision domains without additional fine-tuning, which is critical in open\nscenarios. Recent studies have demonstrated that pre-trained vision-language\nmodels like CLIP exhibit strong generalization with just zero or a few normal\nimages. However, existing methods struggle with designing prompt templates,\ncomplex token interactions, or requiring additional fine-tuning, resulting in\nlimited flexibility. In this work, we present a simple yet effective method\ncalled AdaptCLIP based on two key insights. First, adaptive visual and textual\nrepresentations should be learned alternately rather than jointly. Second,\ncomparative learning between query and normal image prompt should incorporate\nboth contextual and aligned residual features, rather than relying solely on\nresidual features. AdaptCLIP treats CLIP models as a foundational service,\nadding only three simple adapters, visual adapter, textual adapter, and\nprompt-query adapter, at its input or output ends. AdaptCLIP supports\nzero-/few-shot generalization across domains and possesses a training-free\nmanner on target domains once trained on a base dataset. AdaptCLIP achieves\nstate-of-the-art performance on 12 anomaly detection benchmarks from industrial\nand medical domains, significantly outperforming existing competitive methods.\nWe will make the code and model of AdaptCLIP available at\nhttps://github.com/gaobb/AdaptCLIP.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.09926.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "648972ff99f6c45ff6bbd295",
      "avatarUrl": "/avatars/d1fe9d631a786462d55cfe43c565c88e.svg",
      "fullname": "Bin-Bin Gao",
      "name": "csgaobb",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.09666",
      "authors": [
        {
          "_id": "68269a1eaa8aded616d280a0",
          "name": "Yumin Choi",
          "hidden": false
        },
        {
          "_id": "68269a1eaa8aded616d280a1",
          "name": "Jinheon Baek",
          "hidden": false
        },
        {
          "_id": "68269a1eaa8aded616d280a2",
          "name": "Sung Ju Hwang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-14T16:46:15.000Z",
      "submittedOnDailyAt": "2025-05-16T01:05:44.315Z",
      "title": "System Prompt Optimization with Meta-Learning",
      "submittedOnDailyBy": {
        "_id": "63036b6c5c70c21d0ea79d48",
        "avatarUrl": "/avatars/a7eb03f5cbd4eaa09fe807bbed8bc0f7.svg",
        "isPro": false,
        "fullname": "Jinheon Baek",
        "user": "jinheon",
        "type": "user"
      },
      "summary": "Large Language Models (LLMs) have shown remarkable capabilities, with\noptimizing their input prompts playing a pivotal role in maximizing their\nperformance. However, while LLM prompts consist of both the task-agnostic\nsystem prompts and task-specific user prompts, existing work on prompt\noptimization has focused on user prompts specific to individual queries or\ntasks, and largely overlooked the system prompt that is, once optimized,\napplicable across different tasks and domains. Motivated by this, we introduce\nthe novel problem of bilevel system prompt optimization, whose objective is to\ndesign system prompts that are robust to diverse user prompts and transferable\nto unseen tasks. To tackle this problem, we then propose a meta-learning\nframework, which meta-learns the system prompt by optimizing it over various\nuser prompts across multiple datasets, while simultaneously updating the user\nprompts in an iterative manner to ensure synergy between them. We conduct\nexperiments on 14 unseen datasets spanning 5 different domains, on which we\nshow that our approach produces system prompts that generalize effectively to\ndiverse user prompts. Also, our findings reveal that the optimized system\nprompt enables rapid adaptation even to unseen tasks, requiring fewer\noptimization steps for test-time user prompts while achieving improved\nperformance.",
      "upvotes": 1,
      "discussionId": "68269a1eaa8aded616d280d1",
      "ai_keywords": [
        "Large Language Models (LLMs)",
        "bilevel system prompt optimization",
        "meta-learning framework",
        "system prompts",
        "user prompts",
        "unseen datasets",
        "domains",
        "rapid adaptation",
        "test-time user prompts"
      ]
    },
    "publishedAt": "2025-05-14T12:46:15.000Z",
    "title": "System Prompt Optimization with Meta-Learning",
    "summary": "Large Language Models (LLMs) have shown remarkable capabilities, with\noptimizing their input prompts playing a pivotal role in maximizing their\nperformance. However, while LLM prompts consist of both the task-agnostic\nsystem prompts and task-specific user prompts, existing work on prompt\noptimization has focused on user prompts specific to individual queries or\ntasks, and largely overlooked the system prompt that is, once optimized,\napplicable across different tasks and domains. Motivated by this, we introduce\nthe novel problem of bilevel system prompt optimization, whose objective is to\ndesign system prompts that are robust to diverse user prompts and transferable\nto unseen tasks. To tackle this problem, we then propose a meta-learning\nframework, which meta-learns the system prompt by optimizing it over various\nuser prompts across multiple datasets, while simultaneously updating the user\nprompts in an iterative manner to ensure synergy between them. We conduct\nexperiments on 14 unseen datasets spanning 5 different domains, on which we\nshow that our approach produces system prompts that generalize effectively to\ndiverse user prompts. Also, our findings reveal that the optimized system\nprompt enables rapid adaptation even to unseen tasks, requiring fewer\noptimization steps for test-time user prompts while achieving improved\nperformance.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.09666.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63036b6c5c70c21d0ea79d48",
      "avatarUrl": "/avatars/a7eb03f5cbd4eaa09fe807bbed8bc0f7.svg",
      "fullname": "Jinheon Baek",
      "name": "jinheon",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 10
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.09265",
      "authors": [
        {
          "_id": "682547e6501a31b392e78f6a",
          "user": {
            "_id": "648972ff99f6c45ff6bbd295",
            "avatarUrl": "/avatars/d1fe9d631a786462d55cfe43c565c88e.svg",
            "isPro": false,
            "fullname": "Bin-Bin Gao",
            "user": "csgaobb",
            "type": "user"
          },
          "name": "Bin-Bin Gao",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-05-15T01:54:50.125Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-14T10:25:26.000Z",
      "submittedOnDailyAt": "2025-05-16T00:08:33.750Z",
      "title": "MetaUAS: Universal Anomaly Segmentation with One-Prompt Meta-Learning",
      "submittedOnDailyBy": {
        "_id": "648972ff99f6c45ff6bbd295",
        "avatarUrl": "/avatars/d1fe9d631a786462d55cfe43c565c88e.svg",
        "isPro": false,
        "fullname": "Bin-Bin Gao",
        "user": "csgaobb",
        "type": "user"
      },
      "summary": "Zero- and few-shot visual anomaly segmentation relies on powerful\nvision-language models that detect unseen anomalies using manually designed\ntextual prompts. However, visual representations are inherently independent of\nlanguage. In this paper, we explore the potential of a pure visual foundation\nmodel as an alternative to widely used vision-language models for universal\nvisual anomaly segmentation. We present a novel paradigm that unifies anomaly\nsegmentation into change segmentation. This paradigm enables us to leverage\nlarge-scale synthetic image pairs, featuring object-level and local region\nchanges, derived from existing image datasets, which are independent of target\nanomaly datasets. We propose a one-prompt Meta-learning framework for Universal\nAnomaly Segmentation (MetaUAS) that is trained on this synthetic dataset and\nthen generalizes well to segment any novel or unseen visual anomalies in the\nreal world. To handle geometrical variations between prompt and query images,\nwe propose a soft feature alignment module that bridges paired-image change\nperception and single-image semantic segmentation. This is the first work to\nachieve universal anomaly segmentation using a pure vision model without\nrelying on special anomaly detection datasets and pre-trained visual-language\nmodels. Our method effectively and efficiently segments any anomalies with only\none normal image prompt and enjoys training-free without guidance from\nlanguage. Our MetaUAS significantly outperforms previous zero-shot, few-shot,\nand even full-shot anomaly segmentation methods. The code and pre-trained\nmodels are available at https://github.com/gaobb/MetaUAS.",
      "upvotes": 1,
      "discussionId": "682547eb501a31b392e79038",
      "githubRepo": "https://github.com/gaobb/MetaUAS",
      "ai_keywords": [
        "Meta-learning",
        "Universal Anomaly Segmentation (MetaUAS)",
        "synthetic image pairs",
        "object-level changes",
        "local region changes",
        "prompt",
        "query images",
        "soft feature alignment module",
        "paired-image change perception",
        "single-image semantic segmentation",
        "universal anomaly segmentation",
        "pure vision model",
        "zero-shot",
        "few-shot",
        "full-shot anomaly segmentation"
      ]
    },
    "publishedAt": "2025-05-14T06:25:26.000Z",
    "title": "MetaUAS: Universal Anomaly Segmentation with One-Prompt Meta-Learning",
    "summary": "Zero- and few-shot visual anomaly segmentation relies on powerful\nvision-language models that detect unseen anomalies using manually designed\ntextual prompts. However, visual representations are inherently independent of\nlanguage. In this paper, we explore the potential of a pure visual foundation\nmodel as an alternative to widely used vision-language models for universal\nvisual anomaly segmentation. We present a novel paradigm that unifies anomaly\nsegmentation into change segmentation. This paradigm enables us to leverage\nlarge-scale synthetic image pairs, featuring object-level and local region\nchanges, derived from existing image datasets, which are independent of target\nanomaly datasets. We propose a one-prompt Meta-learning framework for Universal\nAnomaly Segmentation (MetaUAS) that is trained on this synthetic dataset and\nthen generalizes well to segment any novel or unseen visual anomalies in the\nreal world. To handle geometrical variations between prompt and query images,\nwe propose a soft feature alignment module that bridges paired-image change\nperception and single-image semantic segmentation. This is the first work to\nachieve universal anomaly segmentation using a pure vision model without\nrelying on special anomaly detection datasets and pre-trained visual-language\nmodels. Our method effectively and efficiently segments any anomalies with only\none normal image prompt and enjoys training-free without guidance from\nlanguage. Our MetaUAS significantly outperforms previous zero-shot, few-shot,\nand even full-shot anomaly segmentation methods. The code and pre-trained\nmodels are available at https://github.com/gaobb/MetaUAS.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.09265.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "648972ff99f6c45ff6bbd295",
      "avatarUrl": "/avatars/d1fe9d631a786462d55cfe43c565c88e.svg",
      "fullname": "Bin-Bin Gao",
      "name": "csgaobb",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.09264",
      "authors": [
        {
          "_id": "682548bff4997d78fe92cd57",
          "user": {
            "_id": "648972ff99f6c45ff6bbd295",
            "avatarUrl": "/avatars/d1fe9d631a786462d55cfe43c565c88e.svg",
            "isPro": false,
            "fullname": "Bin-Bin Gao",
            "user": "csgaobb",
            "type": "user"
          },
          "name": "Bin-Bin Gao",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-05-15T01:54:51.290Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-14T10:25:14.000Z",
      "submittedOnDailyAt": "2025-05-16T00:10:20.034Z",
      "title": "Learning to Detect Multi-class Anomalies with Just One Normal Image\n  Prompt",
      "submittedOnDailyBy": {
        "_id": "648972ff99f6c45ff6bbd295",
        "avatarUrl": "/avatars/d1fe9d631a786462d55cfe43c565c88e.svg",
        "isPro": false,
        "fullname": "Bin-Bin Gao",
        "user": "csgaobb",
        "type": "user"
      },
      "summary": "Unsupervised reconstruction networks using self-attention transformers have\nachieved state-of-the-art performance for multi-class (unified) anomaly\ndetection with a single model. However, these self-attention reconstruction\nmodels primarily operate on target features, which may result in perfect\nreconstruction for both normal and anomaly features due to high consistency\nwith context, leading to failure in detecting anomalies. Additionally, these\nmodels often produce inaccurate anomaly segmentation due to performing\nreconstruction in a low spatial resolution latent space. To enable\nreconstruction models enjoying high efficiency while enhancing their\ngeneralization for unified anomaly detection, we propose a simple yet effective\nmethod that reconstructs normal features and restores anomaly features with\njust One Normal Image Prompt (OneNIP). In contrast to previous work, OneNIP\nallows for the first time to reconstruct or restore anomalies with just one\nnormal image prompt, effectively boosting unified anomaly detection\nperformance. Furthermore, we propose a supervised refiner that regresses\nreconstruction errors by using both real normal and synthesized anomalous\nimages, which significantly improves pixel-level anomaly segmentation. OneNIP\noutperforms previous methods on three industry anomaly detection benchmarks:\nMVTec, BTAD, and VisA. The code and pre-trained models are available at\nhttps://github.com/gaobb/OneNIP.",
      "upvotes": 1,
      "discussionId": "682548c1f4997d78fe92cdbc",
      "githubRepo": "https://github.com/gaobb/OneNIP",
      "ai_keywords": [
        "self-attention transformers",
        "multi-class anomaly detection",
        "self-attention reconstruction models",
        "target features",
        "context",
        "latent space",
        "One Normal Image Prompt (OneNIP)",
        "supervised refiner",
        "reconstruction errors",
        "MVTec",
        "BTAD",
        "VisA"
      ]
    },
    "publishedAt": "2025-05-14T06:25:14.000Z",
    "title": "Learning to Detect Multi-class Anomalies with Just One Normal Image\n  Prompt",
    "summary": "Unsupervised reconstruction networks using self-attention transformers have\nachieved state-of-the-art performance for multi-class (unified) anomaly\ndetection with a single model. However, these self-attention reconstruction\nmodels primarily operate on target features, which may result in perfect\nreconstruction for both normal and anomaly features due to high consistency\nwith context, leading to failure in detecting anomalies. Additionally, these\nmodels often produce inaccurate anomaly segmentation due to performing\nreconstruction in a low spatial resolution latent space. To enable\nreconstruction models enjoying high efficiency while enhancing their\ngeneralization for unified anomaly detection, we propose a simple yet effective\nmethod that reconstructs normal features and restores anomaly features with\njust One Normal Image Prompt (OneNIP). In contrast to previous work, OneNIP\nallows for the first time to reconstruct or restore anomalies with just one\nnormal image prompt, effectively boosting unified anomaly detection\nperformance. Furthermore, we propose a supervised refiner that regresses\nreconstruction errors by using both real normal and synthesized anomalous\nimages, which significantly improves pixel-level anomaly segmentation. OneNIP\noutperforms previous methods on three industry anomaly detection benchmarks:\nMVTec, BTAD, and VisA. The code and pre-trained models are available at\nhttps://github.com/gaobb/OneNIP.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.09264.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "648972ff99f6c45ff6bbd295",
      "avatarUrl": "/avatars/d1fe9d631a786462d55cfe43c565c88e.svg",
      "fullname": "Bin-Bin Gao",
      "name": "csgaobb",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.09263",
      "authors": [
        {
          "_id": "682549a75b5784e023ed7d8a",
          "name": "Guan Gui",
          "hidden": false
        },
        {
          "_id": "682549a75b5784e023ed7d8b",
          "user": {
            "_id": "648972ff99f6c45ff6bbd295",
            "avatarUrl": "/avatars/d1fe9d631a786462d55cfe43c565c88e.svg",
            "isPro": false,
            "fullname": "Bin-Bin Gao",
            "user": "csgaobb",
            "type": "user"
          },
          "name": "Bin-Bin Gao",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-15T10:31:36.508Z",
          "hidden": false
        },
        {
          "_id": "682549a75b5784e023ed7d8c",
          "name": "Jun Liu",
          "hidden": false
        },
        {
          "_id": "682549a75b5784e023ed7d8d",
          "name": "Chengjie Wang",
          "hidden": false
        },
        {
          "_id": "682549a75b5784e023ed7d8e",
          "name": "Yunsheng Wu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-14T10:25:06.000Z",
      "submittedOnDailyAt": "2025-05-16T00:12:32.405Z",
      "title": "Few-Shot Anomaly-Driven Generation for Anomaly Classification and\n  Segmentation",
      "submittedOnDailyBy": {
        "_id": "648972ff99f6c45ff6bbd295",
        "avatarUrl": "/avatars/d1fe9d631a786462d55cfe43c565c88e.svg",
        "isPro": false,
        "fullname": "Bin-Bin Gao",
        "user": "csgaobb",
        "type": "user"
      },
      "summary": "Anomaly detection is a practical and challenging task due to the scarcity of\nanomaly samples in industrial inspection. Some existing anomaly detection\nmethods address this issue by synthesizing anomalies with noise or external\ndata. However, there is always a large semantic gap between synthetic and\nreal-world anomalies, resulting in weak performance in anomaly detection. To\nsolve the problem, we propose a few-shot Anomaly-driven Generation (AnoGen)\nmethod, which guides the diffusion model to generate realistic and diverse\nanomalies with only a few real anomalies, thereby benefiting training anomaly\ndetection models. Specifically, our work is divided into three stages. In the\nfirst stage, we learn the anomaly distribution based on a few given real\nanomalies and inject the learned knowledge into an embedding. In the second\nstage, we use the embedding and given bounding boxes to guide the diffusion\nmodel to generate realistic and diverse anomalies on specific objects (or\ntextures). In the final stage, we propose a weakly-supervised anomaly detection\nmethod to train a more powerful model with generated anomalies. Our method\nbuilds upon DRAEM and DesTSeg as the foundation model and conducts experiments\non the commonly used industrial anomaly detection dataset, MVTec. The\nexperiments demonstrate that our generated anomalies effectively improve the\nmodel performance of both anomaly classification and segmentation tasks\nsimultaneously, \\eg, DRAEM and DseTSeg achieved a 5.8\\% and 1.5\\% improvement\nin AU-PR metric on segmentation task, respectively. The code and generated\nanomalous data are available at https://github.com/gaobb/AnoGen.",
      "upvotes": 1,
      "discussionId": "682549ac5b5784e023ed7e72",
      "ai_keywords": [
        "Anomaly-driven Generation (AnoGen)",
        "diffusion model",
        "anomaly distribution",
        "embedding",
        "bounding boxes",
        "weakly-supervised anomaly detection",
        "DRAEM",
        "DesTSeg",
        "MVTec",
        "AU-PR metric"
      ]
    },
    "publishedAt": "2025-05-14T06:25:06.000Z",
    "title": "Few-Shot Anomaly-Driven Generation for Anomaly Classification and\n  Segmentation",
    "summary": "Anomaly detection is a practical and challenging task due to the scarcity of\nanomaly samples in industrial inspection. Some existing anomaly detection\nmethods address this issue by synthesizing anomalies with noise or external\ndata. However, there is always a large semantic gap between synthetic and\nreal-world anomalies, resulting in weak performance in anomaly detection. To\nsolve the problem, we propose a few-shot Anomaly-driven Generation (AnoGen)\nmethod, which guides the diffusion model to generate realistic and diverse\nanomalies with only a few real anomalies, thereby benefiting training anomaly\ndetection models. Specifically, our work is divided into three stages. In the\nfirst stage, we learn the anomaly distribution based on a few given real\nanomalies and inject the learned knowledge into an embedding. In the second\nstage, we use the embedding and given bounding boxes to guide the diffusion\nmodel to generate realistic and diverse anomalies on specific objects (or\ntextures). In the final stage, we propose a weakly-supervised anomaly detection\nmethod to train a more powerful model with generated anomalies. Our method\nbuilds upon DRAEM and DesTSeg as the foundation model and conducts experiments\non the commonly used industrial anomaly detection dataset, MVTec. The\nexperiments demonstrate that our generated anomalies effectively improve the\nmodel performance of both anomaly classification and segmentation tasks\nsimultaneously, \\eg, DRAEM and DseTSeg achieved a 5.8\\% and 1.5\\% improvement\nin AU-PR metric on segmentation task, respectively. The code and generated\nanomalous data are available at https://github.com/gaobb/AnoGen.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.09263.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "648972ff99f6c45ff6bbd295",
      "avatarUrl": "/avatars/d1fe9d631a786462d55cfe43c565c88e.svg",
      "fullname": "Bin-Bin Gao",
      "name": "csgaobb",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  }
]