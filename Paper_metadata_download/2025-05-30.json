[
  {
    "paper": {
      "id": "2505.22653",
      "authors": [
        {
          "_id": "6838bb282b382ba50bdcddc4",
          "name": "Ang Lv",
          "hidden": false
        },
        {
          "_id": "6838bb282b382ba50bdcddc5",
          "name": "Ruobing Xie",
          "hidden": false
        },
        {
          "_id": "6838bb282b382ba50bdcddc6",
          "name": "Xingwu Sun",
          "hidden": false
        },
        {
          "_id": "6838bb282b382ba50bdcddc7",
          "name": "Zhanhui Kang",
          "hidden": false
        },
        {
          "_id": "6838bb282b382ba50bdcddc8",
          "name": "Rui Yan",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-28T17:59:03.000Z",
      "submittedOnDailyAt": "2025-05-30T00:44:54.555Z",
      "title": "The Climb Carves Wisdom Deeper Than the Summit: On the Noisy Rewards in\n  Learning to Reason",
      "submittedOnDailyBy": {
        "_id": "64b8ca3c5067873176d4b436",
        "avatarUrl": "/avatars/b659d147b2454b47c9a7e89bbed525fc.svg",
        "isPro": false,
        "fullname": "AngLv",
        "user": "AngLv",
        "type": "user"
      },
      "summary": "Recent studies on post-training large language models (LLMs) for reasoning\nthrough reinforcement learning (RL) typically focus on tasks that can be\naccurately verified and rewarded, such as solving math problems. In contrast,\nour research investigates the impact of reward noise, a more practical\nconsideration for real-world scenarios involving the post-training of LLMs\nusing reward models. We found that LLMs demonstrate strong robustness to\nsubstantial reward noise. For example, manually flipping 40% of the reward\nfunction's outputs in math tasks still allows a Qwen-2.5-7B model to achieve\nrapid convergence, improving its performance on math tasks from 5% to 72%,\ncompared to the 75% accuracy achieved by a model trained with noiseless\nrewards. Surprisingly, by only rewarding the appearance of key reasoning\nphrases (namely reasoning pattern reward, RPR), such as ``first, I need\nto''-without verifying the correctness of answers, the model achieved peak\ndownstream performance (over 70% accuracy for Qwen-2.5-7B) comparable to models\ntrained with strict correctness verification and accurate rewards. Recognizing\nthe importance of the reasoning process over the final results, we combined RPR\nwith noisy reward models. RPR helped calibrate the noisy reward models,\nmitigating potential false negatives and enhancing the LLM's performance on\nopen-ended tasks. These findings suggest the importance of improving models'\nfoundational abilities during the pre-training phase while providing insights\nfor advancing post-training techniques. Our code and scripts are available at\nhttps://github.com/trestad/Noisy-Rewards-in-Learning-to-Reason.",
      "upvotes": 12,
      "discussionId": "6838bb2a2b382ba50bdcde1b",
      "githubRepo": "https://github.com/trestad/Noisy-Rewards-in-Learning-to-Reason",
      "ai_summary": "LLMs exhibit robustness to reward noise during post-training and achieve high performance using reasoning pattern rewards (RPR) in conjunction with noisy reward models.",
      "ai_keywords": [
        "large language models (LLMs)",
        "post-training",
        "reinforcement learning (RL)",
        "reward noise",
        "reward models",
        "rapid convergence",
        "reasoning pattern reward (RPR)",
        "false negatives",
        "open-ended tasks"
      ]
    },
    "publishedAt": "2025-05-28T13:59:03.000Z",
    "title": "The Climb Carves Wisdom Deeper Than the Summit: On the Noisy Rewards in\n  Learning to Reason",
    "summary": "Recent studies on post-training large language models (LLMs) for reasoning\nthrough reinforcement learning (RL) typically focus on tasks that can be\naccurately verified and rewarded, such as solving math problems. In contrast,\nour research investigates the impact of reward noise, a more practical\nconsideration for real-world scenarios involving the post-training of LLMs\nusing reward models. We found that LLMs demonstrate strong robustness to\nsubstantial reward noise. For example, manually flipping 40% of the reward\nfunction's outputs in math tasks still allows a Qwen-2.5-7B model to achieve\nrapid convergence, improving its performance on math tasks from 5% to 72%,\ncompared to the 75% accuracy achieved by a model trained with noiseless\nrewards. Surprisingly, by only rewarding the appearance of key reasoning\nphrases (namely reasoning pattern reward, RPR), such as ``first, I need\nto''-without verifying the correctness of answers, the model achieved peak\ndownstream performance (over 70% accuracy for Qwen-2.5-7B) comparable to models\ntrained with strict correctness verification and accurate rewards. Recognizing\nthe importance of the reasoning process over the final results, we combined RPR\nwith noisy reward models. RPR helped calibrate the noisy reward models,\nmitigating potential false negatives and enhancing the LLM's performance on\nopen-ended tasks. These findings suggest the importance of improving models'\nfoundational abilities during the pre-training phase while providing insights\nfor advancing post-training techniques. Our code and scripts are available at\nhttps://github.com/trestad/Noisy-Rewards-in-Learning-to-Reason.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.22653.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64b8ca3c5067873176d4b436",
      "avatarUrl": "/avatars/b659d147b2454b47c9a7e89bbed525fc.svg",
      "fullname": "AngLv",
      "name": "AngLv",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 7
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.23693",
      "authors": [
        {
          "_id": "68390e95b85141ce6c11b50f",
          "name": "Tingyu Song",
          "hidden": false
        },
        {
          "_id": "68390e95b85141ce6c11b510",
          "name": "Tongyan Hu",
          "hidden": false
        },
        {
          "_id": "68390e95b85141ce6c11b511",
          "name": "Guo Gan",
          "hidden": false
        },
        {
          "_id": "68390e95b85141ce6c11b512",
          "name": "Yilun Zhao",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/64dc29d9b5d625e0e9a6ecb9/ZnHMXas2Khv3qS0RIvYR2.png"
      ],
      "publishedAt": "2025-05-29T17:31:13.000Z",
      "submittedOnDailyAt": "2025-05-30T00:24:42.566Z",
      "title": "VF-Eval: Evaluating Multimodal LLMs for Generating Feedback on AIGC\n  Videos",
      "submittedOnDailyBy": {
        "_id": "64dc29d9b5d625e0e9a6ecb9",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/QxGBsnk1cNsBEPqSx4ae-.jpeg",
        "isPro": false,
        "fullname": "Tingyu Song",
        "user": "songtingyu",
        "type": "user"
      },
      "summary": "MLLMs have been widely studied for video question answering recently.\nHowever, most existing assessments focus on natural videos, overlooking\nsynthetic videos, such as AI-generated content (AIGC). Meanwhile, some works in\nvideo generation rely on MLLMs to evaluate the quality of generated videos, but\nthe capabilities of MLLMs on interpreting AIGC videos remain largely\nunderexplored. To address this, we propose a new benchmark, VF-Eval, which\nintroduces four tasks-coherence validation, error awareness, error type\ndetection, and reasoning evaluation-to comprehensively evaluate the abilities\nof MLLMs on AIGC videos. We evaluate 13 frontier MLLMs on VF-Eval and find that\neven the best-performing model, GPT-4.1, struggles to achieve consistently good\nperformance across all tasks. This highlights the challenging nature of our\nbenchmark. Additionally, to investigate the practical applications of VF-Eval\nin improving video generation, we conduct an experiment, RePrompt,\ndemonstrating that aligning MLLMs more closely with human feedback can benefit\nvideo generation.",
      "upvotes": 10,
      "discussionId": "68390e96b85141ce6c11b55c",
      "githubRepo": "https://github.com/SighingSnow/VF-EVAL",
      "ai_summary": "A new benchmark, VF-Eval, evaluates the capabilities of MLLMs in interpreting AI-generated content videos across four tasks, highlighting challenges and demonstrating benefits in video generation through human feedback alignment.",
      "ai_keywords": [
        "MLLMs",
        "video question answering",
        "synthetic videos",
        "AI-generated content (AIGC)",
        "VF-Eval",
        "coherence validation",
        "error awareness",
        "error type detection",
        "reasoning evaluation",
        "GPT-4.1",
        "RePrompt"
      ]
    },
    "publishedAt": "2025-05-29T13:31:13.000Z",
    "title": "VF-Eval: Evaluating Multimodal LLMs for Generating Feedback on AIGC\n  Videos",
    "summary": "MLLMs have been widely studied for video question answering recently.\nHowever, most existing assessments focus on natural videos, overlooking\nsynthetic videos, such as AI-generated content (AIGC). Meanwhile, some works in\nvideo generation rely on MLLMs to evaluate the quality of generated videos, but\nthe capabilities of MLLMs on interpreting AIGC videos remain largely\nunderexplored. To address this, we propose a new benchmark, VF-Eval, which\nintroduces four tasks-coherence validation, error awareness, error type\ndetection, and reasoning evaluation-to comprehensively evaluate the abilities\nof MLLMs on AIGC videos. We evaluate 13 frontier MLLMs on VF-Eval and find that\neven the best-performing model, GPT-4.1, struggles to achieve consistently good\nperformance across all tasks. This highlights the challenging nature of our\nbenchmark. Additionally, to investigate the practical applications of VF-Eval\nin improving video generation, we conduct an experiment, RePrompt,\ndemonstrating that aligning MLLMs more closely with human feedback can benefit\nvideo generation.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/64dc29d9b5d625e0e9a6ecb9/ZnHMXas2Khv3qS0RIvYR2.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.23693.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64dc29d9b5d625e0e9a6ecb9",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/QxGBsnk1cNsBEPqSx4ae-.jpeg",
      "fullname": "Tingyu Song",
      "name": "songtingyu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.23747",
      "authors": [
        {
          "_id": "68391565d762b7c617b1ba81",
          "name": "Diankun Wu",
          "hidden": false
        },
        {
          "_id": "68391565d762b7c617b1ba82",
          "name": "Fangfu Liu",
          "hidden": false
        },
        {
          "_id": "68391565d762b7c617b1ba83",
          "name": "Yi-Hsin Hung",
          "hidden": false
        },
        {
          "_id": "68391565d762b7c617b1ba84",
          "name": "Yueqi Duan",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6505a02f9310ce8c400edc63/eb5xv9-rlab_DrHLk4BMq.mp4"
      ],
      "publishedAt": "2025-05-29T17:59:04.000Z",
      "submittedOnDailyAt": "2025-05-30T00:56:58.237Z",
      "title": "Spatial-MLLM: Boosting MLLM Capabilities in Visual-based Spatial\n  Intelligence",
      "submittedOnDailyBy": {
        "_id": "6505a02f9310ce8c400edc63",
        "avatarUrl": "/avatars/bbf781594fc8c812316711aa8e2797aa.svg",
        "isPro": false,
        "fullname": "Fangfu Liu",
        "user": "Liuff23",
        "type": "user"
      },
      "summary": "Recent advancements in Multimodal Large Language Models (MLLMs) have\nsignificantly enhanced performance on 2D visual tasks. However, improving their\nspatial intelligence remains a challenge. Existing 3D MLLMs always rely on\nadditional 3D or 2.5D data to incorporate spatial awareness, restricting their\nutility in scenarios with only 2D inputs, such as images or videos. In this\npaper, we present Spatial-MLLM, a novel framework for visual-based spatial\nreasoning from purely 2D observations. Unlike conventional video MLLMs which\nrely on CLIP-based visual encoders optimized for semantic understanding, our\nkey insight is to unleash the strong structure prior from the feed-forward\nvisual geometry foundation model. Specifically, we propose a dual-encoder\narchitecture: a pretrained 2D visual encoder to extract semantic features, and\na spatial encoder-initialized from the backbone of the visual geometry model-to\nextract 3D structure features. A connector then integrates both features into\nunified visual tokens for enhanced spatial understanding. Furthermore, we\npropose a space-aware frame sampling strategy at inference time, which selects\nthe spatially informative frames of a video sequence, ensuring that even under\nlimited token length, the model focuses on frames critical for spatial\nreasoning. Beyond architecture improvements, we construct the Spatial-MLLM-120k\ndataset and train the model on it using supervised fine-tuning and GRPO.\nExtensive experiments on various real-world datasets demonstrate that our\nspatial-MLLM achieves state-of-the-art performance in a wide range of\nvisual-based spatial understanding and reasoning tasks. Project page:\nhttps://diankun-wu.github.io/Spatial-MLLM/.",
      "upvotes": 9,
      "discussionId": "68391566d762b7c617b1bae5",
      "projectPage": "https://diankun-wu.github.io/Spatial-MLLM/",
      "githubRepo": "https://github.com/diankun-wu/Spatial-MLLM"
    },
    "publishedAt": "2025-05-29T13:59:04.000Z",
    "title": "Spatial-MLLM: Boosting MLLM Capabilities in Visual-based Spatial\n  Intelligence",
    "summary": "Recent advancements in Multimodal Large Language Models (MLLMs) have\nsignificantly enhanced performance on 2D visual tasks. However, improving their\nspatial intelligence remains a challenge. Existing 3D MLLMs always rely on\nadditional 3D or 2.5D data to incorporate spatial awareness, restricting their\nutility in scenarios with only 2D inputs, such as images or videos. In this\npaper, we present Spatial-MLLM, a novel framework for visual-based spatial\nreasoning from purely 2D observations. Unlike conventional video MLLMs which\nrely on CLIP-based visual encoders optimized for semantic understanding, our\nkey insight is to unleash the strong structure prior from the feed-forward\nvisual geometry foundation model. Specifically, we propose a dual-encoder\narchitecture: a pretrained 2D visual encoder to extract semantic features, and\na spatial encoder-initialized from the backbone of the visual geometry model-to\nextract 3D structure features. A connector then integrates both features into\nunified visual tokens for enhanced spatial understanding. Furthermore, we\npropose a space-aware frame sampling strategy at inference time, which selects\nthe spatially informative frames of a video sequence, ensuring that even under\nlimited token length, the model focuses on frames critical for spatial\nreasoning. Beyond architecture improvements, we construct the Spatial-MLLM-120k\ndataset and train the model on it using supervised fine-tuning and GRPO.\nExtensive experiments on various real-world datasets demonstrate that our\nspatial-MLLM achieves state-of-the-art performance in a wide range of\nvisual-based spatial understanding and reasoning tasks. Project page:\nhttps://diankun-wu.github.io/Spatial-MLLM/.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6505a02f9310ce8c400edc63/eb5xv9-rlab_DrHLk4BMq.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.23747.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6505a02f9310ce8c400edc63",
      "avatarUrl": "/avatars/bbf781594fc8c812316711aa8e2797aa.svg",
      "fullname": "Fangfu Liu",
      "name": "Liuff23",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 8
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.23762",
      "authors": [
        {
          "_id": "68391353d8c153d346e1ddb5",
          "name": "Chenyu Yang",
          "hidden": false
        },
        {
          "_id": "68391353d8c153d346e1ddb6",
          "name": "Shiqian Su",
          "hidden": false
        },
        {
          "_id": "68391353d8c153d346e1ddb7",
          "name": "Shi Liu",
          "hidden": false
        },
        {
          "_id": "68391353d8c153d346e1ddb8",
          "name": "Xuan Dong",
          "hidden": false
        },
        {
          "_id": "68391353d8c153d346e1ddb9",
          "name": "Yue Yu",
          "hidden": false
        },
        {
          "_id": "68391353d8c153d346e1ddba",
          "name": "Weijie Su",
          "hidden": false
        },
        {
          "_id": "68391353d8c153d346e1ddbb",
          "name": "Xuehui Wang",
          "hidden": false
        },
        {
          "_id": "68391353d8c153d346e1ddbc",
          "name": "Zhaoyang Liu",
          "hidden": false
        },
        {
          "_id": "68391353d8c153d346e1ddbd",
          "name": "Jinguo Zhu",
          "hidden": false
        },
        {
          "_id": "68391353d8c153d346e1ddbe",
          "name": "Hao Li",
          "hidden": false
        },
        {
          "_id": "68391353d8c153d346e1ddbf",
          "name": "Wenhai Wang",
          "hidden": false
        },
        {
          "_id": "68391353d8c153d346e1ddc0",
          "name": "Yu Qiao",
          "hidden": false
        },
        {
          "_id": "68391353d8c153d346e1ddc1",
          "name": "Xizhou Zhu",
          "hidden": false
        },
        {
          "_id": "68391353d8c153d346e1ddc2",
          "name": "Jifeng Dai",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-29T17:59:51.000Z",
      "submittedOnDailyAt": "2025-05-30T00:41:52.864Z",
      "title": "ZeroGUI: Automating Online GUI Learning at Zero Human Cost",
      "submittedOnDailyBy": {
        "_id": "637f347a52229c639211bee8",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/637f347a52229c639211bee8/I9_PET-_6SJQJ6hXrACV4.jpeg",
        "isPro": false,
        "fullname": "Chenyu Yang",
        "user": "cyyang822",
        "type": "user"
      },
      "summary": "The rapid advancement of large Vision-Language Models (VLMs) has propelled\nthe development of pure-vision-based GUI Agents, capable of perceiving and\noperating Graphical User Interfaces (GUI) to autonomously fulfill user\ninstructions. However, existing approaches usually adopt an offline learning\nframework, which faces two core limitations: (1) heavy reliance on high-quality\nmanual annotations for element grounding and action supervision, and (2)\nlimited adaptability to dynamic and interactive environments. To address these\nlimitations, we propose ZeroGUI, a scalable, online learning framework for\nautomating GUI Agent training at Zero human cost. Specifically, ZeroGUI\nintegrates (i) VLM-based automatic task generation to produce diverse training\ngoals from the current environment state, (ii) VLM-based automatic reward\nestimation to assess task success without hand-crafted evaluation functions,\nand (iii) two-stage online reinforcement learning to continuously interact with\nand learn from GUI environments. Experiments on two advanced GUI Agents\n(UI-TARS and Aguvis) demonstrate that ZeroGUI significantly boosts performance\nacross OSWorld and AndroidLab environments. The code is available at\nhttps://github.com/OpenGVLab/ZeroGUI.",
      "upvotes": 8,
      "discussionId": "68391354d8c153d346e1de1a",
      "githubRepo": "https://github.com/OpenGVLab/ZeroGUI",
      "ai_summary": "ZeroGUI is an online learning framework that uses Vision-Language Models for task generation and reward estimation, enhancing GUI Agents' performance with minimal human intervention.",
      "ai_keywords": [
        "Vision-Language Models",
        "GUI Agents",
        "element grounding",
        "action supervision",
        "offline learning",
        "automatic task generation",
        "automatic reward estimation",
        "reinforcement learning",
        "OSWorld",
        "AndroidLab"
      ]
    },
    "publishedAt": "2025-05-29T13:59:51.000Z",
    "title": "ZeroGUI: Automating Online GUI Learning at Zero Human Cost",
    "summary": "The rapid advancement of large Vision-Language Models (VLMs) has propelled\nthe development of pure-vision-based GUI Agents, capable of perceiving and\noperating Graphical User Interfaces (GUI) to autonomously fulfill user\ninstructions. However, existing approaches usually adopt an offline learning\nframework, which faces two core limitations: (1) heavy reliance on high-quality\nmanual annotations for element grounding and action supervision, and (2)\nlimited adaptability to dynamic and interactive environments. To address these\nlimitations, we propose ZeroGUI, a scalable, online learning framework for\nautomating GUI Agent training at Zero human cost. Specifically, ZeroGUI\nintegrates (i) VLM-based automatic task generation to produce diverse training\ngoals from the current environment state, (ii) VLM-based automatic reward\nestimation to assess task success without hand-crafted evaluation functions,\nand (iii) two-stage online reinforcement learning to continuously interact with\nand learn from GUI environments. Experiments on two advanced GUI Agents\n(UI-TARS and Aguvis) demonstrate that ZeroGUI significantly boosts performance\nacross OSWorld and AndroidLab environments. The code is available at\nhttps://github.com/OpenGVLab/ZeroGUI.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.23762.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "637f347a52229c639211bee8",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/637f347a52229c639211bee8/I9_PET-_6SJQJ6hXrACV4.jpeg",
      "fullname": "Chenyu Yang",
      "name": "cyyang822",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.23604",
      "authors": [
        {
          "_id": "68390da8f527444e97c4ab95",
          "name": "Guangtao Zeng",
          "hidden": false
        },
        {
          "_id": "68390da8f527444e97c4ab96",
          "user": {
            "_id": "6553c985a7aded0380b5f928",
            "avatarUrl": "/avatars/36109d6f536d2b34d98822b88eac9608.svg",
            "isPro": false,
            "fullname": "Maohao Shen",
            "user": "maohaos2",
            "type": "user"
          },
          "name": "Maohao Shen",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-05-30T01:45:13.275Z",
          "hidden": false
        },
        {
          "_id": "68390da8f527444e97c4ab97",
          "name": "Delin Chen",
          "hidden": false
        },
        {
          "_id": "68390da8f527444e97c4ab98",
          "name": "Zhenting Qi",
          "hidden": false
        },
        {
          "_id": "68390da8f527444e97c4ab99",
          "name": "Subhro Das",
          "hidden": false
        },
        {
          "_id": "68390da8f527444e97c4ab9a",
          "name": "Dan Gutfreund",
          "hidden": false
        },
        {
          "_id": "68390da8f527444e97c4ab9b",
          "name": "David Cox",
          "hidden": false
        },
        {
          "_id": "68390da8f527444e97c4ab9c",
          "name": "Gregory Wornell",
          "hidden": false
        },
        {
          "_id": "68390da8f527444e97c4ab9d",
          "name": "Wei Lu",
          "hidden": false
        },
        {
          "_id": "68390da8f527444e97c4ab9e",
          "name": "Zhang-Wei Hong",
          "hidden": false
        },
        {
          "_id": "68390da8f527444e97c4ab9f",
          "name": "Chuang Gan",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-29T16:15:36.000Z",
      "submittedOnDailyAt": "2025-05-30T00:43:27.169Z",
      "title": "Satori-SWE: Evolutionary Test-Time Scaling for Sample-Efficient Software\n  Engineering",
      "submittedOnDailyBy": {
        "_id": "60ad0de755f970745d4ec28d",
        "avatarUrl": "/avatars/b0de0222b8ed5fdac8dc7cb0336d2ec7.svg",
        "isPro": true,
        "fullname": "GtZeng",
        "user": "chaoscodes",
        "type": "user"
      },
      "summary": "Language models (LMs) perform well on standardized coding benchmarks but\nstruggle with real-world software engineering tasks such as resolving GitHub\nissues in SWE-Bench, especially when model parameters are less than 100B. While\nsmaller models are preferable in practice due to their lower computational\ncost, improving their performance remains challenging. Existing approaches\nprimarily rely on supervised fine-tuning (SFT) with high-quality data, which is\nexpensive to curate at scale. An alternative is test-time scaling: generating\nmultiple outputs, scoring them using a verifier, and selecting the best one.\nAlthough effective, this strategy often requires excessive sampling and costly\nscoring, limiting its practical application. We propose Evolutionary Test-Time\nScaling (EvoScale), a sample-efficient method that treats generation as an\nevolutionary process. By iteratively refining outputs via selection and\nmutation, EvoScale shifts the output distribution toward higher-scoring\nregions, reducing the number of samples needed to find correct solutions. To\nreduce the overhead from repeatedly sampling and selection, we train the model\nto self-evolve using reinforcement learning (RL). Rather than relying on\nexternal verifiers at inference time, the model learns to self-improve the\nscores of its own generations across iterations. Evaluated on\nSWE-Bench-Verified, EvoScale enables our 32B model, Satori-SWE-32B, to match or\nexceed the performance of models with over 100B parameters while using a few\nsamples. Code, data, and models will be fully open-sourced.",
      "upvotes": 8,
      "discussionId": "68390da9f527444e97c4abdf",
      "ai_summary": "EvoScale, an evolutionary and reinforcement learning-based method, enhances small language models' performance on real-world software engineering tasks by iteratively improving and refining outputs.",
      "ai_keywords": [
        "supervised fine-tuning",
        "test-time scaling",
        "Evolutionary Test-Time Scaling",
        "EvoScale",
        "reinforcement learning",
        "SWE-Bench-Verified"
      ]
    },
    "publishedAt": "2025-05-29T12:15:36.000Z",
    "title": "Satori-SWE: Evolutionary Test-Time Scaling for Sample-Efficient Software\n  Engineering",
    "summary": "Language models (LMs) perform well on standardized coding benchmarks but\nstruggle with real-world software engineering tasks such as resolving GitHub\nissues in SWE-Bench, especially when model parameters are less than 100B. While\nsmaller models are preferable in practice due to their lower computational\ncost, improving their performance remains challenging. Existing approaches\nprimarily rely on supervised fine-tuning (SFT) with high-quality data, which is\nexpensive to curate at scale. An alternative is test-time scaling: generating\nmultiple outputs, scoring them using a verifier, and selecting the best one.\nAlthough effective, this strategy often requires excessive sampling and costly\nscoring, limiting its practical application. We propose Evolutionary Test-Time\nScaling (EvoScale), a sample-efficient method that treats generation as an\nevolutionary process. By iteratively refining outputs via selection and\nmutation, EvoScale shifts the output distribution toward higher-scoring\nregions, reducing the number of samples needed to find correct solutions. To\nreduce the overhead from repeatedly sampling and selection, we train the model\nto self-evolve using reinforcement learning (RL). Rather than relying on\nexternal verifiers at inference time, the model learns to self-improve the\nscores of its own generations across iterations. Evaluated on\nSWE-Bench-Verified, EvoScale enables our 32B model, Satori-SWE-32B, to match or\nexceed the performance of models with over 100B parameters while using a few\nsamples. Code, data, and models will be fully open-sourced.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.23604.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60ad0de755f970745d4ec28d",
      "avatarUrl": "/avatars/b0de0222b8ed5fdac8dc7cb0336d2ec7.svg",
      "fullname": "GtZeng",
      "name": "chaoscodes",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 11
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.23559",
      "authors": [
        {
          "_id": "68390eebc260f5fa36eaa27c",
          "user": {
            "_id": "66554507e6ea63012f35824c",
            "avatarUrl": "/avatars/b82de75bd60890e7bb524fc3754b131c.svg",
            "isPro": false,
            "fullname": "Kunlun_Zhu",
            "user": "Leozkl",
            "type": "user"
          },
          "name": "Kunlun Zhu",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-05-30T01:50:37.811Z",
          "hidden": false
        },
        {
          "_id": "68390eebc260f5fa36eaa27d",
          "name": "Jiaxun Zhang",
          "hidden": false
        },
        {
          "_id": "68390eebc260f5fa36eaa27e",
          "name": "Ziheng Qi",
          "hidden": false
        },
        {
          "_id": "68390eebc260f5fa36eaa27f",
          "name": "Nuoxing Shang",
          "hidden": false
        },
        {
          "_id": "68390eebc260f5fa36eaa280",
          "name": "Zijia Liu",
          "hidden": false
        },
        {
          "_id": "68390eebc260f5fa36eaa281",
          "name": "Peixuan Han",
          "hidden": false
        },
        {
          "_id": "68390eebc260f5fa36eaa282",
          "name": "Yue Su",
          "hidden": false
        },
        {
          "_id": "68390eebc260f5fa36eaa283",
          "name": "Haofei Yu",
          "hidden": false
        },
        {
          "_id": "68390eebc260f5fa36eaa284",
          "name": "Jiaxuan You",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-29T15:35:58.000Z",
      "submittedOnDailyAt": "2025-05-30T00:27:19.661Z",
      "title": "SafeScientist: Toward Risk-Aware Scientific Discoveries by LLM Agents",
      "submittedOnDailyBy": {
        "_id": "64c090a9f613170e7be93d2f",
        "avatarUrl": "/avatars/ccbdf444e1f2386d2281e8e42059ebb0.svg",
        "isPro": false,
        "fullname": "KunlunZhu",
        "user": "KunlunZhu",
        "type": "user"
      },
      "summary": "Recent advancements in large language model (LLM) agents have significantly\naccelerated scientific discovery automation, yet concurrently raised critical\nethical and safety concerns. To systematically address these challenges, we\nintroduce SafeScientist, an innovative AI scientist framework\nexplicitly designed to enhance safety and ethical responsibility in AI-driven\nscientific exploration. SafeScientist proactively refuses ethically\ninappropriate or high-risk tasks and rigorously emphasizes safety throughout\nthe research process. To achieve comprehensive safety oversight, we integrate\nmultiple defensive mechanisms, including prompt monitoring, agent-collaboration\nmonitoring, tool-use monitoring, and an ethical reviewer component.\nComplementing SafeScientist, we propose SciSafetyBench, a novel\nbenchmark specifically designed to evaluate AI safety in scientific contexts,\ncomprising 240 high-risk scientific tasks across 6 domains, alongside 30\nspecially designed scientific tools and 120 tool-related risk tasks. Extensive\nexperiments demonstrate that SafeScientist significantly improves safety\nperformance by 35\\% compared to traditional AI scientist frameworks, without\ncompromising scientific output quality. Additionally, we rigorously validate\nthe robustness of our safety pipeline against diverse adversarial attack\nmethods, further confirming the effectiveness of our integrated approach. The\ncode and data will be available at https://github.com/ulab-uiuc/SafeScientist.\nred{Warning: this paper contains example data that may be offensive\nor harmful.}",
      "upvotes": 7,
      "discussionId": "68390eedc260f5fa36eaa319",
      "ai_summary": "SafeScientist is an AI framework that enhances safety in AI-driven scientific research through multiple defensive mechanisms and is validated using the SciSafetyBench benchmark.",
      "ai_keywords": [
        "SafeScientist",
        "AI scientist framework",
        "ethical responsibility",
        "prompt monitoring",
        "agent-collaboration monitoring",
        "tool-use monitoring",
        "ethical reviewer component",
        "SciSafetyBench",
        "benchmark",
        "scientific tasks",
        "scientific tools",
        "safety performance",
        "adversarial attack methods"
      ]
    },
    "publishedAt": "2025-05-29T11:35:58.000Z",
    "title": "SafeScientist: Toward Risk-Aware Scientific Discoveries by LLM Agents",
    "summary": "Recent advancements in large language model (LLM) agents have significantly\naccelerated scientific discovery automation, yet concurrently raised critical\nethical and safety concerns. To systematically address these challenges, we\nintroduce SafeScientist, an innovative AI scientist framework\nexplicitly designed to enhance safety and ethical responsibility in AI-driven\nscientific exploration. SafeScientist proactively refuses ethically\ninappropriate or high-risk tasks and rigorously emphasizes safety throughout\nthe research process. To achieve comprehensive safety oversight, we integrate\nmultiple defensive mechanisms, including prompt monitoring, agent-collaboration\nmonitoring, tool-use monitoring, and an ethical reviewer component.\nComplementing SafeScientist, we propose SciSafetyBench, a novel\nbenchmark specifically designed to evaluate AI safety in scientific contexts,\ncomprising 240 high-risk scientific tasks across 6 domains, alongside 30\nspecially designed scientific tools and 120 tool-related risk tasks. Extensive\nexperiments demonstrate that SafeScientist significantly improves safety\nperformance by 35\\% compared to traditional AI scientist frameworks, without\ncompromising scientific output quality. Additionally, we rigorously validate\nthe robustness of our safety pipeline against diverse adversarial attack\nmethods, further confirming the effectiveness of our integrated approach. The\ncode and data will be available at https://github.com/ulab-uiuc/SafeScientist.\nred{Warning: this paper contains example data that may be offensive\nor harmful.}",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.23559.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64c090a9f613170e7be93d2f",
      "avatarUrl": "/avatars/ccbdf444e1f2386d2281e8e42059ebb0.svg",
      "fullname": "KunlunZhu",
      "name": "KunlunZhu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.22961",
      "authors": [
        {
          "_id": "68390b2ea26b4142b0578d05",
          "name": "Peixuan Han",
          "hidden": false
        },
        {
          "_id": "68390b2ea26b4142b0578d06",
          "name": "Zijia Liu",
          "hidden": false
        },
        {
          "_id": "68390b2ea26b4142b0578d07",
          "name": "Jiaxuan You",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-29T01:03:41.000Z",
      "submittedOnDailyAt": "2025-05-30T00:34:06.330Z",
      "title": "ToMAP: Training Opponent-Aware LLM Persuaders with Theory of Mind",
      "submittedOnDailyBy": {
        "_id": "65d188a4aa309d842e438ef1",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65d188a4aa309d842e438ef1/t5zmBIytp-xnMoHUfYhEa.jpeg",
        "isPro": false,
        "fullname": "Zijia Liu",
        "user": "m-serious",
        "type": "user"
      },
      "summary": "Large language models (LLMs) have shown promising potential in persuasion,\nbut existing works on training LLM persuaders are still preliminary. Notably,\nwhile humans are skilled in modeling their opponent's thoughts and opinions\nproactively and dynamically, current LLMs struggle with such Theory of Mind\n(ToM) reasoning, resulting in limited diversity and opponent awareness. To\naddress this limitation, we introduce Theory of Mind Augmented Persuader\n(ToMAP), a novel approach for building more flexible persuader agents by\nincorporating two theory of mind modules that enhance the persuader's awareness\nand analysis of the opponent's mental state. Specifically, we begin by\nprompting the persuader to consider possible objections to the target central\nclaim, and then use a text encoder paired with a trained MLP classifier to\npredict the opponent's current stance on these counterclaims. Our carefully\ndesigned reinforcement learning schema enables the persuader learns how to\nanalyze opponent-related information and utilize it to generate more effective\narguments. Experiments show that the ToMAP persuader, while containing only 3B\nparameters, outperforms much larger baselines, like GPT-4o, with a relative\ngain of 39.4% across multiple persuadee models and diverse corpora. Notably,\nToMAP exhibits complex reasoning chains and reduced repetition during training,\nwhich leads to more diverse and effective arguments. The opponent-aware feature\nof ToMAP also makes it suitable for long conversations and enables it to employ\nmore logical and opponent-aware strategies. These results underscore our\nmethod's effectiveness and highlight its potential for developing more\npersuasive language agents. Code is available at:\nhttps://github.com/ulab-uiuc/ToMAP.",
      "upvotes": 7,
      "discussionId": "68390b30a26b4142b0578d58",
      "githubRepo": "https://github.com/ulab-uiuc/ToMAP",
      "ai_summary": "ToMAP enhances LLM persuaders with Theory of Mind modules, improving opponent awareness and argument quality.",
      "ai_keywords": [
        "large language models",
        "theory of mind (ToM)",
        "reinforcement learning",
        "text encoder",
        "MLP classifier",
        "opponent awareness",
        "reasoning chains",
        "effective arguments",
        "logical strategies"
      ]
    },
    "publishedAt": "2025-05-28T21:03:41.000Z",
    "title": "ToMAP: Training Opponent-Aware LLM Persuaders with Theory of Mind",
    "summary": "Large language models (LLMs) have shown promising potential in persuasion,\nbut existing works on training LLM persuaders are still preliminary. Notably,\nwhile humans are skilled in modeling their opponent's thoughts and opinions\nproactively and dynamically, current LLMs struggle with such Theory of Mind\n(ToM) reasoning, resulting in limited diversity and opponent awareness. To\naddress this limitation, we introduce Theory of Mind Augmented Persuader\n(ToMAP), a novel approach for building more flexible persuader agents by\nincorporating two theory of mind modules that enhance the persuader's awareness\nand analysis of the opponent's mental state. Specifically, we begin by\nprompting the persuader to consider possible objections to the target central\nclaim, and then use a text encoder paired with a trained MLP classifier to\npredict the opponent's current stance on these counterclaims. Our carefully\ndesigned reinforcement learning schema enables the persuader learns how to\nanalyze opponent-related information and utilize it to generate more effective\narguments. Experiments show that the ToMAP persuader, while containing only 3B\nparameters, outperforms much larger baselines, like GPT-4o, with a relative\ngain of 39.4% across multiple persuadee models and diverse corpora. Notably,\nToMAP exhibits complex reasoning chains and reduced repetition during training,\nwhich leads to more diverse and effective arguments. The opponent-aware feature\nof ToMAP also makes it suitable for long conversations and enables it to employ\nmore logical and opponent-aware strategies. These results underscore our\nmethod's effectiveness and highlight its potential for developing more\npersuasive language agents. Code is available at:\nhttps://github.com/ulab-uiuc/ToMAP.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.22961.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65d188a4aa309d842e438ef1",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65d188a4aa309d842e438ef1/t5zmBIytp-xnMoHUfYhEa.jpeg",
      "fullname": "Zijia Liu",
      "name": "m-serious",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.23735",
      "authors": [
        {
          "_id": "6839158b56bcc85d9f92199b",
          "name": "Ali Behrouz",
          "hidden": false
        },
        {
          "_id": "6839158b56bcc85d9f92199c",
          "name": "Zeman Li",
          "hidden": false
        },
        {
          "_id": "6839158b56bcc85d9f92199d",
          "name": "Praneeth Kacham",
          "hidden": false
        },
        {
          "_id": "6839158b56bcc85d9f92199e",
          "name": "Majid Daliri",
          "hidden": false
        },
        {
          "_id": "6839158b56bcc85d9f92199f",
          "name": "Yuan Deng",
          "hidden": false
        },
        {
          "_id": "6839158b56bcc85d9f9219a0",
          "name": "Peilin Zhong",
          "hidden": false
        },
        {
          "_id": "6839158b56bcc85d9f9219a1",
          "name": "Meisam Razaviyayn",
          "hidden": false
        },
        {
          "_id": "6839158b56bcc85d9f9219a2",
          "name": "Vahab Mirrokni",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-29T17:57:16.000Z",
      "submittedOnDailyAt": "2025-05-30T00:49:34.783Z",
      "title": "ATLAS: Learning to Optimally Memorize the Context at Test Time",
      "submittedOnDailyBy": {
        "_id": "65cccd5134a5d74cbaa9446c",
        "avatarUrl": "/avatars/5255b734628992106598eae4f2c5848f.svg",
        "isPro": false,
        "fullname": "Ali Behrouz",
        "user": "AliBehrouz",
        "type": "user"
      },
      "summary": "Transformers have been established as the most popular backbones in sequence\nmodeling, mainly due to their effectiveness in in-context retrieval tasks and\nthe ability to learn at scale. Their quadratic memory and time complexity,\nhowever, bound their applicability in longer sequences and so has motivated\nresearchers to explore effective alternative architectures such as modern\nrecurrent neural networks (a.k.a long-term recurrent memory module). Despite\ntheir recent success in diverse downstream tasks, they struggle in tasks that\nrequires long context understanding and extrapolation to longer sequences. We\nobserve that these shortcomings come from three disjoint aspects in their\ndesign: (1) limited memory capacity that is bounded by the architecture of\nmemory and feature mapping of the input; (2) online nature of update, i.e.,\noptimizing the memory only with respect to the last input; and (3) less\nexpressive management of their fixed-size memory. To enhance all these three\naspects, we present ATLAS, a long-term memory module with high capacity that\nlearns to memorize the context by optimizing the memory based on the current\nand past tokens, overcoming the online nature of long-term memory models.\nBuilding on this insight, we present a new family of Transformer-like\narchitectures, called DeepTransformers, that are strict generalizations of the\noriginal Transformer architecture. Our experimental results on language\nmodeling, common-sense reasoning, recall-intensive, and long-context\nunderstanding tasks show that ATLAS surpasses the performance of Transformers\nand recent linear recurrent models. ATLAS further improves the long context\nperformance of Titans, achieving +80\\% accuracy in 10M context length of\nBABILong benchmark.",
      "upvotes": 6,
      "discussionId": "6839158c56bcc85d9f9219fc"
    },
    "publishedAt": "2025-05-29T13:57:16.000Z",
    "title": "ATLAS: Learning to Optimally Memorize the Context at Test Time",
    "summary": "Transformers have been established as the most popular backbones in sequence\nmodeling, mainly due to their effectiveness in in-context retrieval tasks and\nthe ability to learn at scale. Their quadratic memory and time complexity,\nhowever, bound their applicability in longer sequences and so has motivated\nresearchers to explore effective alternative architectures such as modern\nrecurrent neural networks (a.k.a long-term recurrent memory module). Despite\ntheir recent success in diverse downstream tasks, they struggle in tasks that\nrequires long context understanding and extrapolation to longer sequences. We\nobserve that these shortcomings come from three disjoint aspects in their\ndesign: (1) limited memory capacity that is bounded by the architecture of\nmemory and feature mapping of the input; (2) online nature of update, i.e.,\noptimizing the memory only with respect to the last input; and (3) less\nexpressive management of their fixed-size memory. To enhance all these three\naspects, we present ATLAS, a long-term memory module with high capacity that\nlearns to memorize the context by optimizing the memory based on the current\nand past tokens, overcoming the online nature of long-term memory models.\nBuilding on this insight, we present a new family of Transformer-like\narchitectures, called DeepTransformers, that are strict generalizations of the\noriginal Transformer architecture. Our experimental results on language\nmodeling, common-sense reasoning, recall-intensive, and long-context\nunderstanding tasks show that ATLAS surpasses the performance of Transformers\nand recent linear recurrent models. ATLAS further improves the long context\nperformance of Titans, achieving +80\\% accuracy in 10M context length of\nBABILong benchmark.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.23735.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65cccd5134a5d74cbaa9446c",
      "avatarUrl": "/avatars/5255b734628992106598eae4f2c5848f.svg",
      "fullname": "Ali Behrouz",
      "name": "AliBehrouz",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.23585",
      "authors": [
        {
          "_id": "683916a7a2d6f83cf12552dd",
          "name": "Yaru Hao",
          "hidden": false
        },
        {
          "_id": "683916a7a2d6f83cf12552de",
          "name": "Li Dong",
          "hidden": false
        },
        {
          "_id": "683916a7a2d6f83cf12552df",
          "name": "Xun Wu",
          "hidden": false
        },
        {
          "_id": "683916a7a2d6f83cf12552e0",
          "name": "Shaohan Huang",
          "hidden": false
        },
        {
          "_id": "683916a7a2d6f83cf12552e1",
          "name": "Zewen Chi",
          "hidden": false
        },
        {
          "_id": "683916a7a2d6f83cf12552e2",
          "name": "Furu Wei",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-29T15:58:04.000Z",
      "submittedOnDailyAt": "2025-05-30T00:53:45.844Z",
      "title": "On-Policy RL with Optimal Reward Baseline",
      "submittedOnDailyBy": {
        "_id": "5df85abada6d0311fd3d5408",
        "avatarUrl": "/avatars/2331cf703c1b5d3a62e2050b1a6eb108.svg",
        "isPro": false,
        "fullname": "Li Dong",
        "user": "unilm",
        "type": "user"
      },
      "summary": "Reinforcement learning algorithms are fundamental to align large language\nmodels with human preferences and to enhance their reasoning capabilities.\nHowever, current reinforcement learning algorithms often suffer from training\ninstability due to loose on-policy constraints and computational inefficiency\ndue to auxiliary models. In this work, we propose On-Policy RL with Optimal\nreward baseline (OPO), a novel and simplified reinforcement learning algorithm\ndesigned to address these challenges. OPO emphasizes the importance of exact\non-policy training, which empirically stabilizes the training process and\nenhances exploration. Moreover, OPO introduces the optimal reward baseline that\ntheoretically minimizes gradient variance. We evaluate OPO on mathematical\nreasoning benchmarks. The results demonstrate its superior performance and\ntraining stability without additional models or regularization terms.\nFurthermore, OPO achieves lower policy shifts and higher output entropy,\nencouraging more diverse and less repetitive responses. These results highlight\nOPO as a promising direction for stable and effective reinforcement learning in\nlarge language model alignment and reasoning tasks. The implementation is\nprovided at https://github.com/microsoft/LMOps/tree/main/opo.",
      "upvotes": 5,
      "discussionId": "683916a8a2d6f83cf1255310"
    },
    "publishedAt": "2025-05-29T11:58:04.000Z",
    "title": "On-Policy RL with Optimal Reward Baseline",
    "summary": "Reinforcement learning algorithms are fundamental to align large language\nmodels with human preferences and to enhance their reasoning capabilities.\nHowever, current reinforcement learning algorithms often suffer from training\ninstability due to loose on-policy constraints and computational inefficiency\ndue to auxiliary models. In this work, we propose On-Policy RL with Optimal\nreward baseline (OPO), a novel and simplified reinforcement learning algorithm\ndesigned to address these challenges. OPO emphasizes the importance of exact\non-policy training, which empirically stabilizes the training process and\nenhances exploration. Moreover, OPO introduces the optimal reward baseline that\ntheoretically minimizes gradient variance. We evaluate OPO on mathematical\nreasoning benchmarks. The results demonstrate its superior performance and\ntraining stability without additional models or regularization terms.\nFurthermore, OPO achieves lower policy shifts and higher output entropy,\nencouraging more diverse and less repetitive responses. These results highlight\nOPO as a promising direction for stable and effective reinforcement learning in\nlarge language model alignment and reasoning tasks. The implementation is\nprovided at https://github.com/microsoft/LMOps/tree/main/opo.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.23585.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "5df85abada6d0311fd3d5408",
      "avatarUrl": "/avatars/2331cf703c1b5d3a62e2050b1a6eb108.svg",
      "fullname": "Li Dong",
      "name": "unilm",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 29
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.23742",
      "authors": [
        {
          "_id": "683917d0ecf59de6ef345e76",
          "name": "Yufan Deng",
          "hidden": false
        },
        {
          "_id": "683917d0ecf59de6ef345e77",
          "name": "Xun Guo",
          "hidden": false
        },
        {
          "_id": "683917d0ecf59de6ef345e78",
          "name": "Yuanyang Yin",
          "hidden": false
        },
        {
          "_id": "683917d0ecf59de6ef345e79",
          "name": "Jacob Zhiyuan Fang",
          "hidden": false
        },
        {
          "_id": "683917d0ecf59de6ef345e7a",
          "name": "Yiding Yang",
          "hidden": false
        },
        {
          "_id": "683917d0ecf59de6ef345e7b",
          "name": "Yizhi Wang",
          "hidden": false
        },
        {
          "_id": "683917d0ecf59de6ef345e7c",
          "name": "Shenghai Yuan",
          "hidden": false
        },
        {
          "_id": "683917d0ecf59de6ef345e7d",
          "name": "Angtian Wang",
          "hidden": false
        },
        {
          "_id": "683917d0ecf59de6ef345e7e",
          "name": "Bo Liu",
          "hidden": false
        },
        {
          "_id": "683917d0ecf59de6ef345e7f",
          "name": "Haibin Huang",
          "hidden": false
        },
        {
          "_id": "683917d0ecf59de6ef345e80",
          "name": "Chongyang Ma",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-29T17:58:15.000Z",
      "submittedOnDailyAt": "2025-05-30T00:58:48.386Z",
      "title": "MAGREF: Masked Guidance for Any-Reference Video Generation",
      "submittedOnDailyBy": {
        "_id": "63468720dd6d90d82ccf3450",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63468720dd6d90d82ccf3450/tVBFlmZNz8FRMkOrDaDID.jpeg",
        "isPro": false,
        "fullname": "YSH",
        "user": "BestWishYsh",
        "type": "user"
      },
      "summary": "Video generation has made substantial strides with the emergence of deep\ngenerative models, especially diffusion-based approaches. However, video\ngeneration based on multiple reference subjects still faces significant\nchallenges in maintaining multi-subject consistency and ensuring high\ngeneration quality. In this paper, we propose MAGREF, a unified framework for\nany-reference video generation that introduces masked guidance to enable\ncoherent multi-subject video synthesis conditioned on diverse reference images\nand a textual prompt. Specifically, we propose (1) a region-aware dynamic\nmasking mechanism that enables a single model to flexibly handle various\nsubject inference, including humans, objects, and backgrounds, without\narchitectural changes, and (2) a pixel-wise channel concatenation mechanism\nthat operates on the channel dimension to better preserve appearance features.\nOur model delivers state-of-the-art video generation quality, generalizing from\nsingle-subject training to complex multi-subject scenarios with coherent\nsynthesis and precise control over individual subjects, outperforming existing\nopen-source and commercial baselines. To facilitate evaluation, we also\nintroduce a comprehensive multi-subject video benchmark. Extensive experiments\ndemonstrate the effectiveness of our approach, paving the way for scalable,\ncontrollable, and high-fidelity multi-subject video synthesis. Code and model\ncan be found at: https://github.com/MAGREF-Video/MAGREF",
      "upvotes": 3,
      "discussionId": "683917d2ecf59de6ef345efd",
      "projectPage": "https://magref-video.github.io/magref.github.io/",
      "githubRepo": "https://github.com/MAGREF-Video/MAGREF"
    },
    "publishedAt": "2025-05-29T13:58:15.000Z",
    "title": "MAGREF: Masked Guidance for Any-Reference Video Generation",
    "summary": "Video generation has made substantial strides with the emergence of deep\ngenerative models, especially diffusion-based approaches. However, video\ngeneration based on multiple reference subjects still faces significant\nchallenges in maintaining multi-subject consistency and ensuring high\ngeneration quality. In this paper, we propose MAGREF, a unified framework for\nany-reference video generation that introduces masked guidance to enable\ncoherent multi-subject video synthesis conditioned on diverse reference images\nand a textual prompt. Specifically, we propose (1) a region-aware dynamic\nmasking mechanism that enables a single model to flexibly handle various\nsubject inference, including humans, objects, and backgrounds, without\narchitectural changes, and (2) a pixel-wise channel concatenation mechanism\nthat operates on the channel dimension to better preserve appearance features.\nOur model delivers state-of-the-art video generation quality, generalizing from\nsingle-subject training to complex multi-subject scenarios with coherent\nsynthesis and precise control over individual subjects, outperforming existing\nopen-source and commercial baselines. To facilitate evaluation, we also\nintroduce a comprehensive multi-subject video benchmark. Extensive experiments\ndemonstrate the effectiveness of our approach, paving the way for scalable,\ncontrollable, and high-fidelity multi-subject video synthesis. Code and model\ncan be found at: https://github.com/MAGREF-Video/MAGREF",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.23742.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63468720dd6d90d82ccf3450",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63468720dd6d90d82ccf3450/tVBFlmZNz8FRMkOrDaDID.jpeg",
      "fullname": "YSH",
      "name": "BestWishYsh",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 55
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.23745",
      "authors": [
        {
          "_id": "683910d50df60182c0dd5b62",
          "name": "Hao Dong",
          "hidden": false
        },
        {
          "_id": "683910d50df60182c0dd5b63",
          "name": "Moru Liu",
          "hidden": false
        },
        {
          "_id": "683910d50df60182c0dd5b64",
          "name": "Jian Liang",
          "hidden": false
        },
        {
          "_id": "683910d50df60182c0dd5b65",
          "name": "Eleni Chatzi",
          "hidden": false
        },
        {
          "_id": "683910d50df60182c0dd5b66",
          "name": "Olga Fink",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-29T17:59:01.000Z",
      "submittedOnDailyAt": "2025-05-30T00:32:10.298Z",
      "title": "To Trust Or Not To Trust Your Vision-Language Model's Prediction",
      "submittedOnDailyBy": {
        "_id": "6649fb62a460da1da20f66d0",
        "avatarUrl": "/avatars/6afa26922ba8abe8c9603e6f7222531b.svg",
        "isPro": false,
        "fullname": "Hao Dong",
        "user": "hdong51",
        "type": "user"
      },
      "summary": "Vision-Language Models (VLMs) have demonstrated strong capabilities in\naligning visual and textual modalities, enabling a wide range of applications\nin multimodal understanding and generation. While they excel in zero-shot and\ntransfer learning scenarios, VLMs remain susceptible to misclassification,\noften yielding confident yet incorrect predictions. This limitation poses a\nsignificant risk in safety-critical domains, where erroneous predictions can\nlead to severe consequences. In this work, we introduce TrustVLM, a\ntraining-free framework designed to address the critical challenge of\nestimating when VLM's predictions can be trusted. Motivated by the observed\nmodality gap in VLMs and the insight that certain concepts are more distinctly\nrepresented in the image embedding space, we propose a novel confidence-scoring\nfunction that leverages this space to improve misclassification detection. We\nrigorously evaluate our approach across 17 diverse datasets, employing 4\narchitectures and 2 VLMs, and demonstrate state-of-the-art performance, with\nimprovements of up to 51.87% in AURC, 9.14% in AUROC, and 32.42% in FPR95\ncompared to existing baselines. By improving the reliability of the model\nwithout requiring retraining, TrustVLM paves the way for safer deployment of\nVLMs in real-world applications. The code will be available at\nhttps://github.com/EPFL-IMOS/TrustVLM.",
      "upvotes": 2,
      "discussionId": "683910d70df60182c0dd5bd3",
      "githubRepo": "https://github.com/EPFL-IMOS/TrustVLM",
      "ai_summary": "TrustVLM enhances the reliability of Vision-Language Models by estimating prediction trustworthiness without retraining, improving misclassification detection in multimodal tasks.",
      "ai_keywords": [
        "Vision-Language Models",
        "VLMs",
        "modality gap",
        "image embedding space",
        "confidence-scoring function",
        "AURC",
        "AUROC",
        "FPR95"
      ]
    },
    "publishedAt": "2025-05-29T13:59:01.000Z",
    "title": "To Trust Or Not To Trust Your Vision-Language Model's Prediction",
    "summary": "Vision-Language Models (VLMs) have demonstrated strong capabilities in\naligning visual and textual modalities, enabling a wide range of applications\nin multimodal understanding and generation. While they excel in zero-shot and\ntransfer learning scenarios, VLMs remain susceptible to misclassification,\noften yielding confident yet incorrect predictions. This limitation poses a\nsignificant risk in safety-critical domains, where erroneous predictions can\nlead to severe consequences. In this work, we introduce TrustVLM, a\ntraining-free framework designed to address the critical challenge of\nestimating when VLM's predictions can be trusted. Motivated by the observed\nmodality gap in VLMs and the insight that certain concepts are more distinctly\nrepresented in the image embedding space, we propose a novel confidence-scoring\nfunction that leverages this space to improve misclassification detection. We\nrigorously evaluate our approach across 17 diverse datasets, employing 4\narchitectures and 2 VLMs, and demonstrate state-of-the-art performance, with\nimprovements of up to 51.87% in AURC, 9.14% in AUROC, and 32.42% in FPR95\ncompared to existing baselines. By improving the reliability of the model\nwithout requiring retraining, TrustVLM paves the way for safer deployment of\nVLMs in real-world applications. The code will be available at\nhttps://github.com/EPFL-IMOS/TrustVLM.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.23745.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6649fb62a460da1da20f66d0",
      "avatarUrl": "/avatars/6afa26922ba8abe8c9603e6f7222531b.svg",
      "fullname": "Hao Dong",
      "name": "hdong51",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.23621",
      "authors": [
        {
          "_id": "68391925d73e6015a1b0f305",
          "name": "Zheyuan Yang",
          "hidden": false
        },
        {
          "_id": "68391925d73e6015a1b0f306",
          "name": "Lyuhao Chen",
          "hidden": false
        },
        {
          "_id": "68391925d73e6015a1b0f307",
          "name": "Arman Cohan",
          "hidden": false
        },
        {
          "_id": "68391925d73e6015a1b0f308",
          "name": "Yilun Zhao",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-29T16:28:50.000Z",
      "submittedOnDailyAt": "2025-05-30T01:04:47.042Z",
      "title": "Table-R1: Inference-Time Scaling for Table Reasoning",
      "submittedOnDailyBy": {
        "_id": "62f662bcc58915315c4eccea",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62f662bcc58915315c4eccea/zOAQLONfMP88zr70sxHK-.jpeg",
        "isPro": true,
        "fullname": "Yilun",
        "user": "yilunzhao",
        "type": "user"
      },
      "summary": "In this work, we present the first study to explore inference-time scaling on\ntable reasoning tasks. We develop and evaluate two post-training strategies to\nenable inference-time scaling: distillation from frontier model reasoning\ntraces and reinforcement learning with verifiable rewards (RLVR). For\ndistillation, we introduce a large-scale dataset of reasoning traces generated\nby DeepSeek-R1, which we use to fine-tune LLMs into the Table-R1-SFT model. For\nRLVR, we propose task-specific verifiable reward functions and apply the GRPO\nalgorithm to obtain the Table-R1-Zero model. We evaluate our Table-R1-series\nmodels across diverse table reasoning tasks, including short-form QA, fact\nverification, and free-form QA. Notably, the Table-R1-Zero model matches or\nexceeds the performance of GPT-4.1 and DeepSeek-R1, while using only a\n7B-parameter LLM. It also demonstrates strong generalization to out-of-domain\ndatasets. Extensive ablation and qualitative analyses reveal the benefits of\ninstruction tuning, model architecture choices, and cross-task generalization,\nas well as emergence of essential table reasoning skills during RL training.",
      "upvotes": 2,
      "discussionId": "68391928d73e6015a1b0f3a8"
    },
    "publishedAt": "2025-05-29T12:28:50.000Z",
    "title": "Table-R1: Inference-Time Scaling for Table Reasoning",
    "summary": "In this work, we present the first study to explore inference-time scaling on\ntable reasoning tasks. We develop and evaluate two post-training strategies to\nenable inference-time scaling: distillation from frontier model reasoning\ntraces and reinforcement learning with verifiable rewards (RLVR). For\ndistillation, we introduce a large-scale dataset of reasoning traces generated\nby DeepSeek-R1, which we use to fine-tune LLMs into the Table-R1-SFT model. For\nRLVR, we propose task-specific verifiable reward functions and apply the GRPO\nalgorithm to obtain the Table-R1-Zero model. We evaluate our Table-R1-series\nmodels across diverse table reasoning tasks, including short-form QA, fact\nverification, and free-form QA. Notably, the Table-R1-Zero model matches or\nexceeds the performance of GPT-4.1 and DeepSeek-R1, while using only a\n7B-parameter LLM. It also demonstrates strong generalization to out-of-domain\ndatasets. Extensive ablation and qualitative analyses reveal the benefits of\ninstruction tuning, model architecture choices, and cross-task generalization,\nas well as emergence of essential table reasoning skills during RL training.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.23621.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62f662bcc58915315c4eccea",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62f662bcc58915315c4eccea/zOAQLONfMP88zr70sxHK-.jpeg",
      "fullname": "Yilun",
      "name": "yilunzhao",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 12
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.23419",
      "authors": [
        {
          "_id": "68391574f85de1fc563cd890",
          "name": "Linghao Zhang",
          "hidden": false
        },
        {
          "_id": "68391574f85de1fc563cd891",
          "name": "Shilin He",
          "hidden": false
        },
        {
          "_id": "68391574f85de1fc563cd892",
          "name": "Chaoyun Zhang",
          "hidden": false
        },
        {
          "_id": "68391574f85de1fc563cd893",
          "name": "Yu Kang",
          "hidden": false
        },
        {
          "_id": "68391574f85de1fc563cd894",
          "name": "Bowen Li",
          "hidden": false
        },
        {
          "_id": "68391574f85de1fc563cd895",
          "name": "Chengxing Xie",
          "hidden": false
        },
        {
          "_id": "68391574f85de1fc563cd896",
          "name": "Junhao Wang",
          "hidden": false
        },
        {
          "_id": "68391574f85de1fc563cd897",
          "name": "Maoquan Wang",
          "hidden": false
        },
        {
          "_id": "68391574f85de1fc563cd898",
          "name": "Yufan Huang",
          "hidden": false
        },
        {
          "_id": "68391574f85de1fc563cd899",
          "name": "Shengyu Fu",
          "hidden": false
        },
        {
          "_id": "68391574f85de1fc563cd89a",
          "name": "Elsie Nallipogu",
          "hidden": false
        },
        {
          "_id": "68391574f85de1fc563cd89b",
          "name": "Qingwei Lin",
          "hidden": false
        },
        {
          "_id": "68391574f85de1fc563cd89c",
          "name": "Yingnong Dang",
          "hidden": false
        },
        {
          "_id": "68391574f85de1fc563cd89d",
          "name": "Saravan Rajmohan",
          "hidden": false
        },
        {
          "_id": "68391574f85de1fc563cd89e",
          "name": "Dongmei Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-29T13:09:44.000Z",
      "submittedOnDailyAt": "2025-05-30T00:49:19.429Z",
      "title": "SWE-bench Goes Live!",
      "submittedOnDailyBy": {
        "_id": "654dbac9938fbf1e696be8aa",
        "avatarUrl": "/avatars/b3c4035c48169c1bfb04a439fce3499f.svg",
        "isPro": false,
        "fullname": "Chaoyun Zhang",
        "user": "vyokky",
        "type": "user"
      },
      "summary": "The issue-resolving task, where a model generates patches to fix real-world\nbugs, has emerged as a critical benchmark for evaluating the capabilities of\nlarge language models (LLMs). While SWE-bench and its variants have become\nstandard in this domain, they suffer from key limitations: they have not been\nupdated since their initial releases, cover a narrow set of repositories, and\ndepend heavily on manual effort for instance construction and environment\nsetup. These factors hinder scalability and introduce risks of overfitting and\ndata contamination. In this work, we present SWE-bench-Live, a\nlive-updatable benchmark designed to overcome these challenges. Our\ninitial release consists of 1,319 tasks derived from real GitHub issues created\nsince 2024, spanning 93 repositories. Each task is accompanied by a dedicated\nDocker image to ensure reproducible execution. Central to our benchmark is\n\\method, an automated curation pipeline that streamlines the entire process\nfrom instance creation to environment setup, removing manual bottlenecks and\nenabling scalability and continuous updates. We evaluate a range of\nstate-of-the-art agent frameworks and LLMs on SWE-bench-Live, revealing a\nsubstantial performance gap compared to static benchmarks like SWE-bench, even\nunder controlled evaluation conditions. To better understand this discrepancy,\nwe perform detailed analyses across repository origin, issue recency, and task\ndifficulty. By providing a fresh, diverse, and executable benchmark grounded in\nlive repository activity, SWE-bench-Live facilitates rigorous,\ncontamination-resistant evaluation of LLMs and agents in dynamic, real-world\nsoftware development settings.",
      "upvotes": 2,
      "discussionId": "68391574f85de1fc563cd8d4"
    },
    "publishedAt": "2025-05-29T09:09:44.000Z",
    "title": "SWE-bench Goes Live!",
    "summary": "The issue-resolving task, where a model generates patches to fix real-world\nbugs, has emerged as a critical benchmark for evaluating the capabilities of\nlarge language models (LLMs). While SWE-bench and its variants have become\nstandard in this domain, they suffer from key limitations: they have not been\nupdated since their initial releases, cover a narrow set of repositories, and\ndepend heavily on manual effort for instance construction and environment\nsetup. These factors hinder scalability and introduce risks of overfitting and\ndata contamination. In this work, we present SWE-bench-Live, a\nlive-updatable benchmark designed to overcome these challenges. Our\ninitial release consists of 1,319 tasks derived from real GitHub issues created\nsince 2024, spanning 93 repositories. Each task is accompanied by a dedicated\nDocker image to ensure reproducible execution. Central to our benchmark is\n\\method, an automated curation pipeline that streamlines the entire process\nfrom instance creation to environment setup, removing manual bottlenecks and\nenabling scalability and continuous updates. We evaluate a range of\nstate-of-the-art agent frameworks and LLMs on SWE-bench-Live, revealing a\nsubstantial performance gap compared to static benchmarks like SWE-bench, even\nunder controlled evaluation conditions. To better understand this discrepancy,\nwe perform detailed analyses across repository origin, issue recency, and task\ndifficulty. By providing a fresh, diverse, and executable benchmark grounded in\nlive repository activity, SWE-bench-Live facilitates rigorous,\ncontamination-resistant evaluation of LLMs and agents in dynamic, real-world\nsoftware development settings.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.23419.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "654dbac9938fbf1e696be8aa",
      "avatarUrl": "/avatars/b3c4035c48169c1bfb04a439fce3499f.svg",
      "fullname": "Chaoyun Zhang",
      "name": "vyokky",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.23359",
      "authors": [
        {
          "_id": "683909a29deef11aa625817c",
          "name": "Yuanxin Liu",
          "hidden": false
        },
        {
          "_id": "683909a29deef11aa625817d",
          "name": "Kun Ouyang",
          "hidden": false
        },
        {
          "_id": "683909a29deef11aa625817e",
          "name": "Haoning Wu",
          "hidden": false
        },
        {
          "_id": "683909a29deef11aa625817f",
          "name": "Yi Liu",
          "hidden": false
        },
        {
          "_id": "683909a29deef11aa6258180",
          "name": "Lin Sui",
          "hidden": false
        },
        {
          "_id": "683909a29deef11aa6258181",
          "name": "Xinhao Li",
          "hidden": false
        },
        {
          "_id": "683909a29deef11aa6258182",
          "name": "Yan Zhong",
          "hidden": false
        },
        {
          "_id": "683909a29deef11aa6258183",
          "name": "Y. Charles",
          "hidden": false
        },
        {
          "_id": "683909a29deef11aa6258184",
          "name": "Xinyu Zhou",
          "hidden": false
        },
        {
          "_id": "683909a29deef11aa6258185",
          "name": "Xu Sun",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6489761dcaea79f577897f98/48SxCF6GVyMdnPR7vZSgC.mp4"
      ],
      "publishedAt": "2025-05-29T11:33:43.000Z",
      "submittedOnDailyAt": "2025-05-30T01:03:01.950Z",
      "title": "VideoReasonBench: Can MLLMs Perform Vision-Centric Complex Video\n  Reasoning?",
      "submittedOnDailyBy": {
        "_id": "6489761dcaea79f577897f98",
        "avatarUrl": "/avatars/8f56dc9c08dc2b672555602d68509a03.svg",
        "isPro": false,
        "fullname": "Yuanxin Liu",
        "user": "lyx97",
        "type": "user"
      },
      "summary": "Recent studies have shown that long chain-of-thought (CoT) reasoning can\nsignificantly enhance the performance of large language models (LLMs) on\ncomplex tasks. However, this benefit is yet to be demonstrated in the domain of\nvideo understanding, since most existing benchmarks lack the reasoning depth\nrequired to demonstrate the advantages of extended CoT chains. While recent\nefforts have proposed benchmarks aimed at video reasoning, the tasks are often\nknowledge-driven and do not rely heavily on visual content. To bridge this gap,\nwe introduce VideoReasonBench, a benchmark designed to evaluate vision-centric,\ncomplex video reasoning. To ensure visual richness and high reasoning\ncomplexity, each video in VideoReasonBench depicts a sequence of fine-grained\noperations on a latent state that is only visible in part of the video. The\nquestions evaluate three escalating levels of video reasoning skills: recalling\nobserved visual information, inferring the content of latent states, and\npredicting information beyond the video. Under such task setting, models have\nto precisely recall multiple operations in the video, and perform step-by-step\nreasoning to get correct final answers for these questions. Using\nVideoReasonBench, we comprehensively evaluate 18 state-of-the-art multimodal\nLLMs (MLLMs), finding that most perform poorly on complex video reasoning,\ne.g., GPT-4o achieves only 6.9% accuracy, while the thinking-enhanced\nGemini-2.5-Pro significantly outperforms others with 56.0% accuracy. Our\ninvestigations on \"test-time scaling\" further reveal that extended thinking\nbudget, while offering none or minimal benefits on existing video benchmarks,\nis essential for improving the performance on VideoReasonBench.",
      "upvotes": 2,
      "discussionId": "683909a39deef11aa62581c2",
      "projectPage": "https://llyx97.github.io/video_reason_bench/",
      "githubRepo": "https://github.com/llyx97/video_reason_bench",
      "ai_summary": "A new benchmark, VideoReasonBench, evaluates complex vision-centric video reasoning, finding that extended thinking budgets are crucial for improved performance compared to existing benchmarks.",
      "ai_keywords": [
        "long chain-of-thought reasoning",
        "large language models",
        "video understanding",
        "VideoReasonBench",
        "vision-centric",
        "complex video reasoning",
        "latent state",
        "visual reasoning",
        "step-by-step reasoning",
        "multimodal language models",
        "thinking-enhanced models",
        "test-time scaling"
      ]
    },
    "publishedAt": "2025-05-29T07:33:43.000Z",
    "title": "VideoReasonBench: Can MLLMs Perform Vision-Centric Complex Video\n  Reasoning?",
    "summary": "Recent studies have shown that long chain-of-thought (CoT) reasoning can\nsignificantly enhance the performance of large language models (LLMs) on\ncomplex tasks. However, this benefit is yet to be demonstrated in the domain of\nvideo understanding, since most existing benchmarks lack the reasoning depth\nrequired to demonstrate the advantages of extended CoT chains. While recent\nefforts have proposed benchmarks aimed at video reasoning, the tasks are often\nknowledge-driven and do not rely heavily on visual content. To bridge this gap,\nwe introduce VideoReasonBench, a benchmark designed to evaluate vision-centric,\ncomplex video reasoning. To ensure visual richness and high reasoning\ncomplexity, each video in VideoReasonBench depicts a sequence of fine-grained\noperations on a latent state that is only visible in part of the video. The\nquestions evaluate three escalating levels of video reasoning skills: recalling\nobserved visual information, inferring the content of latent states, and\npredicting information beyond the video. Under such task setting, models have\nto precisely recall multiple operations in the video, and perform step-by-step\nreasoning to get correct final answers for these questions. Using\nVideoReasonBench, we comprehensively evaluate 18 state-of-the-art multimodal\nLLMs (MLLMs), finding that most perform poorly on complex video reasoning,\ne.g., GPT-4o achieves only 6.9% accuracy, while the thinking-enhanced\nGemini-2.5-Pro significantly outperforms others with 56.0% accuracy. Our\ninvestigations on \"test-time scaling\" further reveal that extended thinking\nbudget, while offering none or minimal benefits on existing video benchmarks,\nis essential for improving the performance on VideoReasonBench.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6489761dcaea79f577897f98/48SxCF6GVyMdnPR7vZSgC.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.23359.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6489761dcaea79f577897f98",
      "avatarUrl": "/avatars/8f56dc9c08dc2b672555602d68509a03.svg",
      "fullname": "Yuanxin Liu",
      "name": "lyx97",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.23606",
      "authors": [
        {
          "_id": "6839189f4a3a71a917b0514e",
          "name": "Qingyu Shi",
          "hidden": false
        },
        {
          "_id": "6839189f4a3a71a917b0514f",
          "name": "Jinbin Bai",
          "hidden": false
        },
        {
          "_id": "6839189f4a3a71a917b05150",
          "name": "Zhuoran Zhao",
          "hidden": false
        },
        {
          "_id": "6839189f4a3a71a917b05151",
          "name": "Wenhao Chai",
          "hidden": false
        },
        {
          "_id": "6839189f4a3a71a917b05152",
          "name": "Kaidong Yu",
          "hidden": false
        },
        {
          "_id": "6839189f4a3a71a917b05153",
          "name": "Jianzong Wu",
          "hidden": false
        },
        {
          "_id": "6839189f4a3a71a917b05154",
          "name": "Shuangyong Song",
          "hidden": false
        },
        {
          "_id": "6839189f4a3a71a917b05155",
          "name": "Yunhai Tong",
          "hidden": false
        },
        {
          "_id": "6839189f4a3a71a917b05156",
          "name": "Xiangtai Li",
          "hidden": false
        },
        {
          "_id": "6839189f4a3a71a917b05157",
          "name": "Xuelong Li",
          "hidden": false
        },
        {
          "_id": "6839189f4a3a71a917b05158",
          "name": "Shuicheng Yan",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-29T16:15:48.000Z",
      "submittedOnDailyAt": "2025-05-30T01:06:53.607Z",
      "title": "Muddit: Liberating Generation Beyond Text-to-Image with a Unified\n  Discrete Diffusion Model",
      "submittedOnDailyBy": {
        "_id": "63fccdac93b993a4ebd7789a",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63fccdac93b993a4ebd7789a/KRx8vpdoDjsZBRw0j8Vg8.jpeg",
        "isPro": false,
        "fullname": "Jinbin Bai",
        "user": "BryanW",
        "type": "user"
      },
      "summary": "Unified generation models aim to handle diverse tasks across modalities --\nsuch as text generation, image generation, and vision-language reasoning --\nwithin a single architecture and decoding paradigm. Autoregressive unified\nmodels suffer from slow inference due to sequential decoding, and\nnon-autoregressive unified models suffer from weak generalization due to\nlimited pretrained backbones. We introduce Muddit, a unified discrete diffusion\ntransformer that enables fast and parallel generation across both text and\nimage modalities. Unlike prior unified diffusion models trained from scratch,\nMuddit integrates strong visual priors from a pretrained text-to-image backbone\nwith a lightweight text decoder, enabling flexible and high-quality multimodal\ngeneration under a unified architecture. Empirical results show that Muddit\nachieves competitive or superior performance compared to significantly larger\nautoregressive models in both quality and efficiency. The work highlights the\npotential of purely discrete diffusion, when equipped with strong visual\npriors, as a scalable and effective backbone for unified generation.",
      "upvotes": 1,
      "discussionId": "683918a14a3a71a917b051ea",
      "githubRepo": "https://github.com/M-E-AGI-Lab/Muddit"
    },
    "publishedAt": "2025-05-29T12:15:48.000Z",
    "title": "Muddit: Liberating Generation Beyond Text-to-Image with a Unified\n  Discrete Diffusion Model",
    "summary": "Unified generation models aim to handle diverse tasks across modalities --\nsuch as text generation, image generation, and vision-language reasoning --\nwithin a single architecture and decoding paradigm. Autoregressive unified\nmodels suffer from slow inference due to sequential decoding, and\nnon-autoregressive unified models suffer from weak generalization due to\nlimited pretrained backbones. We introduce Muddit, a unified discrete diffusion\ntransformer that enables fast and parallel generation across both text and\nimage modalities. Unlike prior unified diffusion models trained from scratch,\nMuddit integrates strong visual priors from a pretrained text-to-image backbone\nwith a lightweight text decoder, enabling flexible and high-quality multimodal\ngeneration under a unified architecture. Empirical results show that Muddit\nachieves competitive or superior performance compared to significantly larger\nautoregressive models in both quality and efficiency. The work highlights the\npotential of purely discrete diffusion, when equipped with strong visual\npriors, as a scalable and effective backbone for unified generation.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.23606.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63fccdac93b993a4ebd7789a",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63fccdac93b993a4ebd7789a/KRx8vpdoDjsZBRw0j8Vg8.jpeg",
      "fullname": "Jinbin Bai",
      "name": "BryanW",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.22943",
      "authors": [
        {
          "_id": "683912ee831cc04b0a3c0c6c",
          "name": "Jaewoo Ahn",
          "hidden": false
        },
        {
          "_id": "683912ee831cc04b0a3c0c6d",
          "name": "Heeseung Yun",
          "hidden": false
        },
        {
          "_id": "683912ee831cc04b0a3c0c6e",
          "name": "Dayoon Ko",
          "hidden": false
        },
        {
          "_id": "683912ee831cc04b0a3c0c6f",
          "name": "Gunhee Kim",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-28T23:45:55.000Z",
      "submittedOnDailyAt": "2025-05-30T00:40:41.829Z",
      "title": "Can LLMs Deceive CLIP? Benchmarking Adversarial Compositionality of\n  Pre-trained Multimodal Representation via Text Updates",
      "submittedOnDailyBy": {
        "_id": "64bb081c01f1983a863654dc",
        "avatarUrl": "/avatars/038894bc72b92ec3f4ecb096cc60b60a.svg",
        "isPro": false,
        "fullname": "Jaewoo Ahn",
        "user": "ahnpersie",
        "type": "user"
      },
      "summary": "While pre-trained multimodal representations (e.g., CLIP) have shown\nimpressive capabilities, they exhibit significant compositional vulnerabilities\nleading to counterintuitive judgments. We introduce Multimodal Adversarial\nCompositionality (MAC), a benchmark that leverages large language models (LLMs)\nto generate deceptive text samples to exploit these vulnerabilities across\ndifferent modalities and evaluates them through both sample-wise attack success\nrate and group-wise entropy-based diversity. To improve zero-shot methods, we\npropose a self-training approach that leverages rejection-sampling fine-tuning\nwith diversity-promoting filtering, which enhances both attack success rate and\nsample diversity. Using smaller language models like Llama-3.1-8B, our approach\ndemonstrates superior performance in revealing compositional vulnerabilities\nacross various multimodal representations, including images, videos, and\naudios.",
      "upvotes": 1,
      "discussionId": "683912ef831cc04b0a3c0cc1",
      "githubRepo": "https://github.com/ahnjaewoo/MAC",
      "ai_summary": "A benchmark using deceptive text samples to evaluate compositional vulnerabilities in multimodal representations is introduced, and a self-training approach improves zero-shot methods by enhancing attack success and sample diversity.",
      "ai_keywords": [
        "Multimodal Adversarial Compositionality (MAC)",
        "multimodal representations",
        "large language models (LLMs)",
        "rejection-sampling fine-tuning",
        "diversity-promoting filtering",
        "images",
        "videos",
        "audios"
      ]
    },
    "publishedAt": "2025-05-28T19:45:55.000Z",
    "title": "Can LLMs Deceive CLIP? Benchmarking Adversarial Compositionality of\n  Pre-trained Multimodal Representation via Text Updates",
    "summary": "While pre-trained multimodal representations (e.g., CLIP) have shown\nimpressive capabilities, they exhibit significant compositional vulnerabilities\nleading to counterintuitive judgments. We introduce Multimodal Adversarial\nCompositionality (MAC), a benchmark that leverages large language models (LLMs)\nto generate deceptive text samples to exploit these vulnerabilities across\ndifferent modalities and evaluates them through both sample-wise attack success\nrate and group-wise entropy-based diversity. To improve zero-shot methods, we\npropose a self-training approach that leverages rejection-sampling fine-tuning\nwith diversity-promoting filtering, which enhances both attack success rate and\nsample diversity. Using smaller language models like Llama-3.1-8B, our approach\ndemonstrates superior performance in revealing compositional vulnerabilities\nacross various multimodal representations, including images, videos, and\naudios.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.22943.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64bb081c01f1983a863654dc",
      "avatarUrl": "/avatars/038894bc72b92ec3f4ecb096cc60b60a.svg",
      "fullname": "Jaewoo Ahn",
      "name": "ahnpersie",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.20199",
      "authors": [
        {
          "_id": "683919611186f2cbf3ed2267",
          "name": "Pengxiang Li",
          "hidden": false
        },
        {
          "_id": "683919611186f2cbf3ed2268",
          "name": "Shilin Yan",
          "hidden": false
        },
        {
          "_id": "683919611186f2cbf3ed2269",
          "name": "Joey Tsai",
          "hidden": false
        },
        {
          "_id": "683919611186f2cbf3ed226a",
          "name": "Renrui Zhang",
          "hidden": false
        },
        {
          "_id": "683919611186f2cbf3ed226b",
          "name": "Ruichuan An",
          "hidden": false
        },
        {
          "_id": "683919611186f2cbf3ed226c",
          "name": "Ziyu Guo",
          "hidden": false
        },
        {
          "_id": "683919611186f2cbf3ed226d",
          "name": "Xiaowei Gao",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-26T16:40:22.000Z",
      "submittedOnDailyAt": "2025-05-30T01:05:23.884Z",
      "title": "Adaptive Classifier-Free Guidance via Dynamic Low-Confidence Masking",
      "submittedOnDailyBy": {
        "_id": "64245f2c089d5fae56b4549a",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64245f2c089d5fae56b4549a/qUHFsL9Svwyj5BKpfMtaY.jpeg",
        "isPro": false,
        "fullname": "Pengxiang Li",
        "user": "pengxiang",
        "type": "user"
      },
      "summary": "Classifier-Free Guidance (CFG) significantly enhances controllability in\ngenerative models by interpolating conditional and unconditional predictions.\nHowever, standard CFG often employs a static unconditional input, which can be\nsuboptimal for iterative generation processes where model uncertainty varies\ndynamically. We introduce Adaptive Classifier-Free Guidance (A-CFG), a novel\nmethod that tailors the unconditional input by leveraging the model's\ninstantaneous predictive confidence. At each step of an iterative (masked)\ndiffusion language model, A-CFG identifies tokens in the currently generated\nsequence for which the model exhibits low confidence. These tokens are\ntemporarily re-masked to create a dynamic, localized unconditional input. This\nfocuses CFG's corrective influence precisely on areas of ambiguity, leading to\nmore effective guidance. We integrate A-CFG into a state-of-the-art masked\ndiffusion language model and demonstrate its efficacy. Experiments on diverse\nlanguage generation benchmarks show that A-CFG yields substantial improvements\nover standard CFG, achieving, for instance, a 3.9 point gain on GPQA. Our work\nhighlights the benefit of dynamically adapting guidance mechanisms to model\nuncertainty in iterative generation.",
      "upvotes": 0,
      "discussionId": "683919611186f2cbf3ed2292"
    },
    "publishedAt": "2025-05-26T12:40:22.000Z",
    "title": "Adaptive Classifier-Free Guidance via Dynamic Low-Confidence Masking",
    "summary": "Classifier-Free Guidance (CFG) significantly enhances controllability in\ngenerative models by interpolating conditional and unconditional predictions.\nHowever, standard CFG often employs a static unconditional input, which can be\nsuboptimal for iterative generation processes where model uncertainty varies\ndynamically. We introduce Adaptive Classifier-Free Guidance (A-CFG), a novel\nmethod that tailors the unconditional input by leveraging the model's\ninstantaneous predictive confidence. At each step of an iterative (masked)\ndiffusion language model, A-CFG identifies tokens in the currently generated\nsequence for which the model exhibits low confidence. These tokens are\ntemporarily re-masked to create a dynamic, localized unconditional input. This\nfocuses CFG's corrective influence precisely on areas of ambiguity, leading to\nmore effective guidance. We integrate A-CFG into a state-of-the-art masked\ndiffusion language model and demonstrate its efficacy. Experiments on diverse\nlanguage generation benchmarks show that A-CFG yields substantial improvements\nover standard CFG, achieving, for instance, a 3.9 point gain on GPQA. Our work\nhighlights the benefit of dynamically adapting guidance mechanisms to model\nuncertainty in iterative generation.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.20199.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64245f2c089d5fae56b4549a",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64245f2c089d5fae56b4549a/qUHFsL9Svwyj5BKpfMtaY.jpeg",
      "fullname": "Pengxiang Li",
      "name": "pengxiang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.18087",
      "authors": [
        {
          "_id": "68351b91037632c04211688d",
          "user": {
            "_id": "6837b9a63ed37b18326c7fff",
            "avatarUrl": "/avatars/7aaf5e1ff783aad45946c937ac36bdd8.svg",
            "isPro": false,
            "fullname": "Hyungyung Lee",
            "user": "ttumyche",
            "type": "user"
          },
          "name": "Hyungyung Lee",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-29T07:44:18.221Z",
          "hidden": false
        },
        {
          "_id": "68351b91037632c04211688e",
          "name": "Geon Choi",
          "hidden": false
        },
        {
          "_id": "68351b91037632c04211688f",
          "name": "Jung-Oh Lee",
          "hidden": false
        },
        {
          "_id": "68351b91037632c042116890",
          "name": "Hangyul Yoon",
          "hidden": false
        },
        {
          "_id": "68351b91037632c042116891",
          "name": "Hyuk Gi Hong",
          "hidden": false
        },
        {
          "_id": "68351b91037632c042116892",
          "name": "Edward Choi",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-23T16:44:21.000Z",
      "submittedOnDailyAt": "2025-05-30T00:18:09.180Z",
      "title": "CXReasonBench: A Benchmark for Evaluating Structured Diagnostic\n  Reasoning in Chest X-rays",
      "submittedOnDailyBy": {
        "_id": "6837b9a63ed37b18326c7fff",
        "avatarUrl": "/avatars/7aaf5e1ff783aad45946c937ac36bdd8.svg",
        "isPro": false,
        "fullname": "Hyungyung Lee",
        "user": "ttumyche",
        "type": "user"
      },
      "summary": "Recent progress in Large Vision-Language Models (LVLMs) has enabled promising\napplications in medical tasks, such as report generation and visual question\nanswering. However, existing benchmarks focus mainly on the final diagnostic\nanswer, offering limited insight into whether models engage in clinically\nmeaningful reasoning. To address this, we present CheXStruct and CXReasonBench,\na structured pipeline and benchmark built on the publicly available\nMIMIC-CXR-JPG dataset. CheXStruct automatically derives a sequence of\nintermediate reasoning steps directly from chest X-rays, such as segmenting\nanatomical regions, deriving anatomical landmarks and diagnostic measurements,\ncomputing diagnostic indices, and applying clinical thresholds. CXReasonBench\nleverages this pipeline to evaluate whether models can perform clinically valid\nreasoning steps and to what extent they can learn from structured guidance,\nenabling fine-grained and transparent assessment of diagnostic reasoning. The\nbenchmark comprises 18,988 QA pairs across 12 diagnostic tasks and 1,200 cases,\neach paired with up to 4 visual inputs, and supports multi-path, multi-stage\nevaluation including visual grounding via anatomical region selection and\ndiagnostic measurements. Even the strongest of 10 evaluated LVLMs struggle with\nstructured reasoning and generalization, often failing to link abstract\nknowledge with anatomically grounded visual interpretation. The code is\navailable at https://github.com/ttumyche/CXReasonBench",
      "upvotes": 0,
      "discussionId": "68351b97037632c0421169d0",
      "githubRepo": "https://github.com/ttumyche/CXReasonBench",
      "ai_summary": "CheXStruct and CXReasonBench evaluate Large Vision-Language Models in clinical diagnosis by assessing structured reasoning, visual grounding, and generalization using the MIMIC-CXR-JPG dataset.",
      "ai_keywords": [
        "Large Vision-Language Models",
        "LVLMs",
        "CheXStruct",
        "CXReasonBench",
        "MIMIC-CXR-JPG",
        "structured reasoning",
        "visual question answering",
        "medical tasks",
        "report generation",
        "anatomical regions",
        "diagnostic measurements",
        "diagnostic indices",
        "clinical thresholds",
        "visual grounding",
        "diagnostic reasoning",
        "multi-path",
        "multi-stage evaluation"
      ]
    },
    "publishedAt": "2025-05-23T12:44:21.000Z",
    "title": "CXReasonBench: A Benchmark for Evaluating Structured Diagnostic\n  Reasoning in Chest X-rays",
    "summary": "Recent progress in Large Vision-Language Models (LVLMs) has enabled promising\napplications in medical tasks, such as report generation and visual question\nanswering. However, existing benchmarks focus mainly on the final diagnostic\nanswer, offering limited insight into whether models engage in clinically\nmeaningful reasoning. To address this, we present CheXStruct and CXReasonBench,\na structured pipeline and benchmark built on the publicly available\nMIMIC-CXR-JPG dataset. CheXStruct automatically derives a sequence of\nintermediate reasoning steps directly from chest X-rays, such as segmenting\nanatomical regions, deriving anatomical landmarks and diagnostic measurements,\ncomputing diagnostic indices, and applying clinical thresholds. CXReasonBench\nleverages this pipeline to evaluate whether models can perform clinically valid\nreasoning steps and to what extent they can learn from structured guidance,\nenabling fine-grained and transparent assessment of diagnostic reasoning. The\nbenchmark comprises 18,988 QA pairs across 12 diagnostic tasks and 1,200 cases,\neach paired with up to 4 visual inputs, and supports multi-path, multi-stage\nevaluation including visual grounding via anatomical region selection and\ndiagnostic measurements. Even the strongest of 10 evaluated LVLMs struggle with\nstructured reasoning and generalization, often failing to link abstract\nknowledge with anatomically grounded visual interpretation. The code is\navailable at https://github.com/ttumyche/CXReasonBench",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.18087.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6837b9a63ed37b18326c7fff",
      "avatarUrl": "/avatars/7aaf5e1ff783aad45946c937ac36bdd8.svg",
      "fullname": "Hyungyung Lee",
      "name": "ttumyche",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  }
]