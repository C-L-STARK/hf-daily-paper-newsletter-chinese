[
  {
    "paper": {
      "id": "2602.17270",
      "authors": [
        {
          "_id": "6997ce527a658569d5a10165",
          "name": "Jonathan Heek",
          "hidden": false
        },
        {
          "_id": "6997ce527a658569d5a10166",
          "name": "Emiel Hoogeboom",
          "hidden": false
        },
        {
          "_id": "6997ce527a658569d5a10167",
          "name": "Thomas Mensink",
          "hidden": false
        },
        {
          "_id": "6997ce527a658569d5a10168",
          "name": "Tim Salimans",
          "hidden": false
        }
      ],
      "publishedAt": "2026-02-19T11:18:12.000Z",
      "submittedOnDailyAt": "2026-02-20T00:30:40.692Z",
      "title": "Unified Latents (UL): How to train your latents",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "We present Unified Latents (UL), a framework for learning latent representations that are jointly regularized by a diffusion prior and decoded by a diffusion model. By linking the encoder's output noise to the prior's minimum noise level, we obtain a simple training objective that provides a tight upper bound on the latent bitrate. On ImageNet-512, our approach achieves competitive FID of 1.4, with high reconstruction quality (PSNR) while requiring fewer training FLOPs than models trained on Stable Diffusion latents. On Kinetics-600, we set a new state-of-the-art FVD of 1.3.",
      "upvotes": 2,
      "discussionId": "6997ce527a658569d5a10169",
      "ai_summary": "Unified Latents framework learns joint latent representations using diffusion prior regularization and diffusion model decoding, achieving competitive FID scores with reduced training compute.",
      "ai_keywords": [
        "diffusion prior",
        "diffusion model",
        "latent representations",
        "training objective",
        "latent bitrate",
        "FID",
        "PSNR",
        "FLOPs",
        "FVD"
      ],
      "organization": {
        "_id": "5e6aca39878b8b2bf9806447",
        "name": "google",
        "fullname": "Google",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/WtA3YYitedOr9n02eHfJe.png"
      }
    },
    "publishedAt": "2026-02-19T06:18:12.000Z",
    "title": "Unified Latents (UL): How to train your latents",
    "summary": "We present Unified Latents (UL), a framework for learning latent representations that are jointly regularized by a diffusion prior and decoded by a diffusion model. By linking the encoder's output noise to the prior's minimum noise level, we obtain a simple training objective that provides a tight upper bound on the latent bitrate. On ImageNet-512, our approach achieves competitive FID of 1.4, with high reconstruction quality (PSNR) while requiring fewer training FLOPs than models trained on Stable Diffusion latents. On Kinetics-600, we set a new state-of-the-art FVD of 1.3.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.17270.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 235,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "5e6aca39878b8b2bf9806447",
      "name": "google",
      "fullname": "Google",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/WtA3YYitedOr9n02eHfJe.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2602.17365",
      "authors": [
        {
          "_id": "6997d0587a658569d5a10173",
          "name": "Yiming Guan",
          "hidden": false
        },
        {
          "_id": "6997d0587a658569d5a10174",
          "name": "Rui Yu",
          "hidden": false
        },
        {
          "_id": "6997d0587a658569d5a10175",
          "name": "John Zhang",
          "hidden": false
        },
        {
          "_id": "6997d0587a658569d5a10176",
          "name": "Lu Wang",
          "hidden": false
        },
        {
          "_id": "6997d0587a658569d5a10177",
          "name": "Chaoyun Zhang",
          "hidden": false
        },
        {
          "_id": "6997d0587a658569d5a10178",
          "name": "Liqun Li",
          "hidden": false
        },
        {
          "_id": "6997d0587a658569d5a10179",
          "name": "Bo Qiao",
          "hidden": false
        },
        {
          "_id": "6997d0587a658569d5a1017a",
          "name": "Si Qin",
          "hidden": false
        },
        {
          "_id": "6997d0587a658569d5a1017b",
          "name": "He Huang",
          "hidden": false
        },
        {
          "_id": "6997d0587a658569d5a1017c",
          "name": "Fangkai Yang",
          "hidden": false
        },
        {
          "_id": "6997d0587a658569d5a1017d",
          "name": "Pu Zhao",
          "hidden": false
        },
        {
          "_id": "6997d0587a658569d5a1017e",
          "name": "Lukas Wutschitz",
          "hidden": false
        },
        {
          "_id": "6997d0587a658569d5a1017f",
          "name": "Samuel Kessler",
          "hidden": false
        },
        {
          "_id": "6997d0587a658569d5a10180",
          "name": "Huseyin A Inan",
          "hidden": false
        },
        {
          "_id": "6997d0587a658569d5a10181",
          "name": "Robert Sim",
          "hidden": false
        },
        {
          "_id": "6997d0587a658569d5a10182",
          "name": "Saravan Rajmohan",
          "hidden": false
        },
        {
          "_id": "6997d0587a658569d5a10183",
          "name": "Qingwei Lin",
          "hidden": false
        },
        {
          "_id": "6997d0587a658569d5a10184",
          "name": "Dongmei Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2026-02-19T13:48:29.000Z",
      "submittedOnDailyAt": "2026-02-20T00:39:22.977Z",
      "title": "Computer-Using World Model",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Agents operating in complex software environments benefit from reasoning about the consequences of their actions, as even a single incorrect user interface (UI) operation can derail long, artifact-preserving workflows. This challenge is particularly acute for computer-using scenarios, where real execution does not support counterfactual exploration, making large-scale trial-and-error learning and planning impractical despite the environment being fully digital and deterministic. We introduce the Computer-Using World Model (CUWM), a world model for desktop software that predicts the next UI state given the current state and a candidate action. CUWM adopts a two-stage factorization of UI dynamics: it first predicts a textual description of agent-relevant state changes, and then realizes these changes visually to synthesize the next screenshot. CUWM is trained on offline UI transitions collected from agents interacting with real Microsoft Office applications, and further refined with a lightweight reinforcement learning stage that aligns textual transition predictions with the structural requirements of computer-using environments. We evaluate CUWM via test-time action search, where a frozen agent uses the world model to simulate and compare candidate actions before execution. Across a range of Office tasks, world-model-guided test-time scaling improves decision quality and execution robustness.",
      "upvotes": 0,
      "discussionId": "6997d0587a658569d5a10185",
      "ai_summary": "A world model for desktop software that predicts UI state changes through textual description followed by visual synthesis, improving decision quality and execution robustness in computer-using tasks.",
      "ai_keywords": [
        "world model",
        "user interface",
        "UI state",
        "action search",
        "reinforcement learning",
        "test-time scaling",
        "computer-using environments",
        "desktop software",
        "textual description",
        "visual synthesis"
      ]
    },
    "publishedAt": "2026-02-19T08:48:29.000Z",
    "title": "Computer-Using World Model",
    "summary": "Agents operating in complex software environments benefit from reasoning about the consequences of their actions, as even a single incorrect user interface (UI) operation can derail long, artifact-preserving workflows. This challenge is particularly acute for computer-using scenarios, where real execution does not support counterfactual exploration, making large-scale trial-and-error learning and planning impractical despite the environment being fully digital and deterministic. We introduce the Computer-Using World Model (CUWM), a world model for desktop software that predicts the next UI state given the current state and a candidate action. CUWM adopts a two-stage factorization of UI dynamics: it first predicts a textual description of agent-relevant state changes, and then realizes these changes visually to synthesize the next screenshot. CUWM is trained on offline UI transitions collected from agents interacting with real Microsoft Office applications, and further refined with a lightweight reinforcement learning stage that aligns textual transition predictions with the structural requirements of computer-using environments. We evaluate CUWM via test-time action search, where a frozen agent uses the world model to simulate and compare candidate actions before execution. Across a range of Office tasks, world-model-guided test-time scaling improves decision quality and execution robustness.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.17365.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 235,
      "isUserFollowing": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2602.17363",
      "authors": [
        {
          "_id": "6997d08a7a658569d5a10187",
          "name": "Gabriel Mongaras",
          "hidden": false
        },
        {
          "_id": "6997d08a7a658569d5a10188",
          "name": "Eric C. Larson",
          "hidden": false
        }
      ],
      "publishedAt": "2026-02-19T13:45:23.000Z",
      "submittedOnDailyAt": "2026-02-20T00:41:59.598Z",
      "title": "2Mamba2Furious: Linear in Complexity, Competitive in Accuracy",
      "submittedOnDailyBy": {
        "_id": "62a1280b88bfb47fc40fe75b",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62a1280b88bfb47fc40fe75b/u6teJWcB6BWdD04G7g6uy.png",
        "isPro": true,
        "fullname": "Gabriel Mongaras",
        "user": "gmongaras",
        "type": "user"
      },
      "summary": "Linear attention transformers have become a strong alternative to softmax attention due to their efficiency. However, linear attention tends to be less expressive and results in reduced accuracy compared to softmax attention. To bridge the accuracy gap between softmax attention and linear attention, we manipulate Mamba-2, a very strong linear attention variant. We first simplify Mamba-2 down to its most fundamental and important components, evaluating which specific choices make it most accurate. From this simplified Mamba variant (Mamba-2S), we improve the A-mask and increase the order of the hidden state, resulting in a method, which we call 2Mamba, that is nearly as accurate as softmax attention, yet much more memory efficient for long context lengths. We also investigate elements to Mamba-2 that help surpass softmax attention accuracy. Code is provided for all our experiments",
      "upvotes": 0,
      "discussionId": "6997d08a7a658569d5a10189",
      "githubRepo": "https://github.com/gmongaras/2Mamba2Furious",
      "githubRepoAddedBy": "user",
      "ai_summary": "Researchers enhance linear attention by simplifying Mamba-2 and improving its architectural components to achieve near-softmax accuracy while maintaining memory efficiency for long sequences.",
      "ai_keywords": [
        "linear attention",
        "softmax attention",
        "Mamba-2",
        "hidden state",
        "A-mask",
        "2Mamba"
      ],
      "githubStars": 0
    },
    "publishedAt": "2026-02-19T08:45:23.000Z",
    "title": "2Mamba2Furious: Linear in Complexity, Competitive in Accuracy",
    "summary": "Linear attention transformers have become a strong alternative to softmax attention due to their efficiency. However, linear attention tends to be less expressive and results in reduced accuracy compared to softmax attention. To bridge the accuracy gap between softmax attention and linear attention, we manipulate Mamba-2, a very strong linear attention variant. We first simplify Mamba-2 down to its most fundamental and important components, evaluating which specific choices make it most accurate. From this simplified Mamba variant (Mamba-2S), we improve the A-mask and increase the order of the hidden state, resulting in a method, which we call 2Mamba, that is nearly as accurate as softmax attention, yet much more memory efficient for long context lengths. We also investigate elements to Mamba-2 that help surpass softmax attention accuracy. Code is provided for all our experiments",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.17363.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62a1280b88bfb47fc40fe75b",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62a1280b88bfb47fc40fe75b/u6teJWcB6BWdD04G7g6uy.png",
      "fullname": "Gabriel Mongaras",
      "name": "gmongaras",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 10,
      "isUserFollowing": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2602.17004",
      "authors": [
        {
          "_id": "6997cd777a658569d5a10149",
          "name": "Varun Singh",
          "hidden": false
        },
        {
          "_id": "6997cd777a658569d5a1014a",
          "name": "Lucas Krauss",
          "hidden": false
        },
        {
          "_id": "6997cd777a658569d5a1014b",
          "name": "Sami Jaghouar",
          "hidden": false
        },
        {
          "_id": "6997cd777a658569d5a1014c",
          "name": "Matej Sirovatka",
          "hidden": false
        },
        {
          "_id": "6997cd777a658569d5a1014d",
          "name": "Charles Goddard",
          "hidden": false
        },
        {
          "_id": "6997cd777a658569d5a1014e",
          "name": "Fares Obied",
          "hidden": false
        },
        {
          "_id": "6997cd777a658569d5a1014f",
          "name": "Jack Min Ong",
          "hidden": false
        },
        {
          "_id": "6997cd777a658569d5a10150",
          "name": "Jannik Straube",
          "hidden": false
        },
        {
          "_id": "6997cd777a658569d5a10151",
          "name": "Fern",
          "hidden": false
        },
        {
          "_id": "6997cd777a658569d5a10152",
          "name": "Aria Harley",
          "hidden": false
        },
        {
          "_id": "6997cd777a658569d5a10153",
          "name": "Conner Stewart",
          "hidden": false
        },
        {
          "_id": "6997cd777a658569d5a10154",
          "name": "Colin Kealty",
          "hidden": false
        },
        {
          "_id": "6997cd777a658569d5a10155",
          "name": "Maziyar Panahi",
          "hidden": false
        },
        {
          "_id": "6997cd777a658569d5a10156",
          "name": "Simon Kirsten",
          "hidden": false
        },
        {
          "_id": "6997cd777a658569d5a10157",
          "name": "Anushka Deshpande",
          "hidden": false
        },
        {
          "_id": "6997cd777a658569d5a10158",
          "name": "Anneketh Vij",
          "hidden": false
        },
        {
          "_id": "6997cd777a658569d5a10159",
          "name": "Arthur Bresnu",
          "hidden": false
        },
        {
          "_id": "6997cd777a658569d5a1015a",
          "name": "Pranav Veldurthi",
          "hidden": false
        },
        {
          "_id": "6997cd777a658569d5a1015b",
          "name": "Raghav Ravishankar",
          "hidden": false
        },
        {
          "_id": "6997cd777a658569d5a1015c",
          "name": "Hardik Bishnoi",
          "hidden": false
        },
        {
          "_id": "6997cd777a658569d5a1015d",
          "name": "DatologyAI Team",
          "hidden": false
        },
        {
          "_id": "6997cd777a658569d5a1015e",
          "name": "Arcee AI Team",
          "hidden": false
        },
        {
          "_id": "6997cd777a658569d5a1015f",
          "name": "Prime Intellect Team",
          "hidden": false
        },
        {
          "_id": "6997cd777a658569d5a10160",
          "name": "Mark McQuade",
          "hidden": false
        },
        {
          "_id": "6997cd777a658569d5a10161",
          "name": "Johannes Hagemann",
          "hidden": false
        },
        {
          "_id": "6997cd777a658569d5a10162",
          "name": "Lucas Atkins",
          "hidden": false
        }
      ],
      "publishedAt": "2026-02-19T01:58:50.000Z",
      "submittedOnDailyAt": "2026-02-20T00:26:56.558Z",
      "title": "Arcee Trinity Large Technical Report",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "We present the technical report for Arcee Trinity Large, a sparse Mixture-of-Experts model with 400B total parameters and 13B activated per token. Additionally, we report on Trinity Nano and Trinity Mini, with Trinity Nano having 6B total parameters with 1B activated per token, Trinity Mini having 26B total parameters with 3B activated per token. The models' modern architecture includes interleaved local and global attention, gated attention, depth-scaled sandwich norm, and sigmoid routing for Mixture-of-Experts. For Trinity Large, we also introduce a new MoE load balancing strategy titled Soft-clamped Momentum Expert Bias Updates (SMEBU). We train the models using the Muon optimizer. All three models completed training with zero loss spikes. Trinity Nano and Trinity Mini were pre-trained on 10 trillion tokens, and Trinity Large was pre-trained on 17 trillion tokens. The model checkpoints are available at https://huggingface.co/arcee-ai.",
      "upvotes": 0,
      "discussionId": "6997cd777a658569d5a10163",
      "ai_summary": "Arcee Trinity models are sparse Mixture-of-Experts architectures with varying parameter counts and activation patterns, utilizing advanced attention mechanisms and training optimizations.",
      "ai_keywords": [
        "Mixture-of-Experts",
        "sparse Mixture-of-Experts",
        "attention",
        "gated attention",
        "depth-scaled sandwich norm",
        "sigmoid routing",
        "Muon optimizer",
        "MoE load balancing",
        "Soft-clamped Momentum Expert Bias Updates"
      ]
    },
    "publishedAt": "2026-02-18T20:58:50.000Z",
    "title": "Arcee Trinity Large Technical Report",
    "summary": "We present the technical report for Arcee Trinity Large, a sparse Mixture-of-Experts model with 400B total parameters and 13B activated per token. Additionally, we report on Trinity Nano and Trinity Mini, with Trinity Nano having 6B total parameters with 1B activated per token, Trinity Mini having 26B total parameters with 3B activated per token. The models' modern architecture includes interleaved local and global attention, gated attention, depth-scaled sandwich norm, and sigmoid routing for Mixture-of-Experts. For Trinity Large, we also introduce a new MoE load balancing strategy titled Soft-clamped Momentum Expert Bias Updates (SMEBU). We train the models using the Muon optimizer. All three models completed training with zero loss spikes. Trinity Nano and Trinity Mini were pre-trained on 10 trillion tokens, and Trinity Large was pre-trained on 17 trillion tokens. The model checkpoints are available at https://huggingface.co/arcee-ai.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.17004.png",
    "numComments": 0,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 235,
      "isUserFollowing": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2602.16968",
      "authors": [
        {
          "_id": "6997ceac7a658569d5a1016b",
          "name": "Dahye Kim",
          "hidden": false
        },
        {
          "_id": "6997ceac7a658569d5a1016c",
          "name": "Deepti Ghadiyaram",
          "hidden": false
        },
        {
          "_id": "6997ceac7a658569d5a1016d",
          "name": "Raghudeep Gadde",
          "hidden": false
        }
      ],
      "publishedAt": "2026-02-19T00:15:20.000Z",
      "submittedOnDailyAt": "2026-02-20T00:32:15.811Z",
      "title": "DDiT: Dynamic Patch Scheduling for Efficient Diffusion Transformers",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Diffusion Transformers (DiTs) have achieved state-of-the-art performance in image and video generation, but their success comes at the cost of heavy computation. This inefficiency is largely due to the fixed tokenization process, which uses constant-sized patches throughout the entire denoising phase, regardless of the content's complexity. We propose dynamic tokenization, an efficient test-time strategy that varies patch sizes based on content complexity and the denoising timestep. Our key insight is that early timesteps only require coarser patches to model global structure, while later iterations demand finer (smaller-sized) patches to refine local details. During inference, our method dynamically reallocates patch sizes across denoising steps for image and video generation and substantially reduces cost while preserving perceptual generation quality. Extensive experiments demonstrate the effectiveness of our approach: it achieves up to 3.52times and 3.2times speedup on FLUX-1.Dev and Wan 2.1, respectively, without compromising the generation quality and prompt adherence.",
      "upvotes": 0,
      "discussionId": "6997ceac7a658569d5a1016e",
      "ai_summary": "Dynamic tokenization improves diffusion transformer efficiency by adjusting patch sizes based on content complexity and denoising timestep, achieving significant speedup without quality loss.",
      "ai_keywords": [
        "diffusion transformers",
        "tokenization",
        "denoising phase",
        "patch sizes",
        "content complexity",
        "denoising timestep",
        "inference",
        "computational efficiency",
        "perceptual quality",
        "speedup"
      ]
    },
    "publishedAt": "2026-02-18T19:15:20.000Z",
    "title": "DDiT: Dynamic Patch Scheduling for Efficient Diffusion Transformers",
    "summary": "Diffusion Transformers (DiTs) have achieved state-of-the-art performance in image and video generation, but their success comes at the cost of heavy computation. This inefficiency is largely due to the fixed tokenization process, which uses constant-sized patches throughout the entire denoising phase, regardless of the content's complexity. We propose dynamic tokenization, an efficient test-time strategy that varies patch sizes based on content complexity and the denoising timestep. Our key insight is that early timesteps only require coarser patches to model global structure, while later iterations demand finer (smaller-sized) patches to refine local details. During inference, our method dynamically reallocates patch sizes across denoising steps for image and video generation and substantially reduces cost while preserving perceptual generation quality. Extensive experiments demonstrate the effectiveness of our approach: it achieves up to 3.52times and 3.2times speedup on FLUX-1.Dev and Wan 2.1, respectively, without compromising the generation quality and prompt adherence.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.16968.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 235,
      "isUserFollowing": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2602.16928",
      "authors": [
        {
          "_id": "6997d19a7a658569d5a1018b",
          "name": "Zun Li",
          "hidden": false
        },
        {
          "_id": "6997d19a7a658569d5a1018c",
          "name": "John Schultz",
          "hidden": false
        },
        {
          "_id": "6997d19a7a658569d5a1018d",
          "name": "Daniel Hennes",
          "hidden": false
        },
        {
          "_id": "6997d19a7a658569d5a1018e",
          "name": "Marc Lanctot",
          "hidden": false
        }
      ],
      "publishedAt": "2026-02-18T22:41:00.000Z",
      "submittedOnDailyAt": "2026-02-20T00:44:41.311Z",
      "title": "Discovering Multiagent Learning Algorithms with Large Language Models",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Much of the advancement of Multi-Agent Reinforcement Learning (MARL) in imperfect-information games has historically depended on manual iterative refinement of baselines. While foundational families like Counterfactual Regret Minimization (CFR) and Policy Space Response Oracles (PSRO) rest on solid theoretical ground, the design of their most effective variants often relies on human intuition to navigate a vast algorithmic design space. In this work, we propose the use of AlphaEvolve, an evolutionary coding agent powered by large language models, to automatically discover new multiagent learning algorithms. We demonstrate the generality of this framework by evolving novel variants for two distinct paradigms of game-theoretic learning. First, in the domain of iterative regret minimization, we evolve the logic governing regret accumulation and policy derivation, discovering a new algorithm, Volatility-Adaptive Discounted (VAD-)CFR. VAD-CFR employs novel, non-intuitive mechanisms-including volatility-sensitive discounting, consistency-enforced optimism, and a hard warm-start policy accumulation schedule-to outperform state-of-the-art baselines like Discounted Predictive CFR+. Second, in the regime of population based training algorithms, we evolve training-time and evaluation-time meta strategy solvers for PSRO, discovering a new variant, Smoothed Hybrid Optimistic Regret (SHOR-)PSRO. SHOR-PSRO introduces a hybrid meta-solver that linearly blends Optimistic Regret Matching with a smoothed, temperature-controlled distribution over best pure strategies. By dynamically annealing this blending factor and diversity bonuses during training, the algorithm automates the transition from population diversity to rigorous equilibrium finding, yielding superior empirical convergence compared to standard static meta-solvers.",
      "upvotes": 0,
      "discussionId": "6997d19a7a658569d5a1018f",
      "ai_summary": "AlphaEvolve, an evolutionary coding agent using large language models, automatically discovers new multiagent learning algorithms for imperfect-information games by evolving regret minimization and population-based training variants.",
      "ai_keywords": [
        "Multi-Agent Reinforcement Learning",
        "imperfect-information games",
        "Counterfactual Regret Minimization",
        "Policy Space Response Oracles",
        "evolutionary coding",
        "large language models",
        "regret accumulation",
        "policy derivation",
        "Volatility-Adaptive Discounted CFR",
        "discounted predictive CFR",
        "population based training",
        "meta strategy solvers",
        "Optimistic Regret Matching",
        "smoothed distribution",
        "equilibrium finding"
      ],
      "organization": {
        "_id": "5e6aca39878b8b2bf9806447",
        "name": "google",
        "fullname": "Google",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/WtA3YYitedOr9n02eHfJe.png"
      }
    },
    "publishedAt": "2026-02-18T17:41:00.000Z",
    "title": "Discovering Multiagent Learning Algorithms with Large Language Models",
    "summary": "Much of the advancement of Multi-Agent Reinforcement Learning (MARL) in imperfect-information games has historically depended on manual iterative refinement of baselines. While foundational families like Counterfactual Regret Minimization (CFR) and Policy Space Response Oracles (PSRO) rest on solid theoretical ground, the design of their most effective variants often relies on human intuition to navigate a vast algorithmic design space. In this work, we propose the use of AlphaEvolve, an evolutionary coding agent powered by large language models, to automatically discover new multiagent learning algorithms. We demonstrate the generality of this framework by evolving novel variants for two distinct paradigms of game-theoretic learning. First, in the domain of iterative regret minimization, we evolve the logic governing regret accumulation and policy derivation, discovering a new algorithm, Volatility-Adaptive Discounted (VAD-)CFR. VAD-CFR employs novel, non-intuitive mechanisms-including volatility-sensitive discounting, consistency-enforced optimism, and a hard warm-start policy accumulation schedule-to outperform state-of-the-art baselines like Discounted Predictive CFR+. Second, in the regime of population based training algorithms, we evolve training-time and evaluation-time meta strategy solvers for PSRO, discovering a new variant, Smoothed Hybrid Optimistic Regret (SHOR-)PSRO. SHOR-PSRO introduces a hybrid meta-solver that linearly blends Optimistic Regret Matching with a smoothed, temperature-controlled distribution over best pure strategies. By dynamically annealing this blending factor and diversity bonuses during training, the algorithm automates the transition from population diversity to rigorous equilibrium finding, yielding superior empirical convergence compared to standard static meta-solvers.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.16928.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 235,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "5e6aca39878b8b2bf9806447",
      "name": "google",
      "fullname": "Google",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/WtA3YYitedOr9n02eHfJe.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2602.13579",
      "authors": [
        {
          "_id": "6996a91e1268a6b79e0d033f",
          "user": {
            "_id": "626c8c89a317717549e4e5aa",
            "avatarUrl": "/avatars/b8bf54005b7b2a8fa0d10ed4557b675d.svg",
            "isPro": false,
            "fullname": "youngsun wi",
            "user": "youngw",
            "type": "user"
          },
          "name": "Youngsun Wi",
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-02-19T09:51:39.647Z",
          "hidden": false
        },
        {
          "_id": "6996a91e1268a6b79e0d0340",
          "name": "Jessica Yin",
          "hidden": false
        },
        {
          "_id": "6996a91e1268a6b79e0d0341",
          "name": "Elvis Xiang",
          "hidden": false
        },
        {
          "_id": "6996a91e1268a6b79e0d0342",
          "name": "Akash Sharma",
          "hidden": false
        },
        {
          "_id": "6996a91e1268a6b79e0d0343",
          "name": "Jitendra Malik",
          "hidden": false
        },
        {
          "_id": "6996a91e1268a6b79e0d0344",
          "name": "Mustafa Mukadam",
          "hidden": false
        },
        {
          "_id": "6996a91e1268a6b79e0d0345",
          "name": "Nima Fazeli",
          "hidden": false
        },
        {
          "_id": "6996a91e1268a6b79e0d0346",
          "name": "Tess Hellebrekers",
          "hidden": false
        }
      ],
      "publishedAt": "2026-02-14T03:31:32.000Z",
      "submittedOnDailyAt": "2026-02-20T00:07:09.361Z",
      "title": "TactAlign: Human-to-Robot Policy Transfer via Tactile Alignment",
      "submittedOnDailyBy": {
        "_id": "626c8c89a317717549e4e5aa",
        "avatarUrl": "/avatars/b8bf54005b7b2a8fa0d10ed4557b675d.svg",
        "isPro": false,
        "fullname": "youngsun wi",
        "user": "youngw",
        "type": "user"
      },
      "summary": "Human demonstrations collected by wearable devices (e.g., tactile gloves) provide fast and dexterous supervision for policy learning, and are guided by rich, natural tactile feedback. However, a key challenge is how to transfer human-collected tactile signals to robots despite the differences in sensing modalities and embodiment. Existing human-to-robot (H2R) approaches that incorporate touch often assume identical tactile sensors, require paired data, and involve little to no embodiment gap between human demonstrator and the robots, limiting scalability and generality. We propose TactAlign, a cross-embodiment tactile alignment method that transfers human-collected tactile signals to a robot with different embodiment. TactAlign transforms human and robot tactile observations into a shared latent representation using a rectified flow, without paired datasets, manual labels, or privileged information. Our method enables low-cost latent transport guided by hand-object interaction-derived pseudo-pairs. We demonstrate that TactAlign improves H2R policy transfer across multiple contact-rich tasks (pivoting, insertion, lid closing), generalizes to unseen objects and tasks with human data (less than 5 minutes), and enables zero-shot H2R transfer on a highly dexterous tasks (light bulb screwing).",
      "upvotes": 0,
      "discussionId": "6996a91e1268a6b79e0d0347",
      "projectPage": "https://yswi.github.io/tactalign/",
      "ai_summary": "TactAlign enables transfer of human tactile demonstrations to robots with different embodiments through cross-embodiment tactile alignment without requiring paired data or manual labels.",
      "ai_keywords": [
        "human-to-robot transfer",
        "tactile alignment",
        "rectified flow",
        "latent representation",
        "pseudo-pairs",
        "contact-rich tasks",
        "zero-shot transfer"
      ],
      "organization": {
        "_id": "63df4874e742e86dc925d67c",
        "name": "umich",
        "fullname": "University of Michigan",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1675577443573-63df328115266dd945fc01f4.png"
      }
    },
    "publishedAt": "2026-02-13T22:31:32.000Z",
    "title": "TactAlign: Human-to-Robot Policy Transfer via Tactile Alignment",
    "summary": "Human demonstrations collected by wearable devices (e.g., tactile gloves) provide fast and dexterous supervision for policy learning, and are guided by rich, natural tactile feedback. However, a key challenge is how to transfer human-collected tactile signals to robots despite the differences in sensing modalities and embodiment. Existing human-to-robot (H2R) approaches that incorporate touch often assume identical tactile sensors, require paired data, and involve little to no embodiment gap between human demonstrator and the robots, limiting scalability and generality. We propose TactAlign, a cross-embodiment tactile alignment method that transfers human-collected tactile signals to a robot with different embodiment. TactAlign transforms human and robot tactile observations into a shared latent representation using a rectified flow, without paired datasets, manual labels, or privileged information. Our method enables low-cost latent transport guided by hand-object interaction-derived pseudo-pairs. We demonstrate that TactAlign improves H2R policy transfer across multiple contact-rich tasks (pivoting, insertion, lid closing), generalizes to unseen objects and tasks with human data (less than 5 minutes), and enables zero-shot H2R transfer on a highly dexterous tasks (light bulb screwing).",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.13579.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "626c8c89a317717549e4e5aa",
      "avatarUrl": "/avatars/b8bf54005b7b2a8fa0d10ed4557b675d.svg",
      "fullname": "youngsun wi",
      "name": "youngw",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "63df4874e742e86dc925d67c",
      "name": "umich",
      "fullname": "University of Michigan",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1675577443573-63df328115266dd945fc01f4.png"
    },
    "isAuthorParticipating": true
  }
]