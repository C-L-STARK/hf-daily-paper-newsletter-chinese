[
  {
    "paper": {
      "id": "2509.14232",
      "authors": [
        {
          "_id": "68cb67a45a7803ff3be42d1e",
          "name": "Zhaokai Wang",
          "hidden": false
        },
        {
          "_id": "68cb67a45a7803ff3be42d1f",
          "name": "Penghao Yin",
          "hidden": false
        },
        {
          "_id": "68cb67a45a7803ff3be42d20",
          "name": "Xiangyu Zhao",
          "hidden": false
        },
        {
          "_id": "68cb67a45a7803ff3be42d21",
          "name": "Changyao Tian",
          "hidden": false
        },
        {
          "_id": "68cb67a45a7803ff3be42d22",
          "name": "Yu Qiao",
          "hidden": false
        },
        {
          "_id": "68cb67a45a7803ff3be42d23",
          "name": "Wenhai Wang",
          "hidden": false
        },
        {
          "_id": "68cb67a45a7803ff3be42d24",
          "name": "Jifeng Dai",
          "hidden": false
        },
        {
          "_id": "68cb67a45a7803ff3be42d25",
          "name": "Gen Luo",
          "hidden": false
        }
      ],
      "publishedAt": "2025-09-17T17:59:14.000Z",
      "submittedOnDailyAt": "2025-09-18T00:30:14.725Z",
      "title": "GenExam: A Multidisciplinary Text-to-Image Exam",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Exams are a fundamental test of expert-level intelligence and require\nintegrated understanding, reasoning, and generation. Existing exam-style\nbenchmarks mainly focus on understanding and reasoning tasks, and current\ngeneration benchmarks emphasize the illustration of world knowledge and visual\nconcepts, neglecting the evaluation of rigorous drawing exams. We introduce\nGenExam, the first benchmark for multidisciplinary text-to-image exams,\nfeaturing 1,000 samples across 10 subjects with exam-style prompts organized\nunder a four-level taxonomy. Each problem is equipped with ground-truth images\nand fine-grained scoring points to enable a precise evaluation of semantic\ncorrectness and visual plausibility. Experiments show that even\nstate-of-the-art models such as GPT-Image-1 and Gemini-2.5-Flash-Image achieve\nless than 15% strict scores, and most models yield almost 0%, suggesting the\ngreat challenge of our benchmark. By framing image generation as an exam,\nGenExam offers a rigorous assessment of models' ability to integrate knowledge,\nreasoning, and generation, providing insights on the path to general AGI.",
      "upvotes": 1,
      "discussionId": "68cb67a45a7803ff3be42d26",
      "githubRepo": "https://github.com/OpenGVLab/GenExam",
      "ai_summary": "GenExam is a benchmark for evaluating text-to-image generation in exam-style settings across multiple disciplines, highlighting the challenges in integrating knowledge, reasoning, and generation.",
      "ai_keywords": [
        "text-to-image",
        "GenExam",
        "multidisciplinary",
        "exam-style prompts",
        "ground-truth images",
        "fine-grained scoring",
        "semantic correctness",
        "visual plausibility",
        "GPT-Image-1",
        "Gemini-2.5-Flash-Image",
        "general AGI"
      ],
      "githubStars": 5
    },
    "publishedAt": "2025-09-17T13:59:14.000Z",
    "title": "GenExam: A Multidisciplinary Text-to-Image Exam",
    "summary": "Exams are a fundamental test of expert-level intelligence and require\nintegrated understanding, reasoning, and generation. Existing exam-style\nbenchmarks mainly focus on understanding and reasoning tasks, and current\ngeneration benchmarks emphasize the illustration of world knowledge and visual\nconcepts, neglecting the evaluation of rigorous drawing exams. We introduce\nGenExam, the first benchmark for multidisciplinary text-to-image exams,\nfeaturing 1,000 samples across 10 subjects with exam-style prompts organized\nunder a four-level taxonomy. Each problem is equipped with ground-truth images\nand fine-grained scoring points to enable a precise evaluation of semantic\ncorrectness and visual plausibility. Experiments show that even\nstate-of-the-art models such as GPT-Image-1 and Gemini-2.5-Flash-Image achieve\nless than 15% strict scores, and most models yield almost 0%, suggesting the\ngreat challenge of our benchmark. By framing image generation as an exam,\nGenExam offers a rigorous assessment of models' ability to integrate knowledge,\nreasoning, and generation, providing insights on the path to general AGI.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.14232.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 106
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2509.14142",
      "authors": [
        {
          "_id": "68cb67d55a7803ff3be42d28",
          "name": "Peng Xu",
          "hidden": false
        },
        {
          "_id": "68cb67d55a7803ff3be42d29",
          "name": "Shengwu Xiong",
          "hidden": false
        },
        {
          "_id": "68cb67d55a7803ff3be42d2a",
          "name": "Jiajun Zhang",
          "hidden": false
        },
        {
          "_id": "68cb67d55a7803ff3be42d2b",
          "name": "Yaxiong Chen",
          "hidden": false
        },
        {
          "_id": "68cb67d55a7803ff3be42d2c",
          "name": "Bowen Zhou",
          "hidden": false
        },
        {
          "_id": "68cb67d55a7803ff3be42d2d",
          "name": "Chen Change Loy",
          "hidden": false
        },
        {
          "_id": "68cb67d55a7803ff3be42d2e",
          "name": "David A. Clifton",
          "hidden": false
        },
        {
          "_id": "68cb67d55a7803ff3be42d2f",
          "name": "Kyoung Mu Lee",
          "hidden": false
        },
        {
          "_id": "68cb67d55a7803ff3be42d30",
          "name": "Luc Van Gool",
          "hidden": false
        },
        {
          "_id": "68cb67d55a7803ff3be42d31",
          "name": "Ruiming He",
          "hidden": false
        },
        {
          "_id": "68cb67d55a7803ff3be42d32",
          "name": "Ruilin Yao",
          "hidden": false
        },
        {
          "_id": "68cb67d55a7803ff3be42d33",
          "name": "Xinwei Long",
          "hidden": false
        },
        {
          "_id": "68cb67d55a7803ff3be42d34",
          "name": "Jirui Huang",
          "hidden": false
        },
        {
          "_id": "68cb67d55a7803ff3be42d35",
          "name": "Kai Tian",
          "hidden": false
        },
        {
          "_id": "68cb67d55a7803ff3be42d36",
          "name": "Sa Yang",
          "hidden": false
        },
        {
          "_id": "68cb67d55a7803ff3be42d37",
          "name": "Yihua Shao",
          "hidden": false
        },
        {
          "_id": "68cb67d55a7803ff3be42d38",
          "name": "Jin Feng",
          "hidden": false
        },
        {
          "_id": "68cb67d55a7803ff3be42d39",
          "name": "Yue Zhong",
          "hidden": false
        },
        {
          "_id": "68cb67d55a7803ff3be42d3a",
          "name": "Jiakai Zhou",
          "hidden": false
        },
        {
          "_id": "68cb67d55a7803ff3be42d3b",
          "name": "Cheng Tang",
          "hidden": false
        },
        {
          "_id": "68cb67d55a7803ff3be42d3c",
          "name": "Tianyu Zou",
          "hidden": false
        },
        {
          "_id": "68cb67d55a7803ff3be42d3d",
          "name": "Yifang Zhang",
          "hidden": false
        },
        {
          "_id": "68cb67d55a7803ff3be42d3e",
          "name": "Junming Liang",
          "hidden": false
        },
        {
          "_id": "68cb67d55a7803ff3be42d3f",
          "name": "Guoyou Li",
          "hidden": false
        },
        {
          "_id": "68cb67d55a7803ff3be42d40",
          "name": "Zhaoxiang Wang",
          "hidden": false
        },
        {
          "_id": "68cb67d55a7803ff3be42d41",
          "name": "Qiang Zhou",
          "hidden": false
        },
        {
          "_id": "68cb67d55a7803ff3be42d42",
          "name": "Yichen Zhao",
          "hidden": false
        },
        {
          "_id": "68cb67d55a7803ff3be42d43",
          "name": "Shili Xiong",
          "hidden": false
        },
        {
          "_id": "68cb67d55a7803ff3be42d44",
          "name": "Hyeongjin Nam",
          "hidden": false
        },
        {
          "_id": "68cb67d55a7803ff3be42d45",
          "name": "Jaerin Lee",
          "hidden": false
        },
        {
          "_id": "68cb67d55a7803ff3be42d46",
          "name": "Jaeyoung Chung",
          "hidden": false
        },
        {
          "_id": "68cb67d55a7803ff3be42d47",
          "name": "JoonKyu Park",
          "hidden": false
        },
        {
          "_id": "68cb67d55a7803ff3be42d48",
          "name": "Junghun Oh",
          "hidden": false
        },
        {
          "_id": "68cb67d55a7803ff3be42d49",
          "name": "Kanggeon Lee",
          "hidden": false
        },
        {
          "_id": "68cb67d55a7803ff3be42d4a",
          "name": "Wooseok Lee",
          "hidden": false
        },
        {
          "_id": "68cb67d55a7803ff3be42d4b",
          "name": "Juneyoung Ro",
          "hidden": false
        },
        {
          "_id": "68cb67d55a7803ff3be42d4c",
          "name": "Turghun Osman",
          "hidden": false
        },
        {
          "_id": "68cb67d55a7803ff3be42d4d",
          "name": "Can Hu",
          "hidden": false
        },
        {
          "_id": "68cb67d55a7803ff3be42d4e",
          "name": "Chaoyang Liao",
          "hidden": false
        },
        {
          "_id": "68cb67d55a7803ff3be42d4f",
          "name": "Cheng Chen",
          "hidden": false
        },
        {
          "_id": "68cb67d55a7803ff3be42d50",
          "name": "Chengcheng Han",
          "hidden": false
        },
        {
          "_id": "68cb67d55a7803ff3be42d51",
          "name": "Chenhao Qiu",
          "hidden": false
        },
        {
          "_id": "68cb67d55a7803ff3be42d52",
          "name": "Chong Peng",
          "hidden": false
        },
        {
          "_id": "68cb67d55a7803ff3be42d53",
          "name": "Cong Xu",
          "hidden": false
        },
        {
          "_id": "68cb67d55a7803ff3be42d54",
          "name": "Dailin Li",
          "hidden": false
        },
        {
          "_id": "68cb67d55a7803ff3be42d55",
          "name": "Feiyu Wang",
          "hidden": false
        },
        {
          "_id": "68cb67d55a7803ff3be42d56",
          "name": "Feng Gao",
          "hidden": false
        },
        {
          "_id": "68cb67d55a7803ff3be42d57",
          "name": "Guibo Zhu",
          "hidden": false
        },
        {
          "_id": "68cb67d55a7803ff3be42d58",
          "name": "Guopeng Tang",
          "hidden": false
        },
        {
          "_id": "68cb67d55a7803ff3be42d59",
          "name": "Haibo Lu",
          "hidden": false
        },
        {
          "_id": "68cb67d55a7803ff3be42d5a",
          "name": "Han Fang",
          "hidden": false
        },
        {
          "_id": "68cb67d55a7803ff3be42d5b",
          "name": "Han Qi",
          "hidden": false
        },
        {
          "_id": "68cb67d55a7803ff3be42d5c",
          "name": "Hanxiao Wu",
          "hidden": false
        },
        {
          "_id": "68cb67d55a7803ff3be42d5d",
          "name": "Haobo Cheng",
          "hidden": false
        },
        {
          "_id": "68cb67d55a7803ff3be42d5e",
          "name": "Hongbo Sun",
          "hidden": false
        },
        {
          "_id": "68cb67d55a7803ff3be42d5f",
          "name": "Hongyao Chen",
          "hidden": false
        },
        {
          "_id": "68cb67d55a7803ff3be42d60",
          "name": "Huayong Hu",
          "hidden": false
        },
        {
          "_id": "68cb67d55a7803ff3be42d61",
          "name": "Hui Li",
          "hidden": false
        },
        {
          "_id": "68cb67d55a7803ff3be42d62",
          "name": "Jiaheng Ma",
          "hidden": false
        },
        {
          "_id": "68cb67d55a7803ff3be42d63",
          "name": "Jiang Yu",
          "hidden": false
        },
        {
          "_id": "68cb67d55a7803ff3be42d64",
          "name": "Jianing Wang",
          "hidden": false
        },
        {
          "_id": "68cb67d55a7803ff3be42d65",
          "name": "Jie Yang",
          "hidden": false
        },
        {
          "_id": "68cb67d55a7803ff3be42d66",
          "name": "Jing He",
          "hidden": false
        },
        {
          "_id": "68cb67d55a7803ff3be42d67",
          "name": "Jinglin Zhou",
          "hidden": false
        },
        {
          "_id": "68cb67d55a7803ff3be42d68",
          "name": "Jingxuan Li",
          "hidden": false
        },
        {
          "_id": "68cb67d55a7803ff3be42d69",
          "name": "Josef Kittler",
          "hidden": false
        },
        {
          "_id": "68cb67d55a7803ff3be42d6a",
          "name": "Lihao Zheng",
          "hidden": false
        },
        {
          "_id": "68cb67d55a7803ff3be42d6b",
          "name": "Linnan Zhao",
          "hidden": false
        },
        {
          "_id": "68cb67d55a7803ff3be42d6c",
          "name": "Mengxi Jia",
          "hidden": false
        },
        {
          "_id": "68cb67d55a7803ff3be42d6d",
          "name": "Muyang Yan",
          "hidden": false
        },
        {
          "_id": "68cb67d55a7803ff3be42d6e",
          "name": "Nguyen Thanh Thien",
          "hidden": false
        },
        {
          "_id": "68cb67d55a7803ff3be42d6f",
          "name": "Pu Luo",
          "hidden": false
        },
        {
          "_id": "68cb67d55a7803ff3be42d70",
          "name": "Qi Li",
          "hidden": false
        },
        {
          "_id": "68cb67d55a7803ff3be42d71",
          "name": "Shien Song",
          "hidden": false
        },
        {
          "_id": "68cb67d55a7803ff3be42d72",
          "name": "Shijie Dong",
          "hidden": false
        },
        {
          "_id": "68cb67d55a7803ff3be42d73",
          "name": "Shuai Shao",
          "hidden": false
        },
        {
          "_id": "68cb67d55a7803ff3be42d74",
          "name": "Shutao Li",
          "hidden": false
        },
        {
          "_id": "68cb67d55a7803ff3be42d75",
          "name": "Taofeng Xue",
          "hidden": false
        },
        {
          "_id": "68cb67d55a7803ff3be42d76",
          "name": "Tianyang Xu",
          "hidden": false
        },
        {
          "_id": "68cb67d55a7803ff3be42d77",
          "name": "Tianyi Gao",
          "hidden": false
        },
        {
          "_id": "68cb67d55a7803ff3be42d78",
          "name": "Tingting Li",
          "hidden": false
        },
        {
          "_id": "68cb67d55a7803ff3be42d79",
          "name": "Wei Zhang",
          "hidden": false
        },
        {
          "_id": "68cb67d55a7803ff3be42d7a",
          "name": "Weiyang Su",
          "hidden": false
        },
        {
          "_id": "68cb67d55a7803ff3be42d7b",
          "name": "Xiaodong Dong",
          "hidden": false
        },
        {
          "_id": "68cb67d55a7803ff3be42d7c",
          "name": "Xiao-Jun Wu",
          "hidden": false
        },
        {
          "_id": "68cb67d55a7803ff3be42d7d",
          "name": "Xiaopeng Zhou",
          "hidden": false
        },
        {
          "_id": "68cb67d55a7803ff3be42d7e",
          "name": "Xin Chen",
          "hidden": false
        },
        {
          "_id": "68cb67d55a7803ff3be42d7f",
          "name": "Xin Wei",
          "hidden": false
        },
        {
          "_id": "68cb67d55a7803ff3be42d80",
          "name": "Xinyi You",
          "hidden": false
        },
        {
          "_id": "68cb67d55a7803ff3be42d81",
          "name": "Xudong Kang",
          "hidden": false
        },
        {
          "_id": "68cb67d55a7803ff3be42d82",
          "name": "Xujie Zhou",
          "hidden": false
        },
        {
          "_id": "68cb67d55a7803ff3be42d83",
          "name": "Xusheng Liu",
          "hidden": false
        },
        {
          "_id": "68cb67d55a7803ff3be42d84",
          "name": "Yanan Wang",
          "hidden": false
        },
        {
          "_id": "68cb67d55a7803ff3be42d85",
          "name": "Yanbin Huang",
          "hidden": false
        },
        {
          "_id": "68cb67d55a7803ff3be42d86",
          "name": "Yang Liu",
          "hidden": false
        },
        {
          "_id": "68cb67d55a7803ff3be42d87",
          "name": "Yang Yang",
          "hidden": false
        },
        {
          "_id": "68cb67d55a7803ff3be42d88",
          "name": "Yanglin Deng",
          "hidden": false
        },
        {
          "_id": "68cb67d55a7803ff3be42d89",
          "name": "Yashu Kang",
          "hidden": false
        },
        {
          "_id": "68cb67d55a7803ff3be42d8a",
          "name": "Ye Yuan",
          "hidden": false
        },
        {
          "_id": "68cb67d55a7803ff3be42d8b",
          "name": "Yi Wen",
          "hidden": false
        },
        {
          "_id": "68cb67d55a7803ff3be42d8c",
          "name": "Yicen Tian",
          "hidden": false
        },
        {
          "_id": "68cb67d55a7803ff3be42d8d",
          "name": "Yilin Tao",
          "hidden": false
        },
        {
          "_id": "68cb67d55a7803ff3be42d8e",
          "name": "Yin Tang",
          "hidden": false
        },
        {
          "_id": "68cb67d55a7803ff3be42d8f",
          "name": "Yipeng Lin",
          "hidden": false
        },
        {
          "_id": "68cb67d55a7803ff3be42d90",
          "name": "Yiqing Wang",
          "hidden": false
        },
        {
          "_id": "68cb67d55a7803ff3be42d91",
          "name": "Yiting Xi",
          "hidden": false
        },
        {
          "_id": "68cb67d55a7803ff3be42d92",
          "name": "Yongkang Yu",
          "hidden": false
        },
        {
          "_id": "68cb67d55a7803ff3be42d93",
          "name": "Yumei Li",
          "hidden": false
        },
        {
          "_id": "68cb67d55a7803ff3be42d94",
          "name": "Yuxin Qin",
          "hidden": false
        },
        {
          "_id": "68cb67d55a7803ff3be42d95",
          "name": "Yuying Chen",
          "hidden": false
        },
        {
          "_id": "68cb67d55a7803ff3be42d96",
          "name": "Yuzhe Cen",
          "hidden": false
        },
        {
          "_id": "68cb67d55a7803ff3be42d97",
          "name": "Zhaofan Zou",
          "hidden": false
        },
        {
          "_id": "68cb67d55a7803ff3be42d98",
          "name": "Zhaohong Liu",
          "hidden": false
        },
        {
          "_id": "68cb67d55a7803ff3be42d99",
          "name": "Zhehao Shen",
          "hidden": false
        },
        {
          "_id": "68cb67d55a7803ff3be42d9a",
          "name": "Zhenglin Du",
          "hidden": false
        },
        {
          "_id": "68cb67d55a7803ff3be42d9b",
          "name": "Zhengyang Li",
          "hidden": false
        },
        {
          "_id": "68cb67d55a7803ff3be42d9c",
          "name": "Zhenni Huang",
          "hidden": false
        },
        {
          "_id": "68cb67d55a7803ff3be42d9d",
          "name": "Zhenwei Shao",
          "hidden": false
        },
        {
          "_id": "68cb67d55a7803ff3be42d9e",
          "name": "Zhilong Song",
          "hidden": false
        },
        {
          "_id": "68cb67d55a7803ff3be42d9f",
          "name": "Zhiyong Feng",
          "hidden": false
        },
        {
          "_id": "68cb67d55a7803ff3be42da0",
          "name": "Zhiyu Wang",
          "hidden": false
        },
        {
          "_id": "68cb67d55a7803ff3be42da1",
          "name": "Zhou Yu",
          "hidden": false
        },
        {
          "_id": "68cb67d55a7803ff3be42da2",
          "name": "Ziang Li",
          "hidden": false
        },
        {
          "_id": "68cb67d55a7803ff3be42da3",
          "name": "Zihan Zhai",
          "hidden": false
        },
        {
          "_id": "68cb67d55a7803ff3be42da4",
          "name": "Zijian Zhang",
          "hidden": false
        },
        {
          "_id": "68cb67d55a7803ff3be42da5",
          "name": "Ziyang Peng",
          "hidden": false
        },
        {
          "_id": "68cb67d55a7803ff3be42da6",
          "name": "Ziyun Xiao",
          "hidden": false
        },
        {
          "_id": "68cb67d55a7803ff3be42da7",
          "name": "Zongshu Li",
          "hidden": false
        }
      ],
      "publishedAt": "2025-09-17T16:21:34.000Z",
      "submittedOnDailyAt": "2025-09-18T00:31:11.292Z",
      "title": "MARS2 2025 Challenge on Multimodal Reasoning: Datasets, Methods,\n  Results, Discussion, and Outlook",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "This paper reviews the MARS2 2025 Challenge on Multimodal Reasoning. We aim\nto bring together different approaches in multimodal machine learning and LLMs\nvia a large benchmark. We hope it better allows researchers to follow the\nstate-of-the-art in this very dynamic area. Meanwhile, a growing number of\ntestbeds have boosted the evolution of general-purpose large language models.\nThus, this year's MARS2 focuses on real-world and specialized scenarios to\nbroaden the multimodal reasoning applications of MLLMs. Our organizing team\nreleased two tailored datasets Lens and AdsQA as test sets, which support\ngeneral reasoning in 12 daily scenarios and domain-specific reasoning in\nadvertisement videos, respectively. We evaluated 40+ baselines that include\nboth generalist MLLMs and task-specific models, and opened up three competition\ntracks, i.e., Visual Grounding in Real-world Scenarios (VG-RS), Visual Question\nAnswering with Spatial Awareness (VQA-SA), and Visual Reasoning in Creative\nAdvertisement Videos (VR-Ads). Finally, 76 teams from the renowned academic and\nindustrial institutions have registered and 40+ valid submissions (out of\n1200+) have been included in our ranking lists. Our datasets, code sets (40+\nbaselines and 15+ participants' methods), and rankings are publicly available\non the MARS2 workshop website and our GitHub organization page\nhttps://github.com/mars2workshop/, where our updates and announcements of\nupcoming events will be continuously provided.",
      "upvotes": 0,
      "discussionId": "68cb67d65a7803ff3be42da8",
      "projectPage": "https://github.com/mars2workshop/",
      "ai_summary": "The MARS2 2025 Challenge focuses on multimodal reasoning with large language models through real-world and specialized scenarios, evaluating 40+ models across three competition tracks using tailored datasets.",
      "ai_keywords": [
        "multimodal machine learning",
        "LLMs",
        "MARS2",
        "multimodal reasoning",
        "large language models",
        "testbeds",
        "general-purpose large language models",
        "real-world scenarios",
        "specialized scenarios",
        "multimodal reasoning applications",
        "MLLMs",
        "Visual Grounding in Real-world Scenarios",
        "Visual Question Answering with Spatial Awareness",
        "Visual Reasoning in Creative Advertisement Videos",
        "datasets",
        "code sets",
        "GitHub organization page"
      ]
    },
    "publishedAt": "2025-09-17T12:21:34.000Z",
    "title": "MARS2 2025 Challenge on Multimodal Reasoning: Datasets, Methods,\n  Results, Discussion, and Outlook",
    "summary": "This paper reviews the MARS2 2025 Challenge on Multimodal Reasoning. We aim\nto bring together different approaches in multimodal machine learning and LLMs\nvia a large benchmark. We hope it better allows researchers to follow the\nstate-of-the-art in this very dynamic area. Meanwhile, a growing number of\ntestbeds have boosted the evolution of general-purpose large language models.\nThus, this year's MARS2 focuses on real-world and specialized scenarios to\nbroaden the multimodal reasoning applications of MLLMs. Our organizing team\nreleased two tailored datasets Lens and AdsQA as test sets, which support\ngeneral reasoning in 12 daily scenarios and domain-specific reasoning in\nadvertisement videos, respectively. We evaluated 40+ baselines that include\nboth generalist MLLMs and task-specific models, and opened up three competition\ntracks, i.e., Visual Grounding in Real-world Scenarios (VG-RS), Visual Question\nAnswering with Spatial Awareness (VQA-SA), and Visual Reasoning in Creative\nAdvertisement Videos (VR-Ads). Finally, 76 teams from the renowned academic and\nindustrial institutions have registered and 40+ valid submissions (out of\n1200+) have been included in our ranking lists. Our datasets, code sets (40+\nbaselines and 15+ participants' methods), and rankings are publicly available\non the MARS2 workshop website and our GitHub organization page\nhttps://github.com/mars2workshop/, where our updates and announcements of\nupcoming events will be continuously provided.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.14142.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 106
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2509.14055",
      "authors": [
        {
          "_id": "68cb68635a7803ff3be42daa",
          "name": "Gang Cheng",
          "hidden": false
        },
        {
          "_id": "68cb68635a7803ff3be42dab",
          "name": "Xin Gao",
          "hidden": false
        },
        {
          "_id": "68cb68635a7803ff3be42dac",
          "name": "Li Hu",
          "hidden": false
        },
        {
          "_id": "68cb68635a7803ff3be42dad",
          "name": "Siqi Hu",
          "hidden": false
        },
        {
          "_id": "68cb68635a7803ff3be42dae",
          "name": "Mingyang Huang",
          "hidden": false
        },
        {
          "_id": "68cb68635a7803ff3be42daf",
          "name": "Chaonan Ji",
          "hidden": false
        },
        {
          "_id": "68cb68635a7803ff3be42db0",
          "name": "Ju Li",
          "hidden": false
        },
        {
          "_id": "68cb68635a7803ff3be42db1",
          "name": "Dechao Meng",
          "hidden": false
        },
        {
          "_id": "68cb68635a7803ff3be42db2",
          "name": "Jinwei Qi",
          "hidden": false
        },
        {
          "_id": "68cb68635a7803ff3be42db3",
          "name": "Penchong Qiao",
          "hidden": false
        },
        {
          "_id": "68cb68635a7803ff3be42db4",
          "name": "Zhen Shen",
          "hidden": false
        },
        {
          "_id": "68cb68635a7803ff3be42db5",
          "name": "Yafei Song",
          "hidden": false
        },
        {
          "_id": "68cb68635a7803ff3be42db6",
          "name": "Ke Sun",
          "hidden": false
        },
        {
          "_id": "68cb68635a7803ff3be42db7",
          "name": "Linrui Tian",
          "hidden": false
        },
        {
          "_id": "68cb68635a7803ff3be42db8",
          "name": "Feng Wang",
          "hidden": false
        },
        {
          "_id": "68cb68635a7803ff3be42db9",
          "name": "Guangyuan Wang",
          "hidden": false
        },
        {
          "_id": "68cb68635a7803ff3be42dba",
          "name": "Qi Wang",
          "hidden": false
        },
        {
          "_id": "68cb68635a7803ff3be42dbb",
          "name": "Zhongjian Wang",
          "hidden": false
        },
        {
          "_id": "68cb68635a7803ff3be42dbc",
          "name": "Jiayu Xiao",
          "hidden": false
        },
        {
          "_id": "68cb68635a7803ff3be42dbd",
          "name": "Sheng Xu",
          "hidden": false
        },
        {
          "_id": "68cb68635a7803ff3be42dbe",
          "name": "Bang Zhang",
          "hidden": false
        },
        {
          "_id": "68cb68635a7803ff3be42dbf",
          "name": "Peng Zhang",
          "hidden": false
        },
        {
          "_id": "68cb68635a7803ff3be42dc0",
          "name": "Xindi Zhang",
          "hidden": false
        },
        {
          "_id": "68cb68635a7803ff3be42dc1",
          "name": "Zhe Zhang",
          "hidden": false
        },
        {
          "_id": "68cb68635a7803ff3be42dc2",
          "name": "Jingren Zhou",
          "hidden": false
        },
        {
          "_id": "68cb68635a7803ff3be42dc3",
          "name": "Lian Zhuo",
          "hidden": false
        }
      ],
      "publishedAt": "2025-09-17T15:00:57.000Z",
      "submittedOnDailyAt": "2025-09-18T00:33:25.099Z",
      "title": "Wan-Animate: Unified Character Animation and Replacement with Holistic\n  Replication",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "We introduce Wan-Animate, a unified framework for character animation and\nreplacement. Given a character image and a reference video, Wan-Animate can\nanimate the character by precisely replicating the expressions and movements of\nthe character in the video to generate high-fidelity character videos.\nAlternatively, it can integrate the animated character into the reference video\nto replace the original character, replicating the scene's lighting and color\ntone to achieve seamless environmental integration. Wan-Animate is built upon\nthe Wan model. To adapt it for character animation tasks, we employ a modified\ninput paradigm to differentiate between reference conditions and regions for\ngeneration. This design unifies multiple tasks into a common symbolic\nrepresentation. We use spatially-aligned skeleton signals to replicate body\nmotion and implicit facial features extracted from source images to reenact\nexpressions, enabling the generation of character videos with high\ncontrollability and expressiveness. Furthermore, to enhance environmental\nintegration during character replacement, we develop an auxiliary Relighting\nLoRA. This module preserves the character's appearance consistency while\napplying the appropriate environmental lighting and color tone. Experimental\nresults demonstrate that Wan-Animate achieves state-of-the-art performance. We\nare committed to open-sourcing the model weights and its source code.",
      "upvotes": 0,
      "discussionId": "68cb68635a7803ff3be42dc4",
      "projectPage": "https://humanaigc.github.io/wan-animate/",
      "ai_summary": "Wan-Animate is a unified framework for character animation and replacement, using spatially-aligned skeleton signals and implicit facial features to generate high-fidelity character videos with seamless environmental integration.",
      "ai_keywords": [
        "character animation",
        "reference video",
        "high-fidelity character videos",
        "Wan model",
        "input paradigm",
        "spatially-aligned skeleton signals",
        "implicit facial features",
        "character replacement",
        "environmental integration",
        "Relighting LoRA",
        "appearance consistency"
      ]
    },
    "publishedAt": "2025-09-17T11:00:57.000Z",
    "title": "Wan-Animate: Unified Character Animation and Replacement with Holistic\n  Replication",
    "summary": "We introduce Wan-Animate, a unified framework for character animation and\nreplacement. Given a character image and a reference video, Wan-Animate can\nanimate the character by precisely replicating the expressions and movements of\nthe character in the video to generate high-fidelity character videos.\nAlternatively, it can integrate the animated character into the reference video\nto replace the original character, replicating the scene's lighting and color\ntone to achieve seamless environmental integration. Wan-Animate is built upon\nthe Wan model. To adapt it for character animation tasks, we employ a modified\ninput paradigm to differentiate between reference conditions and regions for\ngeneration. This design unifies multiple tasks into a common symbolic\nrepresentation. We use spatially-aligned skeleton signals to replicate body\nmotion and implicit facial features extracted from source images to reenact\nexpressions, enabling the generation of character videos with high\ncontrollability and expressiveness. Furthermore, to enhance environmental\nintegration during character replacement, we develop an auxiliary Relighting\nLoRA. This module preserves the character's appearance consistency while\napplying the appropriate environmental lighting and color tone. Experimental\nresults demonstrate that Wan-Animate achieves state-of-the-art performance. We\nare committed to open-sourcing the model weights and its source code.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.14055.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 106
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2509.14033",
      "authors": [
        {
          "_id": "68cb69b55a7803ff3be42dcd",
          "name": "Weijie Yin",
          "hidden": false
        },
        {
          "_id": "68cb69b55a7803ff3be42dce",
          "name": "Yongjie Ye",
          "hidden": false
        },
        {
          "_id": "68cb69b55a7803ff3be42dcf",
          "name": "Fangxun Shu",
          "hidden": false
        },
        {
          "_id": "68cb69b55a7803ff3be42dd0",
          "name": "Yue Liao",
          "hidden": false
        },
        {
          "_id": "68cb69b55a7803ff3be42dd1",
          "name": "Zijian Kang",
          "hidden": false
        },
        {
          "_id": "68cb69b55a7803ff3be42dd2",
          "name": "Hongyuan Dong",
          "hidden": false
        },
        {
          "_id": "68cb69b55a7803ff3be42dd3",
          "name": "Haiyang Yu",
          "hidden": false
        },
        {
          "_id": "68cb69b55a7803ff3be42dd4",
          "name": "Dingkang Yang",
          "hidden": false
        },
        {
          "_id": "68cb69b55a7803ff3be42dd5",
          "name": "Jiacong Wang",
          "hidden": false
        },
        {
          "_id": "68cb69b55a7803ff3be42dd6",
          "name": "Han Wang",
          "hidden": false
        },
        {
          "_id": "68cb69b55a7803ff3be42dd7",
          "name": "Wenzhuo Liu",
          "hidden": false
        },
        {
          "_id": "68cb69b55a7803ff3be42dd8",
          "name": "Xiao Liang",
          "hidden": false
        },
        {
          "_id": "68cb69b55a7803ff3be42dd9",
          "name": "Shuicheng Yan",
          "hidden": false
        },
        {
          "_id": "68cb69b55a7803ff3be42dda",
          "name": "Chao Feng",
          "hidden": false
        }
      ],
      "publishedAt": "2025-09-17T14:34:02.000Z",
      "submittedOnDailyAt": "2025-09-18T00:39:01.459Z",
      "title": "SAIL-VL2 Technical Report",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "We introduce SAIL-VL2, an open-suite vision-language foundation model (LVM)\nfor comprehensive multimodal understanding and reasoning. As the successor to\nSAIL-VL, SAIL-VL2 achieves state-of-the-art performance at the 2B and 8B\nparameter scales across diverse image and video benchmarks, demonstrating\nstrong capabilities from fine-grained perception to complex reasoning. Three\ncore innovations drive its effectiveness. First, a large-scale data curation\npipeline with scoring and filtering strategies enhances both quality and\ndistribution across captioning, OCR, QA, and video data, improving training\nefficiency. Second, a progressive training framework begins with a powerful\npre-trained vision encoder (SAIL-ViT), advances through multimodal\npre-training, and culminates in a thinking-fusion SFT-RL hybrid paradigm that\nsystematically strengthens model capabilities. Third, architectural advances\nextend beyond dense LLMs to efficient sparse Mixture-of-Experts (MoE) designs.\nWith these contributions, SAIL-VL2 demonstrates competitive performance across\n106 datasets and achieves state-of-the-art results on challenging reasoning\nbenchmarks such as MMMU and MathVista. Furthermore, on the OpenCompass\nleaderboard, SAIL-VL2-2B ranks first among officially released open-source\nmodels under the 4B parameter scale, while serving as an efficient and\nextensible foundation for the open-source multimodal community.",
      "upvotes": 0,
      "discussionId": "68cb69b55a7803ff3be42ddb",
      "ai_summary": "SAIL-VL2, a vision-language foundation model, achieves state-of-the-art performance across diverse benchmarks through data curation, progressive training, and sparse MoE architecture.",
      "ai_keywords": [
        "vision-language foundation model",
        "SAIL-VL2",
        "SAIL-VL",
        "parameter scales",
        "image and video benchmarks",
        "fine-grained perception",
        "complex reasoning",
        "large-scale data curation",
        "scoring and filtering strategies",
        "training efficiency",
        "progressive training framework",
        "powerful pre-trained vision encoder",
        "SAIL-ViT",
        "multimodal pre-training",
        "thinking-fusion SFT-RL hybrid paradigm",
        "dense LLMs",
        "efficient sparse Mixture-of-Experts",
        "MoE designs",
        "challenging reasoning benchmarks",
        "MMMU",
        "MathVista",
        "OpenCompass leaderboard"
      ]
    },
    "publishedAt": "2025-09-17T10:34:02.000Z",
    "title": "SAIL-VL2 Technical Report",
    "summary": "We introduce SAIL-VL2, an open-suite vision-language foundation model (LVM)\nfor comprehensive multimodal understanding and reasoning. As the successor to\nSAIL-VL, SAIL-VL2 achieves state-of-the-art performance at the 2B and 8B\nparameter scales across diverse image and video benchmarks, demonstrating\nstrong capabilities from fine-grained perception to complex reasoning. Three\ncore innovations drive its effectiveness. First, a large-scale data curation\npipeline with scoring and filtering strategies enhances both quality and\ndistribution across captioning, OCR, QA, and video data, improving training\nefficiency. Second, a progressive training framework begins with a powerful\npre-trained vision encoder (SAIL-ViT), advances through multimodal\npre-training, and culminates in a thinking-fusion SFT-RL hybrid paradigm that\nsystematically strengthens model capabilities. Third, architectural advances\nextend beyond dense LLMs to efficient sparse Mixture-of-Experts (MoE) designs.\nWith these contributions, SAIL-VL2 demonstrates competitive performance across\n106 datasets and achieves state-of-the-art results on challenging reasoning\nbenchmarks such as MMMU and MathVista. Furthermore, on the OpenCompass\nleaderboard, SAIL-VL2-2B ranks first among officially released open-source\nmodels under the 4B parameter scale, while serving as an efficient and\nextensible foundation for the open-source multimodal community.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.14033.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 106
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2509.13761",
      "authors": [
        {
          "_id": "68cb674b5a7803ff3be42d0e",
          "name": "Qikai Chang",
          "hidden": false
        },
        {
          "_id": "68cb674b5a7803ff3be42d0f",
          "name": "Zhenrong Zhang",
          "hidden": false
        },
        {
          "_id": "68cb674b5a7803ff3be42d10",
          "name": "Pengfei Hu",
          "hidden": false
        },
        {
          "_id": "68cb674b5a7803ff3be42d11",
          "name": "Jiefeng Ma",
          "hidden": false
        },
        {
          "_id": "68cb674b5a7803ff3be42d12",
          "name": "Yicheng Pan",
          "hidden": false
        },
        {
          "_id": "68cb674b5a7803ff3be42d13",
          "name": "Jianshu Zhang",
          "hidden": false
        },
        {
          "_id": "68cb674b5a7803ff3be42d14",
          "name": "Jun Du",
          "hidden": false
        },
        {
          "_id": "68cb674b5a7803ff3be42d15",
          "name": "Quan Liu",
          "hidden": false
        },
        {
          "_id": "68cb674b5a7803ff3be42d16",
          "name": "Jianqing Gao",
          "hidden": false
        }
      ],
      "publishedAt": "2025-09-17T07:16:12.000Z",
      "submittedOnDailyAt": "2025-09-18T00:28:48.602Z",
      "title": "THOR: Tool-Integrated Hierarchical Optimization via RL for Mathematical\n  Reasoning",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Large Language Models (LLMs) have made remarkable progress in mathematical\nreasoning, but still continue to struggle with high-precision tasks like\nnumerical computation and formal symbolic manipulation. Integrating external\ntools has emerged as a promising approach to bridge this gap. Despite recent\nadvances, existing methods struggle with three key challenges: constructing\ntool-integrated reasoning data, performing fine-grained optimization, and\nenhancing inference. To overcome these limitations, we propose THOR\n(Tool-Integrated Hierarchical Optimization via RL). First, we introduce TIRGen,\na multi-agent actor-critic-based pipeline for constructing high-quality\ndatasets of tool-integrated reasoning paths, aligning with the policy and\ngeneralizing well across diverse models. Second, to perform fine-grained\nhierarchical optimization, we introduce an RL strategy that jointly optimizes\nfor both trajectory-level problem solving and step-level code generation. This\nis motivated by our key insight that the success of an intermediate tool call\nis a strong predictor of the final answer's correctness. Finally, THOR\nincorporates a self-correction mechanism that leverages immediate tool feedback\nto dynamically revise erroneous reasoning paths during inference. Our approach\ndemonstrates strong generalization across diverse models, performing\neffectively in both reasoning and non-reasoning models. It further achieves\nstate-of-the-art performance for models of a similar scale on multiple\nmathematical benchmarks, while also delivering consistent improvements on code\nbenchmarks. Our code will be publicly available at\nhttps://github.com/JingMog/THOR.",
      "upvotes": 0,
      "discussionId": "68cb674b5a7803ff3be42d17",
      "githubRepo": "https://github.com/JingMog/THOR",
      "ai_summary": "THOR, a tool-integrated hierarchical optimization framework using RL, enhances mathematical reasoning and code generation by constructing high-quality datasets, optimizing reasoning paths, and correcting errors during inference.",
      "ai_keywords": [
        "Large Language Models",
        "mathematical reasoning",
        "numerical computation",
        "formal symbolic manipulation",
        "tool-integrated reasoning",
        "multi-agent actor-critic",
        "TIRGen",
        "hierarchical optimization",
        "RL strategy",
        "trajectory-level problem solving",
        "step-level code generation",
        "self-correction mechanism",
        "immediate tool feedback",
        "reasoning paths",
        "generalization",
        "mathematical benchmarks",
        "code benchmarks"
      ],
      "githubStars": 2
    },
    "publishedAt": "2025-09-17T03:16:12.000Z",
    "title": "THOR: Tool-Integrated Hierarchical Optimization via RL for Mathematical\n  Reasoning",
    "summary": "Large Language Models (LLMs) have made remarkable progress in mathematical\nreasoning, but still continue to struggle with high-precision tasks like\nnumerical computation and formal symbolic manipulation. Integrating external\ntools has emerged as a promising approach to bridge this gap. Despite recent\nadvances, existing methods struggle with three key challenges: constructing\ntool-integrated reasoning data, performing fine-grained optimization, and\nenhancing inference. To overcome these limitations, we propose THOR\n(Tool-Integrated Hierarchical Optimization via RL). First, we introduce TIRGen,\na multi-agent actor-critic-based pipeline for constructing high-quality\ndatasets of tool-integrated reasoning paths, aligning with the policy and\ngeneralizing well across diverse models. Second, to perform fine-grained\nhierarchical optimization, we introduce an RL strategy that jointly optimizes\nfor both trajectory-level problem solving and step-level code generation. This\nis motivated by our key insight that the success of an intermediate tool call\nis a strong predictor of the final answer's correctness. Finally, THOR\nincorporates a self-correction mechanism that leverages immediate tool feedback\nto dynamically revise erroneous reasoning paths during inference. Our approach\ndemonstrates strong generalization across diverse models, performing\neffectively in both reasoning and non-reasoning models. It further achieves\nstate-of-the-art performance for models of a similar scale on multiple\nmathematical benchmarks, while also delivering consistent improvements on code\nbenchmarks. Our code will be publicly available at\nhttps://github.com/JingMog/THOR.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.13761.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 106
    },
    "isAuthorParticipating": false
  }
]