[
  {
    "paper": {
      "id": "2506.13642",
      "authors": [
        {
          "_id": "685129448a68fee7f6ba4c04",
          "name": "Shaolei Zhang",
          "hidden": false
        },
        {
          "_id": "685129448a68fee7f6ba4c05",
          "name": "Shoutao Guo",
          "hidden": false
        },
        {
          "_id": "685129448a68fee7f6ba4c06",
          "name": "Qingkai Fang",
          "hidden": false
        },
        {
          "_id": "685129448a68fee7f6ba4c07",
          "name": "Yan Zhou",
          "hidden": false
        },
        {
          "_id": "685129448a68fee7f6ba4c08",
          "name": "Yang Feng",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/64803e5dc57f629056c601f1/MBm95m2RAX6iKKBaKTma8.mp4",
        "https://cdn-uploads.huggingface.co/production/uploads/64803e5dc57f629056c601f1/PoLupV32gI1iLxILZccQS.mp4"
      ],
      "publishedAt": "2025-06-16T16:06:45.000Z",
      "submittedOnDailyAt": "2025-06-18T00:16:02.465Z",
      "title": "Stream-Omni: Simultaneous Multimodal Interactions with Large\n  Language-Vision-Speech Model",
      "submittedOnDailyBy": {
        "_id": "64803e5dc57f629056c601f1",
        "avatarUrl": "/avatars/a9e9c97c70714e3a29bef2cf929ee6b3.svg",
        "isPro": false,
        "fullname": "Shaolei Zhang",
        "user": "zhangshaolei",
        "type": "user"
      },
      "summary": "The emergence of GPT-4o-like large multimodal models (LMMs) has raised the\nexploration of integrating text, vision, and speech modalities to support more\nflexible multimodal interaction. Existing LMMs typically concatenate\nrepresentation of modalities along the sequence dimension and feed them into a\nlarge language model (LLM) backbone. While sequence-dimension concatenation is\nstraightforward for modality integration, it often relies heavily on\nlarge-scale data to learn modality alignments. In this paper, we aim to model\nthe relationships between modalities more purposefully, thereby achieving more\nefficient and flexible modality alignments. To this end, we propose\nStream-Omni, a large language-vision-speech model with efficient modality\nalignments, which can simultaneously support interactions under various\nmodality combinations. Stream-Omni employs LLM as the backbone and aligns the\nvision and speech to the text based on their relationships. For vision that is\nsemantically complementary to text, Stream-Omni uses sequence-dimension\nconcatenation to achieve vision-text alignment. For speech that is semantically\nconsistent with text, Stream-Omni introduces a CTC-based layer-dimension\nmapping to achieve speech-text alignment. In this way, Stream-Omni can achieve\nmodality alignments with less data (especially speech), enabling the transfer\nof text capabilities to other modalities. Experiments on various benchmarks\ndemonstrate that Stream-Omni achieves strong performance on visual\nunderstanding, speech interaction, and vision-grounded speech interaction\ntasks. Owing to the layer-dimensional mapping, Stream-Omni can simultaneously\nprovide intermediate text outputs (such as ASR transcriptions and model\nresponses) during speech interaction, offering users a comprehensive multimodal\nexperience.",
      "upvotes": 13,
      "discussionId": "685129458a68fee7f6ba4c09",
      "projectPage": "https://github.com/ictnlp/Stream-Omni",
      "githubRepo": "https://github.com/ictnlp/Stream-Omni",
      "ai_summary": "Stream-Omni, a large multimodal model, integrates text, vision, and speech by efficiently aligning modalities using sequence-dimension concatenation for vision and layer-dimension mapping for speech, achieving strong performance with less data.",
      "ai_keywords": [
        "GPT-4o-like",
        "large multimodal models",
        "LLM backbone",
        "modality alignments",
        "sequence-dimension concatenation",
        "CTC-based layer-dimension mapping",
        "visual understanding",
        "speech interaction",
        "vision-grounded speech interaction",
        "ASR transcriptions",
        "model responses"
      ]
    },
    "publishedAt": "2025-06-16T12:06:45.000Z",
    "title": "Stream-Omni: Simultaneous Multimodal Interactions with Large\n  Language-Vision-Speech Model",
    "summary": "The emergence of GPT-4o-like large multimodal models (LMMs) has raised the\nexploration of integrating text, vision, and speech modalities to support more\nflexible multimodal interaction. Existing LMMs typically concatenate\nrepresentation of modalities along the sequence dimension and feed them into a\nlarge language model (LLM) backbone. While sequence-dimension concatenation is\nstraightforward for modality integration, it often relies heavily on\nlarge-scale data to learn modality alignments. In this paper, we aim to model\nthe relationships between modalities more purposefully, thereby achieving more\nefficient and flexible modality alignments. To this end, we propose\nStream-Omni, a large language-vision-speech model with efficient modality\nalignments, which can simultaneously support interactions under various\nmodality combinations. Stream-Omni employs LLM as the backbone and aligns the\nvision and speech to the text based on their relationships. For vision that is\nsemantically complementary to text, Stream-Omni uses sequence-dimension\nconcatenation to achieve vision-text alignment. For speech that is semantically\nconsistent with text, Stream-Omni introduces a CTC-based layer-dimension\nmapping to achieve speech-text alignment. In this way, Stream-Omni can achieve\nmodality alignments with less data (especially speech), enabling the transfer\nof text capabilities to other modalities. Experiments on various benchmarks\ndemonstrate that Stream-Omni achieves strong performance on visual\nunderstanding, speech interaction, and vision-grounded speech interaction\ntasks. Owing to the layer-dimensional mapping, Stream-Omni can simultaneously\nprovide intermediate text outputs (such as ASR transcriptions and model\nresponses) during speech interaction, offering users a comprehensive multimodal\nexperience.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/64803e5dc57f629056c601f1/MBm95m2RAX6iKKBaKTma8.mp4",
      "https://cdn-uploads.huggingface.co/production/uploads/64803e5dc57f629056c601f1/PoLupV32gI1iLxILZccQS.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.13642.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64803e5dc57f629056c601f1",
      "avatarUrl": "/avatars/a9e9c97c70714e3a29bef2cf929ee6b3.svg",
      "fullname": "Shaolei Zhang",
      "name": "zhangshaolei",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.14606",
      "authors": [
        {
          "_id": "68521f240164cd131671047a",
          "name": "Ahmed Heakl",
          "hidden": false
        },
        {
          "_id": "68521f240164cd131671047b",
          "name": "Sarim Hashmi",
          "hidden": false
        },
        {
          "_id": "68521f240164cd131671047c",
          "name": "Chaimaa Abi",
          "hidden": false
        },
        {
          "_id": "68521f240164cd131671047d",
          "name": "Celine Lee",
          "hidden": false
        },
        {
          "_id": "68521f240164cd131671047e",
          "name": "Abdulrahman Mahmoud",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/656864e12d73834278a8dea7/G_EGzMfb1C6fX_o-yLFbl.png",
        "https://cdn-uploads.huggingface.co/production/uploads/656864e12d73834278a8dea7/G2UhoU9mbZ8tQ0VzmKLV5.png"
      ],
      "publishedAt": "2025-06-17T15:06:54.000Z",
      "submittedOnDailyAt": "2025-06-18T00:38:14.336Z",
      "title": "Guaranteed Guess: A Language Modeling Approach for CISC-to-RISC\n  Transpilation with Testing Guarantees",
      "submittedOnDailyBy": {
        "_id": "656864e12d73834278a8dea7",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/656864e12d73834278a8dea7/sfAWS2eyPtFHb_2GZIypp.jpeg",
        "isPro": true,
        "fullname": "Ahmed Heakl",
        "user": "ahmedheakl",
        "type": "user"
      },
      "summary": "The hardware ecosystem is rapidly evolving, with increasing interest in\ntranslating low-level programs across different instruction set architectures\n(ISAs) in a quick, flexible, and correct way to enhance the portability and\nlongevity of existing code. A particularly challenging class of this\ntranspilation problem is translating between complex- (CISC) and reduced-\n(RISC) hardware architectures, due to fundamental differences in instruction\ncomplexity, memory models, and execution paradigms. In this work, we introduce\nGG (Guaranteed Guess), an ISA-centric transpilation pipeline that combines the\ntranslation power of pre-trained large language models (LLMs) with the rigor of\nestablished software testing constructs. Our method generates candidate\ntranslations using an LLM from one ISA to another, and embeds such translations\nwithin a software-testing framework to build quantifiable confidence in the\ntranslation. We evaluate our GG approach over two diverse datasets, enforce\nhigh code coverage (>98%) across unit tests, and achieve functional/semantic\ncorrectness of 99% on HumanEval programs and 49% on BringupBench programs,\nrespectively. Further, we compare our approach to the state-of-the-art Rosetta\n2 framework on Apple Silicon, showcasing 1.73x faster runtime performance,\n1.47x better energy efficiency, and 2.41x better memory usage for our\ntranspiled code, demonstrating the effectiveness of GG for real-world\nCISC-to-RISC translation tasks. We will open-source our codes, data, models,\nand benchmarks to establish a common foundation for ISA-level code translation\nresearch.",
      "upvotes": 10,
      "discussionId": "68521f240164cd131671047f",
      "projectPage": "https://ahmedheakl.github.io/Guaranteed-Guess/",
      "githubRepo": "https://github.com/ahmedheakl/Guaranteed-Guess",
      "ai_summary": "A novel ISA-centric transpilation pipeline using LLMs and software testing achieves high correctness and efficiency in translating between complex and reduced hardware architectures.",
      "ai_keywords": [
        "pre-trained large language models",
        "software testing constructs",
        "ISA-centric transpilation",
        "complex-instruction set computing (CISC)",
        "reduced-instruction set computing (RISC)",
        "instruction set architecture (ISA)",
        "HumanEval",
        "BringupBench",
        "Rosetta 2 framework",
        "functional/semantic correctness",
        "real-world CISC-to-RISC translation",
        "memory models",
        "execution paradigms",
        "transpilation",
        "hardware ecosystem",
        "low-level programs",
        "code portability",
        "longevity"
      ]
    },
    "publishedAt": "2025-06-17T11:06:54.000Z",
    "title": "Guaranteed Guess: A Language Modeling Approach for CISC-to-RISC\n  Transpilation with Testing Guarantees",
    "summary": "The hardware ecosystem is rapidly evolving, with increasing interest in\ntranslating low-level programs across different instruction set architectures\n(ISAs) in a quick, flexible, and correct way to enhance the portability and\nlongevity of existing code. A particularly challenging class of this\ntranspilation problem is translating between complex- (CISC) and reduced-\n(RISC) hardware architectures, due to fundamental differences in instruction\ncomplexity, memory models, and execution paradigms. In this work, we introduce\nGG (Guaranteed Guess), an ISA-centric transpilation pipeline that combines the\ntranslation power of pre-trained large language models (LLMs) with the rigor of\nestablished software testing constructs. Our method generates candidate\ntranslations using an LLM from one ISA to another, and embeds such translations\nwithin a software-testing framework to build quantifiable confidence in the\ntranslation. We evaluate our GG approach over two diverse datasets, enforce\nhigh code coverage (>98%) across unit tests, and achieve functional/semantic\ncorrectness of 99% on HumanEval programs and 49% on BringupBench programs,\nrespectively. Further, we compare our approach to the state-of-the-art Rosetta\n2 framework on Apple Silicon, showcasing 1.73x faster runtime performance,\n1.47x better energy efficiency, and 2.41x better memory usage for our\ntranspiled code, demonstrating the effectiveness of GG for real-world\nCISC-to-RISC translation tasks. We will open-source our codes, data, models,\nand benchmarks to establish a common foundation for ISA-level code translation\nresearch.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/656864e12d73834278a8dea7/G_EGzMfb1C6fX_o-yLFbl.png",
      "https://cdn-uploads.huggingface.co/production/uploads/656864e12d73834278a8dea7/G2UhoU9mbZ8tQ0VzmKLV5.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.14606.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "656864e12d73834278a8dea7",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/656864e12d73834278a8dea7/sfAWS2eyPtFHb_2GZIypp.jpeg",
      "fullname": "Ahmed Heakl",
      "name": "ahmedheakl",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 41
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.14429",
      "authors": [
        {
          "_id": "68521a9a0164cd131671045c",
          "name": "Xiaoran Liu",
          "hidden": false
        },
        {
          "_id": "68521a9a0164cd131671045d",
          "name": "Zhigeng Liu",
          "hidden": false
        },
        {
          "_id": "68521a9a0164cd131671045e",
          "name": "Zengfeng Huang",
          "hidden": false
        },
        {
          "_id": "68521a9a0164cd131671045f",
          "name": "Qipeng Guo",
          "hidden": false
        },
        {
          "_id": "68521a9a0164cd1316710460",
          "name": "Ziwei He",
          "hidden": false
        },
        {
          "_id": "68521a9a0164cd1316710461",
          "name": "Xipeng Qiu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-17T11:45:37.000Z",
      "submittedOnDailyAt": "2025-06-18T00:18:23.135Z",
      "title": "LongLLaDA: Unlocking Long Context Capabilities in Diffusion LLMs",
      "submittedOnDailyBy": {
        "_id": "64f033ef82c6eea604c4da8b",
        "avatarUrl": "/avatars/51b93fea7fd68b4274ee03701245dcca.svg",
        "isPro": false,
        "fullname": "Liu Xiaoran",
        "user": "LiuXR",
        "type": "user"
      },
      "summary": "Large Language Diffusion Models, or diffusion LLMs, have emerged as a\nsignificant focus in NLP research, with substantial effort directed toward\nunderstanding their scalability and downstream task performance. However, their\nlong-context capabilities remain unexplored, lacking systematic analysis or\nmethods for context extension. In this work, we present the first systematic\ninvestigation comparing the long-context performance of diffusion LLMs and\ntraditional auto-regressive LLMs. We first identify a unique characteristic of\ndiffusion LLMs, unlike auto-regressive LLMs, they maintain remarkably\n\\textit{stable perplexity} during direct context extrapolation.\nFurthermore, where auto-regressive models fail outright during the\nNeedle-In-A-Haystack task with context exceeding their pretrained length, we\ndiscover diffusion LLMs exhibit a distinct \\textit{local perception}\nphenomenon, enabling successful retrieval from recent context segments. We\nexplain both phenomena through the lens of Rotary Position Embedding (RoPE)\nscaling theory. Building on these observations, we propose LongLLaDA, a\ntraining-free method that integrates LLaDA with the NTK-based RoPE\nextrapolation. Our results validate that established extrapolation scaling laws\nremain effective for extending the context windows of diffusion LLMs.\nFurthermore, we identify long-context tasks where diffusion LLMs outperform\nauto-regressive LLMs and others where they fall short. Consequently, this study\nestablishes the first context extrapolation method for diffusion LLMs while\nproviding essential theoretical insights and empirical benchmarks critical for\nadvancing future research on long-context diffusion LLMs.",
      "upvotes": 4,
      "discussionId": "68521a9a0164cd1316710462",
      "ai_summary": "This study investigates long-context performance of diffusion LLMs compared to auto-regressive LLMs, identifies their unique characteristics, and proposes LongLLaDA, a training-free method for extending context windows.",
      "ai_keywords": [
        "diffusion LLMs",
        "auto-regressive LLMs",
        "stable perplexity",
        "local perception",
        "Rotary Position Embedding (RoPE) scaling theory",
        "LongLLaDA",
        "NTK-based RoPE extrapolation",
        "context extrapolation scaling laws",
        "long-context tasks"
      ]
    },
    "publishedAt": "2025-06-17T07:45:37.000Z",
    "title": "LongLLaDA: Unlocking Long Context Capabilities in Diffusion LLMs",
    "summary": "Large Language Diffusion Models, or diffusion LLMs, have emerged as a\nsignificant focus in NLP research, with substantial effort directed toward\nunderstanding their scalability and downstream task performance. However, their\nlong-context capabilities remain unexplored, lacking systematic analysis or\nmethods for context extension. In this work, we present the first systematic\ninvestigation comparing the long-context performance of diffusion LLMs and\ntraditional auto-regressive LLMs. We first identify a unique characteristic of\ndiffusion LLMs, unlike auto-regressive LLMs, they maintain remarkably\n\\textit{stable perplexity} during direct context extrapolation.\nFurthermore, where auto-regressive models fail outright during the\nNeedle-In-A-Haystack task with context exceeding their pretrained length, we\ndiscover diffusion LLMs exhibit a distinct \\textit{local perception}\nphenomenon, enabling successful retrieval from recent context segments. We\nexplain both phenomena through the lens of Rotary Position Embedding (RoPE)\nscaling theory. Building on these observations, we propose LongLLaDA, a\ntraining-free method that integrates LLaDA with the NTK-based RoPE\nextrapolation. Our results validate that established extrapolation scaling laws\nremain effective for extending the context windows of diffusion LLMs.\nFurthermore, we identify long-context tasks where diffusion LLMs outperform\nauto-regressive LLMs and others where they fall short. Consequently, this study\nestablishes the first context extrapolation method for diffusion LLMs while\nproviding essential theoretical insights and empirical benchmarks critical for\nadvancing future research on long-context diffusion LLMs.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.14429.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64f033ef82c6eea604c4da8b",
      "avatarUrl": "/avatars/51b93fea7fd68b4274ee03701245dcca.svg",
      "fullname": "Liu Xiaoran",
      "name": "LiuXR",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.14002",
      "authors": [
        {
          "_id": "685210eb0164cd131671043e",
          "name": "Siyu Chen",
          "hidden": false
        },
        {
          "_id": "685210eb0164cd131671043f",
          "name": "Heejune Sheen",
          "hidden": false
        },
        {
          "_id": "685210eb0164cd1316710440",
          "name": "Xuyuan Xiong",
          "hidden": false
        },
        {
          "_id": "685210eb0164cd1316710441",
          "name": "Tianhao Wang",
          "hidden": false
        },
        {
          "_id": "685210eb0164cd1316710442",
          "name": "Zhuoran Yang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-16T20:58:05.000Z",
      "submittedOnDailyAt": "2025-06-18T00:09:08.222Z",
      "title": "Taming Polysemanticity in LLMs: Provable Feature Recovery via Sparse\n  Autoencoders",
      "submittedOnDailyBy": {
        "_id": "683229900411a9d65cd410c0",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/VqwvpUYF8CQAKPHMNfLyw.png",
        "isPro": false,
        "fullname": "Siyu Chen",
        "user": "Siyuc",
        "type": "user"
      },
      "summary": "We study the challenge of achieving theoretically grounded feature recovery\nusing Sparse Autoencoders (SAEs) for the interpretation of Large Language\nModels. Existing SAE training algorithms often lack rigorous mathematical\nguarantees and suffer from practical limitations such as hyperparameter\nsensitivity and instability. To address these issues, we first propose a novel\nstatistical framework for the feature recovery problem, which includes a new\nnotion of feature identifiability by modeling polysemantic features as sparse\nmixtures of underlying monosemantic concepts. Building on this framework, we\nintroduce a new SAE training algorithm based on ``bias adaptation'', a\ntechnique that adaptively adjusts neural network bias parameters to ensure\nappropriate activation sparsity. We theoretically prove that this\nalgorithm correctly recovers all monosemantic features when input data is\nsampled from our proposed statistical model. Furthermore, we develop an\nimproved empirical variant, Group Bias Adaptation (GBA), and\ndemonstrate its superior performance against benchmark methods when\napplied to LLMs with up to 1.5 billion parameters. This work represents a\nfoundational step in demystifying SAE training by providing the first SAE\nalgorithm with theoretical recovery guarantees, thereby advancing the\ndevelopment of more transparent and trustworthy AI systems through enhanced\nmechanistic interpretability.",
      "upvotes": 3,
      "discussionId": "685210ec0164cd1316710443",
      "ai_summary": "A new statistical framework and training algorithm, Group Bias Adaptation, enhance Sparse Autoencoders for recovering monosemantic features in Large Language Models, offering theoretical guarantees and superior performance.",
      "ai_keywords": [
        "Sparse Autoencoders",
        "feature recovery",
        "statistical framework",
        "feature identifiability",
        "polysemantic features",
        "monosemantic concepts",
        "bias adaptation",
        "Group Bias Adaptation",
        "Large Language Models",
        "theoretical recovery guarantees",
        "mechnistic interpretability"
      ]
    },
    "publishedAt": "2025-06-16T16:58:05.000Z",
    "title": "Taming Polysemanticity in LLMs: Provable Feature Recovery via Sparse\n  Autoencoders",
    "summary": "We study the challenge of achieving theoretically grounded feature recovery\nusing Sparse Autoencoders (SAEs) for the interpretation of Large Language\nModels. Existing SAE training algorithms often lack rigorous mathematical\nguarantees and suffer from practical limitations such as hyperparameter\nsensitivity and instability. To address these issues, we first propose a novel\nstatistical framework for the feature recovery problem, which includes a new\nnotion of feature identifiability by modeling polysemantic features as sparse\nmixtures of underlying monosemantic concepts. Building on this framework, we\nintroduce a new SAE training algorithm based on ``bias adaptation'', a\ntechnique that adaptively adjusts neural network bias parameters to ensure\nappropriate activation sparsity. We theoretically prove that this\nalgorithm correctly recovers all monosemantic features when input data is\nsampled from our proposed statistical model. Furthermore, we develop an\nimproved empirical variant, Group Bias Adaptation (GBA), and\ndemonstrate its superior performance against benchmark methods when\napplied to LLMs with up to 1.5 billion parameters. This work represents a\nfoundational step in demystifying SAE training by providing the first SAE\nalgorithm with theoretical recovery guarantees, thereby advancing the\ndevelopment of more transparent and trustworthy AI systems through enhanced\nmechanistic interpretability.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.14002.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "683229900411a9d65cd410c0",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/VqwvpUYF8CQAKPHMNfLyw.png",
      "fullname": "Siyu Chen",
      "name": "Siyuc",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.14603",
      "authors": [
        {
          "_id": "68521f8a0164cd1316710481",
          "name": "Amirmojtaba Sabour",
          "hidden": false
        },
        {
          "_id": "68521f8a0164cd1316710482",
          "name": "Sanja Fidler",
          "hidden": false
        },
        {
          "_id": "68521f8a0164cd1316710483",
          "name": "Karsten Kreis",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/656015f28138827138c70858/_yuUKzjzngNZzaUJCVwjR.jpeg",
        "https://cdn-uploads.huggingface.co/production/uploads/656015f28138827138c70858/2FmUhOtaDCtAzFkNEx6Di.jpeg",
        "https://cdn-uploads.huggingface.co/production/uploads/656015f28138827138c70858/MC_ZLBqT81dOpANroMLrm.jpeg",
        "https://cdn-uploads.huggingface.co/production/uploads/656015f28138827138c70858/degPI6Z5IEf4v_y2_UL9V.jpeg"
      ],
      "publishedAt": "2025-06-17T15:06:07.000Z",
      "submittedOnDailyAt": "2025-06-18T00:42:20.168Z",
      "title": "Align Your Flow: Scaling Continuous-Time Flow Map Distillation",
      "submittedOnDailyBy": {
        "_id": "656015f28138827138c70858",
        "avatarUrl": "/avatars/b8471ae4d80f078c7c928fc3d8f49126.svg",
        "isPro": false,
        "fullname": "Amirmojtaba Sabour",
        "user": "amsabour",
        "type": "user"
      },
      "summary": "Diffusion- and flow-based models have emerged as state-of-the-art generative\nmodeling approaches, but they require many sampling steps. Consistency models\ncan distill these models into efficient one-step generators; however, unlike\nflow- and diffusion-based methods, their performance inevitably degrades when\nincreasing the number of steps, which we show both analytically and\nempirically. Flow maps generalize these approaches by connecting any two noise\nlevels in a single step and remain effective across all step counts. In this\npaper, we introduce two new continuous-time objectives for training flow maps,\nalong with additional novel training techniques, generalizing existing\nconsistency and flow matching objectives. We further demonstrate that\nautoguidance can improve performance, using a low-quality model for guidance\nduring distillation, and an additional boost can be achieved by adversarial\nfinetuning, with minimal loss in sample diversity. We extensively validate our\nflow map models, called Align Your Flow, on challenging image generation\nbenchmarks and achieve state-of-the-art few-step generation performance on both\nImageNet 64x64 and 512x512, using small and efficient neural networks. Finally,\nwe show text-to-image flow map models that outperform all existing\nnon-adversarially trained few-step samplers in text-conditioned synthesis.",
      "upvotes": 2,
      "discussionId": "68521f8a0164cd1316710484",
      "projectPage": "https://research.nvidia.com/labs/toronto-ai/AlignYourFlow/",
      "ai_summary": "Flow maps, introduced with new continuous-time objectives and training techniques, achieve state-of-the-art performance in few-step image and text-to-image generation.",
      "ai_keywords": [
        "diffusion models",
        "flow-based models",
        "consistency models",
        "flow maps",
        "noise levels",
        "autoguidance",
        "adversarial finetuning",
        "Align Your Flow",
        "ImageNet",
        "text-to-image synthesis"
      ]
    },
    "publishedAt": "2025-06-17T11:06:07.000Z",
    "title": "Align Your Flow: Scaling Continuous-Time Flow Map Distillation",
    "summary": "Diffusion- and flow-based models have emerged as state-of-the-art generative\nmodeling approaches, but they require many sampling steps. Consistency models\ncan distill these models into efficient one-step generators; however, unlike\nflow- and diffusion-based methods, their performance inevitably degrades when\nincreasing the number of steps, which we show both analytically and\nempirically. Flow maps generalize these approaches by connecting any two noise\nlevels in a single step and remain effective across all step counts. In this\npaper, we introduce two new continuous-time objectives for training flow maps,\nalong with additional novel training techniques, generalizing existing\nconsistency and flow matching objectives. We further demonstrate that\nautoguidance can improve performance, using a low-quality model for guidance\nduring distillation, and an additional boost can be achieved by adversarial\nfinetuning, with minimal loss in sample diversity. We extensively validate our\nflow map models, called Align Your Flow, on challenging image generation\nbenchmarks and achieve state-of-the-art few-step generation performance on both\nImageNet 64x64 and 512x512, using small and efficient neural networks. Finally,\nwe show text-to-image flow map models that outperform all existing\nnon-adversarially trained few-step samplers in text-conditioned synthesis.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/656015f28138827138c70858/_yuUKzjzngNZzaUJCVwjR.jpeg",
      "https://cdn-uploads.huggingface.co/production/uploads/656015f28138827138c70858/2FmUhOtaDCtAzFkNEx6Di.jpeg",
      "https://cdn-uploads.huggingface.co/production/uploads/656015f28138827138c70858/MC_ZLBqT81dOpANroMLrm.jpeg",
      "https://cdn-uploads.huggingface.co/production/uploads/656015f28138827138c70858/degPI6Z5IEf4v_y2_UL9V.jpeg"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.14603.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "656015f28138827138c70858",
      "avatarUrl": "/avatars/b8471ae4d80f078c7c928fc3d8f49126.svg",
      "fullname": "Amirmojtaba Sabour",
      "name": "amsabour",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.14755",
      "authors": [
        {
          "_id": "685225250164cd13167104aa",
          "name": "Zhengxiang Cheng",
          "hidden": false
        },
        {
          "_id": "685225250164cd13167104ab",
          "name": "Dongping Chen",
          "hidden": false
        },
        {
          "_id": "685225250164cd13167104ac",
          "name": "Mingyang Fu",
          "hidden": false
        },
        {
          "_id": "685225250164cd13167104ad",
          "name": "Tianyi Zhou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-17T17:50:16.000Z",
      "submittedOnDailyAt": "2025-06-18T01:05:46.554Z",
      "title": "Optimizing Length Compression in Large Reasoning Models",
      "submittedOnDailyBy": {
        "_id": "647f5af5b0e96764589f3b2a",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/VJ4cDyjp5M3V5WmI5gPIU.jpeg",
        "isPro": false,
        "fullname": "Tianyi Zhou",
        "user": "zhoutianyi",
        "type": "user"
      },
      "summary": "Large Reasoning Models (LRMs) have achieved remarkable success, yet they\noften suffer from producing unnecessary and verbose reasoning chains. We\nidentify a core aspect of this issue as \"invalid thinking\" -- models tend to\nrepeatedly double-check their work after having derived the correct answer. To\naddress this specific inefficiency, we move beyond the general principles of\nEfficacy and Efficiency to propose two new, fine-grained principles: Brevity,\nwhich advocates for eliminating redundancy, and Sufficiency, which ensures\ncritical reasoning steps are preserved. Guided by these principles, we\nintroduce LC-R1, a post-training method based on Group Relative Policy\nOptimization (GRPO). LC-R1 employs a novel combination of a Length Reward for\noverall conciseness and a Compress Reward that is specifically designed to\nremove the invalid portion of the thinking process. Extensive experiments on\nmultiple reasoning benchmarks demonstrate that LC-R1 achieves a significant\nreduction in sequence length (~50%) with only a marginal (~2%) drop in\naccuracy, achieving a favorable trade-off point on the Pareto frontier that\nprioritizes high compression. Our analysis further validates the robustness of\nLC-R1 and provides valuable insights for developing more powerful yet\ncomputationally efficient LRMs. Our code is released at\nhttps://github.com/zxiangx/LC-R1.",
      "upvotes": 0,
      "discussionId": "685225250164cd13167104ae",
      "githubRepo": "https://github.com/zxiangx/LC-R1",
      "ai_summary": "LC-R1, a post-training method guided by Brevity and Sufficiency principles, reduces unnecessary reasoning in Large Reasoning Models with minimal accuracy loss.",
      "ai_keywords": [
        "Large Reasoning Models",
        "LRM",
        "post-training method",
        "Group Relative Policy Optimization",
        "GRPO",
        "Length Reward",
        "Compress Reward",
        "reasoning benchmarks",
        "Pareto frontier"
      ]
    },
    "publishedAt": "2025-06-17T13:50:16.000Z",
    "title": "Optimizing Length Compression in Large Reasoning Models",
    "summary": "Large Reasoning Models (LRMs) have achieved remarkable success, yet they\noften suffer from producing unnecessary and verbose reasoning chains. We\nidentify a core aspect of this issue as \"invalid thinking\" -- models tend to\nrepeatedly double-check their work after having derived the correct answer. To\naddress this specific inefficiency, we move beyond the general principles of\nEfficacy and Efficiency to propose two new, fine-grained principles: Brevity,\nwhich advocates for eliminating redundancy, and Sufficiency, which ensures\ncritical reasoning steps are preserved. Guided by these principles, we\nintroduce LC-R1, a post-training method based on Group Relative Policy\nOptimization (GRPO). LC-R1 employs a novel combination of a Length Reward for\noverall conciseness and a Compress Reward that is specifically designed to\nremove the invalid portion of the thinking process. Extensive experiments on\nmultiple reasoning benchmarks demonstrate that LC-R1 achieves a significant\nreduction in sequence length (~50%) with only a marginal (~2%) drop in\naccuracy, achieving a favorable trade-off point on the Pareto frontier that\nprioritizes high compression. Our analysis further validates the robustness of\nLC-R1 and provides valuable insights for developing more powerful yet\ncomputationally efficient LRMs. Our code is released at\nhttps://github.com/zxiangx/LC-R1.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.14755.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "647f5af5b0e96764589f3b2a",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/VJ4cDyjp5M3V5WmI5gPIU.jpeg",
      "fullname": "Tianyi Zhou",
      "name": "zhoutianyi",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 14
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.13387",
      "authors": [
        {
          "_id": "6852239f0164cd13167104a4",
          "name": "Beilei Cui",
          "hidden": false
        },
        {
          "_id": "6852239f0164cd13167104a5",
          "name": "Yiming Huang",
          "hidden": false
        },
        {
          "_id": "6852239f0164cd13167104a6",
          "name": "Long Bai",
          "hidden": false
        },
        {
          "_id": "6852239f0164cd13167104a7",
          "name": "Hongliang Ren",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-16T11:50:00.000Z",
      "submittedOnDailyAt": "2025-06-18T00:57:15.579Z",
      "title": "TR2M: Transferring Monocular Relative Depth to Metric Depth with\n  Language Descriptions and Scale-Oriented Contrast",
      "submittedOnDailyBy": {
        "_id": "68518fb45452a74491857c5b",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/SOYIB8T0LsNZM3ORcHXlr.png",
        "isPro": false,
        "fullname": "Beilei Cui",
        "user": "BeileiCui",
        "type": "user"
      },
      "summary": "This work presents a generalizable framework to transfer relative depth to\nmetric depth. Current monocular depth estimation methods are mainly divided\ninto metric depth estimation (MMDE) and relative depth estimation (MRDE). MMDEs\nestimate depth in metric scale but are often limited to a specific domain.\nMRDEs generalize well across different domains, but with uncertain scales which\nhinders downstream applications. To this end, we aim to build up a framework to\nsolve scale uncertainty and transfer relative depth to metric depth. Previous\nmethods used language as input and estimated two factors for conducting\nrescaling. Our approach, TR2M, utilizes both text description and image as\ninputs and estimates two rescale maps to transfer relative depth to metric\ndepth at pixel level. Features from two modalities are fused with a\ncross-modality attention module to better capture scale information. A strategy\nis designed to construct and filter confident pseudo metric depth for more\ncomprehensive supervision. We also develop scale-oriented contrastive learning\nto utilize depth distribution as guidance to enforce the model learning about\nintrinsic knowledge aligning with the scale distribution. TR2M only exploits a\nsmall number of trainable parameters to train on datasets in various domains\nand experiments not only demonstrate TR2M's great performance in seen datasets\nbut also reveal superior zero-shot capabilities on five unseen datasets. We\nshow the huge potential in pixel-wise transferring relative depth to metric\ndepth with language assistance. (Code is available at:\nhttps://github.com/BeileiCui/TR2M)",
      "upvotes": 0,
      "discussionId": "6852239f0164cd13167104a8",
      "ai_summary": "A framework, TR2M, uses multimodal inputs to rescale relative depth to metric depth, enhancing performance across various datasets through cross-modality attention and contrastive learning.",
      "ai_keywords": [
        "relative depth estimation",
        "metric depth estimation",
        "cross-modality attention",
        "contrastive learning",
        "rescale maps",
        "pseudo metric depth",
        "intrinsically aligned scale distribution"
      ]
    },
    "publishedAt": "2025-06-16T07:50:00.000Z",
    "title": "TR2M: Transferring Monocular Relative Depth to Metric Depth with\n  Language Descriptions and Scale-Oriented Contrast",
    "summary": "This work presents a generalizable framework to transfer relative depth to\nmetric depth. Current monocular depth estimation methods are mainly divided\ninto metric depth estimation (MMDE) and relative depth estimation (MRDE). MMDEs\nestimate depth in metric scale but are often limited to a specific domain.\nMRDEs generalize well across different domains, but with uncertain scales which\nhinders downstream applications. To this end, we aim to build up a framework to\nsolve scale uncertainty and transfer relative depth to metric depth. Previous\nmethods used language as input and estimated two factors for conducting\nrescaling. Our approach, TR2M, utilizes both text description and image as\ninputs and estimates two rescale maps to transfer relative depth to metric\ndepth at pixel level. Features from two modalities are fused with a\ncross-modality attention module to better capture scale information. A strategy\nis designed to construct and filter confident pseudo metric depth for more\ncomprehensive supervision. We also develop scale-oriented contrastive learning\nto utilize depth distribution as guidance to enforce the model learning about\nintrinsic knowledge aligning with the scale distribution. TR2M only exploits a\nsmall number of trainable parameters to train on datasets in various domains\nand experiments not only demonstrate TR2M's great performance in seen datasets\nbut also reveal superior zero-shot capabilities on five unseen datasets. We\nshow the huge potential in pixel-wise transferring relative depth to metric\ndepth with language assistance. (Code is available at:\nhttps://github.com/BeileiCui/TR2M)",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.13387.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "68518fb45452a74491857c5b",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/SOYIB8T0LsNZM3ORcHXlr.png",
      "fullname": "Beilei Cui",
      "name": "BeileiCui",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  }
]