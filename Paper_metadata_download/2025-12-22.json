[
  {
    "paper": {
      "id": "2512.17260",
      "authors": [
        {
          "_id": "6948afc434f46eaf46cbb1f1",
          "name": "Jiangjie Chen",
          "hidden": false
        },
        {
          "_id": "6948afc434f46eaf46cbb1f2",
          "name": "Wenxiang Chen",
          "hidden": false
        },
        {
          "_id": "6948afc434f46eaf46cbb1f3",
          "name": "Jiacheng Du",
          "hidden": false
        },
        {
          "_id": "6948afc434f46eaf46cbb1f4",
          "name": "Jinyi Hu",
          "hidden": false
        },
        {
          "_id": "6948afc434f46eaf46cbb1f5",
          "name": "Zhicheng Jiang",
          "hidden": false
        },
        {
          "_id": "6948afc434f46eaf46cbb1f6",
          "name": "Allan Jie",
          "hidden": false
        },
        {
          "_id": "6948afc434f46eaf46cbb1f7",
          "name": "Xiaoran Jin",
          "hidden": false
        },
        {
          "_id": "6948afc434f46eaf46cbb1f8",
          "name": "Xing Jin",
          "hidden": false
        },
        {
          "_id": "6948afc434f46eaf46cbb1f9",
          "name": "Chenggang Li",
          "hidden": false
        },
        {
          "_id": "6948afc434f46eaf46cbb1fa",
          "name": "Wenlei Shi",
          "hidden": false
        },
        {
          "_id": "6948afc434f46eaf46cbb1fb",
          "name": "Zhihong Wang",
          "hidden": false
        },
        {
          "_id": "6948afc434f46eaf46cbb1fc",
          "name": "Mingxuan Wang",
          "hidden": false
        },
        {
          "_id": "6948afc434f46eaf46cbb1fd",
          "name": "Chenrui Wei",
          "hidden": false
        },
        {
          "_id": "6948afc434f46eaf46cbb1fe",
          "name": "Shufa Wei",
          "hidden": false
        },
        {
          "_id": "6948afc434f46eaf46cbb1ff",
          "name": "Huajian Xin",
          "hidden": false
        },
        {
          "_id": "6948afc434f46eaf46cbb200",
          "name": "Fan Yang",
          "hidden": false
        },
        {
          "_id": "6948afc434f46eaf46cbb201",
          "name": "Weihao Gao",
          "hidden": false
        },
        {
          "_id": "6948afc434f46eaf46cbb202",
          "name": "Zheng Yuan",
          "hidden": false
        },
        {
          "_id": "6948afc434f46eaf46cbb203",
          "name": "Tianyang Zhan",
          "hidden": false
        },
        {
          "_id": "6948afc434f46eaf46cbb204",
          "name": "Zeyu Zheng",
          "hidden": false
        },
        {
          "_id": "6948afc434f46eaf46cbb205",
          "name": "Tianxi Zhou",
          "hidden": false
        },
        {
          "_id": "6948afc434f46eaf46cbb206",
          "name": "Thomas Hanwen Zhu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-12-19T06:19:55.000Z",
      "submittedOnDailyAt": "2025-12-22T00:11:16.085Z",
      "title": "Seed-Prover 1.5: Mastering Undergraduate-Level Theorem Proving via Learning from Experience",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Large language models have recently made significant progress to generate rigorous mathematical proofs. In contrast, utilizing LLMs for theorem proving in formal languages (such as Lean) remains challenging and computationally expensive, particularly when addressing problems at the undergraduate level and beyond. In this work, we present Seed-Prover 1.5, a formal theorem-proving model trained via large-scale agentic reinforcement learning, alongside an efficient test-time scaling (TTS) workflow. Through extensive interactions with Lean and other tools, the model continuously accumulates experience during the RL process, substantially enhancing the capability and efficiency of formal theorem proving. Furthermore, leveraging recent advancements in natural language proving, our TTS workflow efficiently bridges the gap between natural and formal languages. Compared to state-of-the-art methods, Seed-Prover 1.5 achieves superior performance with a smaller compute budget. It solves 88\\% of PutnamBench (undergraduate-level), 80\\% of Fate-H (graduate-level), and 33\\% of Fate-X (PhD-level) problems. Notably, using our system, we solved 11 out of 12 problems from Putnam 2025 within 9 hours. Our findings suggest that scaling learning from experience, driven by high-quality formal feedback, holds immense potential for the future of formal mathematical reasoning.",
      "upvotes": 10,
      "discussionId": "6948afc534f46eaf46cbb207",
      "ai_summary": "Seed-Prover 1.5, a formal theorem-proving model using large-scale agentic reinforcement learning and an efficient test-time scaling workflow, demonstrates superior performance in solving mathematical problems across various levels with reduced computational resources.",
      "ai_keywords": [
        "large language models",
        "theorem proving",
        "formal languages",
        "Lean",
        "reinforcement learning",
        "test-time scaling",
        "PutnamBench",
        "Fate-H",
        "Fate-X",
        "formal mathematical reasoning"
      ],
      "organization": {
        "_id": "67d1140985ea0644e2f14b99",
        "name": "ByteDance-Seed",
        "fullname": "ByteDance Seed",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"
      }
    },
    "publishedAt": "2025-12-19T01:19:55.000Z",
    "title": "Seed-Prover 1.5: Mastering Undergraduate-Level Theorem Proving via Learning from Experience",
    "summary": "Large language models have recently made significant progress to generate rigorous mathematical proofs. In contrast, utilizing LLMs for theorem proving in formal languages (such as Lean) remains challenging and computationally expensive, particularly when addressing problems at the undergraduate level and beyond. In this work, we present Seed-Prover 1.5, a formal theorem-proving model trained via large-scale agentic reinforcement learning, alongside an efficient test-time scaling (TTS) workflow. Through extensive interactions with Lean and other tools, the model continuously accumulates experience during the RL process, substantially enhancing the capability and efficiency of formal theorem proving. Furthermore, leveraging recent advancements in natural language proving, our TTS workflow efficiently bridges the gap between natural and formal languages. Compared to state-of-the-art methods, Seed-Prover 1.5 achieves superior performance with a smaller compute budget. It solves 88\\% of PutnamBench (undergraduate-level), 80\\% of Fate-H (graduate-level), and 33\\% of Fate-X (PhD-level) problems. Notably, using our system, we solved 11 out of 12 problems from Putnam 2025 within 9 hours. Our findings suggest that scaling learning from experience, driven by high-quality formal feedback, holds immense potential for the future of formal mathematical reasoning.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.17260.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 188
    },
    "organization": {
      "_id": "67d1140985ea0644e2f14b99",
      "name": "ByteDance-Seed",
      "fullname": "ByteDance Seed",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2512.17901",
      "authors": [
        {
          "_id": "6948af7534f46eaf46cbb1da",
          "name": "Junyu Zhang",
          "hidden": false
        },
        {
          "_id": "6948af7534f46eaf46cbb1db",
          "name": "Yifan Sun",
          "hidden": false
        },
        {
          "_id": "6948af7534f46eaf46cbb1dc",
          "name": "Tianang Leng",
          "hidden": false
        },
        {
          "_id": "6948af7534f46eaf46cbb1dd",
          "name": "Jingyan Shen",
          "hidden": false
        },
        {
          "_id": "6948af7534f46eaf46cbb1de",
          "name": "Liu Ziyin",
          "hidden": false
        },
        {
          "_id": "6948af7534f46eaf46cbb1df",
          "name": "Paul Pu Liang",
          "hidden": false
        },
        {
          "_id": "6948af7534f46eaf46cbb1e0",
          "name": "Huan Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-12-19T18:59:11.000Z",
      "submittedOnDailyAt": "2025-12-22T00:10:07.525Z",
      "title": "When Reasoning Meets Its Laws",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Despite the superior performance of Large Reasoning Models (LRMs), their reasoning behaviors are often counterintuitive, leading to suboptimal reasoning capabilities. To theoretically formalize the desired reasoning behaviors, this paper presents the Laws of Reasoning (LoRe), a unified framework that characterizes intrinsic reasoning patterns in LRMs. We first propose compute law with the hypothesis that the reasoning compute should scale linearly with question complexity. Beyond compute, we extend LoRe with a supplementary accuracy law. Since the question complexity is difficult to quantify in practice, we examine these hypotheses by two properties of the laws, monotonicity and compositionality. We therefore introduce LoRe-Bench, a benchmark that systematically measures these two tractable properties for large reasoning models. Evaluation shows that most reasoning models exhibit reasonable monotonicity but lack compositionality. In response, we develop an effective finetuning approach that enforces compute-law compositionality. Extensive empirical studies demonstrate that better compliance with compute laws yields consistently improved reasoning performance on multiple benchmarks, and uncovers synergistic effects across properties and laws. Project page: https://lore-project.github.io/",
      "upvotes": 7,
      "discussionId": "6948af7534f46eaf46cbb1e1",
      "projectPage": "https://lore-project.github.io/",
      "githubRepo": "https://github.com/ASTRAL-Group/LoRe",
      "githubRepoAddedBy": "user",
      "ai_summary": "A framework called Laws of Reasoning (LoRe) is introduced to theoretically define desired reasoning behaviors in Large Reasoning Models, with a focus on compute and accuracy laws, and a benchmark (LoRe-Bench) to measure these properties.",
      "ai_keywords": [
        "Laws of Reasoning",
        "LoRe",
        "compute law",
        "accuracy law",
        "question complexity",
        "monotonicity",
        "compositionality",
        "LoRe-Bench",
        "finetuning approach",
        "compute-law compositionality"
      ]
    },
    "publishedAt": "2025-12-19T13:59:11.000Z",
    "title": "When Reasoning Meets Its Laws",
    "summary": "Despite the superior performance of Large Reasoning Models (LRMs), their reasoning behaviors are often counterintuitive, leading to suboptimal reasoning capabilities. To theoretically formalize the desired reasoning behaviors, this paper presents the Laws of Reasoning (LoRe), a unified framework that characterizes intrinsic reasoning patterns in LRMs. We first propose compute law with the hypothesis that the reasoning compute should scale linearly with question complexity. Beyond compute, we extend LoRe with a supplementary accuracy law. Since the question complexity is difficult to quantify in practice, we examine these hypotheses by two properties of the laws, monotonicity and compositionality. We therefore introduce LoRe-Bench, a benchmark that systematically measures these two tractable properties for large reasoning models. Evaluation shows that most reasoning models exhibit reasonable monotonicity but lack compositionality. In response, we develop an effective finetuning approach that enforces compute-law compositionality. Extensive empirical studies demonstrate that better compliance with compute laws yields consistently improved reasoning performance on multiple benchmarks, and uncovers synergistic effects across properties and laws. Project page: https://lore-project.github.io/",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.17901.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 188
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2512.17419",
      "authors": [
        {
          "_id": "6948b03934f46eaf46cbb209",
          "name": "Lilin Wang",
          "hidden": false
        },
        {
          "_id": "6948b03934f46eaf46cbb20a",
          "name": "Lucas Ramalho",
          "hidden": false
        },
        {
          "_id": "6948b03934f46eaf46cbb20b",
          "name": "Alan Celestino",
          "hidden": false
        },
        {
          "_id": "6948b03934f46eaf46cbb20c",
          "name": "Phuc Anthony Pham",
          "hidden": false
        },
        {
          "_id": "6948b03934f46eaf46cbb20d",
          "name": "Yu Liu",
          "hidden": false
        },
        {
          "_id": "6948b03934f46eaf46cbb20e",
          "name": "Umang Kumar Sinha",
          "hidden": false
        },
        {
          "_id": "6948b03934f46eaf46cbb20f",
          "name": "Andres Portillo",
          "hidden": false
        },
        {
          "_id": "6948b03934f46eaf46cbb210",
          "name": "Onassis Osunwa",
          "hidden": false
        },
        {
          "_id": "6948b03934f46eaf46cbb211",
          "name": "Gabriel Maduekwe",
          "hidden": false
        }
      ],
      "publishedAt": "2025-12-19T10:16:51.000Z",
      "submittedOnDailyAt": "2025-12-22T00:13:29.948Z",
      "title": "SWE-Bench++: A Framework for the Scalable Generation of Software Engineering Benchmarks from Open-Source Repositories",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Benchmarks like SWE-bench have standardized the evaluation of Large Language Models (LLMs) on repository-level software engineering tasks. However, these efforts remain limited by manual curation, static datasets, and a focus on Python-based bug fixes. We introduce SWE-Bench++, an automated framework that generates repository-level coding tasks from open-source GitHub projects. Unlike synthetic approaches, our pipeline harvests live pull requests to cover both bug fixes and feature requests across 11 languages. SWE-Bench++ turns GitHub pull requests (PRs) into reproducible, execution-based tasks via four stages: programmatic sourcing, environment synthesis, test oracle extraction, and quality assurance. A final hint-guided trajectory synthesis step converts instances that strong models fail on into training trajectories. Our initial benchmark consists of 11,133 instances from 3,971 repositories across 11 languages. On a subset of 1,782 instances of this benchmark, today's strongest models perform as follows: claude-sonnet-4.5 achieves 36.20% pass@10, gpt-5-2025-08-07 34.57%, gemini/gemini-2.5-pro 24.92%, and gpt-4o 16.89%. We further demonstrate the utility of our dataset by showing that fine-tuning on SWE-Bench++ instances yields measurable improvements on the SWE-bench Multilingual benchmark. SWE-Bench++ provides a scalable, multilingual benchmark for evaluating and improving repository-level code generation.",
      "upvotes": 1,
      "discussionId": "6948b03a34f46eaf46cbb212",
      "projectPage": "https://research.turing.com/swebench",
      "ai_summary": "SWE-Bench++ is an automated framework generating repository-level coding tasks from live GitHub pull requests, offering a scalable, multilingual benchmark for evaluating and improving code generation models.",
      "ai_keywords": [
        "Large Language Models",
        "SWE-bench",
        "SWE-Bench++",
        "GitHub pull requests",
        "programmatic sourcing",
        "environment synthesis",
        "test oracle extraction",
        "quality assurance",
        "hint-guided trajectory synthesis",
        "pass@10",
        "SWE-bench Multilingual"
      ],
      "organization": {
        "_id": "64f91c328a234f114e40735d",
        "name": "TuringEnterprises",
        "fullname": "Turing Inc.",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/64f8ef1db0f25f3fe3c07b08/1fw2pc1Vl9LMGTL4Jirgi.jpeg"
      }
    },
    "publishedAt": "2025-12-19T05:16:51.000Z",
    "title": "SWE-Bench++: A Framework for the Scalable Generation of Software Engineering Benchmarks from Open-Source Repositories",
    "summary": "Benchmarks like SWE-bench have standardized the evaluation of Large Language Models (LLMs) on repository-level software engineering tasks. However, these efforts remain limited by manual curation, static datasets, and a focus on Python-based bug fixes. We introduce SWE-Bench++, an automated framework that generates repository-level coding tasks from open-source GitHub projects. Unlike synthetic approaches, our pipeline harvests live pull requests to cover both bug fixes and feature requests across 11 languages. SWE-Bench++ turns GitHub pull requests (PRs) into reproducible, execution-based tasks via four stages: programmatic sourcing, environment synthesis, test oracle extraction, and quality assurance. A final hint-guided trajectory synthesis step converts instances that strong models fail on into training trajectories. Our initial benchmark consists of 11,133 instances from 3,971 repositories across 11 languages. On a subset of 1,782 instances of this benchmark, today's strongest models perform as follows: claude-sonnet-4.5 achieves 36.20% pass@10, gpt-5-2025-08-07 34.57%, gemini/gemini-2.5-pro 24.92%, and gpt-4o 16.89%. We further demonstrate the utility of our dataset by showing that fine-tuning on SWE-Bench++ instances yields measurable improvements on the SWE-bench Multilingual benchmark. SWE-Bench++ provides a scalable, multilingual benchmark for evaluating and improving repository-level code generation.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.17419.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 188
    },
    "organization": {
      "_id": "64f91c328a234f114e40735d",
      "name": "TuringEnterprises",
      "fullname": "Turing Inc.",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/64f8ef1db0f25f3fe3c07b08/1fw2pc1Vl9LMGTL4Jirgi.jpeg"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2512.17012",
      "authors": [
        {
          "_id": "6948aa3d34f46eaf46cbb1cb",
          "name": "Chiao-An Yang",
          "hidden": false
        },
        {
          "_id": "6948aa3d34f46eaf46cbb1cc",
          "name": "Ryo Hachiuma",
          "hidden": false
        },
        {
          "_id": "6948aa3d34f46eaf46cbb1cd",
          "name": "Sifei Liu",
          "hidden": false
        },
        {
          "_id": "6948aa3d34f46eaf46cbb1ce",
          "name": "Subhashree Radhakrishnan",
          "hidden": false
        },
        {
          "_id": "6948aa3d34f46eaf46cbb1cf",
          "name": "Raymond A. Yeh",
          "hidden": false
        },
        {
          "_id": "6948aa3d34f46eaf46cbb1d0",
          "name": "Yu-Chiang Frank Wang",
          "hidden": false
        },
        {
          "_id": "6948aa3d34f46eaf46cbb1d1",
          "name": "Min-Hung Chen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-12-18T19:13:44.000Z",
      "submittedOnDailyAt": "2025-12-22T00:08:35.679Z",
      "title": "4D-RGPT: Toward Region-level 4D Understanding via Perceptual Distillation",
      "submittedOnDailyBy": {
        "_id": "64ae22dd1aee69ece065cdcd",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ae22dd1aee69ece065cdcd/JG7QaHIrr4i2k4uwR4pZK.png",
        "isPro": false,
        "fullname": "Min-Hung Chen",
        "user": "cmhungsteve",
        "type": "user"
      },
      "summary": "Despite advances in Multimodal LLMs (MLLMs), their ability to reason over 3D structures and temporal dynamics remains limited, constrained by weak 4D perception and temporal understanding. Existing 3D and 4D Video Question Answering (VQA) benchmarks also emphasize static scenes and lack region-level prompting. We tackle these issues by introducing: (a) 4D-RGPT, a specialized MLLM designed to capture 4D representations from video inputs with enhanced temporal perception; (b) Perceptual 4D Distillation (P4D), a training framework that transfers 4D representations from a frozen expert model into 4D-RGPT for comprehensive 4D perception; and (c) R4D-Bench, a benchmark for depth-aware dynamic scenes with region-level prompting, built via a hybrid automated and human-verified pipeline. Our 4D-RGPT achieves notable improvements on both existing 4D VQA benchmarks and the proposed R4D-Bench benchmark.",
      "upvotes": 1,
      "discussionId": "6948aa3d34f46eaf46cbb1d2",
      "projectPage": "https://ca-joe-yang.github.io/resource/projects/4D_RGPT",
      "ai_summary": "4D-RGPT, a specialized multimodal LLM, enhances 4D perception in video inputs through Perceptual 4D Distillation and is evaluated on R4D-Bench, a new benchmark for depth-aware dynamic scenes.",
      "ai_keywords": [
        "4D-RGPT",
        "Multimodal LLMs",
        "4D Video Question Answering",
        "4D representations",
        "Perceptual 4D Distillation",
        "R4D-Bench",
        "depth-aware",
        "dynamic scenes",
        "region-level prompting"
      ],
      "organization": {
        "_id": "60262b67268c201cdc8b7d43",
        "name": "nvidia",
        "fullname": "NVIDIA",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"
      }
    },
    "publishedAt": "2025-12-18T14:13:44.000Z",
    "title": "4D-RGPT: Toward Region-level 4D Understanding via Perceptual Distillation",
    "summary": "Despite advances in Multimodal LLMs (MLLMs), their ability to reason over 3D structures and temporal dynamics remains limited, constrained by weak 4D perception and temporal understanding. Existing 3D and 4D Video Question Answering (VQA) benchmarks also emphasize static scenes and lack region-level prompting. We tackle these issues by introducing: (a) 4D-RGPT, a specialized MLLM designed to capture 4D representations from video inputs with enhanced temporal perception; (b) Perceptual 4D Distillation (P4D), a training framework that transfers 4D representations from a frozen expert model into 4D-RGPT for comprehensive 4D perception; and (c) R4D-Bench, a benchmark for depth-aware dynamic scenes with region-level prompting, built via a hybrid automated and human-verified pipeline. Our 4D-RGPT achieves notable improvements on both existing 4D VQA benchmarks and the proposed R4D-Bench benchmark.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.17012.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64ae22dd1aee69ece065cdcd",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ae22dd1aee69ece065cdcd/JG7QaHIrr4i2k4uwR4pZK.png",
      "fullname": "Min-Hung Chen",
      "name": "cmhungsteve",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 8
    },
    "organization": {
      "_id": "60262b67268c201cdc8b7d43",
      "name": "nvidia",
      "fullname": "NVIDIA",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2512.16969",
      "authors": [
        {
          "_id": "6948b09934f46eaf46cbb214",
          "name": "Wanghan Xu",
          "hidden": false
        },
        {
          "_id": "6948b09934f46eaf46cbb215",
          "name": "Yuhao Zhou",
          "hidden": false
        },
        {
          "_id": "6948b09934f46eaf46cbb216",
          "name": "Yifan Zhou",
          "hidden": false
        },
        {
          "_id": "6948b09934f46eaf46cbb217",
          "name": "Qinglong Cao",
          "hidden": false
        },
        {
          "_id": "6948b09934f46eaf46cbb218",
          "name": "Shuo Li",
          "hidden": false
        },
        {
          "_id": "6948b09934f46eaf46cbb219",
          "name": "Jia Bu",
          "hidden": false
        },
        {
          "_id": "6948b09934f46eaf46cbb21a",
          "name": "Bo Liu",
          "hidden": false
        },
        {
          "_id": "6948b09934f46eaf46cbb21b",
          "name": "Yixin Chen",
          "hidden": false
        },
        {
          "_id": "6948b09934f46eaf46cbb21c",
          "name": "Xuming He",
          "hidden": false
        },
        {
          "_id": "6948b09934f46eaf46cbb21d",
          "name": "Xiangyu Zhao",
          "hidden": false
        },
        {
          "_id": "6948b09934f46eaf46cbb21e",
          "name": "Xiang Zhuang",
          "hidden": false
        },
        {
          "_id": "6948b09934f46eaf46cbb21f",
          "name": "Fengxiang Wang",
          "hidden": false
        },
        {
          "_id": "6948b09934f46eaf46cbb220",
          "name": "Zhiwang Zhou",
          "hidden": false
        },
        {
          "_id": "6948b09934f46eaf46cbb221",
          "name": "Qiantai Feng",
          "hidden": false
        },
        {
          "_id": "6948b09934f46eaf46cbb222",
          "name": "Wenxuan Huang",
          "hidden": false
        },
        {
          "_id": "6948b09934f46eaf46cbb223",
          "name": "Jiaqi Wei",
          "hidden": false
        },
        {
          "_id": "6948b09934f46eaf46cbb224",
          "name": "Hao Wu",
          "hidden": false
        },
        {
          "_id": "6948b09934f46eaf46cbb225",
          "name": "Yuejin Yang",
          "hidden": false
        },
        {
          "_id": "6948b09934f46eaf46cbb226",
          "name": "Guangshuai Wang",
          "hidden": false
        },
        {
          "_id": "6948b09934f46eaf46cbb227",
          "name": "Sheng Xu",
          "hidden": false
        },
        {
          "_id": "6948b09934f46eaf46cbb228",
          "name": "Ziyan Huang",
          "hidden": false
        },
        {
          "_id": "6948b09934f46eaf46cbb229",
          "name": "Xinyao Liu",
          "hidden": false
        },
        {
          "_id": "6948b09934f46eaf46cbb22a",
          "name": "Jiyao Liu",
          "hidden": false
        },
        {
          "_id": "6948b09934f46eaf46cbb22b",
          "name": "Cheng Tang",
          "hidden": false
        },
        {
          "_id": "6948b09934f46eaf46cbb22c",
          "name": "Wei Li",
          "hidden": false
        },
        {
          "_id": "6948b09934f46eaf46cbb22d",
          "name": "Ying Chen",
          "hidden": false
        },
        {
          "_id": "6948b09934f46eaf46cbb22e",
          "name": "Junzhi Ning",
          "hidden": false
        },
        {
          "_id": "6948b09934f46eaf46cbb22f",
          "name": "Pengfei Jiang",
          "hidden": false
        },
        {
          "_id": "6948b09934f46eaf46cbb230",
          "name": "Chenglong Ma",
          "hidden": false
        },
        {
          "_id": "6948b09934f46eaf46cbb231",
          "name": "Ye Du",
          "hidden": false
        },
        {
          "_id": "6948b09934f46eaf46cbb232",
          "name": "Changkai Ji",
          "hidden": false
        },
        {
          "_id": "6948b09934f46eaf46cbb233",
          "name": "Huihui Xu",
          "hidden": false
        },
        {
          "_id": "6948b09934f46eaf46cbb234",
          "name": "Ming Hu",
          "hidden": false
        },
        {
          "_id": "6948b09934f46eaf46cbb235",
          "name": "Jiangbin Zheng",
          "hidden": false
        },
        {
          "_id": "6948b09934f46eaf46cbb236",
          "name": "Xin Chen",
          "hidden": false
        },
        {
          "_id": "6948b09934f46eaf46cbb237",
          "name": "Yucheng Wu",
          "hidden": false
        },
        {
          "_id": "6948b09934f46eaf46cbb238",
          "name": "Feifei Jiang",
          "hidden": false
        },
        {
          "_id": "6948b09934f46eaf46cbb239",
          "name": "Xi Chen",
          "hidden": false
        },
        {
          "_id": "6948b09934f46eaf46cbb23a",
          "name": "Xiangru Tang",
          "hidden": false
        },
        {
          "_id": "6948b09934f46eaf46cbb23b",
          "name": "Yuchen Fu",
          "hidden": false
        },
        {
          "_id": "6948b09934f46eaf46cbb23c",
          "name": "Yingzhou Lu",
          "hidden": false
        },
        {
          "_id": "6948b09934f46eaf46cbb23d",
          "name": "Yuanyuan Zhang",
          "hidden": false
        },
        {
          "_id": "6948b09934f46eaf46cbb23e",
          "name": "Lihao Sun",
          "hidden": false
        },
        {
          "_id": "6948b09934f46eaf46cbb23f",
          "name": "Chengbo Li",
          "hidden": false
        },
        {
          "_id": "6948b09934f46eaf46cbb240",
          "name": "Jinzhe Ma",
          "hidden": false
        },
        {
          "_id": "6948b09934f46eaf46cbb241",
          "name": "Wanhao Liu",
          "hidden": false
        },
        {
          "_id": "6948b09934f46eaf46cbb242",
          "name": "Yating Liu",
          "hidden": false
        },
        {
          "_id": "6948b09934f46eaf46cbb243",
          "name": "Kuo-Cheng Wu",
          "hidden": false
        },
        {
          "_id": "6948b09934f46eaf46cbb244",
          "name": "Shengdu Chai",
          "hidden": false
        },
        {
          "_id": "6948b09934f46eaf46cbb245",
          "name": "Yizhou Wang",
          "hidden": false
        },
        {
          "_id": "6948b09934f46eaf46cbb246",
          "name": "Ouwen Zhangjin",
          "hidden": false
        },
        {
          "_id": "6948b09934f46eaf46cbb247",
          "name": "Chen Tang",
          "hidden": false
        },
        {
          "_id": "6948b09934f46eaf46cbb248",
          "name": "Shufei Zhang",
          "hidden": false
        },
        {
          "_id": "6948b09934f46eaf46cbb249",
          "name": "Wenbo Cao",
          "hidden": false
        },
        {
          "_id": "6948b09934f46eaf46cbb24a",
          "name": "Junjie Ren",
          "hidden": false
        },
        {
          "_id": "6948b09934f46eaf46cbb24b",
          "name": "Taoyong Cui",
          "hidden": false
        },
        {
          "_id": "6948b09934f46eaf46cbb24c",
          "name": "Zhouheng Yao",
          "hidden": false
        },
        {
          "_id": "6948b09934f46eaf46cbb24d",
          "name": "Juntao Deng",
          "hidden": false
        },
        {
          "_id": "6948b09934f46eaf46cbb24e",
          "name": "Yijie Sun",
          "hidden": false
        },
        {
          "_id": "6948b09934f46eaf46cbb24f",
          "name": "Feng Liu",
          "hidden": false
        },
        {
          "_id": "6948b09934f46eaf46cbb250",
          "name": "Wangxu Wei",
          "hidden": false
        },
        {
          "_id": "6948b09934f46eaf46cbb251",
          "name": "Jingyi Xu",
          "hidden": false
        },
        {
          "_id": "6948b09934f46eaf46cbb252",
          "name": "Zhangrui Li",
          "hidden": false
        },
        {
          "_id": "6948b09934f46eaf46cbb253",
          "name": "Junchao Gong",
          "hidden": false
        },
        {
          "_id": "6948b09934f46eaf46cbb254",
          "name": "Zijie Guo",
          "hidden": false
        },
        {
          "_id": "6948b09934f46eaf46cbb255",
          "name": "Zhiyu Yao",
          "hidden": false
        },
        {
          "_id": "6948b09934f46eaf46cbb256",
          "name": "Zaoyu Chen",
          "hidden": false
        },
        {
          "_id": "6948b09934f46eaf46cbb257",
          "name": "Tianhao Peng",
          "hidden": false
        },
        {
          "_id": "6948b09934f46eaf46cbb258",
          "name": "Fangchen Yu",
          "hidden": false
        },
        {
          "_id": "6948b09934f46eaf46cbb259",
          "name": "Bo Zhang",
          "hidden": false
        },
        {
          "_id": "6948b09934f46eaf46cbb25a",
          "name": "Dongzhan Zhou",
          "hidden": false
        },
        {
          "_id": "6948b09934f46eaf46cbb25b",
          "name": "Shixiang Tang",
          "hidden": false
        },
        {
          "_id": "6948b09934f46eaf46cbb25c",
          "name": "Jiaheng Liu",
          "hidden": false
        },
        {
          "_id": "6948b09934f46eaf46cbb25d",
          "name": "Fenghua Ling",
          "hidden": false
        },
        {
          "_id": "6948b09934f46eaf46cbb25e",
          "name": "Yan Lu",
          "hidden": false
        },
        {
          "_id": "6948b09934f46eaf46cbb25f",
          "name": "Yuchen Ren",
          "hidden": false
        },
        {
          "_id": "6948b09934f46eaf46cbb260",
          "name": "Ben Fei",
          "hidden": false
        },
        {
          "_id": "6948b09934f46eaf46cbb261",
          "name": "Zhen Zhao",
          "hidden": false
        },
        {
          "_id": "6948b09934f46eaf46cbb262",
          "name": "Xinyu Gu",
          "hidden": false
        },
        {
          "_id": "6948b09934f46eaf46cbb263",
          "name": "Rui Su",
          "hidden": false
        },
        {
          "_id": "6948b09934f46eaf46cbb264",
          "name": "Xiao-Ming Wu",
          "hidden": false
        },
        {
          "_id": "6948b09934f46eaf46cbb265",
          "name": "Weikang Si",
          "hidden": false
        },
        {
          "_id": "6948b09934f46eaf46cbb266",
          "name": "Yang Liu",
          "hidden": false
        },
        {
          "_id": "6948b09934f46eaf46cbb267",
          "name": "Hao Chen",
          "hidden": false
        },
        {
          "_id": "6948b09934f46eaf46cbb268",
          "name": "Xiangchao Yan",
          "hidden": false
        },
        {
          "_id": "6948b09934f46eaf46cbb269",
          "name": "Xue Yang",
          "hidden": false
        },
        {
          "_id": "6948b09934f46eaf46cbb26a",
          "name": "Junchi Yan",
          "hidden": false
        },
        {
          "_id": "6948b09934f46eaf46cbb26b",
          "name": "Jiamin Wu",
          "hidden": false
        },
        {
          "_id": "6948b09934f46eaf46cbb26c",
          "name": "Qihao Zheng",
          "hidden": false
        },
        {
          "_id": "6948b09934f46eaf46cbb26d",
          "name": "Chenhui Li",
          "hidden": false
        },
        {
          "_id": "6948b09934f46eaf46cbb26e",
          "name": "Zhiqiang Gao",
          "hidden": false
        },
        {
          "_id": "6948b09934f46eaf46cbb26f",
          "name": "Hao Kong",
          "hidden": false
        },
        {
          "_id": "6948b09934f46eaf46cbb270",
          "name": "Junjun He",
          "hidden": false
        },
        {
          "_id": "6948b09934f46eaf46cbb271",
          "name": "Mao Su",
          "hidden": false
        },
        {
          "_id": "6948b09934f46eaf46cbb272",
          "name": "Tianfan Fu",
          "hidden": false
        },
        {
          "_id": "6948b09934f46eaf46cbb273",
          "name": "Peng Ye",
          "hidden": false
        },
        {
          "_id": "6948b09934f46eaf46cbb274",
          "name": "Chunfeng Song",
          "hidden": false
        },
        {
          "_id": "6948b09934f46eaf46cbb275",
          "name": "Nanqing Dong",
          "hidden": false
        },
        {
          "_id": "6948b09934f46eaf46cbb276",
          "name": "Yuqiang Li",
          "hidden": false
        },
        {
          "_id": "6948b09934f46eaf46cbb277",
          "name": "Huazhu Fu",
          "hidden": false
        },
        {
          "_id": "6948b09934f46eaf46cbb278",
          "name": "Siqi Sun",
          "hidden": false
        },
        {
          "_id": "6948b09934f46eaf46cbb279",
          "name": "Lijing Cheng",
          "hidden": false
        },
        {
          "_id": "6948b09934f46eaf46cbb27a",
          "name": "Jintai Lin",
          "hidden": false
        },
        {
          "_id": "6948b09934f46eaf46cbb27b",
          "name": "Wanli Ouyang",
          "hidden": false
        },
        {
          "_id": "6948b09934f46eaf46cbb27c",
          "name": "Bowen Zhou",
          "hidden": false
        },
        {
          "_id": "6948b09934f46eaf46cbb27d",
          "name": "Wenlong Zhang",
          "hidden": false
        },
        {
          "_id": "6948b09934f46eaf46cbb27e",
          "name": "Lei Bai",
          "hidden": false
        }
      ],
      "publishedAt": "2025-12-18T12:44:36.000Z",
      "submittedOnDailyAt": "2025-12-22T00:14:52.424Z",
      "title": "Probing Scientific General Intelligence of LLMs with Scientist-Aligned Workflows",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Despite advances in scientific AI, a coherent framework for Scientific General Intelligence (SGI)-the ability to autonomously conceive, investigate, and reason across scientific domains-remains lacking. We present an operational SGI definition grounded in the Practical Inquiry Model (PIM: Deliberation, Conception, Action, Perception) and operationalize it via four scientist-aligned tasks: deep research, idea generation, dry/wet experiments, and experimental reasoning. SGI-Bench comprises over 1,000 expert-curated, cross-disciplinary samples inspired by Science's 125 Big Questions, enabling systematic evaluation of state-of-the-art LLMs. Results reveal gaps: low exact match (10--20%) in deep research despite step-level alignment; ideas lacking feasibility and detail; high code executability but low execution result accuracy in dry experiments; low sequence fidelity in wet protocols; and persistent multimodal comparative-reasoning challenges. We further introduce Test-Time Reinforcement Learning (TTRL), which optimizes retrieval-augmented novelty rewards at inference, enhancing hypothesis novelty without reference answer. Together, our PIM-grounded definition, workflow-centric benchmark, and empirical insights establish a foundation for AI systems that genuinely participate in scientific discovery.",
      "upvotes": 1,
      "discussionId": "6948b09934f46eaf46cbb27f",
      "projectPage": "https://internscience.github.io/SGI-Page/",
      "githubRepo": "https://github.com/InternScience/SGI-Bench",
      "githubRepoAddedBy": "user",
      "ai_summary": "A framework for Scientific General Intelligence (SGI) is presented, evaluated using SGI-Bench, and improved with Test-Time Reinforcement Learning, highlighting gaps in existing models' scientific capabilities.",
      "ai_keywords": [
        "Scientific General Intelligence",
        "SGI",
        "Practical Inquiry Model",
        "PIM",
        "deep research",
        "idea generation",
        "dry experiments",
        "wet experiments",
        "experimental reasoning",
        "SGI-Bench",
        "Big Questions",
        "Low exact match",
        "feasibility",
        "detail",
        "code executability",
        "execution result accuracy",
        "sequence fidelity",
        "multimodal comparative-reasoning",
        "Test-Time Reinforcement Learning",
        "TTRL",
        "retrieval-augmented novelty rewards",
        "hypothesis novelty"
      ]
    },
    "publishedAt": "2025-12-18T07:44:36.000Z",
    "title": "Probing Scientific General Intelligence of LLMs with Scientist-Aligned Workflows",
    "summary": "Despite advances in scientific AI, a coherent framework for Scientific General Intelligence (SGI)-the ability to autonomously conceive, investigate, and reason across scientific domains-remains lacking. We present an operational SGI definition grounded in the Practical Inquiry Model (PIM: Deliberation, Conception, Action, Perception) and operationalize it via four scientist-aligned tasks: deep research, idea generation, dry/wet experiments, and experimental reasoning. SGI-Bench comprises over 1,000 expert-curated, cross-disciplinary samples inspired by Science's 125 Big Questions, enabling systematic evaluation of state-of-the-art LLMs. Results reveal gaps: low exact match (10--20%) in deep research despite step-level alignment; ideas lacking feasibility and detail; high code executability but low execution result accuracy in dry experiments; low sequence fidelity in wet protocols; and persistent multimodal comparative-reasoning challenges. We further introduce Test-Time Reinforcement Learning (TTRL), which optimizes retrieval-augmented novelty rewards at inference, enhancing hypothesis novelty without reference answer. Together, our PIM-grounded definition, workflow-centric benchmark, and empirical insights establish a foundation for AI systems that genuinely participate in scientific discovery.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.16969.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 188
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2512.17796",
      "authors": [
        {
          "_id": "6948b1b934f46eaf46cbb284",
          "name": "Yitong Wang",
          "hidden": false
        },
        {
          "_id": "6948b1b934f46eaf46cbb285",
          "name": "Fangyun Wei",
          "hidden": false
        },
        {
          "_id": "6948b1b934f46eaf46cbb286",
          "name": "Hongyang Zhang",
          "hidden": false
        },
        {
          "_id": "6948b1b934f46eaf46cbb287",
          "name": "Bo Dai",
          "hidden": false
        },
        {
          "_id": "6948b1b934f46eaf46cbb288",
          "name": "Yan Lu",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/Uv0mhNSuq57dAOMvyE3w0.mp4"
      ],
      "publishedAt": "2025-12-18T18:59:18.000Z",
      "submittedOnDailyAt": "2025-12-22T00:24:30.770Z",
      "title": "Animate Any Character in Any World",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Recent advances in world models have greatly enhanced interactive environment simulation. Existing methods mainly fall into two categories: (1) static world generation models, which construct 3D environments without active agents, and (2) controllable-entity models, which allow a single entity to perform limited actions in an otherwise uncontrollable environment. In this work, we introduce AniX, leveraging the realism and structural grounding of static world generation while extending controllable-entity models to support user-specified characters capable of performing open-ended actions. Users can provide a 3DGS scene and a character, then direct the character through natural language to perform diverse behaviors from basic locomotion to object-centric interactions while freely exploring the environment. AniX synthesizes temporally coherent video clips that preserve visual fidelity with the provided scene and character, formulated as a conditional autoregressive video generation problem. Built upon a pre-trained video generator, our training strategy significantly enhances motion dynamics while maintaining generalization across actions and characters. Our evaluation covers a broad range of aspects, including visual quality, character consistency, action controllability, and long-horizon coherence.",
      "upvotes": 0,
      "discussionId": "6948b1b934f46eaf46cbb289",
      "projectPage": "https://snowflakewang.github.io/AniX/",
      "githubRepo": "https://github.com/snowflakewang/AniX",
      "githubRepoAddedBy": "user",
      "ai_summary": "AniX synthesizes temporally coherent videos by extending controllable-entity models to support diverse, user-defined character interactions in static 3D environments using conditional autoregressive video generation.",
      "ai_keywords": [
        "world models",
        "3D environments",
        "static world generation",
        "controllable-entity models",
        "AniX",
        "3DGS scene",
        "natural language",
        "object-centric interactions",
        "conditional autoregressive video generation",
        "video generator",
        "motion dynamics",
        "generalization",
        "visual quality",
        "character consistency",
        "action controllability",
        "long-horizon coherence"
      ]
    },
    "publishedAt": "2025-12-18T13:59:18.000Z",
    "title": "Animate Any Character in Any World",
    "summary": "Recent advances in world models have greatly enhanced interactive environment simulation. Existing methods mainly fall into two categories: (1) static world generation models, which construct 3D environments without active agents, and (2) controllable-entity models, which allow a single entity to perform limited actions in an otherwise uncontrollable environment. In this work, we introduce AniX, leveraging the realism and structural grounding of static world generation while extending controllable-entity models to support user-specified characters capable of performing open-ended actions. Users can provide a 3DGS scene and a character, then direct the character through natural language to perform diverse behaviors from basic locomotion to object-centric interactions while freely exploring the environment. AniX synthesizes temporally coherent video clips that preserve visual fidelity with the provided scene and character, formulated as a conditional autoregressive video generation problem. Built upon a pre-trained video generator, our training strategy significantly enhances motion dynamics while maintaining generalization across actions and characters. Our evaluation covers a broad range of aspects, including visual quality, character consistency, action controllability, and long-horizon coherence.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/Uv0mhNSuq57dAOMvyE3w0.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.17796.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 188
    },
    "isAuthorParticipating": false
  }
]