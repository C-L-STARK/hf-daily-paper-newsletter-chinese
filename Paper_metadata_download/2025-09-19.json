[
  {
    "paper": {
      "id": "2509.15207",
      "authors": [
        {
          "_id": "68ccb7983df9ac65e93dc626",
          "name": "Xuekai Zhu",
          "hidden": false
        },
        {
          "_id": "68ccb7983df9ac65e93dc627",
          "name": "Daixuan Cheng",
          "hidden": false
        },
        {
          "_id": "68ccb7983df9ac65e93dc628",
          "name": "Dinghuai Zhang",
          "hidden": false
        },
        {
          "_id": "68ccb7983df9ac65e93dc629",
          "name": "Hengli Li",
          "hidden": false
        },
        {
          "_id": "68ccb7983df9ac65e93dc62a",
          "name": "Kaiyan Zhang",
          "hidden": false
        },
        {
          "_id": "68ccb7983df9ac65e93dc62b",
          "name": "Che Jiang",
          "hidden": false
        },
        {
          "_id": "68ccb7983df9ac65e93dc62c",
          "name": "Youbang Sun",
          "hidden": false
        },
        {
          "_id": "68ccb7983df9ac65e93dc62d",
          "name": "Ermo Hua",
          "hidden": false
        },
        {
          "_id": "68ccb7983df9ac65e93dc62e",
          "name": "Yuxin Zuo",
          "hidden": false
        },
        {
          "_id": "68ccb7983df9ac65e93dc62f",
          "name": "Xingtai Lv",
          "hidden": false
        },
        {
          "_id": "68ccb7983df9ac65e93dc630",
          "name": "Qizheng Zhang",
          "hidden": false
        },
        {
          "_id": "68ccb7983df9ac65e93dc631",
          "name": "Lin Chen",
          "hidden": false
        },
        {
          "_id": "68ccb7983df9ac65e93dc632",
          "name": "Fanghao Shao",
          "hidden": false
        },
        {
          "_id": "68ccb7983df9ac65e93dc633",
          "name": "Bo Xue",
          "hidden": false
        },
        {
          "_id": "68ccb7983df9ac65e93dc634",
          "name": "Yunchong Song",
          "hidden": false
        },
        {
          "_id": "68ccb7983df9ac65e93dc635",
          "name": "Zhenjie Yang",
          "hidden": false
        },
        {
          "_id": "68ccb7983df9ac65e93dc636",
          "name": "Ganqu Cui",
          "hidden": false
        },
        {
          "_id": "68ccb7983df9ac65e93dc637",
          "name": "Ning Ding",
          "hidden": false
        },
        {
          "_id": "68ccb7983df9ac65e93dc638",
          "name": "Jianfeng Gao",
          "hidden": false
        },
        {
          "_id": "68ccb7983df9ac65e93dc639",
          "name": "Xiaodong Liu",
          "hidden": false
        },
        {
          "_id": "68ccb7983df9ac65e93dc63a",
          "name": "Bowen Zhou",
          "hidden": false
        },
        {
          "_id": "68ccb7983df9ac65e93dc63b",
          "name": "Hongyuan Mei",
          "hidden": false
        },
        {
          "_id": "68ccb7983df9ac65e93dc63c",
          "name": "Zhouhan Lin",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/649e6761f9134a06ed1e0cea/wtc3vGNJQlj3FdOsBXwp3.png"
      ],
      "publishedAt": "2025-09-18T17:56:36.000Z",
      "submittedOnDailyAt": "2025-09-19T00:24:38.079Z",
      "title": "FlowRL: Matching Reward Distributions for LLM Reasoning",
      "submittedOnDailyBy": {
        "_id": "649e6761f9134a06ed1e0cea",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/649e6761f9134a06ed1e0cea/XNeKceE8xSwI0xWwWUwwJ.jpeg",
        "isPro": false,
        "fullname": "Daixuan Cheng",
        "user": "daixuancheng",
        "type": "user"
      },
      "summary": "We propose FlowRL: matching the full reward distribution via flow balancing\ninstead of maximizing rewards in large language model (LLM) reinforcement\nlearning (RL). Recent advanced reasoning models adopt reward-maximizing methods\n(\\eg, PPO and GRPO), which tend to over-optimize dominant reward signals while\nneglecting less frequent but valid reasoning paths, thus reducing diversity. In\ncontrast, we transform scalar rewards into a normalized target distribution\nusing a learnable partition function, and then minimize the reverse KL\ndivergence between the policy and the target distribution. We implement this\nidea as a flow-balanced optimization method that promotes diverse exploration\nand generalizable reasoning trajectories. We conduct experiments on math and\ncode reasoning tasks: FlowRL achieves a significant average improvement of\n10.0% over GRPO and 5.1% over PPO on math benchmarks, and performs\nconsistently better on code reasoning tasks. These results highlight reward\ndistribution-matching as a key step toward efficient exploration and diverse\nreasoning in LLM reinforcement learning.",
      "upvotes": 25,
      "discussionId": "68ccb7983df9ac65e93dc63d",
      "githubRepo": "https://github.com/Xuekai-Zhu/FlowRL",
      "ai_summary": "FlowRL enhances LLM reinforcement learning by matching the full reward distribution through flow balancing, improving diversity and performance over reward-maximizing methods.",
      "ai_keywords": [
        "FlowRL",
        "reward distribution",
        "flow balancing",
        "reinforcement learning",
        "reward-maximizing methods",
        "PPO",
        "GRPO",
        "normalized target distribution",
        "learnable partition function",
        "reverse KL divergence",
        "diverse exploration",
        "generalizable reasoning trajectories",
        "math reasoning",
        "code reasoning"
      ],
      "githubStars": 4
    },
    "publishedAt": "2025-09-18T13:56:36.000Z",
    "title": "FlowRL: Matching Reward Distributions for LLM Reasoning",
    "summary": "We propose FlowRL: matching the full reward distribution via flow balancing\ninstead of maximizing rewards in large language model (LLM) reinforcement\nlearning (RL). Recent advanced reasoning models adopt reward-maximizing methods\n(\\eg, PPO and GRPO), which tend to over-optimize dominant reward signals while\nneglecting less frequent but valid reasoning paths, thus reducing diversity. In\ncontrast, we transform scalar rewards into a normalized target distribution\nusing a learnable partition function, and then minimize the reverse KL\ndivergence between the policy and the target distribution. We implement this\nidea as a flow-balanced optimization method that promotes diverse exploration\nand generalizable reasoning trajectories. We conduct experiments on math and\ncode reasoning tasks: FlowRL achieves a significant average improvement of\n10.0% over GRPO and 5.1% over PPO on math benchmarks, and performs\nconsistently better on code reasoning tasks. These results highlight reward\ndistribution-matching as a key step toward efficient exploration and diverse\nreasoning in LLM reinforcement learning.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/649e6761f9134a06ed1e0cea/wtc3vGNJQlj3FdOsBXwp3.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.15207.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "649e6761f9134a06ed1e0cea",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/649e6761f9134a06ed1e0cea/XNeKceE8xSwI0xWwWUwwJ.jpeg",
      "fullname": "Daixuan Cheng",
      "name": "daixuancheng",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 11
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2509.15194",
      "authors": [
        {
          "_id": "68ccb4383df9ac65e93dc5d0",
          "name": "Yujun Zhou",
          "hidden": false
        },
        {
          "_id": "68ccb4383df9ac65e93dc5d1",
          "name": "Zhenwen Liang",
          "hidden": false
        },
        {
          "_id": "68ccb4383df9ac65e93dc5d2",
          "name": "Haolin Liu",
          "hidden": false
        },
        {
          "_id": "68ccb4383df9ac65e93dc5d3",
          "name": "Wenhao Yu",
          "hidden": false
        },
        {
          "_id": "68ccb4383df9ac65e93dc5d4",
          "name": "Kishan Panaganti",
          "hidden": false
        },
        {
          "_id": "68ccb4383df9ac65e93dc5d5",
          "name": "Linfeng Song",
          "hidden": false
        },
        {
          "_id": "68ccb4383df9ac65e93dc5d6",
          "name": "Dian Yu",
          "hidden": false
        },
        {
          "_id": "68ccb4383df9ac65e93dc5d7",
          "name": "Xiangliang Zhang",
          "hidden": false
        },
        {
          "_id": "68ccb4383df9ac65e93dc5d8",
          "name": "Haitao Mi",
          "hidden": false
        },
        {
          "_id": "68ccb4383df9ac65e93dc5d9",
          "name": "Dong Yu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-09-18T17:50:04.000Z",
      "submittedOnDailyAt": "2025-09-19T00:09:24.304Z",
      "title": "Evolving Language Models without Labels: Majority Drives Selection,\n  Novelty Promotes Variation",
      "submittedOnDailyBy": {
        "_id": "5feab3a28a3201f8e554c969",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1660795228685-5feab3a28a3201f8e554c969.png",
        "isPro": false,
        "fullname": "Wenhao Yu",
        "user": "wyu1",
        "type": "user"
      },
      "summary": "Large language models (LLMs) are increasingly trained with reinforcement\nlearning from verifiable rewards (RLVR), yet real-world deployment demands\nmodels that can self-improve without labels or external judges. Existing\nlabel-free methods, confidence minimization, self-consistency, or majority-vote\nobjectives, stabilize learning but steadily shrink exploration, causing an\nentropy collapse: generations become shorter, less diverse, and brittle. Unlike\nprior approaches such as Test-Time Reinforcement Learning (TTRL), which\nprimarily adapt models to the immediate unlabeled dataset at hand, our goal is\nbroader: to enable general improvements without sacrificing the model's\ninherent exploration capacity and generalization ability, i.e., evolving. We\nformalize this issue and propose EVolution-Oriented and Label-free\nReinforcement Learning (EVOL-RL), a simple rule that couples stability with\nvariation under a label-free setting. EVOL-RL keeps the majority-voted answer\nas a stable anchor (selection) while adding a novelty-aware reward that favors\nresponses whose reasoning differs from what has already been produced\n(variation), measured in semantic space. Implemented with GRPO, EVOL-RL also\nuses asymmetric clipping to preserve strong signals and an entropy regularizer\nto sustain search. This majority-for-selection + novelty-for-variation design\nprevents collapse, maintains longer and more informative chains of thought, and\nimproves both pass@1 and pass@n. EVOL-RL consistently outperforms the\nmajority-only TTRL baseline; e.g., training on label-free AIME24 lifts\nQwen3-4B-Base AIME25 pass@1 from TTRL's 4.6% to 16.4%, and pass@16 from 18.5%\nto 37.9%. EVOL-RL not only prevents diversity collapse but also unlocks\nstronger generalization across domains (e.g., GPQA). Furthermore, we\ndemonstrate that EVOL-RL also boosts performance in the RLVR setting,\nhighlighting its broad applicability.",
      "upvotes": 18,
      "discussionId": "68ccb4383df9ac65e93dc5da",
      "ai_summary": "EVOL-RL, a label-free reinforcement learning method, enhances large language models by balancing stability and variation, preventing entropy collapse and improving generalization.",
      "ai_keywords": [
        "reinforcement learning from verifiable rewards",
        "RLVR",
        "label-free methods",
        "confidence minimization",
        "self-consistency",
        "majority-vote objectives",
        "entropy collapse",
        "Test-Time Reinforcement Learning",
        "TTRL",
        "EVolution-Oriented and Label-free Reinforcement Learning",
        "EVOL-RL",
        "GRPO",
        "asymmetric clipping",
        "entropy regularizer",
        "pass@1",
        "pass@n",
        "AIME24",
        "Qwen3-4B-Base",
        "AIME25",
        "GPQA"
      ]
    },
    "publishedAt": "2025-09-18T13:50:04.000Z",
    "title": "Evolving Language Models without Labels: Majority Drives Selection,\n  Novelty Promotes Variation",
    "summary": "Large language models (LLMs) are increasingly trained with reinforcement\nlearning from verifiable rewards (RLVR), yet real-world deployment demands\nmodels that can self-improve without labels or external judges. Existing\nlabel-free methods, confidence minimization, self-consistency, or majority-vote\nobjectives, stabilize learning but steadily shrink exploration, causing an\nentropy collapse: generations become shorter, less diverse, and brittle. Unlike\nprior approaches such as Test-Time Reinforcement Learning (TTRL), which\nprimarily adapt models to the immediate unlabeled dataset at hand, our goal is\nbroader: to enable general improvements without sacrificing the model's\ninherent exploration capacity and generalization ability, i.e., evolving. We\nformalize this issue and propose EVolution-Oriented and Label-free\nReinforcement Learning (EVOL-RL), a simple rule that couples stability with\nvariation under a label-free setting. EVOL-RL keeps the majority-voted answer\nas a stable anchor (selection) while adding a novelty-aware reward that favors\nresponses whose reasoning differs from what has already been produced\n(variation), measured in semantic space. Implemented with GRPO, EVOL-RL also\nuses asymmetric clipping to preserve strong signals and an entropy regularizer\nto sustain search. This majority-for-selection + novelty-for-variation design\nprevents collapse, maintains longer and more informative chains of thought, and\nimproves both pass@1 and pass@n. EVOL-RL consistently outperforms the\nmajority-only TTRL baseline; e.g., training on label-free AIME24 lifts\nQwen3-4B-Base AIME25 pass@1 from TTRL's 4.6% to 16.4%, and pass@16 from 18.5%\nto 37.9%. EVOL-RL not only prevents diversity collapse but also unlocks\nstronger generalization across domains (e.g., GPQA). Furthermore, we\ndemonstrate that EVOL-RL also boosts performance in the RLVR setting,\nhighlighting its broad applicability.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.15194.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "5feab3a28a3201f8e554c969",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1660795228685-5feab3a28a3201f8e554c969.png",
      "fullname": "Wenhao Yu",
      "name": "wyu1",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 9
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2509.15221",
      "authors": [
        {
          "_id": "68ccb2e43df9ac65e93dc5b9",
          "name": "Zhaoyang Liu",
          "hidden": false
        },
        {
          "_id": "68ccb2e43df9ac65e93dc5ba",
          "name": "JingJing Xie",
          "hidden": false
        },
        {
          "_id": "68ccb2e43df9ac65e93dc5bb",
          "name": "Zichen Ding",
          "hidden": false
        },
        {
          "_id": "68ccb2e43df9ac65e93dc5bc",
          "name": "Zehao Li",
          "hidden": false
        },
        {
          "_id": "68ccb2e43df9ac65e93dc5bd",
          "name": "Bowen Yang",
          "hidden": false
        },
        {
          "_id": "68ccb2e43df9ac65e93dc5be",
          "name": "Zhenyu Wu",
          "hidden": false
        },
        {
          "_id": "68ccb2e43df9ac65e93dc5bf",
          "name": "Xuehui Wang",
          "hidden": false
        },
        {
          "_id": "68ccb2e43df9ac65e93dc5c0",
          "name": "Qiushi Sun",
          "hidden": false
        },
        {
          "_id": "68ccb2e43df9ac65e93dc5c1",
          "name": "Shi Liu",
          "hidden": false
        },
        {
          "_id": "68ccb2e43df9ac65e93dc5c2",
          "name": "Weiyun Wang",
          "hidden": false
        },
        {
          "_id": "68ccb2e43df9ac65e93dc5c3",
          "name": "Shenglong Ye",
          "hidden": false
        },
        {
          "_id": "68ccb2e43df9ac65e93dc5c4",
          "name": "Qingyun Li",
          "hidden": false
        },
        {
          "_id": "68ccb2e43df9ac65e93dc5c5",
          "name": "Zeyue Tian",
          "hidden": false
        },
        {
          "_id": "68ccb2e43df9ac65e93dc5c6",
          "name": "Gen Luo",
          "hidden": false
        },
        {
          "_id": "68ccb2e43df9ac65e93dc5c7",
          "name": "Xiangyu Yue",
          "hidden": false
        },
        {
          "_id": "68ccb2e43df9ac65e93dc5c8",
          "name": "Biqing Qi",
          "hidden": false
        },
        {
          "_id": "68ccb2e43df9ac65e93dc5c9",
          "name": "Kai Chen",
          "hidden": false
        },
        {
          "_id": "68ccb2e43df9ac65e93dc5ca",
          "name": "Bowen Zhou",
          "hidden": false
        },
        {
          "_id": "68ccb2e43df9ac65e93dc5cb",
          "name": "Yu Qiao",
          "hidden": false
        },
        {
          "_id": "68ccb2e43df9ac65e93dc5cc",
          "name": "Qifeng Chen",
          "hidden": false
        },
        {
          "_id": "68ccb2e43df9ac65e93dc5cd",
          "name": "Wenhai Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-09-18T17:59:22.000Z",
      "submittedOnDailyAt": "2025-09-19T00:03:42.415Z",
      "title": "ScaleCUA: Scaling Open-Source Computer Use Agents with Cross-Platform\n  Data",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Vision-Language Models (VLMs) have enabled computer use agents (CUAs) that\noperate GUIs autonomously, showing great potential, yet progress is limited by\nthe lack of large-scale, open-source computer use data and foundation models.\nIn this work, we introduce ScaleCUA, a step toward scaling open-source CUAs. It\noffers a large-scale dataset spanning 6 operating systems and 3 task domains,\nbuilt via a closed-loop pipeline uniting automated agents with human experts.\nTrained on this scaled-up data, ScaleCUA can operate seamlessly across\nplatforms. Specifically, it delivers strong gains over baselines (+26.6 on\nWebArena-Lite-v2, +10.7 on ScreenSpot-Pro) and sets new state-of-the-art\nresults (94.4% on MMBench-GUI L1-Hard, 60.6% on OSWorld-G, 47.4% on\nWebArena-Lite-v2). These findings underscore the power of data-driven scaling\nfor general-purpose computer use agents. We will release data, models, and code\nto advance future research: https://github.com/OpenGVLab/ScaleCUA.",
      "upvotes": 8,
      "discussionId": "68ccb2e43df9ac65e93dc5ce",
      "githubRepo": "https://github.com/OpenGVLab/ScaleCUA",
      "ai_summary": "ScaleCUA, a large-scale dataset and model for computer use agents, achieves state-of-the-art performance across multiple platforms and tasks by leveraging data-driven scaling.",
      "ai_keywords": [
        "Vision-Language Models",
        "computer use agents",
        "GUIs",
        "closed-loop pipeline",
        "automated agents",
        "human experts",
        "WebArena-Lite-v2",
        "ScreenSpot-Pro",
        "MMBench-GUI L1-Hard",
        "OSWorld-G"
      ],
      "githubStars": 9
    },
    "publishedAt": "2025-09-18T13:59:22.000Z",
    "title": "ScaleCUA: Scaling Open-Source Computer Use Agents with Cross-Platform\n  Data",
    "summary": "Vision-Language Models (VLMs) have enabled computer use agents (CUAs) that\noperate GUIs autonomously, showing great potential, yet progress is limited by\nthe lack of large-scale, open-source computer use data and foundation models.\nIn this work, we introduce ScaleCUA, a step toward scaling open-source CUAs. It\noffers a large-scale dataset spanning 6 operating systems and 3 task domains,\nbuilt via a closed-loop pipeline uniting automated agents with human experts.\nTrained on this scaled-up data, ScaleCUA can operate seamlessly across\nplatforms. Specifically, it delivers strong gains over baselines (+26.6 on\nWebArena-Lite-v2, +10.7 on ScreenSpot-Pro) and sets new state-of-the-art\nresults (94.4% on MMBench-GUI L1-Hard, 60.6% on OSWorld-G, 47.4% on\nWebArena-Lite-v2). These findings underscore the power of data-driven scaling\nfor general-purpose computer use agents. We will release data, models, and code\nto advance future research: https://github.com/OpenGVLab/ScaleCUA.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.15221.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 106
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2509.13160",
      "authors": [
        {
          "_id": "68ccb6e23df9ac65e93dc60d",
          "name": "Liang Hu",
          "hidden": false
        },
        {
          "_id": "68ccb6e23df9ac65e93dc60e",
          "name": "Jianpeng Jiao",
          "hidden": false
        },
        {
          "_id": "68ccb6e23df9ac65e93dc60f",
          "name": "Jiashuo Liu",
          "hidden": false
        },
        {
          "_id": "68ccb6e23df9ac65e93dc610",
          "name": "Yanle Ren",
          "hidden": false
        },
        {
          "_id": "68ccb6e23df9ac65e93dc611",
          "name": "Zhoufutu Wen",
          "hidden": false
        },
        {
          "_id": "68ccb6e23df9ac65e93dc612",
          "name": "Kaiyuan Zhang",
          "hidden": false
        },
        {
          "_id": "68ccb6e23df9ac65e93dc613",
          "name": "Xuanliang Zhang",
          "hidden": false
        },
        {
          "_id": "68ccb6e23df9ac65e93dc614",
          "name": "Xiang Gao",
          "hidden": false
        },
        {
          "_id": "68ccb6e23df9ac65e93dc615",
          "name": "Tianci He",
          "hidden": false
        },
        {
          "_id": "68ccb6e23df9ac65e93dc616",
          "name": "Fei Hu",
          "hidden": false
        },
        {
          "_id": "68ccb6e23df9ac65e93dc617",
          "name": "Yali Liao",
          "hidden": false
        },
        {
          "_id": "68ccb6e23df9ac65e93dc618",
          "name": "Zaiyuan Wang",
          "hidden": false
        },
        {
          "_id": "68ccb6e23df9ac65e93dc619",
          "name": "Chenghao Yang",
          "hidden": false
        },
        {
          "_id": "68ccb6e23df9ac65e93dc61a",
          "name": "Qianyu Yang",
          "hidden": false
        },
        {
          "_id": "68ccb6e23df9ac65e93dc61b",
          "name": "Mingren Yin",
          "hidden": false
        },
        {
          "_id": "68ccb6e23df9ac65e93dc61c",
          "name": "Zhiyuan Zeng",
          "hidden": false
        },
        {
          "_id": "68ccb6e23df9ac65e93dc61d",
          "name": "Ge Zhang",
          "hidden": false
        },
        {
          "_id": "68ccb6e23df9ac65e93dc61e",
          "name": "Xinyi Zhang",
          "hidden": false
        },
        {
          "_id": "68ccb6e23df9ac65e93dc61f",
          "name": "Xiying Zhao",
          "hidden": false
        },
        {
          "_id": "68ccb6e23df9ac65e93dc620",
          "name": "Zhenwei Zhu",
          "hidden": false
        },
        {
          "_id": "68ccb6e23df9ac65e93dc621",
          "name": "Hongseok Namkoong",
          "hidden": false
        },
        {
          "_id": "68ccb6e23df9ac65e93dc622",
          "name": "Wenhao Huang",
          "hidden": false
        },
        {
          "_id": "68ccb6e23df9ac65e93dc623",
          "name": "Yuwen Tang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-09-16T15:13:13.000Z",
      "submittedOnDailyAt": "2025-09-19T00:20:55.990Z",
      "title": "FinSearchComp: Towards a Realistic, Expert-Level Evaluation of Financial\n  Search and Reasoning",
      "submittedOnDailyBy": {
        "_id": "638efcf4c67af472d316d424",
        "avatarUrl": "/avatars/97a57859d7d87a3a8f1bb41d32a72bc2.svg",
        "isPro": false,
        "fullname": "Ge Zhang",
        "user": "zhangysk",
        "type": "user"
      },
      "summary": "Search has emerged as core infrastructure for LLM-based agents and is widely\nviewed as critical on the path toward more general intelligence. Finance is a\nparticularly demanding proving ground: analysts routinely conduct complex,\nmulti-step searches over time-sensitive, domain-specific data, making it ideal\nfor assessing both search proficiency and knowledge-grounded reasoning. Yet no\nexisting open financial datasets evaluate data searching capability of\nend-to-end agents, largely because constructing realistic, complicated tasks\nrequires deep financial expertise and time-sensitive data is hard to evaluate.\nWe present FinSearchComp, the first fully open-source agent benchmark for\nrealistic, open-domain financial search and reasoning. FinSearchComp comprises\nthree tasks -- Time-Sensitive Data Fetching, Simple Historical Lookup, and\nComplex Historical Investigation -- closely reproduce real-world financial\nanalyst workflows. To ensure difficulty and reliability, we engage 70\nprofessional financial experts for annotation and implement a rigorous\nmulti-stage quality-assurance pipeline. The benchmark includes 635 questions\nspanning global and Greater China markets, and we evaluate 21 models (products)\non it. Grok 4 (web) tops the global subset, approaching expert-level accuracy.\nDouBao (web) leads on the Greater China subset. Experimental analyses show that\nequipping agents with web search and financial plugins substantially improves\nresults on FinSearchComp, and the country origin of models and tools impact\nperformance significantly.By aligning with realistic analyst tasks and\nproviding end-to-end evaluation, FinSearchComp offers a professional,\nhigh-difficulty testbed for complex financial search and reasoning.",
      "upvotes": 7,
      "discussionId": "68ccb6e23df9ac65e93dc624",
      "ai_summary": "FinSearchComp is an open-source benchmark for evaluating financial search and reasoning capabilities of end-to-end agents, featuring realistic tasks and professional annotations.",
      "ai_keywords": [
        "LLM-based agents",
        "search proficiency",
        "knowledge-grounded reasoning",
        "FinSearchComp",
        "Time-Sensitive Data Fetching",
        "Simple Historical Lookup",
        "Complex Historical Investigation",
        "web search",
        "financial plugins"
      ]
    },
    "publishedAt": "2025-09-16T11:13:13.000Z",
    "title": "FinSearchComp: Towards a Realistic, Expert-Level Evaluation of Financial\n  Search and Reasoning",
    "summary": "Search has emerged as core infrastructure for LLM-based agents and is widely\nviewed as critical on the path toward more general intelligence. Finance is a\nparticularly demanding proving ground: analysts routinely conduct complex,\nmulti-step searches over time-sensitive, domain-specific data, making it ideal\nfor assessing both search proficiency and knowledge-grounded reasoning. Yet no\nexisting open financial datasets evaluate data searching capability of\nend-to-end agents, largely because constructing realistic, complicated tasks\nrequires deep financial expertise and time-sensitive data is hard to evaluate.\nWe present FinSearchComp, the first fully open-source agent benchmark for\nrealistic, open-domain financial search and reasoning. FinSearchComp comprises\nthree tasks -- Time-Sensitive Data Fetching, Simple Historical Lookup, and\nComplex Historical Investigation -- closely reproduce real-world financial\nanalyst workflows. To ensure difficulty and reliability, we engage 70\nprofessional financial experts for annotation and implement a rigorous\nmulti-stage quality-assurance pipeline. The benchmark includes 635 questions\nspanning global and Greater China markets, and we evaluate 21 models (products)\non it. Grok 4 (web) tops the global subset, approaching expert-level accuracy.\nDouBao (web) leads on the Greater China subset. Experimental analyses show that\nequipping agents with web search and financial plugins substantially improves\nresults on FinSearchComp, and the country origin of models and tools impact\nperformance significantly.By aligning with realistic analyst tasks and\nproviding end-to-end evaluation, FinSearchComp offers a professional,\nhigh-difficulty testbed for complex financial search and reasoning.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.13160.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "638efcf4c67af472d316d424",
      "avatarUrl": "/avatars/97a57859d7d87a3a8f1bb41d32a72bc2.svg",
      "fullname": "Ge Zhang",
      "name": "zhangysk",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 67
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2509.14638",
      "authors": [
        {
          "_id": "68ccb51f3df9ac65e93dc5dc",
          "name": "Mingsong Li",
          "hidden": false
        },
        {
          "_id": "68ccb51f3df9ac65e93dc5dd",
          "name": "Lin Liu",
          "hidden": false
        },
        {
          "_id": "68ccb51f3df9ac65e93dc5de",
          "name": "Hongjun Wang",
          "hidden": false
        },
        {
          "_id": "68ccb51f3df9ac65e93dc5df",
          "name": "Haoxing Chen",
          "hidden": false
        },
        {
          "_id": "68ccb51f3df9ac65e93dc5e0",
          "name": "Xijun Gu",
          "hidden": false
        },
        {
          "_id": "68ccb51f3df9ac65e93dc5e1",
          "name": "Shizhan Liu",
          "hidden": false
        },
        {
          "_id": "68ccb51f3df9ac65e93dc5e2",
          "name": "Dong Gong",
          "hidden": false
        },
        {
          "_id": "68ccb51f3df9ac65e93dc5e3",
          "name": "Junbo Zhao",
          "hidden": false
        },
        {
          "_id": "68ccb51f3df9ac65e93dc5e4",
          "name": "Zhenzhong Lan",
          "hidden": false
        },
        {
          "_id": "68ccb51f3df9ac65e93dc5e5",
          "name": "Jianguo Li",
          "hidden": false
        }
      ],
      "publishedAt": "2025-09-18T05:33:38.000Z",
      "submittedOnDailyAt": "2025-09-19T00:13:05.587Z",
      "title": "MultiEdit: Advancing Instruction-based Image Editing on Diverse and\n  Challenging Tasks",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Current instruction-based image editing (IBIE) methods struggle with\nchallenging editing tasks, as both editing types and sample counts of existing\ndatasets are limited. Moreover, traditional dataset construction often contains\nnoisy image-caption pairs, which may introduce biases and limit model\ncapabilities in complex editing scenarios. To address these limitations, we\nintroduce MultiEdit, a comprehensive dataset featuring over 107K high-quality\nimage editing samples. It encompasses 6 challenging editing tasks through a\ndiverse collection of 18 non-style-transfer editing types and 38 style transfer\noperations, covering a spectrum from sophisticated style transfer to complex\nsemantic operations like person reference editing and in-image text editing. We\nemploy a novel dataset construction pipeline that utilizes two multi-modal\nlarge language models (MLLMs) to generate visual-adaptive editing instructions\nand produce high-fidelity edited images, respectively. Extensive experiments\ndemonstrate that fine-tuning foundational open-source models with our\nMultiEdit-Train set substantially improves models' performance on sophisticated\nediting tasks in our proposed MultiEdit-Test benchmark, while effectively\npreserving their capabilities on the standard editing benchmark. We believe\nMultiEdit provides a valuable resource for advancing research into more diverse\nand challenging IBIE capabilities. Our dataset is available at\nhttps://huggingface.co/datasets/inclusionAI/MultiEdit.",
      "upvotes": 2,
      "discussionId": "68ccb5203df9ac65e93dc5e6",
      "projectPage": "https://huggingface.co/datasets/inclusionAI/MultiEdit",
      "ai_summary": "MultiEdit, a comprehensive dataset with over 107K high-quality image editing samples, improves performance on sophisticated editing tasks using a novel pipeline with multi-modal large language models.",
      "ai_keywords": [
        "instruction-based image editing",
        "IBIE",
        "editing tasks",
        "sample counts",
        "noisy image-caption pairs",
        "biases",
        "dataset construction",
        "editing types",
        "style transfer",
        "semantic operations",
        "person reference editing",
        "in-image text editing",
        "multi-modal large language models",
        "MLLMs",
        "visual-adaptive editing instructions",
        "high-fidelity edited images",
        "fine-tuning",
        "foundational open-source models",
        "MultiEdit-Train",
        "MultiEdit-Test benchmark",
        "standard editing benchmark"
      ]
    },
    "publishedAt": "2025-09-18T01:33:38.000Z",
    "title": "MultiEdit: Advancing Instruction-based Image Editing on Diverse and\n  Challenging Tasks",
    "summary": "Current instruction-based image editing (IBIE) methods struggle with\nchallenging editing tasks, as both editing types and sample counts of existing\ndatasets are limited. Moreover, traditional dataset construction often contains\nnoisy image-caption pairs, which may introduce biases and limit model\ncapabilities in complex editing scenarios. To address these limitations, we\nintroduce MultiEdit, a comprehensive dataset featuring over 107K high-quality\nimage editing samples. It encompasses 6 challenging editing tasks through a\ndiverse collection of 18 non-style-transfer editing types and 38 style transfer\noperations, covering a spectrum from sophisticated style transfer to complex\nsemantic operations like person reference editing and in-image text editing. We\nemploy a novel dataset construction pipeline that utilizes two multi-modal\nlarge language models (MLLMs) to generate visual-adaptive editing instructions\nand produce high-fidelity edited images, respectively. Extensive experiments\ndemonstrate that fine-tuning foundational open-source models with our\nMultiEdit-Train set substantially improves models' performance on sophisticated\nediting tasks in our proposed MultiEdit-Test benchmark, while effectively\npreserving their capabilities on the standard editing benchmark. We believe\nMultiEdit provides a valuable resource for advancing research into more diverse\nand challenging IBIE capabilities. Our dataset is available at\nhttps://huggingface.co/datasets/inclusionAI/MultiEdit.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.14638.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 106
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2509.15212",
      "authors": [
        {
          "_id": "68ccb5443df9ac65e93dc5e8",
          "name": "Yuming Jiang",
          "hidden": false
        },
        {
          "_id": "68ccb5443df9ac65e93dc5e9",
          "name": "Siteng Huang",
          "hidden": false
        },
        {
          "_id": "68ccb5443df9ac65e93dc5ea",
          "name": "Shengke Xue",
          "hidden": false
        },
        {
          "_id": "68ccb5443df9ac65e93dc5eb",
          "name": "Yaxi Zhao",
          "hidden": false
        },
        {
          "_id": "68ccb5443df9ac65e93dc5ec",
          "name": "Jun Cen",
          "hidden": false
        },
        {
          "_id": "68ccb5443df9ac65e93dc5ed",
          "name": "Sicong Leng",
          "hidden": false
        },
        {
          "_id": "68ccb5443df9ac65e93dc5ee",
          "name": "Kehan Li",
          "hidden": false
        },
        {
          "_id": "68ccb5443df9ac65e93dc5ef",
          "name": "Jiayan Guo",
          "hidden": false
        },
        {
          "_id": "68ccb5443df9ac65e93dc5f0",
          "name": "Kexiang Wang",
          "hidden": false
        },
        {
          "_id": "68ccb5443df9ac65e93dc5f1",
          "name": "Mingxiu Chen",
          "hidden": false
        },
        {
          "_id": "68ccb5443df9ac65e93dc5f2",
          "name": "Fan Wang",
          "hidden": false
        },
        {
          "_id": "68ccb5443df9ac65e93dc5f3",
          "name": "Deli Zhao",
          "hidden": false
        },
        {
          "_id": "68ccb5443df9ac65e93dc5f4",
          "name": "Xin Li",
          "hidden": false
        }
      ],
      "publishedAt": "2025-09-18T17:58:02.000Z",
      "submittedOnDailyAt": "2025-09-19T00:13:55.962Z",
      "title": "RynnVLA-001: Using Human Demonstrations to Improve Robot Manipulation",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "This paper presents RynnVLA-001, a vision-language-action(VLA) model built\nupon large-scale video generative pretraining from human demonstrations. We\npropose a novel two-stage pretraining methodology. The first stage, Ego-Centric\nVideo Generative Pretraining, trains an Image-to-Video model on 12M ego-centric\nmanipulation videos to predict future frames conditioned on an initial frame\nand a language instruction. The second stage, Human-Centric Trajectory-Aware\nModeling, extends this by jointly predicting future keypoint trajectories,\nthereby effectively bridging visual frame prediction with action prediction.\nFurthermore, to enhance action representation, we propose ActionVAE, a\nvariational autoencoder that compresses sequences of actions into compact\nlatent embeddings, reducing the complexity of the VLA output space. When\nfinetuned on the same downstream robotics datasets, RynnVLA-001 achieves\nsuperior performance over state-of-the-art baselines, demonstrating that the\nproposed pretraining strategy provides a more effective initialization for VLA\nmodels.",
      "upvotes": 1,
      "discussionId": "68ccb5453df9ac65e93dc5f5",
      "githubRepo": "https://github.com/alibaba-damo-academy/RynnVLA-001",
      "ai_summary": "RynnVLA-001, a vision-language-action model, uses a two-stage pretraining approach and ActionVAE to achieve superior performance on robotics tasks.",
      "ai_keywords": [
        "Ego-Centric Video Generative Pretraining",
        "Human-Centric Trajectory-Aware Modeling",
        "Image-to-Video model",
        "keypoint trajectories",
        "ActionVAE",
        "variational autoencoder",
        "latent embeddings",
        "VLA model"
      ],
      "githubStars": 168
    },
    "publishedAt": "2025-09-18T13:58:02.000Z",
    "title": "RynnVLA-001: Using Human Demonstrations to Improve Robot Manipulation",
    "summary": "This paper presents RynnVLA-001, a vision-language-action(VLA) model built\nupon large-scale video generative pretraining from human demonstrations. We\npropose a novel two-stage pretraining methodology. The first stage, Ego-Centric\nVideo Generative Pretraining, trains an Image-to-Video model on 12M ego-centric\nmanipulation videos to predict future frames conditioned on an initial frame\nand a language instruction. The second stage, Human-Centric Trajectory-Aware\nModeling, extends this by jointly predicting future keypoint trajectories,\nthereby effectively bridging visual frame prediction with action prediction.\nFurthermore, to enhance action representation, we propose ActionVAE, a\nvariational autoencoder that compresses sequences of actions into compact\nlatent embeddings, reducing the complexity of the VLA output space. When\nfinetuned on the same downstream robotics datasets, RynnVLA-001 achieves\nsuperior performance over state-of-the-art baselines, demonstrating that the\nproposed pretraining strategy provides a more effective initialization for VLA\nmodels.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.15212.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 106
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2509.15130",
      "authors": [
        {
          "_id": "68ccb5de3df9ac65e93dc5fc",
          "name": "Chenxi Song",
          "hidden": false
        },
        {
          "_id": "68ccb5de3df9ac65e93dc5fd",
          "name": "Yanming Yang",
          "hidden": false
        },
        {
          "_id": "68ccb5de3df9ac65e93dc5fe",
          "name": "Tong Zhao",
          "hidden": false
        },
        {
          "_id": "68ccb5de3df9ac65e93dc5ff",
          "name": "Ruibo Li",
          "hidden": false
        },
        {
          "_id": "68ccb5de3df9ac65e93dc600",
          "name": "Chi Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-09-18T16:40:47.000Z",
      "submittedOnDailyAt": "2025-09-19T00:16:15.925Z",
      "title": "WorldForge: Unlocking Emergent 3D/4D Generation in Video Diffusion Model\n  via Training-Free Guidance",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Recent video diffusion models demonstrate strong potential in spatial\nintelligence tasks due to their rich latent world priors. However, this\npotential is hindered by their limited controllability and geometric\ninconsistency, creating a gap between their strong priors and their practical\nuse in 3D/4D tasks. As a result, current approaches often rely on retraining or\nfine-tuning, which risks degrading pretrained knowledge and incurs high\ncomputational costs. To address this, we propose WorldForge, a training-free,\ninference-time framework composed of three tightly coupled modules. Intra-Step\nRecursive Refinement introduces a recursive refinement mechanism during\ninference, which repeatedly optimizes network predictions within each denoising\nstep to enable precise trajectory injection. Flow-Gated Latent Fusion leverages\noptical flow similarity to decouple motion from appearance in the latent space\nand selectively inject trajectory guidance into motion-related channels.\nDual-Path Self-Corrective Guidance compares guided and unguided denoising paths\nto adaptively correct trajectory drift caused by noisy or misaligned structural\nsignals. Together, these components inject fine-grained, trajectory-aligned\nguidance without training, achieving both accurate motion control and\nphotorealistic content generation. Extensive experiments across diverse\nbenchmarks validate our method's superiority in realism, trajectory\nconsistency, and visual fidelity. This work introduces a novel plug-and-play\nparadigm for controllable video synthesis, offering a new perspective on\nleveraging generative priors for spatial intelligence.",
      "upvotes": 1,
      "discussionId": "68ccb5df3df9ac65e93dc601",
      "projectPage": "https://worldforge-agi.github.io/",
      "ai_summary": "WorldForge, a training-free framework, enhances video diffusion models with precise motion control and photorealistic content generation through recursive refinement, flow-gated latent fusion, and dual-path self-corrective guidance.",
      "ai_keywords": [
        "video diffusion models",
        "spatial intelligence tasks",
        "latent world priors",
        "controllability",
        "geometric inconsistency",
        "retraining",
        "fine-tuning",
        "WorldForge",
        "inference-time framework",
        "Intra-Step Recursive Refinement",
        "denoising step",
        "Flow-Gated Latent Fusion",
        "optical flow similarity",
        "Dual-Path Self-Corrective Guidance",
        "trajectory injection",
        "trajectory drift",
        "photorealistic content generation",
        "trajectory consistency",
        "visual fidelity",
        "plug-and-play paradigm",
        "generative priors"
      ]
    },
    "publishedAt": "2025-09-18T12:40:47.000Z",
    "title": "WorldForge: Unlocking Emergent 3D/4D Generation in Video Diffusion Model\n  via Training-Free Guidance",
    "summary": "Recent video diffusion models demonstrate strong potential in spatial\nintelligence tasks due to their rich latent world priors. However, this\npotential is hindered by their limited controllability and geometric\ninconsistency, creating a gap between their strong priors and their practical\nuse in 3D/4D tasks. As a result, current approaches often rely on retraining or\nfine-tuning, which risks degrading pretrained knowledge and incurs high\ncomputational costs. To address this, we propose WorldForge, a training-free,\ninference-time framework composed of three tightly coupled modules. Intra-Step\nRecursive Refinement introduces a recursive refinement mechanism during\ninference, which repeatedly optimizes network predictions within each denoising\nstep to enable precise trajectory injection. Flow-Gated Latent Fusion leverages\noptical flow similarity to decouple motion from appearance in the latent space\nand selectively inject trajectory guidance into motion-related channels.\nDual-Path Self-Corrective Guidance compares guided and unguided denoising paths\nto adaptively correct trajectory drift caused by noisy or misaligned structural\nsignals. Together, these components inject fine-grained, trajectory-aligned\nguidance without training, achieving both accurate motion control and\nphotorealistic content generation. Extensive experiments across diverse\nbenchmarks validate our method's superiority in realism, trajectory\nconsistency, and visual fidelity. This work introduces a novel plug-and-play\nparadigm for controllable video synthesis, offering a new perspective on\nleveraging generative priors for spatial intelligence.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.15130.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 106
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2509.14476",
      "authors": [
        {
          "_id": "68ccb6a63df9ac65e93dc603",
          "name": "Jiasen Lu",
          "hidden": false
        },
        {
          "_id": "68ccb6a63df9ac65e93dc604",
          "name": "Liangchen Song",
          "hidden": false
        },
        {
          "_id": "68ccb6a63df9ac65e93dc605",
          "name": "Mingze Xu",
          "hidden": false
        },
        {
          "_id": "68ccb6a63df9ac65e93dc606",
          "name": "Byeongjoo Ahn",
          "hidden": false
        },
        {
          "_id": "68ccb6a63df9ac65e93dc607",
          "name": "Yanjun Wang",
          "hidden": false
        },
        {
          "_id": "68ccb6a63df9ac65e93dc608",
          "name": "Chen Chen",
          "hidden": false
        },
        {
          "_id": "68ccb6a63df9ac65e93dc609",
          "name": "Afshin Dehghan",
          "hidden": false
        },
        {
          "_id": "68ccb6a63df9ac65e93dc60a",
          "name": "Yinfei Yang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-09-17T23:11:18.000Z",
      "submittedOnDailyAt": "2025-09-19T00:19:42.928Z",
      "title": "AToken: A Unified Tokenizer for Vision",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "We present AToken, the first unified visual tokenizer that achieves both\nhigh-fidelity reconstruction and semantic understanding across images, videos,\nand 3D assets. Unlike existing tokenizers that specialize in either\nreconstruction or understanding for single modalities, AToken encodes these\ndiverse visual inputs into a shared 4D latent space, unifying both tasks and\nmodalities in a single framework. Specifically, we introduce a pure transformer\narchitecture with 4D rotary position embeddings to process visual inputs of\narbitrary resolutions and temporal durations. To ensure stable training, we\nintroduce an adversarial-free training objective that combines perceptual and\nGram matrix losses, achieving state-of-the-art reconstruction quality. By\nemploying a progressive training curriculum, AToken gradually expands from\nsingle images, videos, and 3D, and supports both continuous and discrete latent\ntokens. AToken achieves 0.21 rFID with 82.2% ImageNet accuracy for images, 3.01\nrFVD with 32.6% MSRVTT retrieval for videos, and 28.19 PSNR with 90.9%\nclassification accuracy for 3D. In downstream applications, AToken enables both\nvisual generation tasks (e.g., image generation with continuous and discrete\ntokens, text-to-video generation, image-to-3D synthesis) and understanding\ntasks (e.g., multimodal LLMs), achieving competitive performance across all\nbenchmarks. These results shed light on the next-generation multimodal AI\nsystems built upon unified visual tokenization.",
      "upvotes": 1,
      "discussionId": "68ccb6a63df9ac65e93dc60b",
      "ai_summary": "AToken, a unified visual tokenizer, achieves high-fidelity reconstruction and semantic understanding across images, videos, and 3D assets using a 4D transformer architecture with adversarial-free training.",
      "ai_keywords": [
        "unified visual tokenizer",
        "4D latent space",
        "pure transformer architecture",
        "4D rotary position embeddings",
        "adversarial-free training objective",
        "perceptual loss",
        "Gram matrix loss",
        "progressive training curriculum",
        "continuous latent tokens",
        "discrete latent tokens",
        "rFID",
        "ImageNet accuracy",
        "rFVD",
        "MSRVTT retrieval",
        "PSNR",
        "classification accuracy",
        "visual generation tasks",
        "text-to-video generation",
        "image-to-3D synthesis",
        "multimodal LLMs"
      ]
    },
    "publishedAt": "2025-09-17T19:11:18.000Z",
    "title": "AToken: A Unified Tokenizer for Vision",
    "summary": "We present AToken, the first unified visual tokenizer that achieves both\nhigh-fidelity reconstruction and semantic understanding across images, videos,\nand 3D assets. Unlike existing tokenizers that specialize in either\nreconstruction or understanding for single modalities, AToken encodes these\ndiverse visual inputs into a shared 4D latent space, unifying both tasks and\nmodalities in a single framework. Specifically, we introduce a pure transformer\narchitecture with 4D rotary position embeddings to process visual inputs of\narbitrary resolutions and temporal durations. To ensure stable training, we\nintroduce an adversarial-free training objective that combines perceptual and\nGram matrix losses, achieving state-of-the-art reconstruction quality. By\nemploying a progressive training curriculum, AToken gradually expands from\nsingle images, videos, and 3D, and supports both continuous and discrete latent\ntokens. AToken achieves 0.21 rFID with 82.2% ImageNet accuracy for images, 3.01\nrFVD with 32.6% MSRVTT retrieval for videos, and 28.19 PSNR with 90.9%\nclassification accuracy for 3D. In downstream applications, AToken enables both\nvisual generation tasks (e.g., image generation with continuous and discrete\ntokens, text-to-video generation, image-to-3D synthesis) and understanding\ntasks (e.g., multimodal LLMs), achieving competitive performance across all\nbenchmarks. These results shed light on the next-generation multimodal AI\nsystems built upon unified visual tokenization.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.14476.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 106
    },
    "isAuthorParticipating": false
  }
]