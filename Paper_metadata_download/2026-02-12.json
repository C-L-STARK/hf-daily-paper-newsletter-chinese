[
  {
    "paper": {
      "id": "2602.10224",
      "authors": [
        {
          "_id": "698d401f65c0d15a6d162015",
          "name": "Shiting Huang",
          "hidden": false
        },
        {
          "_id": "698d401f65c0d15a6d162016",
          "name": "Zecheng Li",
          "hidden": false
        },
        {
          "_id": "698d401f65c0d15a6d162017",
          "name": "Yu Zeng",
          "hidden": false
        },
        {
          "_id": "698d401f65c0d15a6d162018",
          "name": "Qingnan Ren",
          "hidden": false
        },
        {
          "_id": "698d401f65c0d15a6d162019",
          "name": "Zhen Fang",
          "hidden": false
        },
        {
          "_id": "698d401f65c0d15a6d16201a",
          "name": "Qisheng Su",
          "hidden": false
        },
        {
          "_id": "698d401f65c0d15a6d16201b",
          "name": "Kou Shi",
          "hidden": false
        },
        {
          "_id": "698d401f65c0d15a6d16201c",
          "name": "Lin Chen",
          "hidden": false
        },
        {
          "_id": "698d401f65c0d15a6d16201d",
          "name": "Zehui Chen",
          "hidden": false
        },
        {
          "_id": "698d401f65c0d15a6d16201e",
          "name": "Feng Zhao",
          "hidden": false
        }
      ],
      "publishedAt": "2026-02-10T19:16:09.000Z",
      "submittedOnDailyAt": "2026-02-12T00:27:31.569Z",
      "title": "Internalizing Meta-Experience into Memory for Guided Reinforcement Learning in Large Language Models",
      "submittedOnDailyBy": {
        "_id": "665d652e0f35c005de892108",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/665d652e0f35c005de892108/OGLbgZekX-3XTBkwS8k86.jpeg",
        "isPro": false,
        "fullname": "Yu Zeng",
        "user": "YuZeng260",
        "type": "user"
      },
      "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as an effective approach for enhancing the reasoning capabilities of Large Language Models (LLMs). Despite its efficacy, RLVR faces a meta-learning bottleneck: it lacks mechanisms for error attribution and experience internalization intrinsic to the human learning cycle beyond practice and verification, thereby limiting fine-grained credit assignment and reusable knowledge formation. We term such reusable knowledge representations derived from past errors as meta-experience. Based on this insight, we propose Meta-Experience Learning (MEL), a novel framework that incorporates self-distilled meta-experience into the model's parametric memory. Building upon standard RLVR, we introduce an additional design that leverages the LLM's self-verification capability to conduct contrastive analysis on paired correct and incorrect trajectories, identify the precise bifurcation points where reasoning errors arise, and summarize them into generalizable meta-experience. The meta-experience is further internalized into the LLM's parametric memory by minimizing the negative log-likelihood, which induces a language-modeled reward signal that bridges correct and incorrect reasoning trajectories and facilitates effective knowledge reuse. Experimental results demonstrate that MEL achieves consistent improvements on benchmarks, yielding 3.92%--4.73% Pass@1 gains across varying model sizes.",
      "upvotes": 8,
      "discussionId": "698d401f65c0d15a6d16201f",
      "ai_summary": "Meta-Experience Learning enhances LLM reasoning by incorporating self-distilled error representations into parametric memory through contrastive trajectory analysis and language-modeled reward signals.",
      "ai_keywords": [
        "reinforcement learning",
        "large language models",
        "meta-experience",
        "self-verification",
        "contrastive analysis",
        "parametric memory",
        "negative log-likelihood",
        "language-modeled reward",
        "Pass@1"
      ]
    },
    "publishedAt": "2026-02-10T14:16:09.000Z",
    "title": "Internalizing Meta-Experience into Memory for Guided Reinforcement Learning in Large Language Models",
    "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as an effective approach for enhancing the reasoning capabilities of Large Language Models (LLMs). Despite its efficacy, RLVR faces a meta-learning bottleneck: it lacks mechanisms for error attribution and experience internalization intrinsic to the human learning cycle beyond practice and verification, thereby limiting fine-grained credit assignment and reusable knowledge formation. We term such reusable knowledge representations derived from past errors as meta-experience. Based on this insight, we propose Meta-Experience Learning (MEL), a novel framework that incorporates self-distilled meta-experience into the model's parametric memory. Building upon standard RLVR, we introduce an additional design that leverages the LLM's self-verification capability to conduct contrastive analysis on paired correct and incorrect trajectories, identify the precise bifurcation points where reasoning errors arise, and summarize them into generalizable meta-experience. The meta-experience is further internalized into the LLM's parametric memory by minimizing the negative log-likelihood, which induces a language-modeled reward signal that bridges correct and incorrect reasoning trajectories and facilitates effective knowledge reuse. Experimental results demonstrate that MEL achieves consistent improvements on benchmarks, yielding 3.92%--4.73% Pass@1 gains across varying model sizes.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.10224.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "665d652e0f35c005de892108",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/665d652e0f35c005de892108/OGLbgZekX-3XTBkwS8k86.jpeg",
      "fullname": "Yu Zeng",
      "name": "YuZeng260",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6,
      "isUserFollowing": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2602.09713",
      "authors": [
        {
          "_id": "698beee66052d3bed9630a1c",
          "user": {
            "_id": "641590e5486c7c9a5d13fe35",
            "avatarUrl": "/avatars/2ef3432815e34a0eee45297fd99e5c40.svg",
            "isPro": false,
            "fullname": "Ruisi Zhao",
            "user": "zhaors00",
            "type": "user"
          },
          "name": "Ruisi Zhao",
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-02-11T11:17:15.023Z",
          "hidden": false
        },
        {
          "_id": "698beee66052d3bed9630a1d",
          "user": {
            "_id": "64173f238f689506e71091c4",
            "avatarUrl": "/avatars/dbfe7939d3f2bb846df8447485295cdc.svg",
            "isPro": false,
            "fullname": "Zheng Haoren",
            "user": "Zhroyn",
            "type": "user"
          },
          "name": "Haoren Zheng",
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-02-11T11:14:57.543Z",
          "hidden": false
        },
        {
          "_id": "698beee66052d3bed9630a1e",
          "name": "Zongxin Yang",
          "hidden": false
        },
        {
          "_id": "698beee66052d3bed9630a1f",
          "name": "Hehe Fan",
          "hidden": false
        },
        {
          "_id": "698beee66052d3bed9630a20",
          "name": "Yi Yang",
          "hidden": false
        }
      ],
      "publishedAt": "2026-02-10T12:17:00.000Z",
      "submittedOnDailyAt": "2026-02-12T00:00:01.426Z",
      "title": "Stroke3D: Lifting 2D strokes into rigged 3D model via latent diffusion models",
      "submittedOnDailyBy": {
        "_id": "641590e5486c7c9a5d13fe35",
        "avatarUrl": "/avatars/2ef3432815e34a0eee45297fd99e5c40.svg",
        "isPro": false,
        "fullname": "Ruisi Zhao",
        "user": "zhaors00",
        "type": "user"
      },
      "summary": "Rigged 3D assets are fundamental to 3D deformation and animation. However, existing 3D generation methods face challenges in generating animatable geometry, while rigging techniques lack fine-grained structural control over skeleton creation. To address these limitations, we introduce Stroke3D, a novel framework that directly generates rigged meshes from user inputs: 2D drawn strokes and a descriptive text prompt. Our approach pioneers a two-stage pipeline that separates the generation into: 1) Controllable Skeleton Generation, we employ the Skeletal Graph VAE (Sk-VAE) to encode the skeleton's graph structure into a latent space, where the Skeletal Graph DiT (Sk-DiT) generates a skeletal embedding. The generation process is conditioned on both the text for semantics and the 2D strokes for explicit structural control, with the VAE's decoder reconstructing the final high-quality 3D skeleton; and 2) Enhanced Mesh Synthesis via TextuRig and SKA-DPO, where we then synthesize a textured mesh conditioned on the generated skeleton. For this stage, we first enhance an existing skeleton-to-mesh model by augmenting its training data with TextuRig: a dataset of textured and rigged meshes with captions, curated from Objaverse-XL. Additionally, we employ a preference optimization strategy, SKA-DPO, guided by a skeleton-mesh alignment score, to further improve geometric fidelity. Together, our framework enables a more intuitive workflow for creating ready to animate 3D content. To the best of our knowledge, our work is the first to generate rigged 3D meshes conditioned on user-drawn 2D strokes. Extensive experiments demonstrate that Stroke3D produces plausible skeletons and high-quality meshes.",
      "upvotes": 7,
      "discussionId": "698beee66052d3bed9630a21",
      "projectPage": "https://github.com/Whalesong-zrs/Stroke3D_project_page",
      "githubRepo": "https://github.com/Whalesong-zrs/Stroke3D",
      "githubRepoAddedBy": "user",
      "ai_summary": "Stroke3D generates rigged 3D meshes from 2D strokes and text prompts through a two-stage pipeline combining controllable skeleton generation with enhanced mesh synthesis.",
      "ai_keywords": [
        "Skeletal Graph VAE",
        "Sk-VAE",
        "Skeletal Graph DiT",
        "Sk-DiT",
        "TextuRig",
        "SKA-DPO",
        "skeleton-mesh alignment score",
        "Objaverse-XL",
        "preference optimization",
        "controllable skeleton generation",
        "enhanced mesh synthesis"
      ],
      "githubStars": 16,
      "organization": {
        "_id": "6345aadf5efccdc07f1365a5",
        "name": "ZhejiangUniversity",
        "fullname": "Zhejiang University"
      }
    },
    "publishedAt": "2026-02-10T07:17:00.000Z",
    "title": "Stroke3D: Lifting 2D strokes into rigged 3D model via latent diffusion models",
    "summary": "Rigged 3D assets are fundamental to 3D deformation and animation. However, existing 3D generation methods face challenges in generating animatable geometry, while rigging techniques lack fine-grained structural control over skeleton creation. To address these limitations, we introduce Stroke3D, a novel framework that directly generates rigged meshes from user inputs: 2D drawn strokes and a descriptive text prompt. Our approach pioneers a two-stage pipeline that separates the generation into: 1) Controllable Skeleton Generation, we employ the Skeletal Graph VAE (Sk-VAE) to encode the skeleton's graph structure into a latent space, where the Skeletal Graph DiT (Sk-DiT) generates a skeletal embedding. The generation process is conditioned on both the text for semantics and the 2D strokes for explicit structural control, with the VAE's decoder reconstructing the final high-quality 3D skeleton; and 2) Enhanced Mesh Synthesis via TextuRig and SKA-DPO, where we then synthesize a textured mesh conditioned on the generated skeleton. For this stage, we first enhance an existing skeleton-to-mesh model by augmenting its training data with TextuRig: a dataset of textured and rigged meshes with captions, curated from Objaverse-XL. Additionally, we employ a preference optimization strategy, SKA-DPO, guided by a skeleton-mesh alignment score, to further improve geometric fidelity. Together, our framework enables a more intuitive workflow for creating ready to animate 3D content. To the best of our knowledge, our work is the first to generate rigged 3D meshes conditioned on user-drawn 2D strokes. Extensive experiments demonstrate that Stroke3D produces plausible skeletons and high-quality meshes.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.09713.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "641590e5486c7c9a5d13fe35",
      "avatarUrl": "/avatars/2ef3432815e34a0eee45297fd99e5c40.svg",
      "fullname": "Ruisi Zhao",
      "name": "zhaors00",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "6345aadf5efccdc07f1365a5",
      "name": "ZhejiangUniversity",
      "fullname": "Zhejiang University"
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2602.08253",
      "authors": [
        {
          "_id": "698bf9256052d3bed9630ad3",
          "user": {
            "_id": "698ae59d70be6790cd1e6a53",
            "avatarUrl": "/avatars/66125d952e7880baa4fe1635aafbc160.svg",
            "isPro": false,
            "fullname": "Baoyun Zhao",
            "user": "ZBoyn",
            "type": "user"
          },
          "name": "Baoyun Zhao",
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-02-11T11:14:17.040Z",
          "hidden": false
        },
        {
          "_id": "698bf9256052d3bed9630ad4",
          "name": "He Wang",
          "hidden": false
        },
        {
          "_id": "698bf9256052d3bed9630ad5",
          "name": "Liang Zeng",
          "hidden": false
        }
      ],
      "publishedAt": "2026-02-09T04:13:35.000Z",
      "submittedOnDailyAt": "2026-02-12T00:10:48.214Z",
      "title": "G-LNS: Generative Large Neighborhood Search for LLM-Based Automatic Heuristic Design",
      "submittedOnDailyBy": {
        "_id": "698ae59d70be6790cd1e6a53",
        "avatarUrl": "/avatars/66125d952e7880baa4fe1635aafbc160.svg",
        "isPro": false,
        "fullname": "Baoyun Zhao",
        "user": "ZBoyn",
        "type": "user"
      },
      "summary": "While Large Language Models (LLMs) have recently shown promise in Automated Heuristic Design (AHD), existing approaches typically formulate AHD around constructive priority rules or parameterized local search guidance, thereby restricting the search space to fixed heuristic forms. Such designs offer limited capacity for structural exploration, making it difficult to escape deep local optima in complex Combinatorial Optimization Problems (COPs). In this work, we propose G-LNS, a generative evolutionary framework that extends LLM-based AHD to the automated design of Large Neighborhood Search (LNS) operators. Unlike prior methods that evolve heuristics in isolation, G-LNS leverages LLMs to co-evolve tightly coupled pairs of destroy and repair operators. A cooperative evaluation mechanism explicitly captures their interaction, enabling the discovery of complementary operator logic that jointly performs effective structural disruption and reconstruction. Extensive experiments on challenging COP benchmarks, such as Traveling Salesman Problems (TSP) and Capacitated Vehicle Routing Problems (CVRP), demonstrate that G-LNS significantly outperforms LLM-based AHD methods as well as strong classical solvers. The discovered heuristics not only achieve near-optimal solutions with reduced computational budgets but also exhibit robust generalization across diverse and unseen instance distributions.",
      "upvotes": 6,
      "discussionId": "698bf9256052d3bed9630ad6",
      "projectPage": "https://zboyn.github.io/G-LNS/",
      "githubRepo": "https://github.com/ZBoyn/G-LNS",
      "githubRepoAddedBy": "user",
      "ai_summary": "A generative evolutionary framework extends large language models for automated design of large neighborhood search operators in combinatorial optimization problems.",
      "ai_keywords": [
        "Large Language Models",
        "Automated Heuristic Design",
        "Combinatorial Optimization Problems",
        "Large Neighborhood Search",
        "generative evolutionary framework",
        "constructive priority rules",
        "parameterized local search",
        "deep local optima",
        "Traveling Salesman Problems",
        "Capacitated Vehicle Routing Problems"
      ],
      "githubStars": 11
    },
    "publishedAt": "2026-02-08T23:13:35.000Z",
    "title": "G-LNS: Generative Large Neighborhood Search for LLM-Based Automatic Heuristic Design",
    "summary": "While Large Language Models (LLMs) have recently shown promise in Automated Heuristic Design (AHD), existing approaches typically formulate AHD around constructive priority rules or parameterized local search guidance, thereby restricting the search space to fixed heuristic forms. Such designs offer limited capacity for structural exploration, making it difficult to escape deep local optima in complex Combinatorial Optimization Problems (COPs). In this work, we propose G-LNS, a generative evolutionary framework that extends LLM-based AHD to the automated design of Large Neighborhood Search (LNS) operators. Unlike prior methods that evolve heuristics in isolation, G-LNS leverages LLMs to co-evolve tightly coupled pairs of destroy and repair operators. A cooperative evaluation mechanism explicitly captures their interaction, enabling the discovery of complementary operator logic that jointly performs effective structural disruption and reconstruction. Extensive experiments on challenging COP benchmarks, such as Traveling Salesman Problems (TSP) and Capacitated Vehicle Routing Problems (CVRP), demonstrate that G-LNS significantly outperforms LLM-based AHD methods as well as strong classical solvers. The discovered heuristics not only achieve near-optimal solutions with reduced computational budgets but also exhibit robust generalization across diverse and unseen instance distributions.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.08253.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "698ae59d70be6790cd1e6a53",
      "avatarUrl": "/avatars/66125d952e7880baa4fe1635aafbc160.svg",
      "fullname": "Baoyun Zhao",
      "name": "ZBoyn",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1,
      "isUserFollowing": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2602.10999",
      "authors": [
        {
          "_id": "698d437e65c0d15a6d162132",
          "name": "Yusong Lin",
          "hidden": false
        },
        {
          "_id": "698d437e65c0d15a6d162133",
          "name": "Haiyang Wang",
          "hidden": false
        },
        {
          "_id": "698d437e65c0d15a6d162134",
          "name": "Shuzhe Wu",
          "hidden": false
        },
        {
          "_id": "698d437e65c0d15a6d162135",
          "name": "Lue Fan",
          "hidden": false
        },
        {
          "_id": "698d437e65c0d15a6d162136",
          "name": "Feiyang Pan",
          "hidden": false
        },
        {
          "_id": "698d437e65c0d15a6d162137",
          "name": "Sanyuan Zhao",
          "hidden": false
        },
        {
          "_id": "698d437e65c0d15a6d162138",
          "name": "Dandan Tu",
          "hidden": false
        }
      ],
      "publishedAt": "2026-02-11T16:22:18.000Z",
      "submittedOnDailyAt": "2026-02-12T00:35:37.899Z",
      "title": "CLI-Gym: Scalable CLI Task Generation via Agentic Environment Inversion",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Agentic coding requires agents to effectively interact with runtime environments, e.g., command line interfaces (CLI), so as to complete tasks like resolving dependency issues, fixing system problems, etc. But it remains underexplored how such environment-intensive tasks can be obtained at scale to enhance agents' capabilities. To address this, based on an analogy between the Dockerfile and the agentic task, we propose to employ agents to simulate and explore environment histories, guided by execution feedback. By tracing histories of a healthy environment, its state can be inverted to an earlier one with runtime failures, from which a task can be derived by packing the buggy state and the corresponding error messages. With our method, named CLI-Gym, a total of 1,655 environment-intensive tasks are derived, being the largest collection of its kind. Moreover, with curated successful trajectories, our fine-tuned model, named LiberCoder, achieves substantial absolute improvements of +21.1% (to 46.1%) on Terminal-Bench, outperforming various strong baselines. To our knowledge, this is the first public pipeline for scalable derivation of environment-intensive tasks.",
      "upvotes": 5,
      "discussionId": "698d437e65c0d15a6d162139",
      "githubRepo": "https://github.com/LiberCoders/CLI-Gym",
      "githubRepoAddedBy": "user",
      "ai_summary": "CLI-Gym enables scalable derivation of environment-intensive tasks by simulating and exploring environment histories, while LiberCoder achieves significant performance improvements on Terminal-Bench through fine-tuning.",
      "ai_keywords": [
        "CLI-Gym",
        "LiberCoder",
        "environment-intensive tasks",
        "Dockerfile",
        "agentic task",
        "execution feedback",
        "Terminal-Bench"
      ],
      "githubStars": 5
    },
    "publishedAt": "2026-02-11T11:22:18.000Z",
    "title": "CLI-Gym: Scalable CLI Task Generation via Agentic Environment Inversion",
    "summary": "Agentic coding requires agents to effectively interact with runtime environments, e.g., command line interfaces (CLI), so as to complete tasks like resolving dependency issues, fixing system problems, etc. But it remains underexplored how such environment-intensive tasks can be obtained at scale to enhance agents' capabilities. To address this, based on an analogy between the Dockerfile and the agentic task, we propose to employ agents to simulate and explore environment histories, guided by execution feedback. By tracing histories of a healthy environment, its state can be inverted to an earlier one with runtime failures, from which a task can be derived by packing the buggy state and the corresponding error messages. With our method, named CLI-Gym, a total of 1,655 environment-intensive tasks are derived, being the largest collection of its kind. Moreover, with curated successful trajectories, our fine-tuned model, named LiberCoder, achieves substantial absolute improvements of +21.1% (to 46.1%) on Terminal-Bench, outperforming various strong baselines. To our knowledge, this is the first public pipeline for scalable derivation of environment-intensive tasks.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.10999.png",
    "numComments": 0,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 230,
      "isUserFollowing": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2602.10177",
      "authors": [
        {
          "_id": "698d426265c0d15a6d162113",
          "name": "Tony Feng",
          "hidden": false
        },
        {
          "_id": "698d426265c0d15a6d162114",
          "name": "Trieu H. Trinh",
          "hidden": false
        },
        {
          "_id": "698d426265c0d15a6d162115",
          "name": "Garrett Bingham",
          "hidden": false
        },
        {
          "_id": "698d426265c0d15a6d162116",
          "name": "Dawsen Hwang",
          "hidden": false
        },
        {
          "_id": "698d426265c0d15a6d162117",
          "name": "Yuri Chervonyi",
          "hidden": false
        },
        {
          "_id": "698d426265c0d15a6d162118",
          "name": "Junehyuk Jung",
          "hidden": false
        },
        {
          "_id": "698d426265c0d15a6d162119",
          "name": "Joonkyung Lee",
          "hidden": false
        },
        {
          "_id": "698d426265c0d15a6d16211a",
          "name": "Carlo Pagano",
          "hidden": false
        },
        {
          "_id": "698d426265c0d15a6d16211b",
          "name": "Sang-hyun Kim",
          "hidden": false
        },
        {
          "_id": "698d426265c0d15a6d16211c",
          "name": "Federico Pasqualotto",
          "hidden": false
        },
        {
          "_id": "698d426265c0d15a6d16211d",
          "name": "Sergei Gukov",
          "hidden": false
        },
        {
          "_id": "698d426265c0d15a6d16211e",
          "name": "Jonathan N. Lee",
          "hidden": false
        },
        {
          "_id": "698d426265c0d15a6d16211f",
          "name": "Junsu Kim",
          "hidden": false
        },
        {
          "_id": "698d426265c0d15a6d162120",
          "name": "Kaiying Hou",
          "hidden": false
        },
        {
          "_id": "698d426265c0d15a6d162121",
          "name": "Golnaz Ghiasi",
          "hidden": false
        },
        {
          "_id": "698d426265c0d15a6d162122",
          "name": "Yi Tay",
          "hidden": false
        },
        {
          "_id": "698d426265c0d15a6d162123",
          "name": "YaGuang Li",
          "hidden": false
        },
        {
          "_id": "698d426265c0d15a6d162124",
          "name": "Chenkai Kuang",
          "hidden": false
        },
        {
          "_id": "698d426265c0d15a6d162125",
          "name": "Yuan Liu",
          "hidden": false
        },
        {
          "_id": "698d426265c0d15a6d162126",
          "name": "Hanzhao",
          "hidden": false
        },
        {
          "_id": "698d426265c0d15a6d162127",
          "name": "Lin",
          "hidden": false
        },
        {
          "_id": "698d426265c0d15a6d162128",
          "name": "Evan Zheran Liu",
          "hidden": false
        },
        {
          "_id": "698d426265c0d15a6d162129",
          "name": "Nigamaa Nayakanti",
          "hidden": false
        },
        {
          "_id": "698d426265c0d15a6d16212a",
          "name": "Xiaomeng Yang",
          "hidden": false
        },
        {
          "_id": "698d426265c0d15a6d16212b",
          "name": "Heng-tze Cheng",
          "hidden": false
        },
        {
          "_id": "698d426265c0d15a6d16212c",
          "name": "Demis Hassabis",
          "hidden": false
        },
        {
          "_id": "698d426265c0d15a6d16212d",
          "name": "Koray Kavukcuoglu",
          "hidden": false
        },
        {
          "_id": "698d426265c0d15a6d16212e",
          "name": "Quoc V. Le",
          "hidden": false
        },
        {
          "_id": "698d426265c0d15a6d16212f",
          "name": "Thang Luong",
          "hidden": false
        }
      ],
      "publishedAt": "2026-02-10T18:50:15.000Z",
      "submittedOnDailyAt": "2026-02-12T00:30:55.790Z",
      "title": "Towards Autonomous Mathematics Research",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Recent advances in foundational models have yielded reasoning systems capable of achieving a gold-medal standard at the International Mathematical Olympiad. The transition from competition-level problem-solving to professional research, however, requires navigating vast literature and constructing long-horizon proofs. In this work, we introduce Aletheia, a math research agent that iteratively generates, verifies, and revises solutions end-to-end in natural language. Specifically, Aletheia is powered by an advanced version of Gemini Deep Think for challenging reasoning problems, a novel inference-time scaling law that extends beyond Olympiad-level problems, and intensive tool use to navigate the complexities of mathematical research. We demonstrate the capability of Aletheia from Olympiad problems to PhD-level exercises and most notably, through several distinct milestones in AI-assisted mathematics research: (a) a research paper (Feng26) generated by AI without any human intervention in calculating certain structure constants in arithmetic geometry called eigenweights; (b) a research paper (LeeSeo26) demonstrating human-AI collaboration in proving bounds on systems of interacting particles called independent sets; and (c) an extensive semi-autonomous evaluation (Feng et al., 2026a) of 700 open problems on Bloom's Erdos Conjectures database, including autonomous solutions to four open questions. In order to help the public better understand the developments pertaining to AI and mathematics, we suggest codifying standard levels quantifying autonomy and novelty of AI-assisted results. We conclude with reflections on human-AI collaboration in mathematics.",
      "upvotes": 5,
      "discussionId": "698d426365c0d15a6d162130",
      "ai_summary": "Aletheia, a math research agent, demonstrates advanced reasoning capabilities by generating and verifying solutions end-to-end in natural language, achieving autonomous research outcomes from Olympiad problems to PhD-level exercises and contributing to AI-assisted mathematical research.",
      "ai_keywords": [
        "foundational models",
        "reasoning systems",
        "International Mathematical Olympiad",
        "mathematical research",
        "AI-assisted mathematics",
        "autonomous research",
        "human-AI collaboration",
        "proof construction",
        "inference-time scaling law",
        "tool use",
        "natural language processing",
        "mathematical verification"
      ],
      "organization": {
        "_id": "5e6aca39878b8b2bf9806447",
        "name": "google",
        "fullname": "Google",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/WtA3YYitedOr9n02eHfJe.png"
      }
    },
    "publishedAt": "2026-02-10T13:50:15.000Z",
    "title": "Towards Autonomous Mathematics Research",
    "summary": "Recent advances in foundational models have yielded reasoning systems capable of achieving a gold-medal standard at the International Mathematical Olympiad. The transition from competition-level problem-solving to professional research, however, requires navigating vast literature and constructing long-horizon proofs. In this work, we introduce Aletheia, a math research agent that iteratively generates, verifies, and revises solutions end-to-end in natural language. Specifically, Aletheia is powered by an advanced version of Gemini Deep Think for challenging reasoning problems, a novel inference-time scaling law that extends beyond Olympiad-level problems, and intensive tool use to navigate the complexities of mathematical research. We demonstrate the capability of Aletheia from Olympiad problems to PhD-level exercises and most notably, through several distinct milestones in AI-assisted mathematics research: (a) a research paper (Feng26) generated by AI without any human intervention in calculating certain structure constants in arithmetic geometry called eigenweights; (b) a research paper (LeeSeo26) demonstrating human-AI collaboration in proving bounds on systems of interacting particles called independent sets; and (c) an extensive semi-autonomous evaluation (Feng et al., 2026a) of 700 open problems on Bloom's Erdos Conjectures database, including autonomous solutions to four open questions. In order to help the public better understand the developments pertaining to AI and mathematics, we suggest codifying standard levels quantifying autonomy and novelty of AI-assisted results. We conclude with reflections on human-AI collaboration in mathematics.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.10177.png",
    "numComments": 0,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 230,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "5e6aca39878b8b2bf9806447",
      "name": "google",
      "fullname": "Google",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/WtA3YYitedOr9n02eHfJe.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2602.10604",
      "authors": [
        {
          "_id": "698d417065c0d15a6d162026",
          "name": "Ailin Huang",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d162027",
          "name": "Ang Li",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d162028",
          "name": "Aobo Kong",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d162029",
          "name": "Bin Wang",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d16202a",
          "name": "Binxing Jiao",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d16202b",
          "name": "Bo Dong",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d16202c",
          "name": "Bojun Wang",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d16202d",
          "name": "Boyu Chen",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d16202e",
          "name": "Brian Li",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d16202f",
          "name": "Buyun Ma",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d162030",
          "name": "Chang Su",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d162031",
          "name": "Changxin Miao",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d162032",
          "name": "Changyi Wan",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d162033",
          "name": "Chao Lou",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d162034",
          "name": "Chen Hu",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d162035",
          "name": "Chen Xu",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d162036",
          "name": "Chenfeng Yu",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d162037",
          "name": "Chengting Feng",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d162038",
          "name": "Chengyuan Yao",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d162039",
          "name": "Chunrui Han",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d16203a",
          "name": "Dan Ma",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d16203b",
          "name": "Dapeng Shi",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d16203c",
          "name": "Daxin Jiang",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d16203d",
          "name": "Dehua Ma",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d16203e",
          "name": "Deshan Sun",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d16203f",
          "name": "Di Qi",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d162040",
          "name": "Enle Liu",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d162041",
          "name": "Fajie Zhang",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d162042",
          "name": "Fanqi Wan",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d162043",
          "name": "Guanzhe Huang",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d162044",
          "name": "Gulin Yan",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d162045",
          "name": "Guoliang Cao",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d162046",
          "name": "Guopeng Li",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d162047",
          "name": "Han Cheng",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d162048",
          "name": "Hangyu Guo",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d162049",
          "name": "Hanshan Zhang",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d16204a",
          "name": "Hao Nie",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d16204b",
          "name": "Haonan Jia",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d16204c",
          "name": "Haoran Lv",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d16204d",
          "name": "Hebin Zhou",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d16204e",
          "name": "Hekun Lv",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d16204f",
          "name": "Heng Wang",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d162050",
          "name": "Heung-Yeung Shum",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d162051",
          "name": "Hongbo Huang",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d162052",
          "name": "Hongbo Peng",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d162053",
          "name": "Hongyu Zhou",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d162054",
          "name": "Hongyuan Wang",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d162055",
          "name": "Houyong Chen",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d162056",
          "name": "Huangxi Zhu",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d162057",
          "name": "Huimin Wu",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d162058",
          "name": "Huiyong Guo",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d162059",
          "name": "Jia Wang",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d16205a",
          "name": "Jian Zhou",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d16205b",
          "name": "Jianjian Sun",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d16205c",
          "name": "Jiaoren Wu",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d16205d",
          "name": "Jiaran Zhang",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d16205e",
          "name": "Jiashu Lv",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d16205f",
          "name": "Jiashuo Liu",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d162060",
          "name": "Jiayi Fu",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d162061",
          "name": "Jiayu Liu",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d162062",
          "name": "Jie Cheng",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d162063",
          "name": "Jie Luo",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d162064",
          "name": "Jie Yang",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d162065",
          "name": "Jie Zhou",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d162066",
          "name": "Jieyi Hou",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d162067",
          "name": "Jing Bai",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d162068",
          "name": "Jingcheng Hu",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d162069",
          "name": "Jingjing Xie",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d16206a",
          "name": "Jingwei Wu",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d16206b",
          "name": "Jingyang Zhang",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d16206c",
          "name": "Jishi Zhou",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d16206d",
          "name": "Junfeng Liu",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d16206e",
          "name": "Junzhe Lin",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d16206f",
          "name": "Ka Man Lo",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d162070",
          "name": "Kai Liang",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d162071",
          "name": "Kaibo Liu",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d162072",
          "name": "Kaijun Tan",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d162073",
          "name": "Kaiwen Yan",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d162074",
          "name": "Kaixiang Li",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d162075",
          "name": "Kang An",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d162076",
          "name": "Kangheng Lin",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d162077",
          "name": "Lei Yang",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d162078",
          "name": "Liang Lv",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d162079",
          "name": "Liang Zhao",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d16207a",
          "name": "Liangyu Chen",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d16207b",
          "name": "Lieyu Shi",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d16207c",
          "name": "Liguo Tan",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d16207d",
          "name": "Lin Lin",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d16207e",
          "name": "Lina Chen",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d16207f",
          "name": "Luck Ma",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d162080",
          "name": "Mengqiang Ren",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d162081",
          "name": "Michael Li",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d162082",
          "name": "Ming Li",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d162083",
          "name": "Mingliang Li",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d162084",
          "name": "Mingming Zhang",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d162085",
          "name": "Mingrui Chen",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d162086",
          "name": "Mitt Huang",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d162087",
          "name": "Na Wang",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d162088",
          "name": "Peng Liu",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d162089",
          "name": "Qi Han",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d16208a",
          "name": "Qian Zhao",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d16208b",
          "name": "Qinglin He",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d16208c",
          "name": "Qinxin Du",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d16208d",
          "name": "Qiuping Wu",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d16208e",
          "name": "Quan Sun",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d16208f",
          "name": "Rongqiu Yang",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d162090",
          "name": "Ruihang Miao",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d162091",
          "name": "Ruixin Han",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d162092",
          "name": "Ruosi Wan",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d162093",
          "name": "Ruyan Guo",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d162094",
          "name": "Shan Wang",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d162095",
          "name": "Shaoliang Pang",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d162096",
          "name": "Shaowen Yang",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d162097",
          "name": "Shengjie Fan",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d162098",
          "name": "Shijie Shang",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d162099",
          "name": "Shiliang Yang",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d16209a",
          "name": "Shiwei Li",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d16209b",
          "name": "Shuangshuang Tian",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d16209c",
          "name": "Siqi Liu",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d16209d",
          "name": "Siye Wu",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d16209e",
          "name": "Siyu Chen",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d16209f",
          "name": "Song Yuan",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d1620a0",
          "name": "Tiancheng Cao",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d1620a1",
          "name": "Tianchi Yue",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d1620a2",
          "name": "Tianhao Cheng",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d1620a3",
          "name": "Tianning Li",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d1620a4",
          "name": "Tingdan Luo",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d1620a5",
          "name": "Wang You",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d1620a6",
          "name": "Wei Ji",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d1620a7",
          "name": "Wei Yuan",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d1620a8",
          "name": "Wei Zhang",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d1620a9",
          "name": "Weibo Wu",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d1620aa",
          "name": "Weihao Xie",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d1620ab",
          "name": "Wen Sun",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d1620ac",
          "name": "Wenjin Deng",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d1620ad",
          "name": "Wenzhen Zheng",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d1620ae",
          "name": "Wuxun Xie",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d1620af",
          "name": "Xiangfeng Wang",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d1620b0",
          "name": "Xiangwen Kong",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d1620b1",
          "name": "Xiangyu Liu",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d1620b2",
          "name": "Xiangyu Zhang",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d1620b3",
          "name": "Xiaobo Yang",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d1620b4",
          "name": "Xiaojia Liu",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d1620b5",
          "name": "Xiaolan Yuan",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d1620b6",
          "name": "Xiaoran Jiao",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d1620b7",
          "name": "Xiaoxiao Ren",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d1620b8",
          "name": "Xiaoyun Zhang",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d1620b9",
          "name": "Xin Li",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d1620ba",
          "name": "Xin Liu",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d1620bb",
          "name": "Xin Wu",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d1620bc",
          "name": "Xing Chen",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d1620bd",
          "name": "Xingping Yang",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d1620be",
          "name": "Xinran Wang",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d1620bf",
          "name": "Xu Zhao",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d1620c0",
          "name": "Xuan He",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d1620c1",
          "name": "Xuanti Feng",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d1620c2",
          "name": "Xuedan Cai",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d1620c3",
          "name": "Xuqiang Zhou",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d1620c4",
          "name": "Yanbo Yu",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d1620c5",
          "name": "Yang Li",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d1620c6",
          "name": "Yang Xu",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d1620c7",
          "name": "Yanlin Lai",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d1620c8",
          "name": "Yanming Xu",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d1620c9",
          "name": "Yaoyu Wang",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d1620ca",
          "name": "Yeqing Shen",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d1620cb",
          "name": "Yibo Zhu",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d1620cc",
          "name": "Yichen Lv",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d1620cd",
          "name": "Yicheng Cao",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d1620ce",
          "name": "Yifeng Gong",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d1620cf",
          "name": "Yijing Yang",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d1620d0",
          "name": "Yikun Yang",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d1620d1",
          "name": "Yin Zhao",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d1620d2",
          "name": "Yingxiu Zhao",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d1620d3",
          "name": "Yinmin Zhang",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d1620d4",
          "name": "Yitong Zhang",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d1620d5",
          "name": "Yixuan Zhang",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d1620d6",
          "name": "Yiyang Chen",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d1620d7",
          "name": "Yongchi Zhao",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d1620d8",
          "name": "Yongshen Long",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d1620d9",
          "name": "Yongyao Wang",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d1620da",
          "name": "Yousong Guan",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d1620db",
          "name": "Yu Zhou",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d1620dc",
          "name": "Yuang Peng",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d1620dd",
          "name": "Yuanhao Ding",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d1620de",
          "name": "Yuantao Fan",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d1620df",
          "name": "Yuanzhen Yang",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d1620e0",
          "name": "Yuchu Luo",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d1620e1",
          "name": "Yudi Zhao",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d1620e2",
          "name": "Yue Peng",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d1620e3",
          "name": "Yueqiang Lin",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d1620e4",
          "name": "Yufan Lu",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d1620e5",
          "name": "Yuling Zhao",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d1620e6",
          "name": "Yunzhou Ju",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d1620e7",
          "name": "Yurong Zhang",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d1620e8",
          "name": "Yusheng Li",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d1620e9",
          "name": "Yuxiang Yang",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d1620ea",
          "name": "Yuyang Chen",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d1620eb",
          "name": "Yuzhu Cai",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d1620ec",
          "name": "Zejia Weng",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d1620ed",
          "name": "Zetao Hong",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d1620ee",
          "name": "Zexi Li",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d1620ef",
          "name": "Zhe Xie",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d1620f0",
          "name": "Zheng Ge",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d1620f1",
          "name": "Zheng Gong",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d1620f2",
          "name": "Zheng Zeng",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d1620f3",
          "name": "Zhenyi Lu",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d1620f4",
          "name": "Zhewei Huang",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d1620f5",
          "name": "Zhichao Chang",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d1620f6",
          "name": "Zhiguo Huang",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d1620f7",
          "name": "Zhiheng Hu",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d1620f8",
          "name": "Zidong Yang",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d1620f9",
          "name": "Zili Wang",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d1620fa",
          "name": "Ziqi Ren",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d1620fb",
          "name": "Zixin Zhang",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d1620fc",
          "name": "Zixuan Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2026-02-11T07:53:51.000Z",
      "submittedOnDailyAt": "2026-02-12T00:26:49.880Z",
      "title": "Step 3.5 Flash: Open Frontier-Level Intelligence with 11B Active Parameters",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "We introduce Step 3.5 Flash, a sparse Mixture-of-Experts (MoE) model that bridges frontier-level agentic intelligence and computational efficiency. We focus on what matters most when building agents: sharp reasoning and fast, reliable execution. Step 3.5 Flash pairs a 196B-parameter foundation with 11B active parameters for efficient inference. It is optimized with interleaved 3:1 sliding-window/full attention and Multi-Token Prediction (MTP-3) to reduce the latency and cost of multi-round agentic interactions. To reach frontier-level intelligence, we design a scalable reinforcement learning framework that combines verifiable signals with preference feedback, while remaining stable under large-scale off-policy training, enabling consistent self-improvement across mathematics, code, and tool use. Step 3.5 Flash demonstrates strong performance across agent, coding, and math tasks, achieving 85.4% on IMO-AnswerBench, 86.4% on LiveCodeBench-v6 (2024.08-2025.05), 88.2% on tau2-Bench, 69.0% on BrowseComp (with context management), and 51.0% on Terminal-Bench 2.0, comparable to frontier models such as GPT-5.2 xHigh and Gemini 3.0 Pro. By redefining the efficiency frontier, Step 3.5 Flash provides a high-density foundation for deploying sophisticated agents in real-world industrial environments.",
      "upvotes": 4,
      "discussionId": "698d417165c0d15a6d1620fd",
      "githubRepo": "https://github.com/stepfun-ai/Step-3.5-Flash",
      "githubRepoAddedBy": "user",
      "ai_summary": "Step 3.5 Flash is a sparse Mixture-of-Experts model that achieves frontier-level agentic intelligence through efficient parameter utilization and optimized attention mechanisms, demonstrating strong performance across multiple benchmarks.",
      "ai_keywords": [
        "Mixture-of-Experts",
        "sparse MoE",
        "foundation model",
        "active parameters",
        "interleaved attention",
        "sliding-window attention",
        "full attention",
        "Multi-Token Prediction",
        "reinforcement learning",
        "verifiable signals",
        "preference feedback",
        "off-policy training",
        "self-improvement",
        "IMO-AnswerBench",
        "LiveCodeBench",
        "tau2-Bench",
        "BrowseComp",
        "Terminal-Bench"
      ],
      "githubStars": 1221,
      "organization": {
        "_id": "66e43eae9d477f566f937935",
        "name": "stepfun-ai",
        "fullname": "StepFun",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66935cee39002fc0569c2943/Qv8QPbkgoKE3wR4jTzHiy.png"
      }
    },
    "publishedAt": "2026-02-11T02:53:51.000Z",
    "title": "Step 3.5 Flash: Open Frontier-Level Intelligence with 11B Active Parameters",
    "summary": "We introduce Step 3.5 Flash, a sparse Mixture-of-Experts (MoE) model that bridges frontier-level agentic intelligence and computational efficiency. We focus on what matters most when building agents: sharp reasoning and fast, reliable execution. Step 3.5 Flash pairs a 196B-parameter foundation with 11B active parameters for efficient inference. It is optimized with interleaved 3:1 sliding-window/full attention and Multi-Token Prediction (MTP-3) to reduce the latency and cost of multi-round agentic interactions. To reach frontier-level intelligence, we design a scalable reinforcement learning framework that combines verifiable signals with preference feedback, while remaining stable under large-scale off-policy training, enabling consistent self-improvement across mathematics, code, and tool use. Step 3.5 Flash demonstrates strong performance across agent, coding, and math tasks, achieving 85.4% on IMO-AnswerBench, 86.4% on LiveCodeBench-v6 (2024.08-2025.05), 88.2% on tau2-Bench, 69.0% on BrowseComp (with context management), and 51.0% on Terminal-Bench 2.0, comparable to frontier models such as GPT-5.2 xHigh and Gemini 3.0 Pro. By redefining the efficiency frontier, Step 3.5 Flash provides a high-density foundation for deploying sophisticated agents in real-world industrial environments.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.10604.png",
    "numComments": 0,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 230,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "66e43eae9d477f566f937935",
      "name": "stepfun-ai",
      "fullname": "StepFun",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66935cee39002fc0569c2943/Qv8QPbkgoKE3wR4jTzHiy.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2602.10179",
      "authors": [
        {
          "_id": "698d419065c0d15a6d1620ff",
          "name": "Jiacheng Hou",
          "hidden": false
        },
        {
          "_id": "698d419065c0d15a6d162100",
          "name": "Yining Sun",
          "hidden": false
        },
        {
          "_id": "698d419065c0d15a6d162101",
          "name": "Ruochong Jin",
          "hidden": false
        },
        {
          "_id": "698d419065c0d15a6d162102",
          "name": "Haochen Han",
          "hidden": false
        },
        {
          "_id": "698d419065c0d15a6d162103",
          "name": "Fangming Liu",
          "hidden": false
        },
        {
          "_id": "698d419065c0d15a6d162104",
          "name": "Wai Kin Victor Chan",
          "hidden": false
        },
        {
          "_id": "698d419065c0d15a6d162105",
          "name": "Alex Jinpeng Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2026-02-10T18:59:55.000Z",
      "submittedOnDailyAt": "2026-02-12T00:27:39.577Z",
      "title": "When the Prompt Becomes Visual: Vision-Centric Jailbreak Attacks for Large Image Editing Models",
      "submittedOnDailyBy": {
        "_id": "62333a88fd7bb4a39b92d387",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62333a88fd7bb4a39b92d387/e21AhpcXq37Ak_7rZ-Ca9.png",
        "isPro": false,
        "fullname": "Alex Jinpeng Wang",
        "user": "Awiny",
        "type": "user"
      },
      "summary": "Recent advances in large image editing models have shifted the paradigm from text-driven instructions to vision-prompt editing, where user intent is inferred directly from visual inputs such as marks, arrows, and visual-text prompts. While this paradigm greatly expands usability, it also introduces a critical and underexplored safety risk: the attack surface itself becomes visual. In this work, we propose Vision-Centric Jailbreak Attack (VJA), the first visual-to-visual jailbreak attack that conveys malicious instructions purely through visual inputs. To systematically study this emerging threat, we introduce IESBench, a safety-oriented benchmark for image editing models. Extensive experiments on IESBench demonstrate that VJA effectively compromises state-of-the-art commercial models, achieving attack success rates of up to 80.9% on Nano Banana Pro and 70.1% on GPT-Image-1.5. To mitigate this vulnerability, we propose a training-free defense based on introspective multimodal reasoning, which substantially improves the safety of poorly aligned models to a level comparable with commercial systems, without auxiliary guard models and with negligible computational overhead. Our findings expose new vulnerabilities, provide both a benchmark and practical defense to advance safe and trustworthy modern image editing systems. Warning: This paper contains offensive images created by large image editing models.",
      "upvotes": 4,
      "discussionId": "698d419165c0d15a6d162106",
      "ai_summary": "Visual-to-visual jailbreak attacks compromise image editing models through malicious visual inputs, necessitating new safety benchmarks and defense mechanisms.",
      "ai_keywords": [
        "Vision-Centric Jailbreak Attack",
        "image editing models",
        "visual-to-visual attack",
        "IESBench",
        "introspective multimodal reasoning",
        "safety-oriented benchmark",
        "attack success rate"
      ],
      "organization": {
        "_id": "67ab7720792eebb05080c926",
        "name": "CSU-JPG",
        "fullname": "Jinpeng Group",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/62333a88fd7bb4a39b92d387/MHfLrhVz0KqH6ydx1UrOc.jpeg"
      }
    },
    "publishedAt": "2026-02-10T13:59:55.000Z",
    "title": "When the Prompt Becomes Visual: Vision-Centric Jailbreak Attacks for Large Image Editing Models",
    "summary": "Recent advances in large image editing models have shifted the paradigm from text-driven instructions to vision-prompt editing, where user intent is inferred directly from visual inputs such as marks, arrows, and visual-text prompts. While this paradigm greatly expands usability, it also introduces a critical and underexplored safety risk: the attack surface itself becomes visual. In this work, we propose Vision-Centric Jailbreak Attack (VJA), the first visual-to-visual jailbreak attack that conveys malicious instructions purely through visual inputs. To systematically study this emerging threat, we introduce IESBench, a safety-oriented benchmark for image editing models. Extensive experiments on IESBench demonstrate that VJA effectively compromises state-of-the-art commercial models, achieving attack success rates of up to 80.9% on Nano Banana Pro and 70.1% on GPT-Image-1.5. To mitigate this vulnerability, we propose a training-free defense based on introspective multimodal reasoning, which substantially improves the safety of poorly aligned models to a level comparable with commercial systems, without auxiliary guard models and with negligible computational overhead. Our findings expose new vulnerabilities, provide both a benchmark and practical defense to advance safe and trustworthy modern image editing systems. Warning: This paper contains offensive images created by large image editing models.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.10179.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62333a88fd7bb4a39b92d387",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62333a88fd7bb4a39b92d387/e21AhpcXq37Ak_7rZ-Ca9.png",
      "fullname": "Alex Jinpeng Wang",
      "name": "Awiny",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 11,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "67ab7720792eebb05080c926",
      "name": "CSU-JPG",
      "fullname": "Jinpeng Group",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/62333a88fd7bb4a39b92d387/MHfLrhVz0KqH6ydx1UrOc.jpeg"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2602.09901",
      "authors": [
        {
          "_id": "698d472965c0d15a6d16214b",
          "name": "Jianzhao Huang",
          "hidden": false
        },
        {
          "_id": "698d472965c0d15a6d16214c",
          "name": "Xiaorui Huang",
          "hidden": false
        },
        {
          "_id": "698d472965c0d15a6d16214d",
          "name": "Fei Zhao",
          "hidden": false
        },
        {
          "_id": "698d472965c0d15a6d16214e",
          "name": "Yunpeng Liu",
          "hidden": false
        },
        {
          "_id": "698d472965c0d15a6d16214f",
          "name": "Hui Zhang",
          "hidden": false
        },
        {
          "_id": "698d472965c0d15a6d162150",
          "name": "Fangcheng Shi",
          "hidden": false
        },
        {
          "_id": "698d472965c0d15a6d162151",
          "name": "Congfeng Li",
          "hidden": false
        },
        {
          "_id": "698d472965c0d15a6d162152",
          "name": "Zechen Sun",
          "hidden": false
        },
        {
          "_id": "698d472965c0d15a6d162153",
          "name": "Yi Wu",
          "hidden": false
        },
        {
          "_id": "698d472965c0d15a6d162154",
          "name": "Yao Hu",
          "hidden": false
        },
        {
          "_id": "698d472965c0d15a6d162155",
          "name": "Yunhan Bai",
          "hidden": false
        },
        {
          "_id": "698d472965c0d15a6d162156",
          "name": "Shaosheng Cao",
          "hidden": false
        }
      ],
      "publishedAt": "2026-02-10T15:38:17.000Z",
      "submittedOnDailyAt": "2026-02-12T00:52:56.265Z",
      "title": "QP-OneModel: A Unified Generative LLM for Multi-Task Query Understanding in Xiaohongshu Search",
      "submittedOnDailyBy": {
        "_id": "65328aa39326d6da5ff19b52",
        "avatarUrl": "/avatars/5c3de984cd6eba69616bb608796865c5.svg",
        "isPro": false,
        "fullname": "Fei Zhao",
        "user": "Hiiamein",
        "type": "user"
      },
      "summary": "Query Processing (QP) bridges user intent and content supply in large-scale Social Network Service (SNS) search engines. Traditional QP systems rely on pipelines of isolated discriminative models (e.g., BERT), suffering from limited semantic understanding and high maintenance overhead. While Large Language Models (LLMs) offer a potential solution, existing approaches often optimize sub-tasks in isolation, neglecting intrinsic semantic synergy and necessitating independent iterations. Moreover, standard generative methods often lack grounding in SNS scenarios, failing to bridge the gap between open-domain corpora and informal SNS linguistic patterns, while struggling to adhere to rigorous business definitions. We present QP-OneModel, a Unified Generative LLM for Multi-Task Query Understanding in the SNS domain. We reformulate heterogeneous sub-tasks into a unified sequence generation paradigm, adopting a progressive three-stage alignment strategy culminating in multi-reward Reinforcement Learning. Furthermore, QP-OneModel generates intent descriptions as a novel high-fidelity semantic signal, effectively augmenting downstream tasks such as query rewriting and ranking. Offline evaluations show QP-OneModel achieves a 7.35% overall gain over discriminative baselines, with significant F1 boosts in NER (+9.01%) and Term Weighting (+9.31%). It also exhibits superior generalization, surpassing a 32B model by 7.60% accuracy on unseen tasks. Fully deployed at Xiaohongshu, online A/B tests confirm its industrial value, optimizing retrieval relevance (DCG) by 0.21% and lifting user retention by 0.044%.",
      "upvotes": 4,
      "discussionId": "698d472965c0d15a6d162157",
      "ai_summary": "A unified generative large language model approach for social network search query processing that improves semantic understanding through multi-task learning and reinforcement learning while enhancing downstream task performance.",
      "ai_keywords": [
        "Large Language Models",
        "query processing",
        "discriminative models",
        "sequence generation",
        "progressive three-stage alignment",
        "multi-reward Reinforcement Learning",
        "intent descriptions",
        "query rewriting",
        "ranking",
        "offline evaluations",
        "online A/B tests"
      ]
    },
    "publishedAt": "2026-02-10T10:38:17.000Z",
    "title": "QP-OneModel: A Unified Generative LLM for Multi-Task Query Understanding in Xiaohongshu Search",
    "summary": "Query Processing (QP) bridges user intent and content supply in large-scale Social Network Service (SNS) search engines. Traditional QP systems rely on pipelines of isolated discriminative models (e.g., BERT), suffering from limited semantic understanding and high maintenance overhead. While Large Language Models (LLMs) offer a potential solution, existing approaches often optimize sub-tasks in isolation, neglecting intrinsic semantic synergy and necessitating independent iterations. Moreover, standard generative methods often lack grounding in SNS scenarios, failing to bridge the gap between open-domain corpora and informal SNS linguistic patterns, while struggling to adhere to rigorous business definitions. We present QP-OneModel, a Unified Generative LLM for Multi-Task Query Understanding in the SNS domain. We reformulate heterogeneous sub-tasks into a unified sequence generation paradigm, adopting a progressive three-stage alignment strategy culminating in multi-reward Reinforcement Learning. Furthermore, QP-OneModel generates intent descriptions as a novel high-fidelity semantic signal, effectively augmenting downstream tasks such as query rewriting and ranking. Offline evaluations show QP-OneModel achieves a 7.35% overall gain over discriminative baselines, with significant F1 boosts in NER (+9.01%) and Term Weighting (+9.31%). It also exhibits superior generalization, surpassing a 32B model by 7.60% accuracy on unseen tasks. Fully deployed at Xiaohongshu, online A/B tests confirm its industrial value, optimizing retrieval relevance (DCG) by 0.21% and lifting user retention by 0.044%.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.09901.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65328aa39326d6da5ff19b52",
      "avatarUrl": "/avatars/5c3de984cd6eba69616bb608796865c5.svg",
      "fullname": "Fei Zhao",
      "name": "Hiiamein",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2,
      "isUserFollowing": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2602.10609",
      "authors": [
        {
          "_id": "698d37a66c5152984e4f3e7b",
          "name": "Shuo He",
          "hidden": false
        },
        {
          "_id": "698d37a66c5152984e4f3e7c",
          "name": "Lang Feng",
          "hidden": false
        },
        {
          "_id": "698d37a66c5152984e4f3e7d",
          "name": "Xin Cheng",
          "hidden": false
        },
        {
          "_id": "698d37a66c5152984e4f3e7e",
          "name": "Lei Feng",
          "hidden": false
        },
        {
          "_id": "698d37a66c5152984e4f3e7f",
          "name": "Bo An",
          "hidden": false
        }
      ],
      "publishedAt": "2026-02-11T07:57:43.000Z",
      "submittedOnDailyAt": "2026-02-12T01:08:29.897Z",
      "title": "Online Causal Kalman Filtering for Stable and Effective Policy Optimization",
      "submittedOnDailyBy": {
        "_id": "66ba29dd59e8e7a957154c5f",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66ba29dd59e8e7a957154c5f/VvVS7IZNPUIB023GAEf5u.png",
        "isPro": false,
        "fullname": "Lang Feng",
        "user": "langfeng01",
        "type": "user"
      },
      "summary": "Reinforcement learning for large language models suffers from high-variance token-level importance sampling (IS) ratios, which would destabilize policy optimization at scale. To improve stability, recent methods typically use a fixed sequence-level IS ratio for all tokens in a sequence or adjust each token's IS ratio separately, thereby neglecting temporal off-policy derivation across tokens in a sequence. In this paper, we first empirically identify that local off-policy deviation is structurally inconsistent at the token level, which may distort policy-gradient updates across adjacent tokens and lead to training collapse. To address the issue, we propose Online Causal Kalman Filtering for stable and effective Policy Optimization (KPO). Concretely, we model the desired IS ratio as a latent state that evolves across tokens and apply a Kalman filter to update this state online and autoregressively based on the states of past tokens, regardless of future tokens. The resulting filtered IS ratios preserve token-wise local structure-aware variation while strongly smoothing noise spikes, yielding more stable and effective policy updates. Experimentally, KPO achieves superior results on challenging math reasoning datasets compared with state-of-the-art counterparts.",
      "upvotes": 3,
      "discussionId": "698d37a66c5152984e4f3e80",
      "ai_summary": "Online Causal Kalman Filtering addresses high-variance token-level importance sampling in reinforcement learning for large language models by modeling IS ratios as evolving latent states and using Kalman filtering for stable policy optimization.",
      "ai_keywords": [
        "importance sampling",
        "policy optimization",
        "reinforcement learning",
        "large language models",
        "Kalman filter",
        "token-level",
        "sequence-level",
        "off-policy derivation",
        "policy gradient",
        "training collapse"
      ],
      "organization": {
        "_id": "6508b28cf36bb51c50faad98",
        "name": "NanyangTechnologicalUniversity",
        "fullname": "Nanyang Technological University",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/630ca0817dacb93b33506ce7/ZPD1fvei0bcIGeDXxeSkn.png"
      }
    },
    "publishedAt": "2026-02-11T02:57:43.000Z",
    "title": "Online Causal Kalman Filtering for Stable and Effective Policy Optimization",
    "summary": "Reinforcement learning for large language models suffers from high-variance token-level importance sampling (IS) ratios, which would destabilize policy optimization at scale. To improve stability, recent methods typically use a fixed sequence-level IS ratio for all tokens in a sequence or adjust each token's IS ratio separately, thereby neglecting temporal off-policy derivation across tokens in a sequence. In this paper, we first empirically identify that local off-policy deviation is structurally inconsistent at the token level, which may distort policy-gradient updates across adjacent tokens and lead to training collapse. To address the issue, we propose Online Causal Kalman Filtering for stable and effective Policy Optimization (KPO). Concretely, we model the desired IS ratio as a latent state that evolves across tokens and apply a Kalman filter to update this state online and autoregressively based on the states of past tokens, regardless of future tokens. The resulting filtered IS ratios preserve token-wise local structure-aware variation while strongly smoothing noise spikes, yielding more stable and effective policy updates. Experimentally, KPO achieves superior results on challenging math reasoning datasets compared with state-of-the-art counterparts.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.10609.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "66ba29dd59e8e7a957154c5f",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66ba29dd59e8e7a957154c5f/VvVS7IZNPUIB023GAEf5u.png",
      "fullname": "Lang Feng",
      "name": "langfeng01",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "6508b28cf36bb51c50faad98",
      "name": "NanyangTechnologicalUniversity",
      "fullname": "Nanyang Technological University",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/630ca0817dacb93b33506ce7/ZPD1fvei0bcIGeDXxeSkn.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2602.10560",
      "authors": [
        {
          "_id": "698d420e65c0d15a6d162108",
          "name": "Leheng Sheng",
          "hidden": false
        },
        {
          "_id": "698d420e65c0d15a6d162109",
          "name": "Yongtao Zhang",
          "hidden": false
        },
        {
          "_id": "698d420e65c0d15a6d16210a",
          "name": "Wenchang Ma",
          "hidden": false
        },
        {
          "_id": "698d420e65c0d15a6d16210b",
          "name": "Yaorui Shi",
          "hidden": false
        },
        {
          "_id": "698d420e65c0d15a6d16210c",
          "name": "Ting Huang",
          "hidden": false
        },
        {
          "_id": "698d420e65c0d15a6d16210d",
          "name": "Xiang Wang",
          "hidden": false
        },
        {
          "_id": "698d420e65c0d15a6d16210e",
          "name": "An Zhang",
          "hidden": false
        },
        {
          "_id": "698d420e65c0d15a6d16210f",
          "name": "Ke Shen",
          "hidden": false
        },
        {
          "_id": "698d420e65c0d15a6d162110",
          "name": "Tat-Seng Chua",
          "hidden": false
        }
      ],
      "publishedAt": "2026-02-11T06:14:53.000Z",
      "submittedOnDailyAt": "2026-02-12T00:29:30.545Z",
      "title": "When to Memorize and When to Stop: Gated Recurrent Memory for Long-Context Reasoning",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "While reasoning over long context is crucial for various real-world applications, it remains challenging for large language models (LLMs) as they suffer from performance degradation as the context length grows. Recent work MemAgent has tried to tackle this by processing context chunk-by-chunk in an RNN-like loop and updating a textual memory for final answering. However, this naive recurrent memory update faces two crucial drawbacks: (i) memory can quickly explode because it can update indiscriminately, even on evidence-free chunks; and (ii) the loop lacks an exit mechanism, leading to unnecessary computation after even sufficient evidence is collected. To address these issues, we propose GRU-Mem, which incorporates two text-controlled gates for more stable and efficient long-context reasoning. Specifically, in GRU-Mem, the memory only updates when the update gate is open and the recurrent loop will exit immediately once the exit gate is open. To endow the model with such capabilities, we introduce two reward signals r^{update} and r^{exit} within end-to-end RL, rewarding the correct updating and exiting behaviors respectively. Experiments on various long-context reasoning tasks demonstrate the effectiveness and efficiency of GRU-Mem, which generally outperforms the vanilla MemAgent with up to 400\\% times inference speed acceleration.",
      "upvotes": 3,
      "discussionId": "698d420f65c0d15a6d162111",
      "projectPage": "https://alphalab-ustc.github.io/grumem-alphalab/",
      "ai_summary": "GRU-Mem addresses long-context reasoning challenges in LLMs by incorporating text-controlled gates and reinforcement learning rewards to stabilize memory updates and improve computational efficiency.",
      "ai_keywords": [
        "long-context reasoning",
        "large language models",
        "recurrent memory update",
        "text-controlled gates",
        "reward signals",
        "reinforcement learning",
        "memory explosion",
        "exit mechanism",
        "inference speed acceleration"
      ],
      "organization": {
        "_id": "67d1140985ea0644e2f14b99",
        "name": "ByteDance-Seed",
        "fullname": "ByteDance Seed",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"
      }
    },
    "publishedAt": "2026-02-11T01:14:53.000Z",
    "title": "When to Memorize and When to Stop: Gated Recurrent Memory for Long-Context Reasoning",
    "summary": "While reasoning over long context is crucial for various real-world applications, it remains challenging for large language models (LLMs) as they suffer from performance degradation as the context length grows. Recent work MemAgent has tried to tackle this by processing context chunk-by-chunk in an RNN-like loop and updating a textual memory for final answering. However, this naive recurrent memory update faces two crucial drawbacks: (i) memory can quickly explode because it can update indiscriminately, even on evidence-free chunks; and (ii) the loop lacks an exit mechanism, leading to unnecessary computation after even sufficient evidence is collected. To address these issues, we propose GRU-Mem, which incorporates two text-controlled gates for more stable and efficient long-context reasoning. Specifically, in GRU-Mem, the memory only updates when the update gate is open and the recurrent loop will exit immediately once the exit gate is open. To endow the model with such capabilities, we introduce two reward signals r^{update} and r^{exit} within end-to-end RL, rewarding the correct updating and exiting behaviors respectively. Experiments on various long-context reasoning tasks demonstrate the effectiveness and efficiency of GRU-Mem, which generally outperforms the vanilla MemAgent with up to 400\\% times inference speed acceleration.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.10560.png",
    "numComments": 0,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 230,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "67d1140985ea0644e2f14b99",
      "name": "ByteDance-Seed",
      "fullname": "ByteDance Seed",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2602.11144",
      "authors": [
        {
          "_id": "698d4ad765c0d15a6d162188",
          "name": "Ruichuan An",
          "hidden": false
        },
        {
          "_id": "698d4ad765c0d15a6d162189",
          "name": "Sihan Yang",
          "hidden": false
        },
        {
          "_id": "698d4ad765c0d15a6d16218a",
          "name": "Ziyu Guo",
          "hidden": false
        },
        {
          "_id": "698d4ad765c0d15a6d16218b",
          "name": "Wei Dai",
          "hidden": false
        },
        {
          "_id": "698d4ad765c0d15a6d16218c",
          "name": "Zijun Shen",
          "hidden": false
        },
        {
          "_id": "698d4ad765c0d15a6d16218d",
          "name": "Haodong Li",
          "hidden": false
        },
        {
          "_id": "698d4ad765c0d15a6d16218e",
          "name": "Renrui Zhang",
          "hidden": false
        },
        {
          "_id": "698d4ad765c0d15a6d16218f",
          "name": "Xinyu Wei",
          "hidden": false
        },
        {
          "_id": "698d4ad765c0d15a6d162190",
          "name": "Guopeng Li",
          "hidden": false
        },
        {
          "_id": "698d4ad765c0d15a6d162191",
          "name": "Wenshan Wu",
          "hidden": false
        },
        {
          "_id": "698d4ad765c0d15a6d162192",
          "name": "Wentao Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2026-02-11T18:55:54.000Z",
      "submittedOnDailyAt": "2026-02-12T01:06:56.691Z",
      "title": "GENIUS: Generative Fluid Intelligence Evaluation Suite",
      "submittedOnDailyBy": {
        "_id": "65ddea8b2d26e59a5a33330f",
        "avatarUrl": "/avatars/3104ddafd6dda3c05ea9a771dbf2deeb.svg",
        "isPro": false,
        "fullname": "li haodong",
        "user": "mickyhimself",
        "type": "user"
      },
      "summary": "Unified Multimodal Models (UMMs) have shown remarkable progress in visual generation. Yet, existing benchmarks predominantly assess Crystallized Intelligence, which relies on recalling accumulated knowledge and learned schemas. This focus overlooks Generative Fluid Intelligence (GFI): the capacity to induce patterns, reason through constraints, and adapt to novel scenarios on the fly. To rigorously assess this capability, we introduce GENIUS (GEN Fluid Intelligence EvalUation Suite). We formalize GFI as a synthesis of three primitives. These include Inducing Implicit Patterns (e.g., inferring personalized visual preferences), Executing Ad-hoc Constraints (e.g., visualizing abstract metaphors), and Adapting to Contextual Knowledge (e.g., simulating counter-intuitive physics). Collectively, these primitives challenge models to solve problems grounded entirely in the immediate context. Our systematic evaluation of 12 representative models reveals significant performance deficits in these tasks. Crucially, our diagnostic analysis disentangles these failure modes. It demonstrates that deficits stem from limited context comprehension rather than insufficient intrinsic generative capability. To bridge this gap, we propose a training-free attention intervention strategy. Ultimately, GENIUS establishes a rigorous standard for GFI, guiding the field beyond knowledge utilization toward dynamic, general-purpose reasoning. Our dataset and code will be released at: https://github.com/arctanxarc/GENIUS{https://github.com/arctanxarc/GENIUS}.",
      "upvotes": 2,
      "discussionId": "698d4ad865c0d15a6d162193",
      "ai_summary": "GENIUS evaluates multimodal models' generative fluid intelligence through pattern induction, constraint execution, and contextual adaptation tasks, revealing deficiencies in context comprehension rather than generative capability.",
      "ai_keywords": [
        "Unified Multimodal Models",
        "Generative Fluid Intelligence",
        "Inducing Implicit Patterns",
        "Executing Ad-hoc Constraints",
        "Adapting to Contextual Knowledge",
        "attention intervention strategy"
      ]
    },
    "publishedAt": "2026-02-11T13:55:54.000Z",
    "title": "GENIUS: Generative Fluid Intelligence Evaluation Suite",
    "summary": "Unified Multimodal Models (UMMs) have shown remarkable progress in visual generation. Yet, existing benchmarks predominantly assess Crystallized Intelligence, which relies on recalling accumulated knowledge and learned schemas. This focus overlooks Generative Fluid Intelligence (GFI): the capacity to induce patterns, reason through constraints, and adapt to novel scenarios on the fly. To rigorously assess this capability, we introduce GENIUS (GEN Fluid Intelligence EvalUation Suite). We formalize GFI as a synthesis of three primitives. These include Inducing Implicit Patterns (e.g., inferring personalized visual preferences), Executing Ad-hoc Constraints (e.g., visualizing abstract metaphors), and Adapting to Contextual Knowledge (e.g., simulating counter-intuitive physics). Collectively, these primitives challenge models to solve problems grounded entirely in the immediate context. Our systematic evaluation of 12 representative models reveals significant performance deficits in these tasks. Crucially, our diagnostic analysis disentangles these failure modes. It demonstrates that deficits stem from limited context comprehension rather than insufficient intrinsic generative capability. To bridge this gap, we propose a training-free attention intervention strategy. Ultimately, GENIUS establishes a rigorous standard for GFI, guiding the field beyond knowledge utilization toward dynamic, general-purpose reasoning. Our dataset and code will be released at: https://github.com/arctanxarc/GENIUS{https://github.com/arctanxarc/GENIUS}.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.11144.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65ddea8b2d26e59a5a33330f",
      "avatarUrl": "/avatars/3104ddafd6dda3c05ea9a771dbf2deeb.svg",
      "fullname": "li haodong",
      "name": "mickyhimself",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 0,
      "isUserFollowing": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2602.06008",
      "authors": [
        {
          "_id": "6989932ebeecc443208d2773",
          "user": {
            "_id": "658d6e3729ef008a12cc9817",
            "avatarUrl": "/avatars/f1062c71ad3d76e15875b7becd7a8a13.svg",
            "isPro": false,
            "fullname": "Xianyang Liu",
            "user": "XianyangLiu",
            "type": "user"
          },
          "name": "Xianyang Liu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-02-09T08:27:44.932Z",
          "hidden": false
        },
        {
          "_id": "6989932ebeecc443208d2774",
          "name": "Shangding Gu",
          "hidden": false
        },
        {
          "_id": "6989932ebeecc443208d2775",
          "name": "Dawn Song",
          "hidden": false
        }
      ],
      "publishedAt": "2026-02-05T18:50:36.000Z",
      "submittedOnDailyAt": "2026-02-12T00:19:40.602Z",
      "title": "AgenticPay: A Multi-Agent LLM Negotiation System for Buyer-Seller Transactions",
      "submittedOnDailyBy": {
        "_id": "658d6e3729ef008a12cc9817",
        "avatarUrl": "/avatars/f1062c71ad3d76e15875b7becd7a8a13.svg",
        "isPro": false,
        "fullname": "Xianyang Liu",
        "user": "XianyangLiu",
        "type": "user"
      },
      "summary": "Large language model (LLM)-based agents are increasingly expected to negotiate, coordinate, and transact autonomously, yet existing benchmarks lack principled settings for evaluating language-mediated economic interaction among multiple agents. We introduce AgenticPay, a benchmark and simulation framework for multi-agent buyer-seller negotiation driven by natural language. AgenticPay models markets in which buyers and sellers possess private constraints and product-dependent valuations, and must reach agreements through multi-round linguistic negotiation rather than numeric bidding alone. The framework supports a diverse suite of over 110 tasks ranging from bilateral bargaining to many-to-many markets, with structured action extraction and metrics for feasibility, efficiency, and welfare. Benchmarking state-of-the-art proprietary and open-weight LLMs reveals substantial gaps in negotiation performance and highlights challenges in long-horizon strategic reasoning, establishing AgenticPay as a foundation for studying agentic commerce and language-based market interaction. Code and dataset are available at the link: https://github.com/SafeRL-Lab/AgenticPay.",
      "upvotes": 2,
      "discussionId": "6989932ebeecc443208d2776",
      "projectPage": "https://agenticpay-tutorial.readthedocs.io/en/latest/",
      "githubRepo": "https://github.com/SafeRL-Lab/AgenticPay",
      "githubRepoAddedBy": "admin",
      "ai_summary": "AgenticPay presents a benchmark and simulation framework for evaluating multi-agent language-mediated economic interactions, focusing on negotiation performance and strategic reasoning challenges in complex market scenarios.",
      "ai_keywords": [
        "multi-agent negotiation",
        "language-mediated interaction",
        "strategic reasoning",
        "economic interaction",
        "buyer-seller markets",
        "natural language processing",
        "large language models",
        "agent-based simulation"
      ],
      "githubStars": 6,
      "organization": {
        "_id": "61f20a9ce108f2cba2dc0730",
        "name": "Berkeley",
        "fullname": "UC Berkeley",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/0FjsTg2txEZZ4dEgmMnQL.png"
      }
    },
    "publishedAt": "2026-02-05T13:50:36.000Z",
    "title": "AgenticPay: A Multi-Agent LLM Negotiation System for Buyer-Seller Transactions",
    "summary": "Large language model (LLM)-based agents are increasingly expected to negotiate, coordinate, and transact autonomously, yet existing benchmarks lack principled settings for evaluating language-mediated economic interaction among multiple agents. We introduce AgenticPay, a benchmark and simulation framework for multi-agent buyer-seller negotiation driven by natural language. AgenticPay models markets in which buyers and sellers possess private constraints and product-dependent valuations, and must reach agreements through multi-round linguistic negotiation rather than numeric bidding alone. The framework supports a diverse suite of over 110 tasks ranging from bilateral bargaining to many-to-many markets, with structured action extraction and metrics for feasibility, efficiency, and welfare. Benchmarking state-of-the-art proprietary and open-weight LLMs reveals substantial gaps in negotiation performance and highlights challenges in long-horizon strategic reasoning, establishing AgenticPay as a foundation for studying agentic commerce and language-based market interaction. Code and dataset are available at the link: https://github.com/SafeRL-Lab/AgenticPay.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.06008.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "658d6e3729ef008a12cc9817",
      "avatarUrl": "/avatars/f1062c71ad3d76e15875b7becd7a8a13.svg",
      "fullname": "Xianyang Liu",
      "name": "XianyangLiu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "61f20a9ce108f2cba2dc0730",
      "name": "Berkeley",
      "fullname": "UC Berkeley",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/0FjsTg2txEZZ4dEgmMnQL.png"
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2602.09014",
      "authors": [
        {
          "_id": "698ab6301b2dc6b37d61b008",
          "user": {
            "_id": "64a6a1ecb1f875f03fa05a8d",
            "avatarUrl": "/avatars/4ca06e57d43c66c76b71070f02003d7b.svg",
            "isPro": false,
            "fullname": "Zihan Yang",
            "user": "ymyy307",
            "type": "user"
          },
          "name": "Zihan Yang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2026-02-11T16:06:35.630Z",
          "hidden": false
        },
        {
          "_id": "698ab6301b2dc6b37d61b009",
          "name": "Shuyuan Tu",
          "hidden": false
        },
        {
          "_id": "698ab6301b2dc6b37d61b00a",
          "name": "Licheng Zhang",
          "hidden": false
        },
        {
          "_id": "698ab6301b2dc6b37d61b00b",
          "name": "Qi Dai",
          "hidden": false
        },
        {
          "_id": "698ab6301b2dc6b37d61b00c",
          "name": "Yu-Gang Jiang",
          "hidden": false
        },
        {
          "_id": "698ab6301b2dc6b37d61b00d",
          "name": "Zuxuan Wu",
          "hidden": false
        }
      ],
      "publishedAt": "2026-02-09T18:56:14.000Z",
      "submittedOnDailyAt": "2026-02-12T00:23:58.363Z",
      "title": "ArcFlow: Unleashing 2-Step Text-to-Image Generation via High-Precision Non-Linear Flow Distillation",
      "submittedOnDailyBy": {
        "_id": "64a6a1ecb1f875f03fa05a8d",
        "avatarUrl": "/avatars/4ca06e57d43c66c76b71070f02003d7b.svg",
        "isPro": false,
        "fullname": "Zihan Yang",
        "user": "ymyy307",
        "type": "user"
      },
      "summary": "Diffusion models have achieved remarkable generation quality, but they suffer from significant inference cost due to their reliance on multiple sequential denoising steps, motivating recent efforts to distill this inference process into a few-step regime. However, existing distillation methods typically approximate the teacher trajectory by using linear shortcuts, which makes it difficult to match its constantly changing tangent directions as velocities evolve across timesteps, thereby leading to quality degradation. To address this limitation, we propose ArcFlow, a few-step distillation framework that explicitly employs non-linear flow trajectories to approximate pre-trained teacher trajectories. Concretely, ArcFlow parameterizes the velocity field underlying the inference trajectory as a mixture of continuous momentum processes. This enables ArcFlow to capture velocity evolution and extrapolate coherent velocities to form a continuous non-linear trajectory within each denoising step. Importantly, this parameterization admits an analytical integration of this non-linear trajectory, which circumvents numerical discretization errors and results in high-precision approximation of the teacher trajectory. To train this parameterization into a few-step generator, we implement ArcFlow via trajectory distillation on pre-trained teacher models using lightweight adapters. This strategy ensures fast, stable convergence while preserving generative diversity and quality. Built on large-scale models (Qwen-Image-20B and FLUX.1-dev), ArcFlow only fine-tunes on less than 5% of original parameters and achieves a 40x speedup with 2 NFEs over the original multi-step teachers without significant quality degradation. Experiments on benchmarks show the effectiveness of ArcFlow both qualitatively and quantitatively.",
      "upvotes": 1,
      "discussionId": "698ab6301b2dc6b37d61b00e",
      "githubRepo": "https://github.com/pnotp/ArcFlow",
      "githubRepoAddedBy": "user",
      "ai_summary": "ArcFlow is a few-step distillation framework that uses non-linear flow trajectories to approximate teacher diffusion models, achieving fast inference with minimal quality loss through lightweight adapter training.",
      "ai_keywords": [
        "diffusion models",
        "distillation",
        "velocity field",
        "momentum processes",
        "trajectory distillation",
        "denoising steps",
        "NFEs",
        "teacher trajectory",
        "non-linear flow trajectories",
        "adaptive distillation"
      ],
      "githubStars": 37,
      "organization": {
        "_id": "643cb0625fcffe09fb6ca688",
        "name": "Fudan-University",
        "fullname": "Fudan University",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6437eca0819f3ab20d162e14/kWv0cGlAhAG3iNWVxowkJ.png"
      }
    },
    "publishedAt": "2026-02-09T13:56:14.000Z",
    "title": "ArcFlow: Unleashing 2-Step Text-to-Image Generation via High-Precision Non-Linear Flow Distillation",
    "summary": "Diffusion models have achieved remarkable generation quality, but they suffer from significant inference cost due to their reliance on multiple sequential denoising steps, motivating recent efforts to distill this inference process into a few-step regime. However, existing distillation methods typically approximate the teacher trajectory by using linear shortcuts, which makes it difficult to match its constantly changing tangent directions as velocities evolve across timesteps, thereby leading to quality degradation. To address this limitation, we propose ArcFlow, a few-step distillation framework that explicitly employs non-linear flow trajectories to approximate pre-trained teacher trajectories. Concretely, ArcFlow parameterizes the velocity field underlying the inference trajectory as a mixture of continuous momentum processes. This enables ArcFlow to capture velocity evolution and extrapolate coherent velocities to form a continuous non-linear trajectory within each denoising step. Importantly, this parameterization admits an analytical integration of this non-linear trajectory, which circumvents numerical discretization errors and results in high-precision approximation of the teacher trajectory. To train this parameterization into a few-step generator, we implement ArcFlow via trajectory distillation on pre-trained teacher models using lightweight adapters. This strategy ensures fast, stable convergence while preserving generative diversity and quality. Built on large-scale models (Qwen-Image-20B and FLUX.1-dev), ArcFlow only fine-tunes on less than 5% of original parameters and achieves a 40x speedup with 2 NFEs over the original multi-step teachers without significant quality degradation. Experiments on benchmarks show the effectiveness of ArcFlow both qualitatively and quantitatively.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.09014.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64a6a1ecb1f875f03fa05a8d",
      "avatarUrl": "/avatars/4ca06e57d43c66c76b71070f02003d7b.svg",
      "fullname": "Zihan Yang",
      "name": "ymyy307",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "643cb0625fcffe09fb6ca688",
      "name": "Fudan-University",
      "fullname": "Fudan University",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6437eca0819f3ab20d162e14/kWv0cGlAhAG3iNWVxowkJ.png"
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2602.08995",
      "authors": [
        {
          "_id": "698ab3181b2dc6b37d61af84",
          "name": "Yuting Ning",
          "hidden": false
        },
        {
          "_id": "698ab3181b2dc6b37d61af85",
          "user": {
            "_id": "637029f831af06da86518bc4",
            "avatarUrl": "/avatars/b569b77e7f261ef5dc0b072fed61a5ba.svg",
            "isPro": false,
            "fullname": "Jaylen Jones ",
            "user": "jjones62202",
            "type": "user"
          },
          "name": "Jaylen Jones",
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-02-10T09:06:25.356Z",
          "hidden": false
        },
        {
          "_id": "698ab3181b2dc6b37d61af86",
          "name": "Zhehao Zhang",
          "hidden": false
        },
        {
          "_id": "698ab3181b2dc6b37d61af87",
          "name": "Chentao Ye",
          "hidden": false
        },
        {
          "_id": "698ab3181b2dc6b37d61af88",
          "name": "Weitong Ruan",
          "hidden": false
        },
        {
          "_id": "698ab3181b2dc6b37d61af89",
          "name": "Junyi Li",
          "hidden": false
        },
        {
          "_id": "698ab3181b2dc6b37d61af8a",
          "name": "Rahul Gupta",
          "hidden": false
        },
        {
          "_id": "698ab3181b2dc6b37d61af8b",
          "name": "Huan Sun",
          "hidden": false
        }
      ],
      "publishedAt": "2026-02-09T18:41:15.000Z",
      "submittedOnDailyAt": "2026-02-12T00:25:39.625Z",
      "title": "When Actions Go Off-Task: Detecting and Correcting Misaligned Actions in Computer-Use Agents",
      "submittedOnDailyBy": {
        "_id": "65ace92f64c9b93eca5c2bce",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65ace92f64c9b93eca5c2bce/pG0JRXH-8zEy0IoaEnMNw.jpeg",
        "isPro": false,
        "fullname": "Yuting Ning",
        "user": "nnnyt",
        "type": "user"
      },
      "summary": "Computer-use agents (CUAs) have made tremendous progress in the past year, yet they still frequently produce misaligned actions that deviate from the user's original intent. Such misaligned actions may arise from external attacks (e.g., indirect prompt injection) or from internal limitations (e.g., erroneous reasoning). They not only expose CUAs to safety risks, but also degrade task efficiency and reliability. This work makes the first effort to define and study misaligned action detection in CUAs, with comprehensive coverage of both externally induced and internally arising misaligned actions. We further identify three common categories in real-world CUA deployment and construct MisActBench, a benchmark of realistic trajectories with human-annotated, action-level alignment labels. Moreover, we propose DeAction, a practical and universal guardrail that detects misaligned actions before execution and iteratively corrects them through structured feedback. DeAction outperforms all existing baselines across offline and online evaluations with moderate latency overhead: (1) On MisActBench, it outperforms baselines by over 15% absolute in F1 score; (2) In online evaluation, it reduces attack success rate by over 90% under adversarial settings while preserving or even improving task success rate in benign environments.",
      "upvotes": 1,
      "discussionId": "698ab3181b2dc6b37d61af8c",
      "projectPage": "https://osu-nlp-group.github.io/Misaligned-Action-Detection/",
      "githubRepo": "https://github.com/OSU-NLP-Group/Misaligned-Action-Detection",
      "githubRepoAddedBy": "user",
      "ai_summary": "Computer-use agents face safety risks from misaligned actions caused by external attacks or internal limitations, prompting the development of DeAction, a guardrail that detects and corrects such actions before execution.",
      "ai_keywords": [
        "computer-use agents",
        "misaligned actions",
        "prompt injection",
        "erroneous reasoning",
        "guardrail",
        "structured feedback",
        "attack success rate",
        "task success rate"
      ],
      "githubStars": 2,
      "organization": {
        "_id": "6127b4827dcb442c226129da",
        "name": "osunlp",
        "fullname": "OSU NLP Group",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6477a323dbc2a416f8b852b3/oiPPBo_knuDrz0YN9slKj.png"
      }
    },
    "publishedAt": "2026-02-09T13:41:15.000Z",
    "title": "When Actions Go Off-Task: Detecting and Correcting Misaligned Actions in Computer-Use Agents",
    "summary": "Computer-use agents (CUAs) have made tremendous progress in the past year, yet they still frequently produce misaligned actions that deviate from the user's original intent. Such misaligned actions may arise from external attacks (e.g., indirect prompt injection) or from internal limitations (e.g., erroneous reasoning). They not only expose CUAs to safety risks, but also degrade task efficiency and reliability. This work makes the first effort to define and study misaligned action detection in CUAs, with comprehensive coverage of both externally induced and internally arising misaligned actions. We further identify three common categories in real-world CUA deployment and construct MisActBench, a benchmark of realistic trajectories with human-annotated, action-level alignment labels. Moreover, we propose DeAction, a practical and universal guardrail that detects misaligned actions before execution and iteratively corrects them through structured feedback. DeAction outperforms all existing baselines across offline and online evaluations with moderate latency overhead: (1) On MisActBench, it outperforms baselines by over 15% absolute in F1 score; (2) In online evaluation, it reduces attack success rate by over 90% under adversarial settings while preserving or even improving task success rate in benign environments.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.08995.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65ace92f64c9b93eca5c2bce",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65ace92f64c9b93eca5c2bce/pG0JRXH-8zEy0IoaEnMNw.jpeg",
      "fullname": "Yuting Ning",
      "name": "nnnyt",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "6127b4827dcb442c226129da",
      "name": "osunlp",
      "fullname": "OSU NLP Group",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6477a323dbc2a416f8b852b3/oiPPBo_knuDrz0YN9slKj.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2602.11089",
      "authors": [
        {
          "_id": "698d4b1d65c0d15a6d1621a0",
          "name": "Yicheng Chen",
          "hidden": false
        },
        {
          "_id": "698d4b1d65c0d15a6d1621a1",
          "name": "Zerun Ma",
          "hidden": false
        },
        {
          "_id": "698d4b1d65c0d15a6d1621a2",
          "name": "Xinchen Xie",
          "hidden": false
        },
        {
          "_id": "698d4b1d65c0d15a6d1621a3",
          "name": "Yining Li",
          "hidden": false
        },
        {
          "_id": "698d4b1d65c0d15a6d1621a4",
          "name": "Kai Chen",
          "hidden": false
        }
      ],
      "publishedAt": "2026-02-11T17:56:15.000Z",
      "submittedOnDailyAt": "2026-02-12T01:09:34.064Z",
      "title": "DataChef: Cooking Up Optimal Data Recipes for LLM Adaptation via Reinforcement Learning",
      "submittedOnDailyBy": {
        "_id": "6752dcb6ec430bad7a26c83e",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6752dcb6ec430bad7a26c83e/F685Ij72H1gIve4s9pcup.jpeg",
        "isPro": true,
        "fullname": "Chen Yicheng",
        "user": "yichengchen24",
        "type": "user"
      },
      "summary": "In the current landscape of Large Language Models (LLMs), the curation of large-scale, high-quality training data is a primary driver of model performance. A key lever is the data recipe, which comprises a data processing pipeline to transform raw sources into training corpora. Despite the growing use of LLMs to automate individual data processing steps, such as data synthesis and filtering, the overall design of data recipes remains largely manual and labor-intensive, requiring substantial human expertise and iteration. To bridge this gap, we formulate end-to-end data recipe generation for LLM adaptation. Given a target benchmark and a pool of available data sources, a model is required to output a complete data recipe that adapts a base LLM to the target task. We present DataChef-32B, which performs online reinforcement learning using a proxy reward that predicts downstream performance for candidate recipes. Across six held-out tasks, DataChef-32B produces practical recipes that reach comparable downstream performance to those curated by human experts. Notably, the recipe from DataChef-32B adapts Qwen3-1.7B-Base to the math domain, achieving 66.7 on AIME'25 and surpassing Qwen3-1.7B. This work sheds new light on automating LLM training and developing self-evolving AI systems.",
      "upvotes": 0,
      "discussionId": "698d4b1e65c0d15a6d1621a5",
      "githubRepo": "https://github.com/yichengchen24/DataChef",
      "githubRepoAddedBy": "user",
      "ai_summary": "DataChef-32B automates data recipe generation for LLM adaptation through reinforcement learning with proxy rewards, achieving performance comparable to human-crafted recipes.",
      "ai_keywords": [
        "Large Language Models",
        "data recipe",
        "reinforcement learning",
        "proxy reward",
        "downstream performance",
        "data synthesis",
        "data filtering",
        "automated data processing"
      ],
      "organization": {
        "_id": "6747ee5decec679eafb90450",
        "name": "ShanghaiAiLab",
        "fullname": "shanghai ailab "
      }
    },
    "publishedAt": "2026-02-11T12:56:15.000Z",
    "title": "DataChef: Cooking Up Optimal Data Recipes for LLM Adaptation via Reinforcement Learning",
    "summary": "In the current landscape of Large Language Models (LLMs), the curation of large-scale, high-quality training data is a primary driver of model performance. A key lever is the data recipe, which comprises a data processing pipeline to transform raw sources into training corpora. Despite the growing use of LLMs to automate individual data processing steps, such as data synthesis and filtering, the overall design of data recipes remains largely manual and labor-intensive, requiring substantial human expertise and iteration. To bridge this gap, we formulate end-to-end data recipe generation for LLM adaptation. Given a target benchmark and a pool of available data sources, a model is required to output a complete data recipe that adapts a base LLM to the target task. We present DataChef-32B, which performs online reinforcement learning using a proxy reward that predicts downstream performance for candidate recipes. Across six held-out tasks, DataChef-32B produces practical recipes that reach comparable downstream performance to those curated by human experts. Notably, the recipe from DataChef-32B adapts Qwen3-1.7B-Base to the math domain, achieving 66.7 on AIME'25 and surpassing Qwen3-1.7B. This work sheds new light on automating LLM training and developing self-evolving AI systems.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.11089.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6752dcb6ec430bad7a26c83e",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6752dcb6ec430bad7a26c83e/F685Ij72H1gIve4s9pcup.jpeg",
      "fullname": "Chen Yicheng",
      "name": "yichengchen24",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "6747ee5decec679eafb90450",
      "name": "ShanghaiAiLab",
      "fullname": "shanghai ailab "
    },
    "isAuthorParticipating": false
  }
]