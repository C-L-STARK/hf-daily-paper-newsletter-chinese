[
  {
    "paper": {
      "id": "2510.14979",
      "authors": [
        {
          "_id": "68f19c5d6e0bef323a68fc30",
          "name": "Haiwen Diao",
          "hidden": false
        },
        {
          "_id": "68f19c5d6e0bef323a68fc31",
          "name": "Mingxuan Li",
          "hidden": false
        },
        {
          "_id": "68f19c5d6e0bef323a68fc32",
          "name": "Silei Wu",
          "hidden": false
        },
        {
          "_id": "68f19c5d6e0bef323a68fc33",
          "name": "Linjun Dai",
          "hidden": false
        },
        {
          "_id": "68f19c5d6e0bef323a68fc34",
          "name": "Xiaohua Wang",
          "hidden": false
        },
        {
          "_id": "68f19c5d6e0bef323a68fc35",
          "name": "Hanming Deng",
          "hidden": false
        },
        {
          "_id": "68f19c5d6e0bef323a68fc36",
          "name": "Lewei Lu",
          "hidden": false
        },
        {
          "_id": "68f19c5d6e0bef323a68fc37",
          "name": "Dahua Lin",
          "hidden": false
        },
        {
          "_id": "68f19c5d6e0bef323a68fc38",
          "name": "Ziwei Liu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-16T17:59:58.000Z",
      "submittedOnDailyAt": "2025-10-17T00:18:30.032Z",
      "title": "From Pixels to Words -- Towards Native Vision-Language Primitives at\n  Scale",
      "submittedOnDailyBy": {
        "_id": "64b4a717aa03b6520839e9b8",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b4a717aa03b6520839e9b8/Rt3ERG-6BVEA4hAwOz0_I.jpeg",
        "isPro": false,
        "fullname": "Haiwen Diao",
        "user": "Paranioar",
        "type": "user"
      },
      "summary": "The edifice of native Vision-Language Models (VLMs) has emerged as a rising\ncontender to typical modular VLMs, shaped by evolving model architectures and\ntraining paradigms. Yet, two lingering clouds cast shadows over its widespread\nexploration and promotion: (-) What fundamental constraints set native VLMs\napart from modular ones, and to what extent can these barriers be overcome? (-)\nHow to make research in native VLMs more accessible and democratized, thereby\naccelerating progress in the field. In this paper, we clarify these challenges\nand outline guiding principles for constructing native VLMs. Specifically, one\nnative VLM primitive should: (i) effectively align pixel and word\nrepresentations within a shared semantic space; (ii) seamlessly integrate the\nstrengths of formerly separate vision and language modules; (iii) inherently\nembody various cross-modal properties that support unified vision-language\nencoding, aligning, and reasoning. Hence, we launch NEO, a novel family of\nnative VLMs built from first principles, capable of rivaling top-tier modular\ncounterparts across diverse real-world scenarios. With only 390M image-text\nexamples, NEO efficiently develops visual perception from scratch while\nmitigating vision-language conflicts inside a dense and monolithic model\ncrafted from our elaborate primitives. We position NEO as a cornerstone for\nscalable and powerful native VLMs, paired with a rich set of reusable\ncomponents that foster a cost-effective and extensible ecosystem. Our code and\nmodels are publicly available at: https://github.com/EvolvingLMMs-Lab/NEO.",
      "upvotes": 9,
      "discussionId": "68f19c5e6e0bef323a68fc39",
      "githubRepo": "https://github.com/EvolvingLMMs-Lab/NEO",
      "ai_summary": "NEO, a novel family of native Vision-Language Models, addresses fundamental constraints and integrates vision and language within a unified framework, achieving competitive performance with limited data.",
      "ai_keywords": [
        "Vision-Language Models",
        "native VLMs",
        "modular VLMs",
        "pixel and word representations",
        "semantic space",
        "cross-modal properties",
        "visual perception",
        "vision-language conflicts",
        "monolithic model",
        "reusable components"
      ],
      "organization": {
        "_id": "63203b617196f93bc94b73a2",
        "name": "SenseTime",
        "fullname": "SenseTime",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/No33gl22RKB0HXjo-qpLX.png"
      }
    },
    "publishedAt": "2025-10-16T13:59:58.000Z",
    "title": "From Pixels to Words -- Towards Native Vision-Language Primitives at\n  Scale",
    "summary": "The edifice of native Vision-Language Models (VLMs) has emerged as a rising\ncontender to typical modular VLMs, shaped by evolving model architectures and\ntraining paradigms. Yet, two lingering clouds cast shadows over its widespread\nexploration and promotion: (-) What fundamental constraints set native VLMs\napart from modular ones, and to what extent can these barriers be overcome? (-)\nHow to make research in native VLMs more accessible and democratized, thereby\naccelerating progress in the field. In this paper, we clarify these challenges\nand outline guiding principles for constructing native VLMs. Specifically, one\nnative VLM primitive should: (i) effectively align pixel and word\nrepresentations within a shared semantic space; (ii) seamlessly integrate the\nstrengths of formerly separate vision and language modules; (iii) inherently\nembody various cross-modal properties that support unified vision-language\nencoding, aligning, and reasoning. Hence, we launch NEO, a novel family of\nnative VLMs built from first principles, capable of rivaling top-tier modular\ncounterparts across diverse real-world scenarios. With only 390M image-text\nexamples, NEO efficiently develops visual perception from scratch while\nmitigating vision-language conflicts inside a dense and monolithic model\ncrafted from our elaborate primitives. We position NEO as a cornerstone for\nscalable and powerful native VLMs, paired with a rich set of reusable\ncomponents that foster a cost-effective and extensible ecosystem. Our code and\nmodels are publicly available at: https://github.com/EvolvingLMMs-Lab/NEO.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.14979.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64b4a717aa03b6520839e9b8",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b4a717aa03b6520839e9b8/Rt3ERG-6BVEA4hAwOz0_I.jpeg",
      "fullname": "Haiwen Diao",
      "name": "Paranioar",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "organization": {
      "_id": "63203b617196f93bc94b73a2",
      "name": "SenseTime",
      "fullname": "SenseTime",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/No33gl22RKB0HXjo-qpLX.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.14943",
      "authors": [
        {
          "_id": "68f19d716e0bef323a68fc51",
          "name": "Wenkai Yang",
          "hidden": false
        },
        {
          "_id": "68f19d716e0bef323a68fc52",
          "name": "Weijie Liu",
          "hidden": false
        },
        {
          "_id": "68f19d716e0bef323a68fc53",
          "name": "Ruobing Xie",
          "hidden": false
        },
        {
          "_id": "68f19d716e0bef323a68fc54",
          "name": "Yiju Guo",
          "hidden": false
        },
        {
          "_id": "68f19d716e0bef323a68fc55",
          "name": "Lulu Wu",
          "hidden": false
        },
        {
          "_id": "68f19d716e0bef323a68fc56",
          "name": "Saiyong Yang",
          "hidden": false
        },
        {
          "_id": "68f19d716e0bef323a68fc57",
          "name": "Yankai Lin",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-16T17:55:11.000Z",
      "submittedOnDailyAt": "2025-10-17T00:25:17.471Z",
      "title": "LaSeR: Reinforcement Learning with Last-Token Self-Rewarding",
      "submittedOnDailyBy": {
        "_id": "64b7df742f5a966b973e25f7",
        "avatarUrl": "/avatars/e24e7769188d441317b3b7d10ef8fd60.svg",
        "isPro": false,
        "fullname": "Wenkai Yang",
        "user": "Keven16",
        "type": "user"
      },
      "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has recently emerged as\na core paradigm for enhancing the reasoning capabilities of Large Language\nModels (LLMs). To address the lack of verification signals at test time, prior\nstudies incorporate the training of model's self-verification capability into\nthe standard RLVR process, thereby unifying reasoning and verification\ncapabilities within a single LLM. However, previous practice requires the LLM\nto sequentially generate solutions and self-verifications using two separate\nprompt templates, which significantly reduces efficiency. In this work, we\ntheoretically reveal that the closed-form solution to the RL objective of\nself-verification can be reduced to a remarkably simple form: the true\nreasoning reward of a solution is equal to its last-token self-rewarding score,\nwhich is computed as the difference between the policy model's next-token\nlog-probability assigned to any pre-specified token at the solution's last\ntoken and a pre-calculated constant, scaled by the KL coefficient. Based on\nthis insight, we propose LaSeR (Reinforcement Learning with Last-Token\nSelf-Rewarding), an algorithm that simply augments the original RLVR loss with\na MSE loss that aligns the last-token self-rewarding scores with verifier-based\nreasoning rewards, jointly optimizing the reasoning and self-rewarding\ncapabilities of LLMs. The optimized self-rewarding scores can be utilized in\nboth training and testing to enhance model performance. Notably, our algorithm\nderives these scores from the predicted next-token probability distribution of\nthe last token immediately after generation, incurring only the minimal extra\ncost of one additional token inference. Experiments show that our method not\nonly improves the model's reasoning performance but also equips it with\nremarkable self-rewarding capability, thereby boosting its inference-time\nscaling performance.",
      "upvotes": 9,
      "discussionId": "68f19d716e0bef323a68fc58",
      "githubRepo": "https://github.com/RUCBM/LaSeR",
      "ai_summary": "LaSeR, a reinforcement learning algorithm, enhances Large Language Models by aligning last-token self-rewarding scores with verifier-based reasoning rewards, improving reasoning performance and inference-time scaling.",
      "ai_keywords": [
        "Reinforcement Learning with Verifiable Rewards",
        "Large Language Models",
        "self-verification",
        "RLVR",
        "last-token self-rewarding",
        "policy model",
        "next-token log-probability",
        "KL coefficient",
        "MSE loss",
        "reasoning performance",
        "inference-time scaling"
      ],
      "githubStars": 0,
      "organization": {
        "_id": "6645f953c39288df638dbdd5",
        "name": "Tencent-Hunyuan",
        "fullname": "Tencent Hunyuan",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/62d22496c58f969c152bcefd/woKSjt2wXvBNKussyYPsa.png"
      }
    },
    "publishedAt": "2025-10-16T13:55:11.000Z",
    "title": "LaSeR: Reinforcement Learning with Last-Token Self-Rewarding",
    "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has recently emerged as\na core paradigm for enhancing the reasoning capabilities of Large Language\nModels (LLMs). To address the lack of verification signals at test time, prior\nstudies incorporate the training of model's self-verification capability into\nthe standard RLVR process, thereby unifying reasoning and verification\ncapabilities within a single LLM. However, previous practice requires the LLM\nto sequentially generate solutions and self-verifications using two separate\nprompt templates, which significantly reduces efficiency. In this work, we\ntheoretically reveal that the closed-form solution to the RL objective of\nself-verification can be reduced to a remarkably simple form: the true\nreasoning reward of a solution is equal to its last-token self-rewarding score,\nwhich is computed as the difference between the policy model's next-token\nlog-probability assigned to any pre-specified token at the solution's last\ntoken and a pre-calculated constant, scaled by the KL coefficient. Based on\nthis insight, we propose LaSeR (Reinforcement Learning with Last-Token\nSelf-Rewarding), an algorithm that simply augments the original RLVR loss with\na MSE loss that aligns the last-token self-rewarding scores with verifier-based\nreasoning rewards, jointly optimizing the reasoning and self-rewarding\ncapabilities of LLMs. The optimized self-rewarding scores can be utilized in\nboth training and testing to enhance model performance. Notably, our algorithm\nderives these scores from the predicted next-token probability distribution of\nthe last token immediately after generation, incurring only the minimal extra\ncost of one additional token inference. Experiments show that our method not\nonly improves the model's reasoning performance but also equips it with\nremarkable self-rewarding capability, thereby boosting its inference-time\nscaling performance.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.14943.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64b7df742f5a966b973e25f7",
      "avatarUrl": "/avatars/e24e7769188d441317b3b7d10ef8fd60.svg",
      "fullname": "Wenkai Yang",
      "name": "Keven16",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 7
    },
    "organization": {
      "_id": "6645f953c39288df638dbdd5",
      "name": "Tencent-Hunyuan",
      "fullname": "Tencent Hunyuan",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/62d22496c58f969c152bcefd/woKSjt2wXvBNKussyYPsa.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.14975",
      "authors": [
        {
          "_id": "68f1a2616e0bef323a68fcd1",
          "name": "Hengyuan Xu",
          "hidden": false
        },
        {
          "_id": "68f1a2616e0bef323a68fcd2",
          "name": "Wei Cheng",
          "hidden": false
        },
        {
          "_id": "68f1a2616e0bef323a68fcd3",
          "name": "Peng Xing",
          "hidden": false
        },
        {
          "_id": "68f1a2616e0bef323a68fcd4",
          "name": "Yixiao Fang",
          "hidden": false
        },
        {
          "_id": "68f1a2616e0bef323a68fcd5",
          "name": "Shuhan Wu",
          "hidden": false
        },
        {
          "_id": "68f1a2616e0bef323a68fcd6",
          "name": "Rui Wang",
          "hidden": false
        },
        {
          "_id": "68f1a2616e0bef323a68fcd7",
          "name": "Xianfang Zeng",
          "hidden": false
        },
        {
          "_id": "68f1a2616e0bef323a68fcd8",
          "name": "Daxin Jiang",
          "hidden": false
        },
        {
          "_id": "68f1a2616e0bef323a68fcd9",
          "name": "Gang Yu",
          "hidden": false
        },
        {
          "_id": "68f1a2616e0bef323a68fcda",
          "name": "Xingjun Ma",
          "hidden": false
        },
        {
          "_id": "68f1a2616e0bef323a68fcdb",
          "name": "Yu-Gang Jiang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-16T17:59:54.000Z",
      "submittedOnDailyAt": "2025-10-17T00:27:22.105Z",
      "title": "WithAnyone: Towards Controllable and ID Consistent Image Generation",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Identity-consistent generation has become an important focus in text-to-image\nresearch, with recent models achieving notable success in producing images\naligned with a reference identity. Yet, the scarcity of large-scale paired\ndatasets containing multiple images of the same individual forces most\napproaches to adopt reconstruction-based training. This reliance often leads to\na failure mode we term copy-paste, where the model directly replicates the\nreference face rather than preserving identity across natural variations in\npose, expression, or lighting. Such over-similarity undermines controllability\nand limits the expressive power of generation. To address these limitations, we\n(1) construct a large-scale paired dataset MultiID-2M, tailored for\nmulti-person scenarios, providing diverse references for each identity; (2)\nintroduce a benchmark that quantifies both copy-paste artifacts and the\ntrade-off between identity fidelity and variation; and (3) propose a novel\ntraining paradigm with a contrastive identity loss that leverages paired data\nto balance fidelity with diversity. These contributions culminate in\nWithAnyone, a diffusion-based model that effectively mitigates copy-paste while\npreserving high identity similarity. Extensive qualitative and quantitative\nexperiments demonstrate that WithAnyone significantly reduces copy-paste\nartifacts, improves controllability over pose and expression, and maintains\nstrong perceptual quality. User studies further validate that our method\nachieves high identity fidelity while enabling expressive controllable\ngeneration.",
      "upvotes": 7,
      "discussionId": "68f1a2616e0bef323a68fcdc",
      "projectPage": "https://doby-xu.github.io/WithAnyone/",
      "githubRepo": "https://github.com/Doby-Xu/WithAnyone",
      "ai_summary": "A diffusion-based model addresses copy-paste artifacts in text-to-image generation by using a large-scale paired dataset and a contrastive identity loss to balance identity fidelity and variation.",
      "ai_keywords": [
        "identity-consistent generation",
        "text-to-image",
        "reconstruction-based training",
        "copy-paste",
        "MultiID-2M",
        "contrastive identity loss",
        "diffusion-based model",
        "identity fidelity",
        "variation",
        "perceptual quality"
      ],
      "githubStars": 4,
      "organization": {
        "_id": "66e43eae9d477f566f937935",
        "name": "stepfun-ai",
        "fullname": "StepFun",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66935cee39002fc0569c2943/Qv8QPbkgoKE3wR4jTzHiy.png"
      }
    },
    "publishedAt": "2025-10-16T13:59:54.000Z",
    "title": "WithAnyone: Towards Controllable and ID Consistent Image Generation",
    "summary": "Identity-consistent generation has become an important focus in text-to-image\nresearch, with recent models achieving notable success in producing images\naligned with a reference identity. Yet, the scarcity of large-scale paired\ndatasets containing multiple images of the same individual forces most\napproaches to adopt reconstruction-based training. This reliance often leads to\na failure mode we term copy-paste, where the model directly replicates the\nreference face rather than preserving identity across natural variations in\npose, expression, or lighting. Such over-similarity undermines controllability\nand limits the expressive power of generation. To address these limitations, we\n(1) construct a large-scale paired dataset MultiID-2M, tailored for\nmulti-person scenarios, providing diverse references for each identity; (2)\nintroduce a benchmark that quantifies both copy-paste artifacts and the\ntrade-off between identity fidelity and variation; and (3) propose a novel\ntraining paradigm with a contrastive identity loss that leverages paired data\nto balance fidelity with diversity. These contributions culminate in\nWithAnyone, a diffusion-based model that effectively mitigates copy-paste while\npreserving high identity similarity. Extensive qualitative and quantitative\nexperiments demonstrate that WithAnyone significantly reduces copy-paste\nartifacts, improves controllability over pose and expression, and maintains\nstrong perceptual quality. User studies further validate that our method\nachieves high identity fidelity while enabling expressive controllable\ngeneration.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.14975.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 127
    },
    "organization": {
      "_id": "66e43eae9d477f566f937935",
      "name": "stepfun-ai",
      "fullname": "StepFun",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66935cee39002fc0569c2943/Qv8QPbkgoKE3wR4jTzHiy.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.13217",
      "authors": [
        {
          "_id": "68f19ecf6e0bef323a68fc75",
          "name": "Nilesh Gupta",
          "hidden": false
        },
        {
          "_id": "68f19ecf6e0bef323a68fc76",
          "name": "Wei-Cheng Chang",
          "hidden": false
        },
        {
          "_id": "68f19ecf6e0bef323a68fc77",
          "name": "Ngot Bui",
          "hidden": false
        },
        {
          "_id": "68f19ecf6e0bef323a68fc78",
          "name": "Cho-Jui Hsieh",
          "hidden": false
        },
        {
          "_id": "68f19ecf6e0bef323a68fc79",
          "name": "Inderjit S. Dhillon",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-15T07:05:17.000Z",
      "submittedOnDailyAt": "2025-10-17T00:12:11.927Z",
      "title": "LLM-guided Hierarchical Retrieval",
      "submittedOnDailyBy": {
        "_id": "6527151d5606f146974d60d8",
        "avatarUrl": "/avatars/00ac8ab005ceadae866dea5471f6aab9.svg",
        "isPro": false,
        "fullname": "Nilesh Gupta",
        "user": "quicktensor",
        "type": "user"
      },
      "summary": "Modern IR systems are increasingly tasked with answering complex,\nmulti-faceted queries that require deep reasoning rather than simple keyword or\nsemantic matching. While LLM-based IR has shown great promise, the prevailing\nretrieve-then-rerank paradigm inherits the limitations of embedding-based\nretrieval; parametric generative approaches are difficult to update with new\ninformation; and long-context methods that place the entire corpus in context\nare computationally infeasible for large document collections. To address these\nchallenges, we introduce LATTICE, a hierarchical retrieval framework that\nenables an LLM to reason over and navigate large corpora with logarithmic\nsearch complexity by imposing a semantic tree structure on the corpus. Our\napproach consists of two stages: (1) an offline phase that organizes the corpus\ninto a semantic hierarchy via either a bottom-up agglomerative strategy or a\ntop-down divisive strategy using multi-level summaries and (2) an online\ntraversal phase where a search LLM navigates this tree. A central challenge in\nsuch LLM-guided search is that the model's relevance judgments are noisy,\ncontext-dependent, and unaware of the hierarchy, making cross-branch and\ncross-level comparisons difficult. To overcome this, we propose a traversal\nalgorithm that estimates calibrated latent relevance scores from local LLM\noutputs and aggregates them into a global path relevance metric. Our\ntraining-free framework achieves state-of-the-art zero-shot performance on the\nreasoning-intensive BRIGHT benchmark, demonstrating up to 9% improvement in\nRecall@100 and 5% in nDCG@10 over the next best zero-shot baseline.\nFurthermore, compared to the fine-tuned SOTA method DIVER-v2, LATTICE attains\ncomparable results on BRIGHT subsets that use a static corpus for evaluation.",
      "upvotes": 4,
      "discussionId": "68f19ed06e0bef323a68fc7a",
      "projectPage": "https://nilesh2797.github.io/publications/lattice/",
      "githubRepo": "https://github.com/nilesh2797/lattice",
      "ai_summary": "LATTICE, a hierarchical retrieval framework, enables efficient and accurate reasoning over large document collections using a semantic tree structure and a traversal algorithm that calibrates relevance scores.",
      "ai_keywords": [
        "LLM-based IR",
        "retrieve-then-rerank",
        "embedding-based retrieval",
        "parametric generative approaches",
        "long-context methods",
        "hierarchical retrieval framework",
        "semantic tree structure",
        "offline phase",
        "bottom-up agglomerative strategy",
        "top-down divisive strategy",
        "multi-level summaries",
        "online traversal phase",
        "search LLM",
        "relevance judgments",
        "latent relevance scores",
        "global path relevance metric",
        "zero-shot performance",
        "BRIGHT benchmark",
        "Recall@100",
        "nDCG@10",
        "DIVER-v2"
      ],
      "githubStars": 1,
      "organization": {
        "_id": "5e6aca39878b8b2bf9806447",
        "name": "google",
        "fullname": "Google",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/WtA3YYitedOr9n02eHfJe.png"
      }
    },
    "publishedAt": "2025-10-15T03:05:17.000Z",
    "title": "LLM-guided Hierarchical Retrieval",
    "summary": "Modern IR systems are increasingly tasked with answering complex,\nmulti-faceted queries that require deep reasoning rather than simple keyword or\nsemantic matching. While LLM-based IR has shown great promise, the prevailing\nretrieve-then-rerank paradigm inherits the limitations of embedding-based\nretrieval; parametric generative approaches are difficult to update with new\ninformation; and long-context methods that place the entire corpus in context\nare computationally infeasible for large document collections. To address these\nchallenges, we introduce LATTICE, a hierarchical retrieval framework that\nenables an LLM to reason over and navigate large corpora with logarithmic\nsearch complexity by imposing a semantic tree structure on the corpus. Our\napproach consists of two stages: (1) an offline phase that organizes the corpus\ninto a semantic hierarchy via either a bottom-up agglomerative strategy or a\ntop-down divisive strategy using multi-level summaries and (2) an online\ntraversal phase where a search LLM navigates this tree. A central challenge in\nsuch LLM-guided search is that the model's relevance judgments are noisy,\ncontext-dependent, and unaware of the hierarchy, making cross-branch and\ncross-level comparisons difficult. To overcome this, we propose a traversal\nalgorithm that estimates calibrated latent relevance scores from local LLM\noutputs and aggregates them into a global path relevance metric. Our\ntraining-free framework achieves state-of-the-art zero-shot performance on the\nreasoning-intensive BRIGHT benchmark, demonstrating up to 9% improvement in\nRecall@100 and 5% in nDCG@10 over the next best zero-shot baseline.\nFurthermore, compared to the fine-tuned SOTA method DIVER-v2, LATTICE attains\ncomparable results on BRIGHT subsets that use a static corpus for evaluation.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.13217.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6527151d5606f146974d60d8",
      "avatarUrl": "/avatars/00ac8ab005ceadae866dea5471f6aab9.svg",
      "fullname": "Nilesh Gupta",
      "name": "quicktensor",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "organization": {
      "_id": "5e6aca39878b8b2bf9806447",
      "name": "google",
      "fullname": "Google",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/WtA3YYitedOr9n02eHfJe.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.14967",
      "authors": [
        {
          "_id": "68f1a04a6e0bef323a68fcac",
          "name": "Guoqing Wang",
          "hidden": false
        },
        {
          "_id": "68f1a04a6e0bef323a68fcad",
          "name": "Sunhao Dai",
          "hidden": false
        },
        {
          "_id": "68f1a04a6e0bef323a68fcae",
          "name": "Guangze Ye",
          "hidden": false
        },
        {
          "_id": "68f1a04a6e0bef323a68fcaf",
          "name": "Zeyu Gan",
          "hidden": false
        },
        {
          "_id": "68f1a04a6e0bef323a68fcb0",
          "name": "Wei Yao",
          "hidden": false
        },
        {
          "_id": "68f1a04a6e0bef323a68fcb1",
          "name": "Yong Deng",
          "hidden": false
        },
        {
          "_id": "68f1a04a6e0bef323a68fcb2",
          "name": "Xiaofeng Wu",
          "hidden": false
        },
        {
          "_id": "68f1a04a6e0bef323a68fcb3",
          "name": "Zhenzhe Ying",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-16T17:59:32.000Z",
      "submittedOnDailyAt": "2025-10-17T00:19:11.288Z",
      "title": "Information Gain-based Policy Optimization: A Simple and Effective\n  Approach for Multi-Turn LLM Agents",
      "submittedOnDailyBy": {
        "_id": "64db88993725f8d9a908c077",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64db88993725f8d9a908c077/JZEGk0kius6mrANlwOWw9.jpeg",
        "isPro": false,
        "fullname": "Sunhao Dai",
        "user": "KID-22",
        "type": "user"
      },
      "summary": "Large language model (LLM)-based agents are increasingly trained with\nreinforcement learning (RL) to enhance their ability to interact with external\nenvironments through tool use, particularly in search-based settings that\nrequire multi-turn reasoning and knowledge acquisition. However, existing\napproaches typically rely on outcome-based rewards that are only provided at\nthe final answer. This reward sparsity becomes particularly problematic in\nmulti-turn settings, where long trajectories exacerbate two critical issues:\n(i) advantage collapse, where all rollouts receive identical rewards and\nprovide no useful learning signals, and (ii) lack of fine-grained credit\nassignment, where dependencies between turns are obscured, especially in\nlong-horizon tasks. In this paper, we propose Information Gain-based Policy\nOptimization (IGPO), a simple yet effective RL framework that provides dense\nand intrinsic supervision for multi-turn agent training. IGPO models each\ninteraction turn as an incremental process of acquiring information about the\nground truth, and defines turn-level rewards as the marginal increase in the\npolicy's probability of producing the correct answer. Unlike prior\nprocess-level reward approaches that depend on external reward models or costly\nMonte Carlo estimation, IGPO derives intrinsic rewards directly from the\nmodel's own belief updates. These intrinsic turn-level rewards are combined\nwith outcome-level supervision to form dense reward trajectories. Extensive\nexperiments on both in-domain and out-of-domain benchmarks demonstrate that\nIGPO consistently outperforms strong baselines in multi-turn scenarios,\nachieving higher accuracy and improved sample efficiency.",
      "upvotes": 3,
      "discussionId": "68f1a04a6e0bef323a68fcb4",
      "ai_summary": "Information Gain-based Policy Optimization (IGPO) enhances multi-turn reasoning in large language models by providing dense intrinsic rewards derived from the model's belief updates, improving accuracy and sample efficiency.",
      "ai_keywords": [
        "reinforcement learning",
        "large language model",
        "tool use",
        "multi-turn reasoning",
        "knowledge acquisition",
        "reward sparsity",
        "advantage collapse",
        "fine-grained credit assignment",
        "Information Gain-based Policy Optimization",
        "intrinsic rewards",
        "belief updates",
        "dense reward trajectories",
        "sample efficiency"
      ]
    },
    "publishedAt": "2025-10-16T13:59:32.000Z",
    "title": "Information Gain-based Policy Optimization: A Simple and Effective\n  Approach for Multi-Turn LLM Agents",
    "summary": "Large language model (LLM)-based agents are increasingly trained with\nreinforcement learning (RL) to enhance their ability to interact with external\nenvironments through tool use, particularly in search-based settings that\nrequire multi-turn reasoning and knowledge acquisition. However, existing\napproaches typically rely on outcome-based rewards that are only provided at\nthe final answer. This reward sparsity becomes particularly problematic in\nmulti-turn settings, where long trajectories exacerbate two critical issues:\n(i) advantage collapse, where all rollouts receive identical rewards and\nprovide no useful learning signals, and (ii) lack of fine-grained credit\nassignment, where dependencies between turns are obscured, especially in\nlong-horizon tasks. In this paper, we propose Information Gain-based Policy\nOptimization (IGPO), a simple yet effective RL framework that provides dense\nand intrinsic supervision for multi-turn agent training. IGPO models each\ninteraction turn as an incremental process of acquiring information about the\nground truth, and defines turn-level rewards as the marginal increase in the\npolicy's probability of producing the correct answer. Unlike prior\nprocess-level reward approaches that depend on external reward models or costly\nMonte Carlo estimation, IGPO derives intrinsic rewards directly from the\nmodel's own belief updates. These intrinsic turn-level rewards are combined\nwith outcome-level supervision to form dense reward trajectories. Extensive\nexperiments on both in-domain and out-of-domain benchmarks demonstrate that\nIGPO consistently outperforms strong baselines in multi-turn scenarios,\nachieving higher accuracy and improved sample efficiency.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.14967.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64db88993725f8d9a908c077",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64db88993725f8d9a908c077/JZEGk0kius6mrANlwOWw9.jpeg",
      "fullname": "Sunhao Dai",
      "name": "KID-22",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.10518",
      "authors": [
        {
          "_id": "68f0de8336f8b025381e1b61",
          "name": "Qunzhong Wang",
          "hidden": false
        },
        {
          "_id": "68f0de8336f8b025381e1b62",
          "name": "Jie Liu",
          "hidden": false
        },
        {
          "_id": "68f0de8336f8b025381e1b63",
          "name": "Jiajun Liang",
          "hidden": false
        },
        {
          "_id": "68f0de8336f8b025381e1b64",
          "name": "Yilei Jiang",
          "hidden": false
        },
        {
          "_id": "68f0de8336f8b025381e1b65",
          "name": "Yuanxing Zhang",
          "hidden": false
        },
        {
          "_id": "68f0de8336f8b025381e1b66",
          "name": "Jinyuan Chen",
          "hidden": false
        },
        {
          "_id": "68f0de8336f8b025381e1b67",
          "name": "Yaozhi Zheng",
          "hidden": false
        },
        {
          "_id": "68f0de8336f8b025381e1b68",
          "name": "Xintao Wang",
          "hidden": false
        },
        {
          "_id": "68f0de8336f8b025381e1b69",
          "name": "Pengfei Wan",
          "hidden": false
        },
        {
          "_id": "68f0de8336f8b025381e1b6a",
          "name": "Xiangyu Yue",
          "hidden": false
        },
        {
          "_id": "68f0de8336f8b025381e1b6b",
          "name": "Jiaheng Liu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-12T09:29:50.000Z",
      "submittedOnDailyAt": "2025-10-17T00:46:35.185Z",
      "title": "VR-Thinker: Boosting Video Reward Models through Thinking-with-Image\n  Reasoning",
      "submittedOnDailyBy": {
        "_id": "65377c30e48353201e6fdda0",
        "avatarUrl": "/avatars/a8f803b6f2e598eaee9c52c0d2ddfc16.svg",
        "isPro": false,
        "fullname": "Jiaheng Liu",
        "user": "CheeryLJH",
        "type": "user"
      },
      "summary": "Recent advancements in multimodal reward models (RMs) have substantially\nimproved post-training for visual generative models. However, current RMs face\ninherent limitations: (1) visual inputs consume large context budgets, forcing\nfewer frames and causing loss of fine-grained details; and (2) all visual\ninformation is packed into the initial prompt, exacerbating hallucination and\nforgetting during chain-of-thought reasoning. To overcome these issues, we\nintroduce VideoReward Thinker (VR-Thinker), a thinking-with-image framework\nthat equips the RM with visual reasoning operations (e.g., select frame) and a\nconfigurable visual memory window. This allows the RM to actively acquire and\nupdate visual evidence within context limits, improving reasoning fidelity and\nreliability. We activate visual reasoning via a reinforcement fine-tuning\npipeline: (i) Cold Start with curated visual chain-of-thought data to distill\nbasic reasoning skills and operation formatting; (ii) select samples whose\nper-dimension and overall judgments are all correct, then conduct Rejection\nsampling Fine-Tuning on these high-quality traces to further enhance reasoning;\nand (iii) apply Group Relative Policy Optimization (GRPO) to strengthen\nreasoning. Our approach delivers state-of-the-art accuracy among open-source\nmodels on video preference benchmarks, especially for longer videos: a 7B\nVR-Thinker achieves 80.5% on VideoGen Reward, 82.3% on GenAI-Bench, and 75.6%\non MJ-Bench-Video. These results validate the effectiveness and promise of\nthinking-with-image multimodal reward modeling.",
      "upvotes": 3,
      "discussionId": "68f0de8336f8b025381e1b6c",
      "ai_summary": "VideoReward Thinker enhances multimodal reward models with visual reasoning operations and a configurable memory window, improving accuracy on video preference benchmarks.",
      "ai_keywords": [
        "multimodal reward models",
        "visual generative models",
        "visual reasoning operations",
        "visual memory window",
        "reinforcement fine-tuning",
        "curated visual chain-of-thought data",
        "Rejection sampling Fine-Tuning",
        "Group Relative Policy Optimization",
        "VideoGen Reward",
        "GenAI-Bench",
        "MJ-Bench-Video"
      ]
    },
    "publishedAt": "2025-10-12T05:29:50.000Z",
    "title": "VR-Thinker: Boosting Video Reward Models through Thinking-with-Image\n  Reasoning",
    "summary": "Recent advancements in multimodal reward models (RMs) have substantially\nimproved post-training for visual generative models. However, current RMs face\ninherent limitations: (1) visual inputs consume large context budgets, forcing\nfewer frames and causing loss of fine-grained details; and (2) all visual\ninformation is packed into the initial prompt, exacerbating hallucination and\nforgetting during chain-of-thought reasoning. To overcome these issues, we\nintroduce VideoReward Thinker (VR-Thinker), a thinking-with-image framework\nthat equips the RM with visual reasoning operations (e.g., select frame) and a\nconfigurable visual memory window. This allows the RM to actively acquire and\nupdate visual evidence within context limits, improving reasoning fidelity and\nreliability. We activate visual reasoning via a reinforcement fine-tuning\npipeline: (i) Cold Start with curated visual chain-of-thought data to distill\nbasic reasoning skills and operation formatting; (ii) select samples whose\nper-dimension and overall judgments are all correct, then conduct Rejection\nsampling Fine-Tuning on these high-quality traces to further enhance reasoning;\nand (iii) apply Group Relative Policy Optimization (GRPO) to strengthen\nreasoning. Our approach delivers state-of-the-art accuracy among open-source\nmodels on video preference benchmarks, especially for longer videos: a 7B\nVR-Thinker achieves 80.5% on VideoGen Reward, 82.3% on GenAI-Bench, and 75.6%\non MJ-Bench-Video. These results validate the effectiveness and promise of\nthinking-with-image multimodal reward modeling.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.10518.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65377c30e48353201e6fdda0",
      "avatarUrl": "/avatars/a8f803b6f2e598eaee9c52c0d2ddfc16.svg",
      "fullname": "Jiaheng Liu",
      "name": "CheeryLJH",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 19
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.14978",
      "authors": [
        {
          "_id": "68f1a20a6e0bef323a68fcc5",
          "name": "Nupur Kumari",
          "hidden": false
        },
        {
          "_id": "68f1a20a6e0bef323a68fcc6",
          "name": "Sheng-Yu Wang",
          "hidden": false
        },
        {
          "_id": "68f1a20a6e0bef323a68fcc7",
          "name": "Nanxuan Zhao",
          "hidden": false
        },
        {
          "_id": "68f1a20a6e0bef323a68fcc8",
          "name": "Yotam Nitzan",
          "hidden": false
        },
        {
          "_id": "68f1a20a6e0bef323a68fcc9",
          "name": "Yuheng Li",
          "hidden": false
        },
        {
          "_id": "68f1a20a6e0bef323a68fcca",
          "name": "Krishna Kumar Singh",
          "hidden": false
        },
        {
          "_id": "68f1a20a6e0bef323a68fccb",
          "name": "Richard Zhang",
          "hidden": false
        },
        {
          "_id": "68f1a20a6e0bef323a68fccc",
          "name": "Eli Shechtman",
          "hidden": false
        },
        {
          "_id": "68f1a20a6e0bef323a68fccd",
          "name": "Jun-Yan Zhu",
          "hidden": false
        },
        {
          "_id": "68f1a20a6e0bef323a68fcce",
          "name": "Xun Huang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-16T17:59:57.000Z",
      "submittedOnDailyAt": "2025-10-17T00:25:36.586Z",
      "title": "Learning an Image Editing Model without Image Editing Pairs",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Recent image editing models have achieved impressive results while following\nnatural language editing instructions, but they rely on supervised fine-tuning\nwith large datasets of input-target pairs. This is a critical bottleneck, as\nsuch naturally occurring pairs are hard to curate at scale. Current workarounds\nuse synthetic training pairs that leverage the zero-shot capabilities of\nexisting models. However, this can propagate and magnify the artifacts of the\npretrained model into the final trained model. In this work, we present a new\ntraining paradigm that eliminates the need for paired data entirely. Our\napproach directly optimizes a few-step diffusion model by unrolling it during\ntraining and leveraging feedback from vision-language models (VLMs). For each\ninput and editing instruction, the VLM evaluates if an edit follows the\ninstruction and preserves unchanged content, providing direct gradients for\nend-to-end optimization. To ensure visual fidelity, we incorporate distribution\nmatching loss (DMD), which constrains generated images to remain within the\nimage manifold learned by pretrained models. We evaluate our method on standard\nbenchmarks and include an extensive ablation study. Without any paired data,\nour method performs on par with various image editing diffusion models trained\non extensive supervised paired data, under the few-step setting. Given the same\nVLM as the reward model, we also outperform RL-based techniques like Flow-GRPO.",
      "upvotes": 2,
      "discussionId": "68f1a20a6e0bef323a68fccf",
      "ai_summary": "A new training paradigm for image editing models uses unrolled diffusion models and vision-language feedback to achieve performance comparable to supervised models without paired data.",
      "ai_keywords": [
        "diffusion model",
        "unrolling",
        "vision-language models",
        "distribution matching loss",
        "image manifold",
        "few-step setting",
        "RL-based techniques",
        "Flow-GRPO"
      ],
      "organization": {
        "_id": "61e5d14f77496de0a6d95c6b",
        "name": "adobe",
        "fullname": "Adobe",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1645217431826-61e35e517ac6b6d06cfa8081.png"
      }
    },
    "publishedAt": "2025-10-16T13:59:57.000Z",
    "title": "Learning an Image Editing Model without Image Editing Pairs",
    "summary": "Recent image editing models have achieved impressive results while following\nnatural language editing instructions, but they rely on supervised fine-tuning\nwith large datasets of input-target pairs. This is a critical bottleneck, as\nsuch naturally occurring pairs are hard to curate at scale. Current workarounds\nuse synthetic training pairs that leverage the zero-shot capabilities of\nexisting models. However, this can propagate and magnify the artifacts of the\npretrained model into the final trained model. In this work, we present a new\ntraining paradigm that eliminates the need for paired data entirely. Our\napproach directly optimizes a few-step diffusion model by unrolling it during\ntraining and leveraging feedback from vision-language models (VLMs). For each\ninput and editing instruction, the VLM evaluates if an edit follows the\ninstruction and preserves unchanged content, providing direct gradients for\nend-to-end optimization. To ensure visual fidelity, we incorporate distribution\nmatching loss (DMD), which constrains generated images to remain within the\nimage manifold learned by pretrained models. We evaluate our method on standard\nbenchmarks and include an extensive ablation study. Without any paired data,\nour method performs on par with various image editing diffusion models trained\non extensive supervised paired data, under the few-step setting. Given the same\nVLM as the reward model, we also outperform RL-based techniques like Flow-GRPO.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.14978.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 127
    },
    "organization": {
      "_id": "61e5d14f77496de0a6d95c6b",
      "name": "adobe",
      "fullname": "Adobe",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1645217431826-61e35e517ac6b6d06cfa8081.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.13454",
      "authors": [
        {
          "_id": "68f19cc36e0bef323a68fc3b",
          "name": "Hyojun Go",
          "hidden": false
        },
        {
          "_id": "68f19cc36e0bef323a68fc3c",
          "name": "Dominik Narnhofer",
          "hidden": false
        },
        {
          "_id": "68f19cc36e0bef323a68fc3d",
          "name": "Goutam Bhat",
          "hidden": false
        },
        {
          "_id": "68f19cc36e0bef323a68fc3e",
          "name": "Prune Truong",
          "hidden": false
        },
        {
          "_id": "68f19cc36e0bef323a68fc3f",
          "name": "Federico Tombari",
          "hidden": false
        },
        {
          "_id": "68f19cc36e0bef323a68fc40",
          "name": "Konrad Schindler",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-15T11:55:08.000Z",
      "submittedOnDailyAt": "2025-10-17T00:09:34.641Z",
      "title": "VIST3A: Text-to-3D by Stitching a Multi-view Reconstruction Network to a\n  Video Generator",
      "submittedOnDailyBy": {
        "_id": "649f65a4ca03a1a35e3dac14",
        "avatarUrl": "/avatars/b0dcd8ad795b1e666ee247b2ac024d53.svg",
        "isPro": false,
        "fullname": "Hyojun GO",
        "user": "HJGO",
        "type": "user"
      },
      "summary": "The rapid progress of large, pretrained models for both visual content\ngeneration and 3D reconstruction opens up new possibilities for text-to-3D\ngeneration. Intuitively, one could obtain a formidable 3D scene generator if\none were able to combine the power of a modern latent text-to-video model as\n\"generator\" with the geometric abilities of a recent (feedforward) 3D\nreconstruction system as \"decoder\". We introduce VIST3A, a general framework\nthat does just that, addressing two main challenges. First, the two components\nmust be joined in a way that preserves the rich knowledge encoded in their\nweights. We revisit model stitching, i.e., we identify the layer in the 3D\ndecoder that best matches the latent representation produced by the\ntext-to-video generator and stitch the two parts together. That operation\nrequires only a small dataset and no labels. Second, the text-to-video\ngenerator must be aligned with the stitched 3D decoder, to ensure that the\ngenerated latents are decodable into consistent, perceptually convincing 3D\nscene geometry. To that end, we adapt direct reward finetuning, a popular\ntechnique for human preference alignment. We evaluate the proposed VIST3A\napproach with different video generators and 3D reconstruction models. All\ntested pairings markedly improve over prior text-to-3D models that output\nGaussian splats. Moreover, by choosing a suitable 3D base model, VIST3A also\nenables high-quality text-to-pointmap generation.",
      "upvotes": 2,
      "discussionId": "68f19cc86e0bef323a68fc41",
      "projectPage": "https://gohyojun15.github.io/VIST3A/",
      "githubRepo": "https://github.com/gohyojun15/VIST3A",
      "ai_summary": "VIST3A combines latent text-to-video models and 3D reconstruction systems to generate high-quality 3D scenes from text, improving upon prior methods.",
      "ai_keywords": [
        "latent text-to-video model",
        "3D reconstruction system",
        "model stitching",
        "direct reward finetuning",
        "Gaussian splats",
        "text-to-pointmap generation"
      ],
      "githubStars": 17
    },
    "publishedAt": "2025-10-15T07:55:08.000Z",
    "title": "VIST3A: Text-to-3D by Stitching a Multi-view Reconstruction Network to a\n  Video Generator",
    "summary": "The rapid progress of large, pretrained models for both visual content\ngeneration and 3D reconstruction opens up new possibilities for text-to-3D\ngeneration. Intuitively, one could obtain a formidable 3D scene generator if\none were able to combine the power of a modern latent text-to-video model as\n\"generator\" with the geometric abilities of a recent (feedforward) 3D\nreconstruction system as \"decoder\". We introduce VIST3A, a general framework\nthat does just that, addressing two main challenges. First, the two components\nmust be joined in a way that preserves the rich knowledge encoded in their\nweights. We revisit model stitching, i.e., we identify the layer in the 3D\ndecoder that best matches the latent representation produced by the\ntext-to-video generator and stitch the two parts together. That operation\nrequires only a small dataset and no labels. Second, the text-to-video\ngenerator must be aligned with the stitched 3D decoder, to ensure that the\ngenerated latents are decodable into consistent, perceptually convincing 3D\nscene geometry. To that end, we adapt direct reward finetuning, a popular\ntechnique for human preference alignment. We evaluate the proposed VIST3A\napproach with different video generators and 3D reconstruction models. All\ntested pairings markedly improve over prior text-to-3D models that output\nGaussian splats. Moreover, by choosing a suitable 3D base model, VIST3A also\nenables high-quality text-to-pointmap generation.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.13454.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "649f65a4ca03a1a35e3dac14",
      "avatarUrl": "/avatars/b0dcd8ad795b1e666ee247b2ac024d53.svg",
      "fullname": "Hyojun GO",
      "name": "HJGO",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.14974",
      "authors": [
        {
          "_id": "68f19b086e0bef323a68fc1a",
          "name": "Hansheng Chen",
          "hidden": false
        },
        {
          "_id": "68f19b086e0bef323a68fc1b",
          "name": "Kai Zhang",
          "hidden": false
        },
        {
          "_id": "68f19b086e0bef323a68fc1c",
          "name": "Hao Tan",
          "hidden": false
        },
        {
          "_id": "68f19b086e0bef323a68fc1d",
          "name": "Leonidas Guibas",
          "hidden": false
        },
        {
          "_id": "68f19b086e0bef323a68fc1e",
          "name": "Gordon Wetzstein",
          "hidden": false
        },
        {
          "_id": "68f19b086e0bef323a68fc1f",
          "name": "Sai Bi",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-16T17:59:51.000Z",
      "submittedOnDailyAt": "2025-10-17T00:12:55.031Z",
      "title": "pi-Flow: Policy-Based Few-Step Generation via Imitation Distillation",
      "submittedOnDailyBy": {
        "_id": "638067fcb334960c987fbeda",
        "avatarUrl": "/avatars/87f1eaaf6b3a9c0d47d6f406261ccc18.svg",
        "isPro": false,
        "fullname": "Hansheng Chen",
        "user": "Lakonik",
        "type": "user"
      },
      "summary": "Few-step diffusion or flow-based generative models typically distill a\nvelocity-predicting teacher into a student that predicts a shortcut towards\ndenoised data. This format mismatch has led to complex distillation procedures\nthat often suffer from a quality-diversity trade-off. To address this, we\npropose policy-based flow models (pi-Flow). pi-Flow modifies the output\nlayer of a student flow model to predict a network-free policy at one timestep.\nThe policy then produces dynamic flow velocities at future substeps with\nnegligible overhead, enabling fast and accurate ODE integration on these\nsubsteps without extra network evaluations. To match the policy's ODE\ntrajectory to the teacher's, we introduce a novel imitation distillation\napproach, which matches the policy's velocity to the teacher's along the\npolicy's trajectory using a standard ell_2 flow matching loss. By simply\nmimicking the teacher's behavior, pi-Flow enables stable and scalable\ntraining and avoids the quality-diversity trade-off. On ImageNet 256^2, it\nattains a 1-NFE FID of 2.85, outperforming MeanFlow of the same DiT\narchitecture. On FLUX.1-12B and Qwen-Image-20B at 4 NFEs, pi-Flow achieves\nsubstantially better diversity than state-of-the-art few-step methods, while\nmaintaining teacher-level quality.",
      "upvotes": 1,
      "discussionId": "68f19b086e0bef323a68fc20",
      "ai_summary": "Policy-based flow models enable efficient and high-quality image generation by distilling teacher models into student models with dynamic flow velocities, improving diversity and quality.",
      "ai_keywords": [
        "velocity-predicting teacher",
        "student flow model",
        "policy-based flow models",
        "$\\pi$-Flow",
        "dynamic flow velocities",
        "ODE integration",
        "imitation distillation",
        "$\\ell_2$ flow matching loss",
        "ImageNet 256$^2$",
        "FID",
        "MeanFlow",
        "DiT architecture",
        "FLUX.1-12B",
        "Qwen-Image-20B",
        "NFE"
      ],
      "organization": {
        "_id": "61e5d14f77496de0a6d95c6b",
        "name": "adobe",
        "fullname": "Adobe",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1645217431826-61e35e517ac6b6d06cfa8081.png"
      }
    },
    "publishedAt": "2025-10-16T13:59:51.000Z",
    "title": "pi-Flow: Policy-Based Few-Step Generation via Imitation Distillation",
    "summary": "Few-step diffusion or flow-based generative models typically distill a\nvelocity-predicting teacher into a student that predicts a shortcut towards\ndenoised data. This format mismatch has led to complex distillation procedures\nthat often suffer from a quality-diversity trade-off. To address this, we\npropose policy-based flow models (pi-Flow). pi-Flow modifies the output\nlayer of a student flow model to predict a network-free policy at one timestep.\nThe policy then produces dynamic flow velocities at future substeps with\nnegligible overhead, enabling fast and accurate ODE integration on these\nsubsteps without extra network evaluations. To match the policy's ODE\ntrajectory to the teacher's, we introduce a novel imitation distillation\napproach, which matches the policy's velocity to the teacher's along the\npolicy's trajectory using a standard ell_2 flow matching loss. By simply\nmimicking the teacher's behavior, pi-Flow enables stable and scalable\ntraining and avoids the quality-diversity trade-off. On ImageNet 256^2, it\nattains a 1-NFE FID of 2.85, outperforming MeanFlow of the same DiT\narchitecture. On FLUX.1-12B and Qwen-Image-20B at 4 NFEs, pi-Flow achieves\nsubstantially better diversity than state-of-the-art few-step methods, while\nmaintaining teacher-level quality.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.14974.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "638067fcb334960c987fbeda",
      "avatarUrl": "/avatars/87f1eaaf6b3a9c0d47d6f406261ccc18.svg",
      "fullname": "Hansheng Chen",
      "name": "Lakonik",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 11
    },
    "organization": {
      "_id": "61e5d14f77496de0a6d95c6b",
      "name": "adobe",
      "fullname": "Adobe",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1645217431826-61e35e517ac6b6d06cfa8081.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.14969",
      "authors": [
        {
          "_id": "68f19da26e0bef323a68fc68",
          "name": "Yiming Wang",
          "hidden": false
        },
        {
          "_id": "68f19da26e0bef323a68fc69",
          "name": "Da Yin",
          "hidden": false
        },
        {
          "_id": "68f19da26e0bef323a68fc6a",
          "name": "Yuedong Cui",
          "hidden": false
        },
        {
          "_id": "68f19da26e0bef323a68fc6b",
          "name": "Ruichen Zheng",
          "hidden": false
        },
        {
          "_id": "68f19da26e0bef323a68fc6c",
          "name": "Zhiqian Li",
          "hidden": false
        },
        {
          "_id": "68f19da26e0bef323a68fc6d",
          "name": "Zongyu Lin",
          "hidden": false
        },
        {
          "_id": "68f19da26e0bef323a68fc6e",
          "name": "Di Wu",
          "hidden": false
        },
        {
          "_id": "68f19da26e0bef323a68fc6f",
          "name": "Xueqing Wu",
          "hidden": false
        },
        {
          "_id": "68f19da26e0bef323a68fc70",
          "name": "Chenchen Ye",
          "hidden": false
        },
        {
          "_id": "68f19da26e0bef323a68fc71",
          "name": "Yu Zhou",
          "hidden": false
        },
        {
          "_id": "68f19da26e0bef323a68fc72",
          "name": "Kai-Wei Chang",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/634e4670a51d5df8c2d92fce/hP00XFC26Wxs0TvDfDWas.mp4"
      ],
      "publishedAt": "2025-10-16T17:59:38.000Z",
      "submittedOnDailyAt": "2025-10-17T00:20:18.195Z",
      "title": "LLMs as Scalable, General-Purpose Simulators For Evolving Digital Agent\n  Training",
      "submittedOnDailyBy": {
        "_id": "634e4670a51d5df8c2d92fce",
        "avatarUrl": "/avatars/c52d7150b4de6a2eb2d83b345d35cbc2.svg",
        "isPro": false,
        "fullname": "Da Yin",
        "user": "DaYin",
        "type": "user"
      },
      "summary": "Digital agents require diverse, large-scale UI trajectories to generalize\nacross real-world tasks, yet collecting such data is prohibitively expensive in\nboth human annotation, infra and engineering perspectives. To this end, we\nintroduce UI-Simulator, a scalable paradigm that generates\nstructured UI states and transitions to synthesize training trajectories at\nscale. Our paradigm integrates a digital world simulator for diverse UI states,\na guided rollout process for coherent exploration, and a trajectory wrapper\nthat produces high-quality and diverse trajectories for agent training. We\nfurther propose UI-Simulator-Grow, a targeted scaling strategy that\nenables more rapid and data-efficient scaling by prioritizing high-impact tasks\nand synthesizes informative trajectory variants. Experiments on WebArena and\nAndroidWorld show that UI-Simulator rivals or surpasses open-source agents\ntrained on real UIs with significantly better robustness, despite using weaker\nteacher models. Moreover, UI-Simulator-Grow matches the performance of\nLlama-3-70B-Instruct using only Llama-3-8B-Instruct as the base model,\nhighlighting the potential of targeted synthesis scaling paradigm to\ncontinuously and efficiently enhance the digital agents.",
      "upvotes": 1,
      "discussionId": "68f19da26e0bef323a68fc73",
      "ai_summary": "UI-Simulator generates diverse UI trajectories for digital agents using a scalable paradigm, improving robustness and performance with targeted scaling strategies.",
      "ai_keywords": [
        "UI-Simulator",
        "digital world simulator",
        "guided rollout process",
        "trajectory wrapper",
        "UI-Simulator-Grow",
        "WebArena",
        "AndroidWorld",
        "Llama-3-70B-Instruct",
        "Llama-3-8B-Instruct"
      ],
      "organization": {
        "_id": "60b492a78e9589ce25930346",
        "name": "uclanlp",
        "fullname": "UCLA NLP",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1622446751930-5ec0135ded25d76864d553f1.png"
      }
    },
    "publishedAt": "2025-10-16T13:59:38.000Z",
    "title": "LLMs as Scalable, General-Purpose Simulators For Evolving Digital Agent\n  Training",
    "summary": "Digital agents require diverse, large-scale UI trajectories to generalize\nacross real-world tasks, yet collecting such data is prohibitively expensive in\nboth human annotation, infra and engineering perspectives. To this end, we\nintroduce UI-Simulator, a scalable paradigm that generates\nstructured UI states and transitions to synthesize training trajectories at\nscale. Our paradigm integrates a digital world simulator for diverse UI states,\na guided rollout process for coherent exploration, and a trajectory wrapper\nthat produces high-quality and diverse trajectories for agent training. We\nfurther propose UI-Simulator-Grow, a targeted scaling strategy that\nenables more rapid and data-efficient scaling by prioritizing high-impact tasks\nand synthesizes informative trajectory variants. Experiments on WebArena and\nAndroidWorld show that UI-Simulator rivals or surpasses open-source agents\ntrained on real UIs with significantly better robustness, despite using weaker\nteacher models. Moreover, UI-Simulator-Grow matches the performance of\nLlama-3-70B-Instruct using only Llama-3-8B-Instruct as the base model,\nhighlighting the potential of targeted synthesis scaling paradigm to\ncontinuously and efficiently enhance the digital agents.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/634e4670a51d5df8c2d92fce/hP00XFC26Wxs0TvDfDWas.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.14969.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "634e4670a51d5df8c2d92fce",
      "avatarUrl": "/avatars/c52d7150b4de6a2eb2d83b345d35cbc2.svg",
      "fullname": "Da Yin",
      "name": "DaYin",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 0
    },
    "organization": {
      "_id": "60b492a78e9589ce25930346",
      "name": "uclanlp",
      "fullname": "UCLA NLP",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1622446751930-5ec0135ded25d76864d553f1.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.14958",
      "authors": [
        {
          "_id": "68f1a3346e0bef323a68fcde",
          "name": "Weikang Shi",
          "hidden": false
        },
        {
          "_id": "68f1a3346e0bef323a68fcdf",
          "name": "Aldrich Yu",
          "hidden": false
        },
        {
          "_id": "68f1a3346e0bef323a68fce0",
          "name": "Rongyao Fang",
          "hidden": false
        },
        {
          "_id": "68f1a3346e0bef323a68fce1",
          "name": "Houxing Ren",
          "hidden": false
        },
        {
          "_id": "68f1a3346e0bef323a68fce2",
          "name": "Ke Wang",
          "hidden": false
        },
        {
          "_id": "68f1a3346e0bef323a68fce3",
          "name": "Aojun Zhou",
          "hidden": false
        },
        {
          "_id": "68f1a3346e0bef323a68fce4",
          "name": "Changyao Tian",
          "hidden": false
        },
        {
          "_id": "68f1a3346e0bef323a68fce5",
          "name": "Xinyu Fu",
          "hidden": false
        },
        {
          "_id": "68f1a3346e0bef323a68fce6",
          "name": "Yuxuan Hu",
          "hidden": false
        },
        {
          "_id": "68f1a3346e0bef323a68fce7",
          "name": "Zimu Lu",
          "hidden": false
        },
        {
          "_id": "68f1a3346e0bef323a68fce8",
          "name": "Linjiang Huang",
          "hidden": false
        },
        {
          "_id": "68f1a3346e0bef323a68fce9",
          "name": "Si Liu",
          "hidden": false
        },
        {
          "_id": "68f1a3346e0bef323a68fcea",
          "name": "Rui Liu",
          "hidden": false
        },
        {
          "_id": "68f1a3346e0bef323a68fceb",
          "name": "Hongsheng Li",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-16T17:58:58.000Z",
      "submittedOnDailyAt": "2025-10-17T00:30:38.950Z",
      "title": "MathCanvas: Intrinsic Visual Chain-of-Thought for Multimodal\n  Mathematical Reasoning",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "While Large Language Models (LLMs) have excelled in textual reasoning, they\nstruggle with mathematical domains like geometry that intrinsically rely on\nvisual aids. Existing approaches to Visual Chain-of-Thought (VCoT) are often\nlimited by rigid external tools or fail to generate the high-fidelity,\nstrategically-timed diagrams necessary for complex problem-solving. To bridge\nthis gap, we introduce MathCanvas, a comprehensive framework designed to endow\nunified Large Multimodal Models (LMMs) with intrinsic VCoT capabilities for\nmathematics. Our approach consists of two phases. First, a Visual Manipulation\nstage pre-trains the model on a novel 15.2M-pair corpus, comprising 10M\ncaption-to-diagram pairs (MathCanvas-Imagen) and 5.2M step-by-step editing\ntrajectories (MathCanvas-Edit), to master diagram generation and editing.\nSecond, a Strategic Visual-Aided Reasoning stage fine-tunes the model on\nMathCanvas-Instruct, a new 219K-example dataset of interleaved visual-textual\nreasoning paths, teaching it when and how to leverage visual aids. To\nfacilitate rigorous evaluation, we introduce MathCanvas-Bench, a challenging\nbenchmark with 3K problems that require models to produce interleaved\nvisual-textual solutions. Our model, BAGEL-Canvas, trained under this\nframework, achieves an 86% relative improvement over strong LMM baselines on\nMathCanvas-Bench, demonstrating excellent generalization to other public math\nbenchmarks. Our work provides a complete toolkit-framework, datasets, and\nbenchmark-to unlock complex, human-like visual-aided reasoning in LMMs. Project\nPage: https://mathcanvas.github.io/",
      "upvotes": 1,
      "discussionId": "68f1a3346e0bef323a68fcec",
      "projectPage": "https://mathcanvas.github.io/",
      "githubRepo": "https://github.com/shiwk24/MathCanvas",
      "ai_summary": "MathCanvas enhances Large Multimodal Models with Visual Chain-of-Thought capabilities for mathematics through pre-training on diagram generation and fine-tuning on visual-textual reasoning, achieving significant improvements on math benchmarks.",
      "ai_keywords": [
        "Large Language Models",
        "Visual Chain-of-Thought",
        "Large Multimodal Models",
        "Visual Manipulation",
        "Strategic Visual-Aided Reasoning",
        "MathCanvas-Imagen",
        "MathCanvas-Edit",
        "MathCanvas-Instruct",
        "MathCanvas-Bench",
        "BAGEL-Canvas"
      ],
      "githubStars": 1
    },
    "publishedAt": "2025-10-16T13:58:58.000Z",
    "title": "MathCanvas: Intrinsic Visual Chain-of-Thought for Multimodal\n  Mathematical Reasoning",
    "summary": "While Large Language Models (LLMs) have excelled in textual reasoning, they\nstruggle with mathematical domains like geometry that intrinsically rely on\nvisual aids. Existing approaches to Visual Chain-of-Thought (VCoT) are often\nlimited by rigid external tools or fail to generate the high-fidelity,\nstrategically-timed diagrams necessary for complex problem-solving. To bridge\nthis gap, we introduce MathCanvas, a comprehensive framework designed to endow\nunified Large Multimodal Models (LMMs) with intrinsic VCoT capabilities for\nmathematics. Our approach consists of two phases. First, a Visual Manipulation\nstage pre-trains the model on a novel 15.2M-pair corpus, comprising 10M\ncaption-to-diagram pairs (MathCanvas-Imagen) and 5.2M step-by-step editing\ntrajectories (MathCanvas-Edit), to master diagram generation and editing.\nSecond, a Strategic Visual-Aided Reasoning stage fine-tunes the model on\nMathCanvas-Instruct, a new 219K-example dataset of interleaved visual-textual\nreasoning paths, teaching it when and how to leverage visual aids. To\nfacilitate rigorous evaluation, we introduce MathCanvas-Bench, a challenging\nbenchmark with 3K problems that require models to produce interleaved\nvisual-textual solutions. Our model, BAGEL-Canvas, trained under this\nframework, achieves an 86% relative improvement over strong LMM baselines on\nMathCanvas-Bench, demonstrating excellent generalization to other public math\nbenchmarks. Our work provides a complete toolkit-framework, datasets, and\nbenchmark-to unlock complex, human-like visual-aided reasoning in LMMs. Project\nPage: https://mathcanvas.github.io/",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.14958.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 127
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.14545",
      "authors": [
        {
          "_id": "68f1a44e6e0bef323a68fd02",
          "name": "Guanting Dong",
          "hidden": false
        },
        {
          "_id": "68f1a44e6e0bef323a68fd03",
          "name": "Licheng Bao",
          "hidden": false
        },
        {
          "_id": "68f1a44e6e0bef323a68fd04",
          "name": "Zhongyuan Wang",
          "hidden": false
        },
        {
          "_id": "68f1a44e6e0bef323a68fd05",
          "name": "Kangzhi Zhao",
          "hidden": false
        },
        {
          "_id": "68f1a44e6e0bef323a68fd06",
          "name": "Xiaoxi Li",
          "hidden": false
        },
        {
          "_id": "68f1a44e6e0bef323a68fd07",
          "name": "Jiajie Jin",
          "hidden": false
        },
        {
          "_id": "68f1a44e6e0bef323a68fd08",
          "name": "Jinghan Yang",
          "hidden": false
        },
        {
          "_id": "68f1a44e6e0bef323a68fd09",
          "name": "Hangyu Mao",
          "hidden": false
        },
        {
          "_id": "68f1a44e6e0bef323a68fd0a",
          "name": "Fuzheng Zhang",
          "hidden": false
        },
        {
          "_id": "68f1a44e6e0bef323a68fd0b",
          "name": "Kun Gai",
          "hidden": false
        },
        {
          "_id": "68f1a44e6e0bef323a68fd0c",
          "name": "Guorui Zhou",
          "hidden": false
        },
        {
          "_id": "68f1a44e6e0bef323a68fd0d",
          "name": "Yutao Zhu",
          "hidden": false
        },
        {
          "_id": "68f1a44e6e0bef323a68fd0e",
          "name": "Ji-Rong Wen",
          "hidden": false
        },
        {
          "_id": "68f1a44e6e0bef323a68fd0f",
          "name": "Zhicheng Dou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-16T10:40:52.000Z",
      "submittedOnDailyAt": "2025-10-17T00:37:49.509Z",
      "title": "Agentic Entropy-Balanced Policy Optimization",
      "submittedOnDailyBy": {
        "_id": "61cd4b833dd34ba1985e0753",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/61cd4b833dd34ba1985e0753/BfHfrwotoMESpXZOHiIe4.png",
        "isPro": false,
        "fullname": "KABI",
        "user": "dongguanting",
        "type": "user"
      },
      "summary": "Recently, Agentic Reinforcement Learning (Agentic RL) has made significant\nprogress in incentivizing the multi-turn, long-horizon tool-use capabilities of\nweb agents. While mainstream agentic RL algorithms autonomously explore\nhigh-uncertainty tool-call steps under the guidance of entropy, excessive\nreliance on entropy signals can impose further constraints, leading to the\ntraining collapse. In this paper, we delve into the challenges caused by\nentropy and propose the Agentic Entropy-Balanced Policy Optimization (AEPO), an\nagentic RL algorithm designed to balance entropy in both the rollout and policy\nupdate phases. AEPO comprises two core components: (1) a dynamic\nentropy-balanced rollout mechanism that adaptively allocate global and branch\nsampling budget through entropy pre-monitoring, while imposing a branch penalty\non consecutive high-entropy tool-call steps to prevent over-branching issues;\nand (2) Entropy-Balanced Policy Optimization that inserts a stop-gradient\noperation into the high-entropy clipping term to preserve and properly rescale\ngradients on high-entropy tokens, while incorporating entropy-aware advantage\nestimation to prioritize learning on high-uncertainty tokens. Results across 14\nchallenging datasets show that AEPO consistently outperforms 7 mainstream RL\nalgorithms. With just 1K RL samples, Qwen3-14B with AEPO achieves impressive\nresults: 47.6% on GAIA, 11.2% on Humanity's Last Exam, and 43.0% on WebWalker\nfor Pass@1; 65.0% on GAIA, 26.0% on Humanity's Last Exam, and 70.0% on\nWebWalker for Pass@5. Further analysis reveals that AEPO improves rollout\nsampling diversity while maintaining stable policy entropy, facilitating\nscalable web agent training.",
      "upvotes": 1,
      "discussionId": "68f1a44e6e0bef323a68fd10",
      "ai_summary": "AEPO, an agentic RL algorithm, addresses entropy-related challenges in web agent training, enhancing performance and stability across various datasets.",
      "ai_keywords": [
        "Agentic Reinforcement Learning",
        "AEPO",
        "entropy",
        "rollout",
        "policy update",
        "dynamic entropy-balanced rollout",
        "branch penalty",
        "stop-gradient operation",
        "high-entropy clipping",
        "entropy-aware advantage estimation",
        "rollout sampling diversity",
        "policy entropy"
      ],
      "organization": {
        "_id": "622177ac43826d6f261f8208",
        "name": "RUC",
        "fullname": "Renmin University of China",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/670IAX9A2-BflqA5MiSBW.jpeg"
      }
    },
    "publishedAt": "2025-10-16T06:40:52.000Z",
    "title": "Agentic Entropy-Balanced Policy Optimization",
    "summary": "Recently, Agentic Reinforcement Learning (Agentic RL) has made significant\nprogress in incentivizing the multi-turn, long-horizon tool-use capabilities of\nweb agents. While mainstream agentic RL algorithms autonomously explore\nhigh-uncertainty tool-call steps under the guidance of entropy, excessive\nreliance on entropy signals can impose further constraints, leading to the\ntraining collapse. In this paper, we delve into the challenges caused by\nentropy and propose the Agentic Entropy-Balanced Policy Optimization (AEPO), an\nagentic RL algorithm designed to balance entropy in both the rollout and policy\nupdate phases. AEPO comprises two core components: (1) a dynamic\nentropy-balanced rollout mechanism that adaptively allocate global and branch\nsampling budget through entropy pre-monitoring, while imposing a branch penalty\non consecutive high-entropy tool-call steps to prevent over-branching issues;\nand (2) Entropy-Balanced Policy Optimization that inserts a stop-gradient\noperation into the high-entropy clipping term to preserve and properly rescale\ngradients on high-entropy tokens, while incorporating entropy-aware advantage\nestimation to prioritize learning on high-uncertainty tokens. Results across 14\nchallenging datasets show that AEPO consistently outperforms 7 mainstream RL\nalgorithms. With just 1K RL samples, Qwen3-14B with AEPO achieves impressive\nresults: 47.6% on GAIA, 11.2% on Humanity's Last Exam, and 43.0% on WebWalker\nfor Pass@1; 65.0% on GAIA, 26.0% on Humanity's Last Exam, and 70.0% on\nWebWalker for Pass@5. Further analysis reveals that AEPO improves rollout\nsampling diversity while maintaining stable policy entropy, facilitating\nscalable web agent training.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.14545.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "61cd4b833dd34ba1985e0753",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/61cd4b833dd34ba1985e0753/BfHfrwotoMESpXZOHiIe4.png",
      "fullname": "KABI",
      "name": "dongguanting",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 47
    },
    "organization": {
      "_id": "622177ac43826d6f261f8208",
      "name": "RUC",
      "fullname": "Renmin University of China",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/670IAX9A2-BflqA5MiSBW.jpeg"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.14528",
      "authors": [
        {
          "_id": "68f1a43f6e0bef323a68fcee",
          "name": "Cheng Cui",
          "hidden": false
        },
        {
          "_id": "68f1a43f6e0bef323a68fcef",
          "name": "Ting Sun",
          "hidden": false
        },
        {
          "_id": "68f1a43f6e0bef323a68fcf0",
          "name": "Suyin Liang",
          "hidden": false
        },
        {
          "_id": "68f1a43f6e0bef323a68fcf1",
          "name": "Tingquan Gao",
          "hidden": false
        },
        {
          "_id": "68f1a43f6e0bef323a68fcf2",
          "name": "Zelun Zhang",
          "hidden": false
        },
        {
          "_id": "68f1a43f6e0bef323a68fcf3",
          "name": "Jiaxuan Liu",
          "hidden": false
        },
        {
          "_id": "68f1a43f6e0bef323a68fcf4",
          "name": "Xueqing Wang",
          "hidden": false
        },
        {
          "_id": "68f1a43f6e0bef323a68fcf5",
          "name": "Changda Zhou",
          "hidden": false
        },
        {
          "_id": "68f1a43f6e0bef323a68fcf6",
          "name": "Hongen Liu",
          "hidden": false
        },
        {
          "_id": "68f1a43f6e0bef323a68fcf7",
          "name": "Manhui Lin",
          "hidden": false
        },
        {
          "_id": "68f1a43f6e0bef323a68fcf8",
          "name": "Yue Zhang",
          "hidden": false
        },
        {
          "_id": "68f1a43f6e0bef323a68fcf9",
          "name": "Yubo Zhang",
          "hidden": false
        },
        {
          "_id": "68f1a43f6e0bef323a68fcfa",
          "name": "Handong Zheng",
          "hidden": false
        },
        {
          "_id": "68f1a43f6e0bef323a68fcfb",
          "name": "Jing Zhang",
          "hidden": false
        },
        {
          "_id": "68f1a43f6e0bef323a68fcfc",
          "name": "Jun Zhang",
          "hidden": false
        },
        {
          "_id": "68f1a43f6e0bef323a68fcfd",
          "name": "Yi Liu",
          "hidden": false
        },
        {
          "_id": "68f1a43f6e0bef323a68fcfe",
          "name": "Dianhai Yu",
          "hidden": false
        },
        {
          "_id": "68f1a43f6e0bef323a68fcff",
          "name": "Yanjun Ma",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-16T10:18:48.000Z",
      "submittedOnDailyAt": "2025-10-17T00:35:15.163Z",
      "title": "PaddleOCR-VL: Boosting Multilingual Document Parsing via a 0.9B\n  Ultra-Compact Vision-Language Model",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "In this report, we propose PaddleOCR-VL, a SOTA and resource-efficient model\ntailored for document parsing. Its core component is PaddleOCR-VL-0.9B, a\ncompact yet powerful vision-language model (VLM) that integrates a NaViT-style\ndynamic resolution visual encoder with the ERNIE-4.5-0.3B language model to\nenable accurate element recognition. This innovative model efficiently supports\n109 languages and excels in recognizing complex elements (e.g., text, tables,\nformulas, and charts), while maintaining minimal resource consumption. Through\ncomprehensive evaluations on widely used public benchmarks and in-house\nbenchmarks, PaddleOCR-VL achieves SOTA performance in both page-level document\nparsing and element-level recognition. It significantly outperforms existing\nsolutions, exhibits strong competitiveness against top-tier VLMs, and delivers\nfast inference speeds. These strengths make it highly suitable for practical\ndeployment in real-world scenarios.",
      "upvotes": 1,
      "discussionId": "68f1a4406e0bef323a68fd00",
      "ai_summary": "PaddleOCR-VL, a vision-language model combining NaViT-style visual encoder and ERNIE-4.5 language model, achieves state-of-the-art performance in document parsing with minimal resource consumption.",
      "ai_keywords": [
        "vision-language model",
        "NaViT-style",
        "dynamic resolution visual encoder",
        "ERNIE-4.5",
        "element recognition",
        "page-level document parsing",
        "element-level recognition",
        "inference speeds"
      ],
      "organization": {
        "_id": "62067d5d3906f102bc9658bd",
        "name": "PaddlePaddle",
        "fullname": "PaddlePaddle",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1654942635336-5f3ff69679c1ba4c353d0c5a.png"
      }
    },
    "publishedAt": "2025-10-16T06:18:48.000Z",
    "title": "PaddleOCR-VL: Boosting Multilingual Document Parsing via a 0.9B\n  Ultra-Compact Vision-Language Model",
    "summary": "In this report, we propose PaddleOCR-VL, a SOTA and resource-efficient model\ntailored for document parsing. Its core component is PaddleOCR-VL-0.9B, a\ncompact yet powerful vision-language model (VLM) that integrates a NaViT-style\ndynamic resolution visual encoder with the ERNIE-4.5-0.3B language model to\nenable accurate element recognition. This innovative model efficiently supports\n109 languages and excels in recognizing complex elements (e.g., text, tables,\nformulas, and charts), while maintaining minimal resource consumption. Through\ncomprehensive evaluations on widely used public benchmarks and in-house\nbenchmarks, PaddleOCR-VL achieves SOTA performance in both page-level document\nparsing and element-level recognition. It significantly outperforms existing\nsolutions, exhibits strong competitiveness against top-tier VLMs, and delivers\nfast inference speeds. These strengths make it highly suitable for practical\ndeployment in real-world scenarios.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.14528.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 127
    },
    "organization": {
      "_id": "62067d5d3906f102bc9658bd",
      "name": "PaddlePaddle",
      "fullname": "PaddlePaddle",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1654942635336-5f3ff69679c1ba4c353d0c5a.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.14276",
      "authors": [
        {
          "_id": "68f1a6d46e0bef323a68fd29",
          "name": "Haiquan Zhao",
          "hidden": false
        },
        {
          "_id": "68f1a6d46e0bef323a68fd2a",
          "name": "Chenhan Yuan",
          "hidden": false
        },
        {
          "_id": "68f1a6d46e0bef323a68fd2b",
          "name": "Fei Huang",
          "hidden": false
        },
        {
          "_id": "68f1a6d46e0bef323a68fd2c",
          "name": "Xiaomeng Hu",
          "hidden": false
        },
        {
          "_id": "68f1a6d46e0bef323a68fd2d",
          "name": "Yichang Zhang",
          "hidden": false
        },
        {
          "_id": "68f1a6d46e0bef323a68fd2e",
          "name": "An Yang",
          "hidden": false
        },
        {
          "_id": "68f1a6d46e0bef323a68fd2f",
          "name": "Bowen Yu",
          "hidden": false
        },
        {
          "_id": "68f1a6d46e0bef323a68fd30",
          "name": "Dayiheng Liu",
          "hidden": false
        },
        {
          "_id": "68f1a6d46e0bef323a68fd31",
          "name": "Jingren Zhou",
          "hidden": false
        },
        {
          "_id": "68f1a6d46e0bef323a68fd32",
          "name": "Junyang Lin",
          "hidden": false
        },
        {
          "_id": "68f1a6d46e0bef323a68fd33",
          "name": "Baosong Yang",
          "hidden": false
        },
        {
          "_id": "68f1a6d46e0bef323a68fd34",
          "name": "Chen Cheng",
          "hidden": false
        },
        {
          "_id": "68f1a6d46e0bef323a68fd35",
          "name": "Jialong Tang",
          "hidden": false
        },
        {
          "_id": "68f1a6d46e0bef323a68fd36",
          "name": "Jiandong Jiang",
          "hidden": false
        },
        {
          "_id": "68f1a6d46e0bef323a68fd37",
          "name": "Jianwei Zhang",
          "hidden": false
        },
        {
          "_id": "68f1a6d46e0bef323a68fd38",
          "name": "Jijie Xu",
          "hidden": false
        },
        {
          "_id": "68f1a6d46e0bef323a68fd39",
          "name": "Ming Yan",
          "hidden": false
        },
        {
          "_id": "68f1a6d46e0bef323a68fd3a",
          "name": "Minmin Sun",
          "hidden": false
        },
        {
          "_id": "68f1a6d46e0bef323a68fd3b",
          "name": "Pei Zhang",
          "hidden": false
        },
        {
          "_id": "68f1a6d46e0bef323a68fd3c",
          "name": "Pengjun Xie",
          "hidden": false
        },
        {
          "_id": "68f1a6d46e0bef323a68fd3d",
          "name": "Qiaoyu Tang",
          "hidden": false
        },
        {
          "_id": "68f1a6d46e0bef323a68fd3e",
          "name": "Qin Zhu",
          "hidden": false
        },
        {
          "_id": "68f1a6d46e0bef323a68fd3f",
          "name": "Rong Zhang",
          "hidden": false
        },
        {
          "_id": "68f1a6d46e0bef323a68fd40",
          "name": "Shibin Wu",
          "hidden": false
        },
        {
          "_id": "68f1a6d46e0bef323a68fd41",
          "name": "Shuo Zhang",
          "hidden": false
        },
        {
          "_id": "68f1a6d46e0bef323a68fd42",
          "name": "Tao He",
          "hidden": false
        },
        {
          "_id": "68f1a6d46e0bef323a68fd43",
          "name": "Tianyi Tang",
          "hidden": false
        },
        {
          "_id": "68f1a6d46e0bef323a68fd44",
          "name": "Tingyu Xia",
          "hidden": false
        },
        {
          "_id": "68f1a6d46e0bef323a68fd45",
          "name": "Wei Liao",
          "hidden": false
        },
        {
          "_id": "68f1a6d46e0bef323a68fd46",
          "name": "Weizhou Shen",
          "hidden": false
        },
        {
          "_id": "68f1a6d46e0bef323a68fd47",
          "name": "Wenbiao Yin",
          "hidden": false
        },
        {
          "_id": "68f1a6d46e0bef323a68fd48",
          "name": "Wenmeng Zhou",
          "hidden": false
        },
        {
          "_id": "68f1a6d46e0bef323a68fd49",
          "name": "Wenyuan Yu",
          "hidden": false
        },
        {
          "_id": "68f1a6d46e0bef323a68fd4a",
          "name": "Xiaobin Wang",
          "hidden": false
        },
        {
          "_id": "68f1a6d46e0bef323a68fd4b",
          "name": "Xiaodong Deng",
          "hidden": false
        },
        {
          "_id": "68f1a6d46e0bef323a68fd4c",
          "name": "Xiaodong Xu",
          "hidden": false
        },
        {
          "_id": "68f1a6d46e0bef323a68fd4d",
          "name": "Xinyu Zhang",
          "hidden": false
        },
        {
          "_id": "68f1a6d46e0bef323a68fd4e",
          "name": "Yang Liu",
          "hidden": false
        },
        {
          "_id": "68f1a6d46e0bef323a68fd4f",
          "name": "Yeqiu Li",
          "hidden": false
        },
        {
          "_id": "68f1a6d46e0bef323a68fd50",
          "name": "Yi Zhang",
          "hidden": false
        },
        {
          "_id": "68f1a6d46e0bef323a68fd51",
          "name": "Yong Jiang",
          "hidden": false
        },
        {
          "_id": "68f1a6d46e0bef323a68fd52",
          "name": "Yu Wan",
          "hidden": false
        },
        {
          "_id": "68f1a6d46e0bef323a68fd53",
          "name": "Yuxin Zhou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-16T04:00:18.000Z",
      "submittedOnDailyAt": "2025-10-17T00:46:03.670Z",
      "title": "Qwen3Guard Technical Report",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "As large language models (LLMs) become more capable and widely used, ensuring\nthe safety of their outputs is increasingly critical. Existing guardrail\nmodels, though useful in static evaluation settings, face two major limitations\nin real-world applications: (1) they typically output only binary \"safe/unsafe\"\nlabels, which can be interpreted inconsistently across diverse safety policies,\nrendering them incapable of accommodating varying safety tolerances across\ndomains; and (2) they require complete model outputs before performing safety\nchecks, making them fundamentally incompatible with streaming LLM inference,\nthereby preventing timely intervention during generation and increasing\nexposure to harmful partial outputs. To address these challenges, we present\nQwen3Guard, a series of multilingual safety guardrail models with two\nspecialized variants: Generative Qwen3Guard, which casts safety classification\nas an instruction-following task to enable fine-grained tri-class judgments\n(safe, controversial, unsafe); and Stream Qwen3Guard, which introduces a\ntoken-level classification head for real-time safety monitoring during\nincremental text generation. Both variants are available in three sizes (0.6B,\n4B, and 8B parameters) and support up to 119 languages and dialects, providing\ncomprehensive, scalable, and low-latency safety moderation for global LLM\ndeployments. Evaluated across English, Chinese, and multilingual benchmarks,\nQwen3Guard achieves state-of-the-art performance in both prompt and response\nsafety classification. All models are released under the Apache 2.0 license for\npublic use.",
      "upvotes": 1,
      "discussionId": "68f1a6d56e0bef323a68fd54",
      "githubRepo": "https://github.com/QwenLM/Qwen3Guard",
      "ai_summary": "Qwen3Guard introduces multilingual safety guardrail models with fine-grained tri-class judgments and real-time token-level safety monitoring for large language models.",
      "ai_keywords": [
        "large language models",
        "safety guardrail models",
        "binary labels",
        "safety policies",
        "safety tolerances",
        "streaming inference",
        "Generative Qwen3Guard",
        "Stream Qwen3Guard",
        "instruction-following task",
        "tri-class judgments",
        "token-level classification",
        "multilingual",
        "prompt safety",
        "response safety",
        "Apache 2.0 license"
      ],
      "organization": {
        "_id": "64c8b5837fe12ecd0a7e92eb",
        "name": "Qwen",
        "fullname": "Qwen",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/620760a26e3b7210c2ff1943/-s1gyJfvbE1RgO5iBeNOi.png"
      }
    },
    "publishedAt": "2025-10-16T00:00:18.000Z",
    "title": "Qwen3Guard Technical Report",
    "summary": "As large language models (LLMs) become more capable and widely used, ensuring\nthe safety of their outputs is increasingly critical. Existing guardrail\nmodels, though useful in static evaluation settings, face two major limitations\nin real-world applications: (1) they typically output only binary \"safe/unsafe\"\nlabels, which can be interpreted inconsistently across diverse safety policies,\nrendering them incapable of accommodating varying safety tolerances across\ndomains; and (2) they require complete model outputs before performing safety\nchecks, making them fundamentally incompatible with streaming LLM inference,\nthereby preventing timely intervention during generation and increasing\nexposure to harmful partial outputs. To address these challenges, we present\nQwen3Guard, a series of multilingual safety guardrail models with two\nspecialized variants: Generative Qwen3Guard, which casts safety classification\nas an instruction-following task to enable fine-grained tri-class judgments\n(safe, controversial, unsafe); and Stream Qwen3Guard, which introduces a\ntoken-level classification head for real-time safety monitoring during\nincremental text generation. Both variants are available in three sizes (0.6B,\n4B, and 8B parameters) and support up to 119 languages and dialects, providing\ncomprehensive, scalable, and low-latency safety moderation for global LLM\ndeployments. Evaluated across English, Chinese, and multilingual benchmarks,\nQwen3Guard achieves state-of-the-art performance in both prompt and response\nsafety classification. All models are released under the Apache 2.0 license for\npublic use.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.14276.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 127
    },
    "organization": {
      "_id": "64c8b5837fe12ecd0a7e92eb",
      "name": "Qwen",
      "fullname": "Qwen",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/620760a26e3b7210c2ff1943/-s1gyJfvbE1RgO5iBeNOi.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.14252",
      "authors": [
        {
          "_id": "68f1a46e6e0bef323a68fd12",
          "name": "Jihao Zhao",
          "hidden": false
        },
        {
          "_id": "68f1a46e6e0bef323a68fd13",
          "name": "Zhiyuan Ji",
          "hidden": false
        },
        {
          "_id": "68f1a46e6e0bef323a68fd14",
          "name": "Simin Niu",
          "hidden": false
        },
        {
          "_id": "68f1a46e6e0bef323a68fd15",
          "name": "Hanyu Wang",
          "hidden": false
        },
        {
          "_id": "68f1a46e6e0bef323a68fd16",
          "name": "Feiyu Xiong",
          "hidden": false
        },
        {
          "_id": "68f1a46e6e0bef323a68fd17",
          "name": "Zhiyu Li",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-16T03:09:51.000Z",
      "submittedOnDailyAt": "2025-10-17T00:42:57.762Z",
      "title": "MoM: Mixtures of Scenario-Aware Document Memories for\n  Retrieval-Augmented Generation Systems",
      "submittedOnDailyBy": {
        "_id": "658e85bb5b7553ca5c29ba89",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/658e85bb5b7553ca5c29ba89/KK6UpS9agtrxevvBoup5N.jpeg",
        "isPro": false,
        "fullname": "Jihao Zhao",
        "user": "Robot2050",
        "type": "user"
      },
      "summary": "The traditional RAG paradigm, which typically engages in the comprehension of\nrelevant text chunks in response to received queries, inherently restricts both\nthe depth of knowledge internalization and reasoning capabilities. To address\nthis limitation, our research transforms the text processing in RAG from\npassive chunking to proactive understanding, defining this process as document\nmemory extraction with the objective of simulating human cognitive processes\nduring reading. Building upon this, we propose the Mixtures of scenario-aware\ndocument Memories (MoM) framework, engineered to efficiently handle documents\nfrom multiple domains and train small language models (SLMs) to acquire the\nability to proactively explore and construct document memories. The MoM\ninitially instructs large language models (LLMs) to simulate domain experts in\ngenerating document logical outlines, thereby directing structured chunking and\ncore content extraction. It employs a multi-path sampling and multi-perspective\nevaluation mechanism, specifically designing comprehensive metrics that\nrepresent chunk clarity and extraction completeness to select the optimal\ndocument memories. Additionally, to infuse deeper human-like reading abilities\nduring the training of SLMs, we incorporate a reverse reasoning strategy, which\ndeduces refined expert thinking paths from high-quality outcomes. Finally,\nleveraging diverse forms of content generated by MoM, we develop a three-layer\ndocument memory retrieval mechanism, which is grounded in our theoretical proof\nfrom the perspective of probabilistic modeling. Extensive experimental results\nacross three distinct domains demonstrate that the MoM framework not only\nresolves text chunking challenges in existing RAG systems, providing LLMs with\nsemantically complete document memories, but also paves the way for SLMs to\nachieve human-centric intelligent text processing.",
      "upvotes": 1,
      "discussionId": "68f1a46e6e0bef323a68fd18",
      "ai_summary": "The MoM framework enhances RAG by transforming text processing from passive chunking to proactive understanding, enabling LLMs to generate structured document memories and SLMs to develop human-like reading abilities.",
      "ai_keywords": [
        "document memory extraction",
        "Mixtures of scenario-aware document Memories (MoM)",
        "small language models (SLMs)",
        "large language models (LLMs)",
        "document logical outlines",
        "multi-path sampling",
        "multi-perspective evaluation",
        "reverse reasoning strategy",
        "three-layer document memory retrieval mechanism",
        "probabilistic modeling"
      ]
    },
    "publishedAt": "2025-10-15T23:09:51.000Z",
    "title": "MoM: Mixtures of Scenario-Aware Document Memories for\n  Retrieval-Augmented Generation Systems",
    "summary": "The traditional RAG paradigm, which typically engages in the comprehension of\nrelevant text chunks in response to received queries, inherently restricts both\nthe depth of knowledge internalization and reasoning capabilities. To address\nthis limitation, our research transforms the text processing in RAG from\npassive chunking to proactive understanding, defining this process as document\nmemory extraction with the objective of simulating human cognitive processes\nduring reading. Building upon this, we propose the Mixtures of scenario-aware\ndocument Memories (MoM) framework, engineered to efficiently handle documents\nfrom multiple domains and train small language models (SLMs) to acquire the\nability to proactively explore and construct document memories. The MoM\ninitially instructs large language models (LLMs) to simulate domain experts in\ngenerating document logical outlines, thereby directing structured chunking and\ncore content extraction. It employs a multi-path sampling and multi-perspective\nevaluation mechanism, specifically designing comprehensive metrics that\nrepresent chunk clarity and extraction completeness to select the optimal\ndocument memories. Additionally, to infuse deeper human-like reading abilities\nduring the training of SLMs, we incorporate a reverse reasoning strategy, which\ndeduces refined expert thinking paths from high-quality outcomes. Finally,\nleveraging diverse forms of content generated by MoM, we develop a three-layer\ndocument memory retrieval mechanism, which is grounded in our theoretical proof\nfrom the perspective of probabilistic modeling. Extensive experimental results\nacross three distinct domains demonstrate that the MoM framework not only\nresolves text chunking challenges in existing RAG systems, providing LLMs with\nsemantically complete document memories, but also paves the way for SLMs to\nachieve human-centric intelligent text processing.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.14252.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "658e85bb5b7553ca5c29ba89",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/658e85bb5b7553ca5c29ba89/KK6UpS9agtrxevvBoup5N.jpeg",
      "fullname": "Jihao Zhao",
      "name": "Robot2050",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.14359",
      "authors": [
        {
          "_id": "68f1a6b16e0bef323a68fd1a",
          "name": "Zichen Wen",
          "hidden": false
        },
        {
          "_id": "68f1a6b16e0bef323a68fd1b",
          "name": "Yiyu Wang",
          "hidden": false
        },
        {
          "_id": "68f1a6b16e0bef323a68fd1c",
          "name": "Chenfei Liao",
          "hidden": false
        },
        {
          "_id": "68f1a6b16e0bef323a68fd1d",
          "name": "Boxue Yang",
          "hidden": false
        },
        {
          "_id": "68f1a6b16e0bef323a68fd1e",
          "name": "Junxian Li",
          "hidden": false
        },
        {
          "_id": "68f1a6b16e0bef323a68fd1f",
          "name": "Weifeng Liu",
          "hidden": false
        },
        {
          "_id": "68f1a6b16e0bef323a68fd20",
          "name": "Haocong He",
          "hidden": false
        },
        {
          "_id": "68f1a6b16e0bef323a68fd21",
          "name": "Bolong Feng",
          "hidden": false
        },
        {
          "_id": "68f1a6b16e0bef323a68fd22",
          "name": "Xuyang Liu",
          "hidden": false
        },
        {
          "_id": "68f1a6b16e0bef323a68fd23",
          "name": "Yuanhuiyi Lyu",
          "hidden": false
        },
        {
          "_id": "68f1a6b16e0bef323a68fd24",
          "name": "Xu Zheng",
          "hidden": false
        },
        {
          "_id": "68f1a6b16e0bef323a68fd25",
          "name": "Xuming Hu",
          "hidden": false
        },
        {
          "_id": "68f1a6b16e0bef323a68fd26",
          "name": "Linfeng Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-16T06:55:28.000Z",
      "submittedOnDailyAt": "2025-10-17T00:46:23.869Z",
      "title": "AI for Service: Proactive Assistance with AI Glasses",
      "submittedOnDailyBy": {
        "_id": "653b8c3e97a4d71d950e2f20",
        "avatarUrl": "/avatars/b68880022e14556d0be58c69615db3be.svg",
        "isPro": false,
        "fullname": "Zichen Wen",
        "user": "zichenwen",
        "type": "user"
      },
      "summary": "In an era where AI is evolving from a passive tool into an active and\nadaptive companion, we introduce AI for Service (AI4Service), a new paradigm\nthat enables proactive and real-time assistance in daily life. Existing AI\nservices remain largely reactive, responding only to explicit user commands. We\nargue that a truly intelligent and helpful assistant should be capable of\nanticipating user needs and taking actions proactively when appropriate. To\nrealize this vision, we propose Alpha-Service, a unified framework that\naddresses two fundamental challenges: Know When to intervene by detecting\nservice opportunities from egocentric video streams, and Know How to provide\nboth generalized and personalized services. Inspired by the von Neumann\ncomputer architecture and based on AI glasses, Alpha-Service consists of five\nkey components: an Input Unit for perception, a Central Processing Unit for\ntask scheduling, an Arithmetic Logic Unit for tool utilization, a Memory Unit\nfor long-term personalization, and an Output Unit for natural human\ninteraction. As an initial exploration, we implement Alpha-Service through a\nmulti-agent system deployed on AI glasses. Case studies, including a real-time\nBlackjack advisor, a museum tour guide, and a shopping fit assistant,\ndemonstrate its ability to seamlessly perceive the environment, infer user\nintent, and provide timely and useful assistance without explicit prompts.",
      "upvotes": 0,
      "discussionId": "68f1a6b16e0bef323a68fd27",
      "ai_summary": "Alpha-Service, a unified framework for proactive AI assistance, uses a multi-agent system on AI glasses to detect service opportunities and provide timely, personalized assistance.",
      "ai_keywords": [
        "AI4Service",
        "Alpha-Service",
        "egocentric video streams",
        "multi-agent system",
        "AI glasses",
        "Input Unit",
        "Central Processing Unit",
        "Arithmetic Logic Unit",
        "Memory Unit",
        "Output Unit",
        "natural human interaction"
      ],
      "organization": {
        "_id": "63e5ef7bf2e9a8f22c515654",
        "name": "SJTU",
        "fullname": "Shanghai Jiao Tong University",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1676013394657-63e5ee22b6a40bf941da0928.png"
      }
    },
    "publishedAt": "2025-10-16T02:55:28.000Z",
    "title": "AI for Service: Proactive Assistance with AI Glasses",
    "summary": "In an era where AI is evolving from a passive tool into an active and\nadaptive companion, we introduce AI for Service (AI4Service), a new paradigm\nthat enables proactive and real-time assistance in daily life. Existing AI\nservices remain largely reactive, responding only to explicit user commands. We\nargue that a truly intelligent and helpful assistant should be capable of\nanticipating user needs and taking actions proactively when appropriate. To\nrealize this vision, we propose Alpha-Service, a unified framework that\naddresses two fundamental challenges: Know When to intervene by detecting\nservice opportunities from egocentric video streams, and Know How to provide\nboth generalized and personalized services. Inspired by the von Neumann\ncomputer architecture and based on AI glasses, Alpha-Service consists of five\nkey components: an Input Unit for perception, a Central Processing Unit for\ntask scheduling, an Arithmetic Logic Unit for tool utilization, a Memory Unit\nfor long-term personalization, and an Output Unit for natural human\ninteraction. As an initial exploration, we implement Alpha-Service through a\nmulti-agent system deployed on AI glasses. Case studies, including a real-time\nBlackjack advisor, a museum tour guide, and a shopping fit assistant,\ndemonstrate its ability to seamlessly perceive the environment, infer user\nintent, and provide timely and useful assistance without explicit prompts.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.14359.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "653b8c3e97a4d71d950e2f20",
      "avatarUrl": "/avatars/b68880022e14556d0be58c69615db3be.svg",
      "fullname": "Zichen Wen",
      "name": "zichenwen",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 10
    },
    "organization": {
      "_id": "63e5ef7bf2e9a8f22c515654",
      "name": "SJTU",
      "fullname": "Shanghai Jiao Tong University",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1676013394657-63e5ee22b6a40bf941da0928.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.14351",
      "authors": [
        {
          "_id": "68f193bd6e0bef323a68fbcd",
          "name": "Perapard Ngokpol",
          "hidden": false
        },
        {
          "_id": "68f193bd6e0bef323a68fbce",
          "name": "Kun Kerdthaisong",
          "hidden": false
        },
        {
          "_id": "68f193bd6e0bef323a68fbcf",
          "name": "Pasin Buakhaw",
          "hidden": false
        },
        {
          "_id": "68f193bd6e0bef323a68fbd0",
          "name": "Pitikorn Khlaisamniang",
          "hidden": false
        },
        {
          "_id": "68f193bd6e0bef323a68fbd1",
          "name": "Supasate Vorathammathorn",
          "hidden": false
        },
        {
          "_id": "68f193bd6e0bef323a68fbd2",
          "name": "Piyalitt Ittichaiwong",
          "hidden": false
        },
        {
          "_id": "68f193bd6e0bef323a68fbd3",
          "name": "Nutchanon Yongsatianchot",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-16T06:39:27.000Z",
      "submittedOnDailyAt": "2025-10-17T00:01:32.280Z",
      "title": "Beyond One World: Benchmarking Super Heros in Role-Playing Across\n  Multiversal Contexts",
      "submittedOnDailyBy": {
        "_id": "63a83c5432ed73936eb8363e",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a83c5432ed73936eb8363e/8iJN61whs49R0bccdL7b5.jpeg",
        "isPro": false,
        "fullname": "kun kerdthaisong",
        "user": "augustus2011",
        "type": "user"
      },
      "summary": "Large language models (LLMs) are increasingly used as role-playing agents,\nyet their capacity to faithfully and consistently portray version-specific\ncharacters -- for example, superheroes across comic and cinematic universes --\nremains underexplored. Superhero canons such as Marvel and DC provide a rich\ntestbed: decades of storytelling yield multiple incarnations of the same\ncharacter with distinct histories, values, and moral codes. To study this\nproblem, we introduce Beyond One World, a benchmark for character-grounded\nroleplay spanning 30 iconic heroes and 90 canon-specific versions. The\nbenchmark comprises two tasks: (i) Canon Events, which probes factual recall of\npivotal life stages, and (ii) Moral Dilemmas, which confronts models with\nethically charged scenarios. We score responses for canonical accuracy and\nreasoning fidelity under a framework that separates internal deliberation\n(\"thinking\") from outward decisions (\"acting\"). We further propose Think-Act\nMatching, a metric that quantifies alignment between reasons and actions and\nserves as a proxy for model trustworthiness. Experiments across reasoning- and\nnon-reasoning-oriented models yield three findings: (1) chain-of-thought\nprompting improves narrative coherence in weaker models but can reduce\ncanonical accuracy in stronger ones; (2) cross-version generalization within a\ncharacter remains a major obstacle; and (3) models often excel at either\nthinking or acting, but rarely both. Beyond One World exposes critical gaps in\nmultiversal consistency and reasoning alignment, offering a challenging\nevaluation for role-playing LLMs.",
      "upvotes": 0,
      "discussionId": "68f193be6e0bef323a68fbd4",
      "githubRepo": "https://github.com/Augustus2011/Beyond_One_World.git",
      "ai_summary": "Beyond One World benchmark evaluates LLMs' ability to consistently portray version-specific superheroes across different canons through factual recall and ethical reasoning tasks.",
      "ai_keywords": [
        "large language models",
        "role-playing agents",
        "character-grounded roleplay",
        "Canon Events",
        "Moral Dilemmas",
        "canonical accuracy",
        "reasoning fidelity",
        "Think-Act Matching",
        "chain-of-thought prompting",
        "cross-version generalization"
      ],
      "githubStars": 0,
      "organization": {
        "_id": "67aaba5d8d478dcb4b2f4281",
        "name": "Character-lab",
        "fullname": "Character-lab"
      }
    },
    "publishedAt": "2025-10-16T02:39:27.000Z",
    "title": "Beyond One World: Benchmarking Super Heros in Role-Playing Across\n  Multiversal Contexts",
    "summary": "Large language models (LLMs) are increasingly used as role-playing agents,\nyet their capacity to faithfully and consistently portray version-specific\ncharacters -- for example, superheroes across comic and cinematic universes --\nremains underexplored. Superhero canons such as Marvel and DC provide a rich\ntestbed: decades of storytelling yield multiple incarnations of the same\ncharacter with distinct histories, values, and moral codes. To study this\nproblem, we introduce Beyond One World, a benchmark for character-grounded\nroleplay spanning 30 iconic heroes and 90 canon-specific versions. The\nbenchmark comprises two tasks: (i) Canon Events, which probes factual recall of\npivotal life stages, and (ii) Moral Dilemmas, which confronts models with\nethically charged scenarios. We score responses for canonical accuracy and\nreasoning fidelity under a framework that separates internal deliberation\n(\"thinking\") from outward decisions (\"acting\"). We further propose Think-Act\nMatching, a metric that quantifies alignment between reasons and actions and\nserves as a proxy for model trustworthiness. Experiments across reasoning- and\nnon-reasoning-oriented models yield three findings: (1) chain-of-thought\nprompting improves narrative coherence in weaker models but can reduce\ncanonical accuracy in stronger ones; (2) cross-version generalization within a\ncharacter remains a major obstacle; and (3) models often excel at either\nthinking or acting, but rarely both. Beyond One World exposes critical gaps in\nmultiversal consistency and reasoning alignment, offering a challenging\nevaluation for role-playing LLMs.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.14351.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63a83c5432ed73936eb8363e",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a83c5432ed73936eb8363e/8iJN61whs49R0bccdL7b5.jpeg",
      "fullname": "kun kerdthaisong",
      "name": "augustus2011",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "organization": {
      "_id": "67aaba5d8d478dcb4b2f4281",
      "name": "Character-lab",
      "fullname": "Character-lab"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.13054",
      "authors": [
        {
          "_id": "68f0817e36f8b025381e1a20",
          "name": "Ankit Goyal",
          "hidden": false
        },
        {
          "_id": "68f0817e36f8b025381e1a21",
          "name": "Hugo Hadfield",
          "hidden": false
        },
        {
          "_id": "68f0817e36f8b025381e1a22",
          "name": "Xuning Yang",
          "hidden": false
        },
        {
          "_id": "68f0817e36f8b025381e1a23",
          "name": "Valts Blukis",
          "hidden": false
        },
        {
          "_id": "68f0817e36f8b025381e1a24",
          "name": "Fabio Ramos",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/666c6f67bc840e6748554a91/0p5wqkJj8rDIMzA2SRj6z.mp4"
      ],
      "publishedAt": "2025-10-15T00:31:10.000Z",
      "submittedOnDailyAt": "2025-10-17T00:51:30.866Z",
      "title": "VLA-0: Building State-of-the-Art VLAs with Zero Modification",
      "submittedOnDailyBy": {
        "_id": "666c6f67bc840e6748554a91",
        "avatarUrl": "/avatars/72121715e14f720e5c1d029b7f00d55d.svg",
        "isPro": false,
        "fullname": "Ankit Goyal",
        "user": "ankgoyal",
        "type": "user"
      },
      "summary": "Vision-Language-Action models (VLAs) hold immense promise for enabling\ngeneralist robot manipulation. However, the best way to build them remains an\nopen question. Current approaches often add complexity, such as modifying the\nexisting vocabulary of a Vision-Language Model (VLM) with action tokens or\nintroducing special action heads. Curiously, the simplest strategy of\nrepresenting actions directly as text has remained largely unexplored. This\nwork introduces VLA-0 to investigate this idea. We find that VLA-0 is not only\neffective; it is surprisingly powerful. With the right design, VLA-0\noutperforms more involved models. On LIBERO, a popular benchmark for evaluating\nVLAs, VLA-0 outperforms all existing methods trained on the same robotic data,\nincluding pi_0.5-KI, OpenVLA-OFT and SmolVLA. Furthermore, without\nlarge-scale robotics-specific training, it outperforms methods trained on\nlarge-scale robotic data, like pi_0.5-KI, pi_0, GR00T-N1 and MolmoAct.\nThese findings also translate to the real world, where VLA-0 outperforms\nSmolVLA, a VLA model pre-trained on large-scale real data. This paper\nsummarizes our unexpected findings and spells out the specific techniques\nrequired to unlock the high performance of this simple yet potent VLA design.\nVisual results, code, and trained models are provided here:\nhttps://vla0.github.io/.",
      "upvotes": 0,
      "discussionId": "68f0817e36f8b025381e1a25",
      "ai_summary": "A simple VLA model, VLA-0, outperforms more complex models on robotic manipulation tasks by representing actions as text without additional modifications or large-scale training.",
      "ai_keywords": [
        "Vision-Language-Action models",
        "Vision-Language Model",
        "action tokens",
        "action heads",
        "LIBERO",
        "$\\pi_0.5$-KI",
        "OpenVLA-OFT",
        "SmolVLA",
        "GR00T-N1",
        "MolmoAct"
      ],
      "organization": {
        "_id": "60262b67268c201cdc8b7d43",
        "name": "nvidia",
        "fullname": "NVIDIA",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"
      }
    },
    "publishedAt": "2025-10-14T20:31:10.000Z",
    "title": "VLA-0: Building State-of-the-Art VLAs with Zero Modification",
    "summary": "Vision-Language-Action models (VLAs) hold immense promise for enabling\ngeneralist robot manipulation. However, the best way to build them remains an\nopen question. Current approaches often add complexity, such as modifying the\nexisting vocabulary of a Vision-Language Model (VLM) with action tokens or\nintroducing special action heads. Curiously, the simplest strategy of\nrepresenting actions directly as text has remained largely unexplored. This\nwork introduces VLA-0 to investigate this idea. We find that VLA-0 is not only\neffective; it is surprisingly powerful. With the right design, VLA-0\noutperforms more involved models. On LIBERO, a popular benchmark for evaluating\nVLAs, VLA-0 outperforms all existing methods trained on the same robotic data,\nincluding pi_0.5-KI, OpenVLA-OFT and SmolVLA. Furthermore, without\nlarge-scale robotics-specific training, it outperforms methods trained on\nlarge-scale robotic data, like pi_0.5-KI, pi_0, GR00T-N1 and MolmoAct.\nThese findings also translate to the real world, where VLA-0 outperforms\nSmolVLA, a VLA model pre-trained on large-scale real data. This paper\nsummarizes our unexpected findings and spells out the specific techniques\nrequired to unlock the high performance of this simple yet potent VLA design.\nVisual results, code, and trained models are provided here:\nhttps://vla0.github.io/.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/666c6f67bc840e6748554a91/0p5wqkJj8rDIMzA2SRj6z.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.13054.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "666c6f67bc840e6748554a91",
      "avatarUrl": "/avatars/72121715e14f720e5c1d029b7f00d55d.svg",
      "fullname": "Ankit Goyal",
      "name": "ankgoyal",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "organization": {
      "_id": "60262b67268c201cdc8b7d43",
      "name": "nvidia",
      "fullname": "NVIDIA",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.09033",
      "authors": [
        {
          "_id": "68f1a6f96e0bef323a68fd64",
          "name": "Chi Seng Cheang",
          "hidden": false
        },
        {
          "_id": "68f1a6f96e0bef323a68fd65",
          "name": "Hou Pong Chan",
          "hidden": false
        },
        {
          "_id": "68f1a6f96e0bef323a68fd66",
          "name": "Wenxuan Zhang",
          "hidden": false
        },
        {
          "_id": "68f1a6f96e0bef323a68fd67",
          "name": "Yang Deng",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-10T06:09:04.000Z",
      "submittedOnDailyAt": "2025-10-17T00:49:36.375Z",
      "title": "Large Language Models Do NOT Really Know What They Don't Know",
      "submittedOnDailyBy": {
        "_id": "604f67ef0fe8ff3ec13d71ef",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/604f67ef0fe8ff3ec13d71ef/KhUwWvZ3OJ9nEee3B-SXO.png",
        "isPro": false,
        "fullname": "Hou Pong (Ken) Chan",
        "user": "kenchan0226",
        "type": "user"
      },
      "summary": "Recent work suggests that large language models (LLMs) encode factuality\nsignals in their internal representations, such as hidden states, attention\nweights, or token probabilities, implying that LLMs may \"know what they don't\nknow\". However, LLMs can also produce factual errors by relying on shortcuts or\nspurious associations. These error are driven by the same training objective\nthat encourage correct predictions, raising the question of whether internal\ncomputations can reliably distinguish between factual and hallucinated outputs.\nIn this work, we conduct a mechanistic analysis of how LLMs internally process\nfactual queries by comparing two types of hallucinations based on their\nreliance on subject information. We find that when hallucinations are\nassociated with subject knowledge, LLMs employ the same internal recall process\nas for correct responses, leading to overlapping and indistinguishable\nhidden-state geometries. In contrast, hallucinations detached from subject\nknowledge produce distinct, clustered representations that make them\ndetectable. These findings reveal a fundamental limitation: LLMs do not encode\ntruthfulness in their internal states but only patterns of knowledge recall,\ndemonstrating that \"LLMs don't really know what they don't know\".",
      "upvotes": 0,
      "discussionId": "68f1a6fa6e0bef323a68fd68",
      "ai_summary": "LLMs process factual queries and hallucinations similarly when associated with subject knowledge, leading to indistinguishable internal representations, but produce distinct representations for hallucinations without subject knowledge.",
      "ai_keywords": [
        "large language models",
        "LLMs",
        "hidden states",
        "attention weights",
        "token probabilities",
        "factual queries",
        "hallucinations",
        "subject information",
        "internal recall process",
        "hidden-state geometries",
        "truthfulness",
        "knowledge recall"
      ],
      "organization": {
        "_id": "6296f43820dc74838613d1ba",
        "name": "SingaporeManagementUniversity",
        "fullname": "Singapore Management University",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1654060072621-6296f3e0b493a00f946220de.png"
      }
    },
    "publishedAt": "2025-10-10T02:09:04.000Z",
    "title": "Large Language Models Do NOT Really Know What They Don't Know",
    "summary": "Recent work suggests that large language models (LLMs) encode factuality\nsignals in their internal representations, such as hidden states, attention\nweights, or token probabilities, implying that LLMs may \"know what they don't\nknow\". However, LLMs can also produce factual errors by relying on shortcuts or\nspurious associations. These error are driven by the same training objective\nthat encourage correct predictions, raising the question of whether internal\ncomputations can reliably distinguish between factual and hallucinated outputs.\nIn this work, we conduct a mechanistic analysis of how LLMs internally process\nfactual queries by comparing two types of hallucinations based on their\nreliance on subject information. We find that when hallucinations are\nassociated with subject knowledge, LLMs employ the same internal recall process\nas for correct responses, leading to overlapping and indistinguishable\nhidden-state geometries. In contrast, hallucinations detached from subject\nknowledge produce distinct, clustered representations that make them\ndetectable. These findings reveal a fundamental limitation: LLMs do not encode\ntruthfulness in their internal states but only patterns of knowledge recall,\ndemonstrating that \"LLMs don't really know what they don't know\".",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.09033.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "604f67ef0fe8ff3ec13d71ef",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/604f67ef0fe8ff3ec13d71ef/KhUwWvZ3OJ9nEee3B-SXO.png",
      "fullname": "Hou Pong (Ken) Chan",
      "name": "kenchan0226",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 13
    },
    "organization": {
      "_id": "6296f43820dc74838613d1ba",
      "name": "SingaporeManagementUniversity",
      "fullname": "Singapore Management University",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1654060072621-6296f3e0b493a00f946220de.png"
    },
    "isAuthorParticipating": false
  }
]