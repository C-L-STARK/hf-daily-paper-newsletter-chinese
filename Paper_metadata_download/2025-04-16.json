[
  {
    "paper": {
      "id": "2504.10481",
      "authors": [
        {
          "_id": "67fdc1b41d1bc292f7b9358e",
          "user": {
            "_id": "64e18e9ec20c27fcc8df384e",
            "avatarUrl": "/avatars/64ef866b9fa385efcefb34ea76b76802.svg",
            "isPro": false,
            "fullname": "Ding Chen",
            "user": "Hush-cd",
            "type": "user"
          },
          "name": "Ding Chen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-15T07:54:22.449Z",
          "hidden": false
        },
        {
          "_id": "67fdc1b41d1bc292f7b9358f",
          "name": "Qingchen Yu",
          "hidden": false
        },
        {
          "_id": "67fdc1b41d1bc292f7b93590",
          "name": "Pengyuan Wang",
          "hidden": false
        },
        {
          "_id": "67fdc1b41d1bc292f7b93591",
          "name": "Wentao Zhang",
          "hidden": false
        },
        {
          "_id": "67fdc1b41d1bc292f7b93592",
          "name": "Bo Tang",
          "hidden": false
        },
        {
          "_id": "67fdc1b41d1bc292f7b93593",
          "name": "Feiyu Xiong",
          "hidden": false
        },
        {
          "_id": "67fdc1b41d1bc292f7b93594",
          "name": "Xinchi Li",
          "hidden": false
        },
        {
          "_id": "67fdc1b41d1bc292f7b93595",
          "name": "Minchuan Yang",
          "hidden": false
        },
        {
          "_id": "67fdc1b41d1bc292f7b93596",
          "name": "Zhiyu Li",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-14T17:59:36.000Z",
      "submittedOnDailyAt": "2025-04-16T00:53:50.942Z",
      "title": "xVerify: Efficient Answer Verifier for Reasoning Model Evaluations",
      "submittedOnDailyBy": {
        "_id": "64e18e9ec20c27fcc8df384e",
        "avatarUrl": "/avatars/64ef866b9fa385efcefb34ea76b76802.svg",
        "isPro": false,
        "fullname": "Ding Chen",
        "user": "Hush-cd",
        "type": "user"
      },
      "summary": "With the release of the o1 model by OpenAI, reasoning models adopting slow\nthinking strategies have gradually emerged. As the responses generated by such\nmodels often include complex reasoning, intermediate steps, and\nself-reflection, existing evaluation methods are often inadequate. They\nstruggle to determine whether the LLM output is truly equivalent to the\nreference answer, and also have difficulty identifying and extracting the final\nanswer from long, complex responses. To address this issue, we propose xVerify,\nan efficient answer verifier for reasoning model evaluations. xVerify\ndemonstrates strong capability in equivalence judgment, enabling it to\neffectively determine whether the answers produced by reasoning models are\nequivalent to reference answers across various types of objective questions. To\ntrain and evaluate xVerify, we construct the VAR dataset by collecting\nquestion-answer pairs generated by multiple LLMs across various datasets,\nleveraging multiple reasoning models and challenging evaluation sets designed\nspecifically for reasoning model assessment. A multi-round annotation process\nis employed to ensure label accuracy. Based on the VAR dataset, we train\nmultiple xVerify models of different scales. In evaluation experiments\nconducted on both the test set and generalization set, all xVerify models\nachieve overall F1 scores and accuracy exceeding 95\\%. Notably, the smallest\nvariant, xVerify-0.5B-I, outperforms all evaluation methods except GPT-4o,\nwhile xVerify-3B-Ib surpasses GPT-4o in overall performance. These results\nvalidate the effectiveness and generalizability of xVerify.",
      "upvotes": 10,
      "discussionId": "67fdc1b51d1bc292f7b935e8",
      "ai_keywords": [
        "reasoning models",
        "o1 model",
        "slow thinking strategies",
        "complex reasoning",
        "intermediate steps",
        "self-reflection",
        "evaluation methods",
        "LLM output",
        "reference answer",
        "final answer",
        "xVerify",
        "equivalence judgment",
        "VAR dataset",
        "multi-round annotation process",
        "F1 scores",
        "xVerify-0.5B-I",
        "xVerify-3B-Ib",
        "GPT-4o"
      ]
    },
    "publishedAt": "2025-04-14T13:59:36.000Z",
    "title": "xVerify: Efficient Answer Verifier for Reasoning Model Evaluations",
    "summary": "With the release of the o1 model by OpenAI, reasoning models adopting slow\nthinking strategies have gradually emerged. As the responses generated by such\nmodels often include complex reasoning, intermediate steps, and\nself-reflection, existing evaluation methods are often inadequate. They\nstruggle to determine whether the LLM output is truly equivalent to the\nreference answer, and also have difficulty identifying and extracting the final\nanswer from long, complex responses. To address this issue, we propose xVerify,\nan efficient answer verifier for reasoning model evaluations. xVerify\ndemonstrates strong capability in equivalence judgment, enabling it to\neffectively determine whether the answers produced by reasoning models are\nequivalent to reference answers across various types of objective questions. To\ntrain and evaluate xVerify, we construct the VAR dataset by collecting\nquestion-answer pairs generated by multiple LLMs across various datasets,\nleveraging multiple reasoning models and challenging evaluation sets designed\nspecifically for reasoning model assessment. A multi-round annotation process\nis employed to ensure label accuracy. Based on the VAR dataset, we train\nmultiple xVerify models of different scales. In evaluation experiments\nconducted on both the test set and generalization set, all xVerify models\nachieve overall F1 scores and accuracy exceeding 95\\%. Notably, the smallest\nvariant, xVerify-0.5B-I, outperforms all evaluation methods except GPT-4o,\nwhile xVerify-3B-Ib surpasses GPT-4o in overall performance. These results\nvalidate the effectiveness and generalizability of xVerify.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.10481.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64e18e9ec20c27fcc8df384e",
      "avatarUrl": "/avatars/64ef866b9fa385efcefb34ea76b76802.svg",
      "fullname": "Ding Chen",
      "name": "Hush-cd",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.10337",
      "authors": [
        {
          "_id": "67fddae99a03686367721718",
          "user": {
            "_id": "6471a24381ded91f253ceb1c",
            "avatarUrl": "/avatars/31d447068fe1fd4200ab5d08ab31eed4.svg",
            "isPro": false,
            "fullname": "Wesley Shi",
            "user": "WesleyShi",
            "type": "user"
          },
          "name": "Wenlei Shi",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-04-15T06:43:40.277Z",
          "hidden": false
        },
        {
          "_id": "67fddae99a03686367721719",
          "name": "Xing Jin",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-14T15:46:33.000Z",
      "submittedOnDailyAt": "2025-04-16T00:52:23.733Z",
      "title": "Heimdall: test-time scaling on the generative verification",
      "submittedOnDailyBy": {
        "_id": "6471a24381ded91f253ceb1c",
        "avatarUrl": "/avatars/31d447068fe1fd4200ab5d08ab31eed4.svg",
        "isPro": false,
        "fullname": "Wesley Shi",
        "user": "WesleyShi",
        "type": "user"
      },
      "summary": "An AI system can create and maintain knowledge only to the extent that it can\nverify that knowledge itself. Recent work on long Chain-of-Thought reasoning\nhas demonstrated great potential of LLMs on solving competitive problems, but\ntheir verification ability remains to be weak and not sufficiently\ninvestigated. In this paper, we propose Heimdall, the long CoT verification LLM\nthat can accurately judge the correctness of solutions. With pure reinforcement\nlearning, we boost the verification accuracy from 62.5% to 94.5% on competitive\nmath problems. By scaling with repeated sampling, the accuracy further\nincreases to 97.5%. Through human evaluation, Heimdall demonstrates impressive\ngeneralization capabilities, successfully detecting most issues in challenging\nmath proofs, the type of which is not included during training. Furthermore, we\npropose Pessimistic Verification to extend the functionality of Heimdall to\nscaling up the problem solving. It calls Heimdall to judge the solutions from a\nsolver model and based on the pessimistic principle, selects the most likely\ncorrect solution with the least uncertainty. Taking\nDeepSeek-R1-Distill-Qwen-32B as the solver model, Pessimistic Verification\nimproves the solution accuracy on AIME2025 from 54.2% to 70.0% with 16x compute\nbudget and to 83.3% with more compute budget. With the stronger solver Gemini\n2.5 Pro, the score reaches 93.0%. Finally, we prototype an automatic knowledge\ndiscovery system, a ternary system where one poses questions, another provides\nsolutions, and the third verifies the solutions. Using the data synthesis work\nNuminaMath for the first two components, Heimdall effectively identifies\nproblematic records within the dataset and reveals that nearly half of the data\nis flawed, which interestingly aligns with the recent ablation studies from\nNuminaMath.",
      "upvotes": 2,
      "discussionId": "67fddaea9a03686367721776",
      "ai_keywords": [
        "Chain-of-Thought reasoning",
        "LLMs (Large Language Models)",
        "Heimdall",
        "long CoT verification",
        "pure reinforcement learning",
        "synthetic math problems",
        "human evaluation",
        "generalization capabilities",
        "Pessimistic Verification",
        "DeepSeek-R1-Distill-Qwen-32B",
        "AIME2025",
        "Gemini 2.5 Pro",
        "solution accuracy",
        "automatic knowledge discovery system",
        "ternary system",
        "NuminaMath",
        "data synthesis",
        "data records",
        "flawed data"
      ]
    },
    "publishedAt": "2025-04-14T11:46:33.000Z",
    "title": "Heimdall: test-time scaling on the generative verification",
    "summary": "An AI system can create and maintain knowledge only to the extent that it can\nverify that knowledge itself. Recent work on long Chain-of-Thought reasoning\nhas demonstrated great potential of LLMs on solving competitive problems, but\ntheir verification ability remains to be weak and not sufficiently\ninvestigated. In this paper, we propose Heimdall, the long CoT verification LLM\nthat can accurately judge the correctness of solutions. With pure reinforcement\nlearning, we boost the verification accuracy from 62.5% to 94.5% on competitive\nmath problems. By scaling with repeated sampling, the accuracy further\nincreases to 97.5%. Through human evaluation, Heimdall demonstrates impressive\ngeneralization capabilities, successfully detecting most issues in challenging\nmath proofs, the type of which is not included during training. Furthermore, we\npropose Pessimistic Verification to extend the functionality of Heimdall to\nscaling up the problem solving. It calls Heimdall to judge the solutions from a\nsolver model and based on the pessimistic principle, selects the most likely\ncorrect solution with the least uncertainty. Taking\nDeepSeek-R1-Distill-Qwen-32B as the solver model, Pessimistic Verification\nimproves the solution accuracy on AIME2025 from 54.2% to 70.0% with 16x compute\nbudget and to 83.3% with more compute budget. With the stronger solver Gemini\n2.5 Pro, the score reaches 93.0%. Finally, we prototype an automatic knowledge\ndiscovery system, a ternary system where one poses questions, another provides\nsolutions, and the third verifies the solutions. Using the data synthesis work\nNuminaMath for the first two components, Heimdall effectively identifies\nproblematic records within the dataset and reveals that nearly half of the data\nis flawed, which interestingly aligns with the recent ablation studies from\nNuminaMath.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.10337.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6471a24381ded91f253ceb1c",
      "avatarUrl": "/avatars/31d447068fe1fd4200ab5d08ab31eed4.svg",
      "fullname": "Wesley Shi",
      "name": "WesleyShi",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.11447",
      "authors": [
        {
          "_id": "67ff1026f8afab940cc23f88",
          "name": "An Zhaol",
          "hidden": false
        },
        {
          "_id": "67ff1026f8afab940cc23f89",
          "name": "Shengyuan Zhang",
          "hidden": false
        },
        {
          "_id": "67ff1026f8afab940cc23f8a",
          "name": "Ling Yang",
          "hidden": false
        },
        {
          "_id": "67ff1026f8afab940cc23f8b",
          "name": "Zejian Li",
          "hidden": false
        },
        {
          "_id": "67ff1026f8afab940cc23f8c",
          "name": "Jiale Wu",
          "hidden": false
        },
        {
          "_id": "67ff1026f8afab940cc23f8d",
          "name": "Haoran Xu",
          "hidden": false
        },
        {
          "_id": "67ff1026f8afab940cc23f8e",
          "name": "AnYang Wei",
          "hidden": false
        },
        {
          "_id": "67ff1026f8afab940cc23f8f",
          "name": "Perry Pengyun GU Lingyun Sun",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-15T17:57:13.000Z",
      "submittedOnDailyAt": "2025-04-16T00:35:02.754Z",
      "title": "Diffusion Distillation With Direct Preference Optimization For Efficient\n  3D LiDAR Scene Completion",
      "submittedOnDailyBy": {
        "_id": "63943c882b9483beb473ec25",
        "avatarUrl": "/avatars/abd2aae43e68c34770159c15a01c8297.svg",
        "isPro": false,
        "fullname": "Shengyuan Zhang",
        "user": "SYZhang0805",
        "type": "user"
      },
      "summary": "The application of diffusion models in 3D LiDAR scene completion is limited\ndue to diffusion's slow sampling speed. Score distillation accelerates\ndiffusion sampling but with performance degradation, while post-training with\ndirect policy optimization (DPO) boosts performance using preference data. This\npaper proposes Distillation-DPO, a novel diffusion distillation framework for\nLiDAR scene completion with preference aligment. First, the student model\ngenerates paired completion scenes with different initial noises. Second, using\nLiDAR scene evaluation metrics as preference, we construct winning and losing\nsample pairs. Such construction is reasonable, since most LiDAR scene metrics\nare informative but non-differentiable to be optimized directly. Third,\nDistillation-DPO optimizes the student model by exploiting the difference in\nscore functions between the teacher and student models on the paired completion\nscenes. Such procedure is repeated until convergence. Extensive experiments\ndemonstrate that, compared to state-of-the-art LiDAR scene completion diffusion\nmodels, Distillation-DPO achieves higher-quality scene completion while\naccelerating the completion speed by more than 5-fold. Our method is the first\nto explore adopting preference learning in distillation to the best of our\nknowledge and provide insights into preference-aligned distillation. Our code\nis public available on https://github.com/happyw1nd/DistillationDPO.",
      "upvotes": 1,
      "discussionId": "67ff1027f8afab940cc23fd4",
      "ai_keywords": [
        "diffusion models",
        "LiDAR scene completion",
        "score distillation",
        "direct policy optimization (DPO)",
        "preference alignment",
        "student model",
        "paired completion scenes",
        "LiDAR scene evaluation metrics",
        "winning and losing sample pairs",
        "score functions",
        "preference learning"
      ]
    },
    "publishedAt": "2025-04-15T13:57:13.000Z",
    "title": "Diffusion Distillation With Direct Preference Optimization For Efficient\n  3D LiDAR Scene Completion",
    "summary": "The application of diffusion models in 3D LiDAR scene completion is limited\ndue to diffusion's slow sampling speed. Score distillation accelerates\ndiffusion sampling but with performance degradation, while post-training with\ndirect policy optimization (DPO) boosts performance using preference data. This\npaper proposes Distillation-DPO, a novel diffusion distillation framework for\nLiDAR scene completion with preference aligment. First, the student model\ngenerates paired completion scenes with different initial noises. Second, using\nLiDAR scene evaluation metrics as preference, we construct winning and losing\nsample pairs. Such construction is reasonable, since most LiDAR scene metrics\nare informative but non-differentiable to be optimized directly. Third,\nDistillation-DPO optimizes the student model by exploiting the difference in\nscore functions between the teacher and student models on the paired completion\nscenes. Such procedure is repeated until convergence. Extensive experiments\ndemonstrate that, compared to state-of-the-art LiDAR scene completion diffusion\nmodels, Distillation-DPO achieves higher-quality scene completion while\naccelerating the completion speed by more than 5-fold. Our method is the first\nto explore adopting preference learning in distillation to the best of our\nknowledge and provide insights into preference-aligned distillation. Our code\nis public available on https://github.com/happyw1nd/DistillationDPO.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.11447.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63943c882b9483beb473ec25",
      "avatarUrl": "/avatars/abd2aae43e68c34770159c15a01c8297.svg",
      "fullname": "Shengyuan Zhang",
      "name": "SYZhang0805",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.10766",
      "authors": [
        {
          "_id": "67ff114a3026f8abc4bf7e43",
          "name": "Ming Li",
          "hidden": false
        },
        {
          "_id": "67ff114a3026f8abc4bf7e44",
          "name": "Yanhong Li",
          "hidden": false
        },
        {
          "_id": "67ff114a3026f8abc4bf7e45",
          "name": "Ziyue Li",
          "hidden": false
        },
        {
          "_id": "67ff114a3026f8abc4bf7e46",
          "name": "Tianyi Zhou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-14T23:53:47.000Z",
      "submittedOnDailyAt": "2025-04-16T00:40:29.697Z",
      "title": "How Instruction and Reasoning Data shape Post-Training: Data Quality\n  through the Lens of Layer-wise Gradients",
      "submittedOnDailyBy": {
        "_id": "647f5af5b0e96764589f3b2a",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/VJ4cDyjp5M3V5WmI5gPIU.jpeg",
        "isPro": false,
        "fullname": "Tianyi Zhou",
        "user": "zhoutianyi",
        "type": "user"
      },
      "summary": "As the post-training of large language models (LLMs) advances from\ninstruction-following to complex reasoning tasks, understanding how different\ndata affect finetuning dynamics remains largely unexplored. In this paper, we\npresent a spectral analysis of layer-wise gradients induced by low/high-quality\ninstruction and reasoning data for LLM post-training. Our analysis reveals that\nwidely-studied metrics for data evaluation, e.g., IFD, InsTag, Difficulty, and\nReward, can be explained and unified by spectral properties computed from\ngradients' singular value decomposition (SVD). Specifically, higher-quality\ndata are usually associated with lower nuclear norms and higher effective\nranks. Notably, effective rank exhibits better robustness and resolution than\nnuclear norm in capturing subtle quality differences. For example, reasoning\ndata achieves substantially higher effective ranks than instruction data,\nimplying richer gradient structures on more complex tasks. Our experiments also\nhighlight that models within the same family share similar gradient patterns\nregardless of their sizes, whereas different model families diverge\nsignificantly. Providing a unified view on the effects of data quality across\ninstruction and reasoning data, this work illuminates the interplay between\ndata quality and training stability, shedding novel insights into developing\nbetter data exploration strategies for post-training.",
      "upvotes": 1,
      "discussionId": "67ff11503026f8abc4bf7fed",
      "githubRepo": "https://github.com/MingLiiii/Gradient_Unified",
      "ai_keywords": [
        "spectral analysis",
        "layer-wise gradients",
        "low/high-quality instruction",
        "reasoning data",
        "IFD",
        "InsTag",
        "Difficulty",
        "Reward",
        "singular value decomposition (SVD)",
        "nuclear norms",
        "effective ranks",
        "gradient structures",
        "training stability"
      ]
    },
    "publishedAt": "2025-04-14T19:53:47.000Z",
    "title": "How Instruction and Reasoning Data shape Post-Training: Data Quality\n  through the Lens of Layer-wise Gradients",
    "summary": "As the post-training of large language models (LLMs) advances from\ninstruction-following to complex reasoning tasks, understanding how different\ndata affect finetuning dynamics remains largely unexplored. In this paper, we\npresent a spectral analysis of layer-wise gradients induced by low/high-quality\ninstruction and reasoning data for LLM post-training. Our analysis reveals that\nwidely-studied metrics for data evaluation, e.g., IFD, InsTag, Difficulty, and\nReward, can be explained and unified by spectral properties computed from\ngradients' singular value decomposition (SVD). Specifically, higher-quality\ndata are usually associated with lower nuclear norms and higher effective\nranks. Notably, effective rank exhibits better robustness and resolution than\nnuclear norm in capturing subtle quality differences. For example, reasoning\ndata achieves substantially higher effective ranks than instruction data,\nimplying richer gradient structures on more complex tasks. Our experiments also\nhighlight that models within the same family share similar gradient patterns\nregardless of their sizes, whereas different model families diverge\nsignificantly. Providing a unified view on the effects of data quality across\ninstruction and reasoning data, this work illuminates the interplay between\ndata quality and training stability, shedding novel insights into developing\nbetter data exploration strategies for post-training.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.10766.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "647f5af5b0e96764589f3b2a",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/VJ4cDyjp5M3V5WmI5gPIU.jpeg",
      "fullname": "Tianyi Zhou",
      "name": "zhoutianyi",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 14
    },
    "isAuthorParticipating": false
  }
]