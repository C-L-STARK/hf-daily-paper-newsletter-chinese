[
  {
    "paper": {
      "id": "2507.01949",
      "authors": [
        {
          "_id": "6865e6218c83dab5f72d1e47",
          "name": "Kwai Keye Team",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e48",
          "name": "Biao Yang",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e49",
          "name": "Bin Wen",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e4a",
          "name": "Changyi Liu",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e4b",
          "name": "Chenglong Chu",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e4c",
          "name": "Chengru Song",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e4d",
          "name": "Chongling Rao",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e4e",
          "name": "Chuan Yi",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e4f",
          "name": "Da Li",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e50",
          "name": "Dunju Zang",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e51",
          "name": "Fan Yang",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e52",
          "name": "Guorui Zhou",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e53",
          "name": "Hao Peng",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e54",
          "name": "Haojie Ding",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e55",
          "name": "Jiaming Huang",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e56",
          "name": "Jiangxia Cao",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e57",
          "name": "Jiankang Chen",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e58",
          "name": "Jingyun Hua",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e59",
          "name": "Jin Ouyang",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e5a",
          "name": "Kaibing Chen",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e5b",
          "name": "Kaiyu Jiang",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e5c",
          "name": "Kaiyu Tang",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e5d",
          "name": "Kun Gai",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e5e",
          "name": "Shengnan Zhang",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e5f",
          "name": "Siyang Mao",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e60",
          "name": "Sui Huang",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e61",
          "name": "Tianke Zhang",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e62",
          "name": "Tingting Gao",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e63",
          "name": "Wei Chen",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e64",
          "name": "Wei Yuan",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e65",
          "name": "Xiangyu Wu",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e66",
          "name": "Xiao Hu",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e67",
          "name": "Xingyu Lu",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e68",
          "name": "Yang Zhou",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e69",
          "name": "Yi-Fan Zhang",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e6a",
          "name": "Yiping Yang",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e6b",
          "name": "Yulong Chen",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e6c",
          "name": "Zhenhua Wu",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e6d",
          "name": "Zhenyu Li",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e6e",
          "name": "Zhixin Ling",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e6f",
          "name": "Ziming Li",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e70",
          "name": "Dehua Ma",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e71",
          "name": "Di Xu",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e72",
          "name": "Haixuan Gao",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e73",
          "name": "Hang Li",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e74",
          "name": "Jiawei Guo",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e75",
          "name": "Jing Wang",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e76",
          "name": "Lejian Ren",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e77",
          "name": "Muhao Wei",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e78",
          "name": "Qianqian Wang",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e79",
          "name": "Qigen Hu",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e7a",
          "name": "Shiyao Wang",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e7b",
          "name": "Tao Yu",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e7c",
          "name": "Xinchen Luo",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e7d",
          "name": "Yan Li",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e7e",
          "name": "Yiming Liang",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e7f",
          "name": "Yuhang Hu",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e80",
          "name": "Zeyi Lu",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e81",
          "name": "Zhuoran Yang",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e82",
          "name": "Zixing Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-02T17:57:28.000Z",
      "submittedOnDailyAt": "2025-07-03T00:40:58.759Z",
      "title": "Kwai Keye-VL Technical Report",
      "submittedOnDailyBy": {
        "_id": "623d8ca4c29adf5ef6175615",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/623d8ca4c29adf5ef6175615/q7lHao7UPwU1u7YLSP56m.jpeg",
        "isPro": false,
        "fullname": "Yi-Fan Zhang",
        "user": "yifanzhang114",
        "type": "user"
      },
      "summary": "While Multimodal Large Language Models (MLLMs) demonstrate remarkable\ncapabilities on static images, they often fall short in comprehending dynamic,\ninformation-dense short-form videos, a dominant medium in today's digital\nlandscape. To bridge this gap, we introduce Kwai Keye-VL, an\n8-billion-parameter multimodal foundation model engineered for leading-edge\nperformance in short-video understanding while maintaining robust\ngeneral-purpose vision-language abilities. The development of Keye-VL rests on\ntwo core pillars: a massive, high-quality dataset exceeding 600 billion tokens\nwith a strong emphasis on video, and an innovative training recipe. This recipe\nfeatures a four-stage pre-training process for solid vision-language alignment,\nfollowed by a meticulous two-phase post-training process. The first\npost-training stage enhances foundational capabilities like instruction\nfollowing, while the second phase focuses on stimulating advanced reasoning. In\nthis second phase, a key innovation is our five-mode ``cold-start'' data\nmixture, which includes ``thinking'', ``non-thinking'', ``auto-think'', ``think\nwith image'', and high-quality video data. This mixture teaches the model to\ndecide when and how to reason. Subsequent reinforcement learning (RL) and\nalignment steps further enhance these reasoning capabilities and correct\nabnormal model behaviors, such as repetitive outputs. To validate our approach,\nwe conduct extensive evaluations, showing that Keye-VL achieves\nstate-of-the-art results on public video benchmarks and remains highly\ncompetitive on general image-based tasks (Figure 1). Furthermore, we develop\nand release the KC-MMBench, a new benchmark tailored for real-world\nshort-video scenarios, where Keye-VL shows a significant advantage.",
      "upvotes": 14,
      "discussionId": "6865e6218c83dab5f72d1e83",
      "projectPage": "https://kwai-keye.github.io/",
      "githubRepo": "https://github.com/Kwai-Keye/Keye"
    },
    "publishedAt": "2025-07-02T13:57:28.000Z",
    "title": "Kwai Keye-VL Technical Report",
    "summary": "While Multimodal Large Language Models (MLLMs) demonstrate remarkable\ncapabilities on static images, they often fall short in comprehending dynamic,\ninformation-dense short-form videos, a dominant medium in today's digital\nlandscape. To bridge this gap, we introduce Kwai Keye-VL, an\n8-billion-parameter multimodal foundation model engineered for leading-edge\nperformance in short-video understanding while maintaining robust\ngeneral-purpose vision-language abilities. The development of Keye-VL rests on\ntwo core pillars: a massive, high-quality dataset exceeding 600 billion tokens\nwith a strong emphasis on video, and an innovative training recipe. This recipe\nfeatures a four-stage pre-training process for solid vision-language alignment,\nfollowed by a meticulous two-phase post-training process. The first\npost-training stage enhances foundational capabilities like instruction\nfollowing, while the second phase focuses on stimulating advanced reasoning. In\nthis second phase, a key innovation is our five-mode ``cold-start'' data\nmixture, which includes ``thinking'', ``non-thinking'', ``auto-think'', ``think\nwith image'', and high-quality video data. This mixture teaches the model to\ndecide when and how to reason. Subsequent reinforcement learning (RL) and\nalignment steps further enhance these reasoning capabilities and correct\nabnormal model behaviors, such as repetitive outputs. To validate our approach,\nwe conduct extensive evaluations, showing that Keye-VL achieves\nstate-of-the-art results on public video benchmarks and remains highly\ncompetitive on general image-based tasks (Figure 1). Furthermore, we develop\nand release the KC-MMBench, a new benchmark tailored for real-world\nshort-video scenarios, where Keye-VL shows a significant advantage.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.01949.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "623d8ca4c29adf5ef6175615",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/623d8ca4c29adf5ef6175615/q7lHao7UPwU1u7YLSP56m.jpeg",
      "fullname": "Yi-Fan Zhang",
      "name": "yifanzhang114",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 10
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.01634",
      "authors": [
        {
          "_id": "6865e04b8c83dab5f72d1e2d",
          "name": "Boyuan Sun",
          "hidden": false
        },
        {
          "_id": "6865e04b8c83dab5f72d1e2e",
          "name": "Modi Jin",
          "hidden": false
        },
        {
          "_id": "6865e04b8c83dab5f72d1e2f",
          "name": "Bowen Yin",
          "hidden": false
        },
        {
          "_id": "6865e04b8c83dab5f72d1e30",
          "name": "Qibin Hou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-02T12:05:57.000Z",
      "submittedOnDailyAt": "2025-07-03T00:16:17.577Z",
      "title": "Depth Anything at Any Condition",
      "submittedOnDailyBy": {
        "_id": "66ef2611fcc1c455f8dce832",
        "avatarUrl": "/avatars/c73ef2dfcd1e6ec8414a31226ad38e3b.svg",
        "isPro": false,
        "fullname": "Boyuan Sun",
        "user": "BBBBCHAN",
        "type": "user"
      },
      "summary": "We present Depth Anything at Any Condition (DepthAnything-AC), a foundation\nmonocular depth estimation (MDE) model capable of handling diverse\nenvironmental conditions. Previous foundation MDE models achieve impressive\nperformance across general scenes but not perform well in complex open-world\nenvironments that involve challenging conditions, such as illumination\nvariations, adverse weather, and sensor-induced distortions. To overcome the\nchallenges of data scarcity and the inability of generating high-quality\npseudo-labels from corrupted images, we propose an unsupervised consistency\nregularization finetuning paradigm that requires only a relatively small amount\nof unlabeled data. Furthermore, we propose the Spatial Distance Constraint to\nexplicitly enforce the model to learn patch-level relative relationships,\nresulting in clearer semantic boundaries and more accurate details.\nExperimental results demonstrate the zero-shot capabilities of DepthAnything-AC\nacross diverse benchmarks, including real-world adverse weather benchmarks,\nsynthetic corruption benchmarks, and general benchmarks.\n  Project Page: https://ghost233lism.github.io/depthanything-AC-page\n  Code: https://github.com/HVision-NKU/DepthAnythingAC",
      "upvotes": 11,
      "discussionId": "6865e04b8c83dab5f72d1e31",
      "projectPage": "https://ghost233lism.github.io/depthanything-AC-page/",
      "githubRepo": "https://github.com/HVision-NKU/DepthAnythingAC",
      "githubStars": 32
    },
    "publishedAt": "2025-07-02T08:05:57.000Z",
    "title": "Depth Anything at Any Condition",
    "summary": "We present Depth Anything at Any Condition (DepthAnything-AC), a foundation\nmonocular depth estimation (MDE) model capable of handling diverse\nenvironmental conditions. Previous foundation MDE models achieve impressive\nperformance across general scenes but not perform well in complex open-world\nenvironments that involve challenging conditions, such as illumination\nvariations, adverse weather, and sensor-induced distortions. To overcome the\nchallenges of data scarcity and the inability of generating high-quality\npseudo-labels from corrupted images, we propose an unsupervised consistency\nregularization finetuning paradigm that requires only a relatively small amount\nof unlabeled data. Furthermore, we propose the Spatial Distance Constraint to\nexplicitly enforce the model to learn patch-level relative relationships,\nresulting in clearer semantic boundaries and more accurate details.\nExperimental results demonstrate the zero-shot capabilities of DepthAnything-AC\nacross diverse benchmarks, including real-world adverse weather benchmarks,\nsynthetic corruption benchmarks, and general benchmarks.\n  Project Page: https://ghost233lism.github.io/depthanything-AC-page\n  Code: https://github.com/HVision-NKU/DepthAnythingAC",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.01634.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66ef2611fcc1c455f8dce832",
      "avatarUrl": "/avatars/c73ef2dfcd1e6ec8414a31226ad38e3b.svg",
      "fullname": "Boyuan Sun",
      "name": "BBBBCHAN",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.01945",
      "authors": [
        {
          "_id": "6865e4b88c83dab5f72d1e41",
          "name": "Nan Chen",
          "hidden": false
        },
        {
          "_id": "6865e4b88c83dab5f72d1e42",
          "name": "Mengqi Huang",
          "hidden": false
        },
        {
          "_id": "6865e4b88c83dab5f72d1e43",
          "name": "Yihao Meng",
          "hidden": false
        },
        {
          "_id": "6865e4b88c83dab5f72d1e44",
          "name": "Zhendong Mao",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6629d7c9fa14eaccf07d8633/0WDfsnDIDJ9hzI6iJym9L.mp4"
      ],
      "publishedAt": "2025-07-02T17:55:50.000Z",
      "submittedOnDailyAt": "2025-07-03T00:57:17.831Z",
      "title": "LongAnimation: Long Animation Generation with Dynamic Global-Local\n  Memory",
      "submittedOnDailyBy": {
        "_id": "6629d7c9fa14eaccf07d8633",
        "avatarUrl": "/avatars/dceb2f6c804c583adf15a3536c8c995b.svg",
        "isPro": false,
        "fullname": "Nan Chen",
        "user": "CNcreator0331",
        "type": "user"
      },
      "summary": "Animation colorization is a crucial part of real animation industry\nproduction. Long animation colorization has high labor costs. Therefore,\nautomated long animation colorization based on the video generation model has\nsignificant research value. Existing studies are limited to short-term\ncolorization. These studies adopt a local paradigm, fusing overlapping features\nto achieve smooth transitions between local segments. However, the local\nparadigm neglects global information, failing to maintain long-term color\nconsistency. In this study, we argue that ideal long-term color consistency can\nbe achieved through a dynamic global-local paradigm, i.e., dynamically\nextracting global color-consistent features relevant to the current generation.\nSpecifically, we propose LongAnimation, a novel framework, which mainly\nincludes a SketchDiT, a Dynamic Global-Local Memory (DGLM), and a Color\nConsistency Reward. The SketchDiT captures hybrid reference features to support\nthe DGLM module. The DGLM module employs a long video understanding model to\ndynamically compress global historical features and adaptively fuse them with\nthe current generation features. To refine the color consistency, we introduce\na Color Consistency Reward. During inference, we propose a color consistency\nfusion to smooth the video segment transition. Extensive experiments on both\nshort-term (14 frames) and long-term (average 500 frames) animations show the\neffectiveness of LongAnimation in maintaining short-term and long-term color\nconsistency for open-domain animation colorization task. The code can be found\nat https://cn-makers.github.io/long_animation_web/.",
      "upvotes": 7,
      "discussionId": "6865e4b88c83dab5f72d1e45",
      "projectPage": "https://cn-makers.github.io/long_animation_web/",
      "githubRepo": "https://github.com/CN-makers/LongAnimation"
    },
    "publishedAt": "2025-07-02T13:55:50.000Z",
    "title": "LongAnimation: Long Animation Generation with Dynamic Global-Local\n  Memory",
    "summary": "Animation colorization is a crucial part of real animation industry\nproduction. Long animation colorization has high labor costs. Therefore,\nautomated long animation colorization based on the video generation model has\nsignificant research value. Existing studies are limited to short-term\ncolorization. These studies adopt a local paradigm, fusing overlapping features\nto achieve smooth transitions between local segments. However, the local\nparadigm neglects global information, failing to maintain long-term color\nconsistency. In this study, we argue that ideal long-term color consistency can\nbe achieved through a dynamic global-local paradigm, i.e., dynamically\nextracting global color-consistent features relevant to the current generation.\nSpecifically, we propose LongAnimation, a novel framework, which mainly\nincludes a SketchDiT, a Dynamic Global-Local Memory (DGLM), and a Color\nConsistency Reward. The SketchDiT captures hybrid reference features to support\nthe DGLM module. The DGLM module employs a long video understanding model to\ndynamically compress global historical features and adaptively fuse them with\nthe current generation features. To refine the color consistency, we introduce\na Color Consistency Reward. During inference, we propose a color consistency\nfusion to smooth the video segment transition. Extensive experiments on both\nshort-term (14 frames) and long-term (average 500 frames) animations show the\neffectiveness of LongAnimation in maintaining short-term and long-term color\nconsistency for open-domain animation colorization task. The code can be found\nat https://cn-makers.github.io/long_animation_web/.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6629d7c9fa14eaccf07d8633/0WDfsnDIDJ9hzI6iJym9L.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.01945.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6629d7c9fa14eaccf07d8633",
      "avatarUrl": "/avatars/dceb2f6c804c583adf15a3536c8c995b.svg",
      "fullname": "Nan Chen",
      "name": "CNcreator0331",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.23552",
      "authors": [
        {
          "_id": "6865e0148c83dab5f72d1e26",
          "name": "Mingi Kwon",
          "hidden": false
        },
        {
          "_id": "6865e0148c83dab5f72d1e27",
          "name": "Joonghyuk Shin",
          "hidden": false
        },
        {
          "_id": "6865e0148c83dab5f72d1e28",
          "name": "Jaeseok Jung",
          "hidden": false
        },
        {
          "_id": "6865e0148c83dab5f72d1e29",
          "name": "Jaesik Park",
          "hidden": false
        },
        {
          "_id": "6865e0148c83dab5f72d1e2a",
          "name": "Youngjung Uh",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-30T06:51:40.000Z",
      "submittedOnDailyAt": "2025-07-03T00:15:16.476Z",
      "title": "JAM-Flow: Joint Audio-Motion Synthesis with Flow Matching",
      "submittedOnDailyBy": {
        "_id": "631074d895c34b95407945f0",
        "avatarUrl": "/avatars/699baf06ec818650dec5752aca87c5b4.svg",
        "isPro": false,
        "fullname": "Joonghyuk Shin",
        "user": "alex4727",
        "type": "user"
      },
      "summary": "The intrinsic link between facial motion and speech is often overlooked in\ngenerative modeling, where talking head synthesis and text-to-speech (TTS) are\ntypically addressed as separate tasks. This paper introduces JAM-Flow, a\nunified framework to simultaneously synthesize and condition on both facial\nmotion and speech. Our approach leverages flow matching and a novel Multi-Modal\nDiffusion Transformer (MM-DiT) architecture, integrating specialized Motion-DiT\nand Audio-DiT modules. These are coupled via selective joint attention layers\nand incorporate key architectural choices, such as temporally aligned\npositional embeddings and localized joint attention masking, to enable\neffective cross-modal interaction while preserving modality-specific strengths.\nTrained with an inpainting-style objective, JAM-Flow supports a wide array of\nconditioning inputs-including text, reference audio, and reference\nmotion-facilitating tasks such as synchronized talking head generation from\ntext, audio-driven animation, and much more, within a single, coherent model.\nJAM-Flow significantly advances multi-modal generative modeling by providing a\npractical solution for holistic audio-visual synthesis. project page:\nhttps://joonghyuk.com/jamflow-web",
      "upvotes": 2,
      "discussionId": "6865e0148c83dab5f72d1e2b"
    },
    "publishedAt": "2025-06-30T02:51:40.000Z",
    "title": "JAM-Flow: Joint Audio-Motion Synthesis with Flow Matching",
    "summary": "The intrinsic link between facial motion and speech is often overlooked in\ngenerative modeling, where talking head synthesis and text-to-speech (TTS) are\ntypically addressed as separate tasks. This paper introduces JAM-Flow, a\nunified framework to simultaneously synthesize and condition on both facial\nmotion and speech. Our approach leverages flow matching and a novel Multi-Modal\nDiffusion Transformer (MM-DiT) architecture, integrating specialized Motion-DiT\nand Audio-DiT modules. These are coupled via selective joint attention layers\nand incorporate key architectural choices, such as temporally aligned\npositional embeddings and localized joint attention masking, to enable\neffective cross-modal interaction while preserving modality-specific strengths.\nTrained with an inpainting-style objective, JAM-Flow supports a wide array of\nconditioning inputs-including text, reference audio, and reference\nmotion-facilitating tasks such as synchronized talking head generation from\ntext, audio-driven animation, and much more, within a single, coherent model.\nJAM-Flow significantly advances multi-modal generative modeling by providing a\npractical solution for holistic audio-visual synthesis. project page:\nhttps://joonghyuk.com/jamflow-web",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.23552.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "631074d895c34b95407945f0",
      "avatarUrl": "/avatars/699baf06ec818650dec5752aca87c5b4.svg",
      "fullname": "Joonghyuk Shin",
      "name": "alex4727",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  }
]