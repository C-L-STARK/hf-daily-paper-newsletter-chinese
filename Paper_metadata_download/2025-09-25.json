[
  {
    "paper": {
      "id": "2509.19760",
      "authors": [
        {
          "_id": "68d4a3f236950a9dff156891",
          "name": "Xiangyang Chen",
          "hidden": false
        },
        {
          "_id": "68d4a3f236950a9dff156892",
          "name": "Shuzhao Li",
          "hidden": false
        },
        {
          "_id": "68d4a3f236950a9dff156893",
          "name": "Xiuwen Zhu",
          "hidden": false
        },
        {
          "_id": "68d4a3f236950a9dff156894",
          "name": "Yongfan Chen",
          "hidden": false
        },
        {
          "_id": "68d4a3f236950a9dff156895",
          "name": "Fan Yang",
          "hidden": false
        },
        {
          "_id": "68d4a3f236950a9dff156896",
          "name": "Cheng Fang",
          "hidden": false
        },
        {
          "_id": "68d4a3f236950a9dff156897",
          "name": "Lin Qu",
          "hidden": false
        },
        {
          "_id": "68d4a3f236950a9dff156898",
          "name": "Xiaoxiao Xu",
          "hidden": false
        },
        {
          "_id": "68d4a3f236950a9dff156899",
          "name": "Hu Wei",
          "hidden": false
        },
        {
          "_id": "68d4a3f236950a9dff15689a",
          "name": "Minggang Wu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-09-24T04:54:37.000Z",
      "submittedOnDailyAt": "2025-09-25T00:38:03.982Z",
      "title": "Logics-Parsing Technical Report",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Recent advances in Large Vision-Language models (LVLM) have spurred\nsignificant progress in document parsing task. Compared to traditional\npipeline-based methods, end-to-end paradigms have shown their excellence in\nconverting PDF images into structured outputs through integrated Optical\nCharacter Recognition (OCR), table recognition, mathematical formula\nrecognition and so on. However, the absence of explicit analytical stages for\ndocument layouts and reading orders limits the LVLM's capability in handling\ncomplex document types such as multi-column newspapers or posters. To address\nthis limitation, we propose in this report Logics-Parsing: an end-to-end\nLVLM-based model augmented with reinforcement learning. Our model incorporates\nmeticulously designed reward mechanisms to optimize complex layout analysis and\nreading order inference. In addition, we expand the model's versatility by\nincorporating diverse data types such as chemical formulas and handwritten\nChinese characters into supervised fine-tuning. Finally, to enable rigorous\nevaluation of our approach, we introduce LogicsParsingBench, a curated set of\n1,078 page-level PDF images spanning nine major categories and over twenty\nsub-categories, which will be released later. Comprehensive experiments\nconducted on LogicsParsingBench have validated the efficacy and\nState-of-the-art (SOTA) performance of our proposed model across diverse\ndocument analysis scenarios. Project Page:\nhttps://github.com/alibaba/Logics-Parsing",
      "upvotes": 2,
      "discussionId": "68d4a3f336950a9dff15689b",
      "githubRepo": "https://github.com/alibaba/Logics-Parsing",
      "ai_summary": "Logics-Parsing, an end-to-end LVLM model enhanced with reinforcement learning, improves document parsing by optimizing layout analysis and reading order inference, achieving state-of-the-art performance on a diverse benchmark.",
      "ai_keywords": [
        "Large Vision-Language models",
        "LVLM",
        "end-to-end paradigms",
        "Optical Character Recognition",
        "OCR",
        "table recognition",
        "mathematical formula recognition",
        "reinforcement learning",
        "reward mechanisms",
        "layout analysis",
        "reading order inference",
        "supervised fine-tuning",
        "chemical formulas",
        "handwritten Chinese characters",
        "LogicsParsingBench"
      ],
      "githubStars": 14
    },
    "publishedAt": "2025-09-24T00:54:37.000Z",
    "title": "Logics-Parsing Technical Report",
    "summary": "Recent advances in Large Vision-Language models (LVLM) have spurred\nsignificant progress in document parsing task. Compared to traditional\npipeline-based methods, end-to-end paradigms have shown their excellence in\nconverting PDF images into structured outputs through integrated Optical\nCharacter Recognition (OCR), table recognition, mathematical formula\nrecognition and so on. However, the absence of explicit analytical stages for\ndocument layouts and reading orders limits the LVLM's capability in handling\ncomplex document types such as multi-column newspapers or posters. To address\nthis limitation, we propose in this report Logics-Parsing: an end-to-end\nLVLM-based model augmented with reinforcement learning. Our model incorporates\nmeticulously designed reward mechanisms to optimize complex layout analysis and\nreading order inference. In addition, we expand the model's versatility by\nincorporating diverse data types such as chemical formulas and handwritten\nChinese characters into supervised fine-tuning. Finally, to enable rigorous\nevaluation of our approach, we introduce LogicsParsingBench, a curated set of\n1,078 page-level PDF images spanning nine major categories and over twenty\nsub-categories, which will be released later. Comprehensive experiments\nconducted on LogicsParsingBench have validated the efficacy and\nState-of-the-art (SOTA) performance of our proposed model across diverse\ndocument analysis scenarios. Project Page:\nhttps://github.com/alibaba/Logics-Parsing",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.19760.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 109
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2509.20360",
      "authors": [
        {
          "_id": "68d49f9f36950a9dff156865",
          "name": "Xuan Ju",
          "hidden": false
        },
        {
          "_id": "68d49f9f36950a9dff156866",
          "name": "Tianyu Wang",
          "hidden": false
        },
        {
          "_id": "68d49f9f36950a9dff156867",
          "name": "Yuqian Zhou",
          "hidden": false
        },
        {
          "_id": "68d49f9f36950a9dff156868",
          "name": "He Zhang",
          "hidden": false
        },
        {
          "_id": "68d49f9f36950a9dff156869",
          "name": "Qing Liu",
          "hidden": false
        },
        {
          "_id": "68d49f9f36950a9dff15686a",
          "name": "Nanxuan Zhao",
          "hidden": false
        },
        {
          "_id": "68d49f9f36950a9dff15686b",
          "name": "Zhifei Zhang",
          "hidden": false
        },
        {
          "_id": "68d49f9f36950a9dff15686c",
          "name": "Yijun Li",
          "hidden": false
        },
        {
          "_id": "68d49f9f36950a9dff15686d",
          "name": "Yuanhao Cai",
          "hidden": false
        },
        {
          "_id": "68d49f9f36950a9dff15686e",
          "name": "Shaoteng Liu",
          "hidden": false
        },
        {
          "_id": "68d49f9f36950a9dff15686f",
          "name": "Daniil Pakhomov",
          "hidden": false
        },
        {
          "_id": "68d49f9f36950a9dff156870",
          "name": "Zhe Lin",
          "hidden": false
        },
        {
          "_id": "68d49f9f36950a9dff156871",
          "name": "Soo Ye Kim",
          "hidden": false
        },
        {
          "_id": "68d49f9f36950a9dff156872",
          "name": "Qiang Xu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-09-24T17:59:30.000Z",
      "submittedOnDailyAt": "2025-09-25T00:19:27.642Z",
      "title": "EditVerse: Unifying Image and Video Editing and Generation with\n  In-Context Learning",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Recent advances in foundation models highlight a clear trend toward\nunification and scaling, showing emergent capabilities across diverse domains.\nWhile image generation and editing have rapidly transitioned from task-specific\nto unified frameworks, video generation and editing remain fragmented due to\narchitectural limitations and data scarcity. In this work, we introduce\nEditVerse, a unified framework for image and video generation and editing\nwithin a single model. By representing all modalities, i.e., text, image, and\nvideo, as a unified token sequence, EditVerse leverages self-attention to\nachieve robust in-context learning, natural cross-modal knowledge transfer, and\nflexible handling of inputs and outputs with arbitrary resolutions and\ndurations. To address the lack of video editing training data, we design a\nscalable data pipeline that curates 232K video editing samples and combines\nthem with large-scale image and video datasets for joint training. Furthermore,\nwe present EditVerseBench, the first benchmark for instruction-based video\nediting covering diverse tasks and resolutions. Extensive experiments and user\nstudies demonstrate that EditVerse achieves state-of-the-art performance,\nsurpassing existing open-source and commercial models, while exhibiting\nemergent editing and generation abilities across modalities.",
      "upvotes": 1,
      "discussionId": "68d49f9f36950a9dff156873",
      "ai_summary": "EditVerse is a unified framework using self-attention for image and video generation and editing, achieving state-of-the-art performance with a scalable data pipeline and benchmark.",
      "ai_keywords": [
        "self-attention",
        "in-context learning",
        "cross-modal knowledge transfer",
        "EditVerse",
        "EditVerseBench",
        "instruction-based video editing"
      ]
    },
    "publishedAt": "2025-09-24T13:59:30.000Z",
    "title": "EditVerse: Unifying Image and Video Editing and Generation with\n  In-Context Learning",
    "summary": "Recent advances in foundation models highlight a clear trend toward\nunification and scaling, showing emergent capabilities across diverse domains.\nWhile image generation and editing have rapidly transitioned from task-specific\nto unified frameworks, video generation and editing remain fragmented due to\narchitectural limitations and data scarcity. In this work, we introduce\nEditVerse, a unified framework for image and video generation and editing\nwithin a single model. By representing all modalities, i.e., text, image, and\nvideo, as a unified token sequence, EditVerse leverages self-attention to\nachieve robust in-context learning, natural cross-modal knowledge transfer, and\nflexible handling of inputs and outputs with arbitrary resolutions and\ndurations. To address the lack of video editing training data, we design a\nscalable data pipeline that curates 232K video editing samples and combines\nthem with large-scale image and video datasets for joint training. Furthermore,\nwe present EditVerseBench, the first benchmark for instruction-based video\nediting covering diverse tasks and resolutions. Extensive experiments and user\nstudies demonstrate that EditVerse achieves state-of-the-art performance,\nsurpassing existing open-source and commercial models, while exhibiting\nemergent editing and generation abilities across modalities.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.20360.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 109
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2509.20358",
      "authors": [
        {
          "_id": "68d4a00136950a9dff156875",
          "name": "Chen Wang",
          "hidden": false
        },
        {
          "_id": "68d4a00136950a9dff156876",
          "name": "Chuhao Chen",
          "hidden": false
        },
        {
          "_id": "68d4a00136950a9dff156877",
          "name": "Yiming Huang",
          "hidden": false
        },
        {
          "_id": "68d4a00136950a9dff156878",
          "name": "Zhiyang Dou",
          "hidden": false
        },
        {
          "_id": "68d4a00136950a9dff156879",
          "name": "Yuan Liu",
          "hidden": false
        },
        {
          "_id": "68d4a00136950a9dff15687a",
          "name": "Jiatao Gu",
          "hidden": false
        },
        {
          "_id": "68d4a00136950a9dff15687b",
          "name": "Lingjie Liu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-09-24T17:58:04.000Z",
      "submittedOnDailyAt": "2025-09-25T00:21:11.761Z",
      "title": "PhysCtrl: Generative Physics for Controllable and Physics-Grounded Video\n  Generation",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Existing video generation models excel at producing photo-realistic videos\nfrom text or images, but often lack physical plausibility and 3D\ncontrollability. To overcome these limitations, we introduce PhysCtrl, a novel\nframework for physics-grounded image-to-video generation with physical\nparameters and force control. At its core is a generative physics network that\nlearns the distribution of physical dynamics across four materials (elastic,\nsand, plasticine, and rigid) via a diffusion model conditioned on physics\nparameters and applied forces. We represent physical dynamics as 3D point\ntrajectories and train on a large-scale synthetic dataset of 550K animations\ngenerated by physics simulators. We enhance the diffusion model with a novel\nspatiotemporal attention block that emulates particle interactions and\nincorporates physics-based constraints during training to enforce physical\nplausibility. Experiments show that PhysCtrl generates realistic,\nphysics-grounded motion trajectories which, when used to drive image-to-video\nmodels, yield high-fidelity, controllable videos that outperform existing\nmethods in both visual quality and physical plausibility. Project Page:\nhttps://cwchenwang.github.io/physctrl",
      "upvotes": 1,
      "discussionId": "68d4a00136950a9dff15687c",
      "projectPage": "https://cwchenwang.github.io/physctrl/",
      "ai_summary": "PhysCtrl is a physics-grounded framework for generating realistic, controllable videos from images using a diffusion model with spatiotemporal attention and physics-based constraints.",
      "ai_keywords": [
        "generative physics network",
        "diffusion model",
        "physical parameters",
        "force control",
        "3D point trajectories",
        "spatiotemporal attention block",
        "physics-based constraints",
        "physical plausibility",
        "image-to-video models",
        "high-fidelity videos"
      ]
    },
    "publishedAt": "2025-09-24T13:58:04.000Z",
    "title": "PhysCtrl: Generative Physics for Controllable and Physics-Grounded Video\n  Generation",
    "summary": "Existing video generation models excel at producing photo-realistic videos\nfrom text or images, but often lack physical plausibility and 3D\ncontrollability. To overcome these limitations, we introduce PhysCtrl, a novel\nframework for physics-grounded image-to-video generation with physical\nparameters and force control. At its core is a generative physics network that\nlearns the distribution of physical dynamics across four materials (elastic,\nsand, plasticine, and rigid) via a diffusion model conditioned on physics\nparameters and applied forces. We represent physical dynamics as 3D point\ntrajectories and train on a large-scale synthetic dataset of 550K animations\ngenerated by physics simulators. We enhance the diffusion model with a novel\nspatiotemporal attention block that emulates particle interactions and\nincorporates physics-based constraints during training to enforce physical\nplausibility. Experiments show that PhysCtrl generates realistic,\nphysics-grounded motion trajectories which, when used to drive image-to-video\nmodels, yield high-fidelity, controllable videos that outperform existing\nmethods in both visual quality and physical plausibility. Project Page:\nhttps://cwchenwang.github.io/physctrl",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.20358.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 109
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2509.20354",
      "authors": [
        {
          "_id": "68d49f4e36950a9dff15680b",
          "name": "Henrique Schechter Vera",
          "hidden": false
        },
        {
          "_id": "68d49f4e36950a9dff15680c",
          "name": "Sahil Dua",
          "hidden": false
        },
        {
          "_id": "68d49f4e36950a9dff15680d",
          "name": "Biao Zhang",
          "hidden": false
        },
        {
          "_id": "68d49f4e36950a9dff15680e",
          "name": "Daniel Salz",
          "hidden": false
        },
        {
          "_id": "68d49f4e36950a9dff15680f",
          "name": "Ryan Mullins",
          "hidden": false
        },
        {
          "_id": "68d49f4e36950a9dff156810",
          "name": "Sindhu Raghuram Panyam",
          "hidden": false
        },
        {
          "_id": "68d49f4e36950a9dff156811",
          "name": "Sara Smoot",
          "hidden": false
        },
        {
          "_id": "68d49f4e36950a9dff156812",
          "name": "Iftekhar Naim",
          "hidden": false
        },
        {
          "_id": "68d49f4e36950a9dff156813",
          "name": "Joe Zou",
          "hidden": false
        },
        {
          "_id": "68d49f4e36950a9dff156814",
          "name": "Feiyang Chen",
          "hidden": false
        },
        {
          "_id": "68d49f4e36950a9dff156815",
          "name": "Daniel Cer",
          "hidden": false
        },
        {
          "_id": "68d49f4e36950a9dff156816",
          "name": "Alice Lisak",
          "hidden": false
        },
        {
          "_id": "68d49f4e36950a9dff156817",
          "name": "Min Choi",
          "hidden": false
        },
        {
          "_id": "68d49f4e36950a9dff156818",
          "name": "Lucas Gonzalez",
          "hidden": false
        },
        {
          "_id": "68d49f4e36950a9dff156819",
          "name": "Omar Sanseviero",
          "hidden": false
        },
        {
          "_id": "68d49f4e36950a9dff15681a",
          "name": "Glenn Cameron",
          "hidden": false
        },
        {
          "_id": "68d49f4e36950a9dff15681b",
          "name": "Ian Ballantyne",
          "hidden": false
        },
        {
          "_id": "68d49f4e36950a9dff15681c",
          "name": "Kat Black",
          "hidden": false
        },
        {
          "_id": "68d49f4e36950a9dff15681d",
          "name": "Kaifeng Chen",
          "hidden": false
        },
        {
          "_id": "68d49f4e36950a9dff15681e",
          "name": "Weiyi Wang",
          "hidden": false
        },
        {
          "_id": "68d49f4e36950a9dff15681f",
          "name": "Zhe Li",
          "hidden": false
        },
        {
          "_id": "68d49f4e36950a9dff156820",
          "name": "Gus Martins",
          "hidden": false
        },
        {
          "_id": "68d49f4e36950a9dff156821",
          "name": "Jinhyuk Lee",
          "hidden": false
        },
        {
          "_id": "68d49f4e36950a9dff156822",
          "name": "Mark Sherwood",
          "hidden": false
        },
        {
          "_id": "68d49f4e36950a9dff156823",
          "name": "Juyeong Ji",
          "hidden": false
        },
        {
          "_id": "68d49f4e36950a9dff156824",
          "name": "Renjie Wu",
          "hidden": false
        },
        {
          "_id": "68d49f4e36950a9dff156825",
          "name": "Jingxiao Zheng",
          "hidden": false
        },
        {
          "_id": "68d49f4e36950a9dff156826",
          "name": "Jyotinder Singh",
          "hidden": false
        },
        {
          "_id": "68d49f4e36950a9dff156827",
          "name": "Abheesht Sharma",
          "hidden": false
        },
        {
          "_id": "68d49f4e36950a9dff156828",
          "name": "Divya Sreepat",
          "hidden": false
        },
        {
          "_id": "68d49f4e36950a9dff156829",
          "name": "Aashi Jain",
          "hidden": false
        },
        {
          "_id": "68d49f4e36950a9dff15682a",
          "name": "Adham Elarabawy",
          "hidden": false
        },
        {
          "_id": "68d49f4e36950a9dff15682b",
          "name": "AJ Co",
          "hidden": false
        },
        {
          "_id": "68d49f4e36950a9dff15682c",
          "name": "Andreas Doumanoglou",
          "hidden": false
        },
        {
          "_id": "68d49f4e36950a9dff15682d",
          "name": "Babak Samari",
          "hidden": false
        },
        {
          "_id": "68d49f4e36950a9dff15682e",
          "name": "Ben Hora",
          "hidden": false
        },
        {
          "_id": "68d49f4e36950a9dff15682f",
          "name": "Brian Potetz",
          "hidden": false
        },
        {
          "_id": "68d49f4e36950a9dff156830",
          "name": "Dahun Kim",
          "hidden": false
        },
        {
          "_id": "68d49f4e36950a9dff156831",
          "name": "Enrique Alfonseca",
          "hidden": false
        },
        {
          "_id": "68d49f4e36950a9dff156832",
          "name": "Fedor Moiseev",
          "hidden": false
        },
        {
          "_id": "68d49f4e36950a9dff156833",
          "name": "Feng Han",
          "hidden": false
        },
        {
          "_id": "68d49f4e36950a9dff156834",
          "name": "Frank Palma Gomez",
          "hidden": false
        },
        {
          "_id": "68d49f4e36950a9dff156835",
          "name": "Gustavo Hernández Ábrego",
          "hidden": false
        },
        {
          "_id": "68d49f4e36950a9dff156836",
          "name": "Hesen Zhang",
          "hidden": false
        },
        {
          "_id": "68d49f4e36950a9dff156837",
          "name": "Hui Hui",
          "hidden": false
        },
        {
          "_id": "68d49f4e36950a9dff156838",
          "name": "Jay Han",
          "hidden": false
        },
        {
          "_id": "68d49f4e36950a9dff156839",
          "name": "Karan Gill",
          "hidden": false
        },
        {
          "_id": "68d49f4e36950a9dff15683a",
          "name": "Ke Chen",
          "hidden": false
        },
        {
          "_id": "68d49f4e36950a9dff15683b",
          "name": "Koert Chen",
          "hidden": false
        },
        {
          "_id": "68d49f4e36950a9dff15683c",
          "name": "Madhuri Shanbhogue",
          "hidden": false
        },
        {
          "_id": "68d49f4e36950a9dff15683d",
          "name": "Michael Boratko",
          "hidden": false
        },
        {
          "_id": "68d49f4e36950a9dff15683e",
          "name": "Paul Suganthan",
          "hidden": false
        },
        {
          "_id": "68d49f4e36950a9dff15683f",
          "name": "Sai Meher Karthik Duddu",
          "hidden": false
        },
        {
          "_id": "68d49f4e36950a9dff156840",
          "name": "Sandeep Mariserla",
          "hidden": false
        },
        {
          "_id": "68d49f4e36950a9dff156841",
          "name": "Setareh Ariafar",
          "hidden": false
        },
        {
          "_id": "68d49f4e36950a9dff156842",
          "name": "Shanfeng Zhang",
          "hidden": false
        },
        {
          "_id": "68d49f4e36950a9dff156843",
          "name": "Shijie Zhang",
          "hidden": false
        },
        {
          "_id": "68d49f4e36950a9dff156844",
          "name": "Simon Baumgartner",
          "hidden": false
        },
        {
          "_id": "68d49f4e36950a9dff156845",
          "name": "Sonam Goenka",
          "hidden": false
        },
        {
          "_id": "68d49f4e36950a9dff156846",
          "name": "Steve Qiu",
          "hidden": false
        },
        {
          "_id": "68d49f4e36950a9dff156847",
          "name": "Tanmaya Dabral",
          "hidden": false
        },
        {
          "_id": "68d49f4e36950a9dff156848",
          "name": "Trevor Walker",
          "hidden": false
        },
        {
          "_id": "68d49f4e36950a9dff156849",
          "name": "Vikram Rao",
          "hidden": false
        },
        {
          "_id": "68d49f4e36950a9dff15684a",
          "name": "Waleed Khawaja",
          "hidden": false
        },
        {
          "_id": "68d49f4e36950a9dff15684b",
          "name": "Wenlei Zhou",
          "hidden": false
        },
        {
          "_id": "68d49f4e36950a9dff15684c",
          "name": "Xiaoqi Ren",
          "hidden": false
        },
        {
          "_id": "68d49f4e36950a9dff15684d",
          "name": "Ye Xia",
          "hidden": false
        },
        {
          "_id": "68d49f4e36950a9dff15684e",
          "name": "Yichang Chen",
          "hidden": false
        },
        {
          "_id": "68d49f4e36950a9dff15684f",
          "name": "Yi-Ting Chen",
          "hidden": false
        },
        {
          "_id": "68d49f4e36950a9dff156850",
          "name": "Zhe Dong",
          "hidden": false
        },
        {
          "_id": "68d49f4e36950a9dff156851",
          "name": "Zhongli Ding",
          "hidden": false
        },
        {
          "_id": "68d49f4e36950a9dff156852",
          "name": "Francesco Visin",
          "hidden": false
        },
        {
          "_id": "68d49f4e36950a9dff156853",
          "name": "Gaël Liu",
          "hidden": false
        },
        {
          "_id": "68d49f4e36950a9dff156854",
          "name": "Jiageng Zhang",
          "hidden": false
        },
        {
          "_id": "68d49f4e36950a9dff156855",
          "name": "Kathleen Kenealy",
          "hidden": false
        },
        {
          "_id": "68d49f4e36950a9dff156856",
          "name": "Michelle Casbon",
          "hidden": false
        },
        {
          "_id": "68d49f4e36950a9dff156857",
          "name": "Ravin Kumar",
          "hidden": false
        },
        {
          "_id": "68d49f4e36950a9dff156858",
          "name": "Thomas Mesnard",
          "hidden": false
        },
        {
          "_id": "68d49f4e36950a9dff156859",
          "name": "Zach Gleicher",
          "hidden": false
        },
        {
          "_id": "68d49f4e36950a9dff15685a",
          "name": "Cormac Brick",
          "hidden": false
        },
        {
          "_id": "68d49f4e36950a9dff15685b",
          "name": "Olivier Lacombe",
          "hidden": false
        },
        {
          "_id": "68d49f4e36950a9dff15685c",
          "name": "Adam Roberts",
          "hidden": false
        },
        {
          "_id": "68d49f4e36950a9dff15685d",
          "name": "Yunhsuan Sung",
          "hidden": false
        },
        {
          "_id": "68d49f4e36950a9dff15685e",
          "name": "Raphael Hoffmann",
          "hidden": false
        },
        {
          "_id": "68d49f4e36950a9dff15685f",
          "name": "Tris Warkentin",
          "hidden": false
        },
        {
          "_id": "68d49f4e36950a9dff156860",
          "name": "Armand Joulin",
          "hidden": false
        },
        {
          "_id": "68d49f4e36950a9dff156861",
          "name": "Tom Duerig",
          "hidden": false
        },
        {
          "_id": "68d49f4e36950a9dff156862",
          "name": "Mojtaba Seyedhosseini",
          "hidden": false
        }
      ],
      "publishedAt": "2025-09-24T17:56:51.000Z",
      "submittedOnDailyAt": "2025-09-25T00:18:05.591Z",
      "title": "EmbeddingGemma: Powerful and Lightweight Text Representations",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "We introduce EmbeddingGemma, a new lightweight, open text embedding model\nbased on the Gemma 3 language model family. Our innovative training recipe\nstrategically captures knowledge from larger models via encoder-decoder\ninitialization and geometric embedding distillation. We improve model\nrobustness and expressiveness with a spread-out regularizer, and ensure\ngeneralizability by merging checkpoints from varied, optimized mixtures.\nEvaluated on the Massive Text Embedding Benchmark (MTEB) across multilingual,\nEnglish, and code domains, EmbeddingGemma (300M) achieves state-of-the-art\nresults. Notably, it outperforms prior top models, both proprietary and open,\nwith fewer than 500M parameters, and provides performance comparable to models\ndouble its size, offering an exceptional performance-to-cost ratio. Remarkably,\nthis lead persists when quantizing model weights or truncating embedding\noutputs. This makes EmbeddingGemma particularly well-suited for low-latency and\nhigh-throughput use cases such as on-device applications. We provide ablation\nstudies exploring our key design choices. We release EmbeddingGemma to the\ncommunity to promote further research.",
      "upvotes": 1,
      "discussionId": "68d49f4e36950a9dff156863",
      "ai_summary": "EmbeddingGemma, a lightweight text embedding model based on Gemma 3, achieves state-of-the-art performance with fewer parameters through encoder-decoder initialization, geometric embedding distillation, and spread-out regularization.",
      "ai_keywords": [
        "Gemma 3",
        "encoder-decoder initialization",
        "geometric embedding distillation",
        "spread-out regularizer",
        "Massive Text Embedding Benchmark (MTEB)",
        "multilingual",
        "English",
        "code domains",
        "quantizing model weights",
        "truncating embedding outputs"
      ]
    },
    "publishedAt": "2025-09-24T13:56:51.000Z",
    "title": "EmbeddingGemma: Powerful and Lightweight Text Representations",
    "summary": "We introduce EmbeddingGemma, a new lightweight, open text embedding model\nbased on the Gemma 3 language model family. Our innovative training recipe\nstrategically captures knowledge from larger models via encoder-decoder\ninitialization and geometric embedding distillation. We improve model\nrobustness and expressiveness with a spread-out regularizer, and ensure\ngeneralizability by merging checkpoints from varied, optimized mixtures.\nEvaluated on the Massive Text Embedding Benchmark (MTEB) across multilingual,\nEnglish, and code domains, EmbeddingGemma (300M) achieves state-of-the-art\nresults. Notably, it outperforms prior top models, both proprietary and open,\nwith fewer than 500M parameters, and provides performance comparable to models\ndouble its size, offering an exceptional performance-to-cost ratio. Remarkably,\nthis lead persists when quantizing model weights or truncating embedding\noutputs. This makes EmbeddingGemma particularly well-suited for low-latency and\nhigh-throughput use cases such as on-device applications. We provide ablation\nstudies exploring our key design choices. We release EmbeddingGemma to the\ncommunity to promote further research.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.20354.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 109
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2509.20328",
      "authors": [
        {
          "_id": "68d4a16436950a9dff15687e",
          "name": "Thaddäus Wiedemer",
          "hidden": false
        },
        {
          "_id": "68d4a16436950a9dff15687f",
          "name": "Yuxuan Li",
          "hidden": false
        },
        {
          "_id": "68d4a16436950a9dff156880",
          "name": "Paul Vicol",
          "hidden": false
        },
        {
          "_id": "68d4a16436950a9dff156881",
          "name": "Shixiang Shane Gu",
          "hidden": false
        },
        {
          "_id": "68d4a16436950a9dff156882",
          "name": "Nick Matarese",
          "hidden": false
        },
        {
          "_id": "68d4a16436950a9dff156883",
          "name": "Kevin Swersky",
          "hidden": false
        },
        {
          "_id": "68d4a16436950a9dff156884",
          "name": "Been Kim",
          "hidden": false
        },
        {
          "_id": "68d4a16436950a9dff156885",
          "name": "Priyank Jaini",
          "hidden": false
        },
        {
          "_id": "68d4a16436950a9dff156886",
          "name": "Robert Geirhos",
          "hidden": false
        }
      ],
      "publishedAt": "2025-09-24T17:17:27.000Z",
      "submittedOnDailyAt": "2025-09-25T00:27:00.745Z",
      "title": "Video models are zero-shot learners and reasoners",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "The remarkable zero-shot capabilities of Large Language Models (LLMs) have\npropelled natural language processing from task-specific models to unified,\ngeneralist foundation models. This transformation emerged from simple\nprimitives: large, generative models trained on web-scale data. Curiously, the\nsame primitives apply to today's generative video models. Could video models be\non a trajectory towards general-purpose vision understanding, much like LLMs\ndeveloped general-purpose language understanding? We demonstrate that Veo 3 can\nsolve a broad variety of tasks it wasn't explicitly trained for: segmenting\nobjects, detecting edges, editing images, understanding physical properties,\nrecognizing object affordances, simulating tool use, and more. These abilities\nto perceive, model, and manipulate the visual world enable early forms of\nvisual reasoning like maze and symmetry solving. Veo's emergent zero-shot\ncapabilities indicate that video models are on a path to becoming unified,\ngeneralist vision foundation models.",
      "upvotes": 1,
      "discussionId": "68d4a16436950a9dff156887",
      "projectPage": "https://video-zero-shot.github.io/",
      "ai_summary": "Veo 3, a generative video model, exhibits zero-shot capabilities across various visual tasks, suggesting a trajectory towards becoming a unified, generalist vision foundation model.",
      "ai_keywords": [
        "Large Language Models",
        "LLMs",
        "generative models",
        "web-scale data",
        "generative video models",
        "zero-shot capabilities",
        "object segmentation",
        "edge detection",
        "image editing",
        "physical properties",
        "object affordances",
        "tool use simulation",
        "visual reasoning",
        "maze solving",
        "symmetry solving",
        "unified",
        "generalist vision foundation models"
      ]
    },
    "publishedAt": "2025-09-24T13:17:27.000Z",
    "title": "Video models are zero-shot learners and reasoners",
    "summary": "The remarkable zero-shot capabilities of Large Language Models (LLMs) have\npropelled natural language processing from task-specific models to unified,\ngeneralist foundation models. This transformation emerged from simple\nprimitives: large, generative models trained on web-scale data. Curiously, the\nsame primitives apply to today's generative video models. Could video models be\non a trajectory towards general-purpose vision understanding, much like LLMs\ndeveloped general-purpose language understanding? We demonstrate that Veo 3 can\nsolve a broad variety of tasks it wasn't explicitly trained for: segmenting\nobjects, detecting edges, editing images, understanding physical properties,\nrecognizing object affordances, simulating tool use, and more. These abilities\nto perceive, model, and manipulate the visual world enable early forms of\nvisual reasoning like maze and symmetry solving. Veo's emergent zero-shot\ncapabilities indicate that video models are on a path to becoming unified,\ngeneralist vision foundation models.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.20328.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 109
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2509.14745",
      "authors": [
        {
          "_id": "68d036918adc5cd018d15a02",
          "name": "Miku Watanabe",
          "hidden": false
        },
        {
          "_id": "68d036918adc5cd018d15a03",
          "user": {
            "_id": "62b4f3b7464e664268bf4e85",
            "avatarUrl": "/avatars/16e79e11a8734b1d241e0f0c55a54045.svg",
            "isPro": false,
            "fullname": "Leo",
            "user": "hao-li",
            "type": "user"
          },
          "name": "Hao Li",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-09-23T10:07:21.461Z",
          "hidden": false
        },
        {
          "_id": "68d036918adc5cd018d15a04",
          "name": "Yutaro Kashiwa",
          "hidden": false
        },
        {
          "_id": "68d036918adc5cd018d15a05",
          "name": "Brittany Reid",
          "hidden": false
        },
        {
          "_id": "68d036918adc5cd018d15a06",
          "name": "Hajimu Iida",
          "hidden": false
        },
        {
          "_id": "68d036918adc5cd018d15a07",
          "name": "Ahmed E. Hassan",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/62b4f3b7464e664268bf4e85/fQaBQAcup5Rkv8yQHQiZE.png"
      ],
      "publishedAt": "2025-09-18T08:48:32.000Z",
      "submittedOnDailyAt": "2025-09-25T00:30:52.921Z",
      "title": "On the Use of Agentic Coding: An Empirical Study of Pull Requests on\n  GitHub",
      "submittedOnDailyBy": {
        "_id": "62b4f3b7464e664268bf4e85",
        "avatarUrl": "/avatars/16e79e11a8734b1d241e0f0c55a54045.svg",
        "isPro": false,
        "fullname": "Leo",
        "user": "hao-li",
        "type": "user"
      },
      "summary": "Large language models (LLMs) are increasingly being integrated into software\ndevelopment processes. The ability to generate code and submit pull requests\nwith minimal human intervention, through the use of autonomous AI agents, is\npoised to become a standard practice. However, little is known about the\npractical usefulness of these pull requests and the extent to which their\ncontributions are accepted in real-world projects. In this paper, we\nempirically study 567 GitHub pull requests (PRs) generated using Claude Code,\nan agentic coding tool, across 157 diverse open-source projects. Our analysis\nreveals that developers tend to rely on agents for tasks such as refactoring,\ndocumentation, and testing. The results indicate that 83.8% of these\nagent-assisted PRs are eventually accepted and merged by project maintainers,\nwith 54.9% of the merged PRs are integrated without further modification. The\nremaining 45.1% require additional changes benefit from human revisions,\nespecially for bug fixes, documentation, and adherence to project-specific\nstandards. These findings suggest that while agent-assisted PRs are largely\nacceptable, they still benefit from human oversight and refinement.",
      "upvotes": 1,
      "discussionId": "68d036918adc5cd018d15a08",
      "ai_summary": "Agent-assisted pull requests generated by Claude Code are largely accepted in open-source projects, with most requiring minimal human modification.",
      "ai_keywords": [
        "large language models",
        "LLMs",
        "autonomous AI agents",
        "GitHub pull requests",
        "PRs",
        "agentic coding tool",
        "refactoring",
        "documentation",
        "testing",
        "project maintainers",
        "human revisions",
        "bug fixes",
        "project-specific standards"
      ]
    },
    "publishedAt": "2025-09-18T04:48:32.000Z",
    "title": "On the Use of Agentic Coding: An Empirical Study of Pull Requests on\n  GitHub",
    "summary": "Large language models (LLMs) are increasingly being integrated into software\ndevelopment processes. The ability to generate code and submit pull requests\nwith minimal human intervention, through the use of autonomous AI agents, is\npoised to become a standard practice. However, little is known about the\npractical usefulness of these pull requests and the extent to which their\ncontributions are accepted in real-world projects. In this paper, we\nempirically study 567 GitHub pull requests (PRs) generated using Claude Code,\nan agentic coding tool, across 157 diverse open-source projects. Our analysis\nreveals that developers tend to rely on agents for tasks such as refactoring,\ndocumentation, and testing. The results indicate that 83.8% of these\nagent-assisted PRs are eventually accepted and merged by project maintainers,\nwith 54.9% of the merged PRs are integrated without further modification. The\nremaining 45.1% require additional changes benefit from human revisions,\nespecially for bug fixes, documentation, and adherence to project-specific\nstandards. These findings suggest that while agent-assisted PRs are largely\nacceptable, they still benefit from human oversight and refinement.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/62b4f3b7464e664268bf4e85/fQaBQAcup5Rkv8yQHQiZE.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.14745.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62b4f3b7464e664268bf4e85",
      "avatarUrl": "/avatars/16e79e11a8734b1d241e0f0c55a54045.svg",
      "fullname": "Leo",
      "name": "hao-li",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  }
]