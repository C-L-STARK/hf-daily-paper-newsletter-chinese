[
  {
    "paper": {
      "id": "2509.09265",
      "authors": [
        {
          "_id": "68c37f86fc1747b9124039de",
          "name": "Jiawei Wang",
          "hidden": false
        },
        {
          "_id": "68c37f86fc1747b9124039df",
          "name": "Jiacai Liu",
          "hidden": false
        },
        {
          "_id": "68c37f86fc1747b9124039e0",
          "name": "Yuqian Fu",
          "hidden": false
        },
        {
          "_id": "68c37f86fc1747b9124039e1",
          "name": "Yingru Li",
          "hidden": false
        },
        {
          "_id": "68c37f86fc1747b9124039e2",
          "name": "Xintao Wang",
          "hidden": false
        },
        {
          "_id": "68c37f86fc1747b9124039e3",
          "name": "Yuan Lin",
          "hidden": false
        },
        {
          "_id": "68c37f86fc1747b9124039e4",
          "name": "Yu Yue",
          "hidden": false
        },
        {
          "_id": "68c37f86fc1747b9124039e5",
          "name": "Lin Zhang",
          "hidden": false
        },
        {
          "_id": "68c37f86fc1747b9124039e6",
          "name": "Yang Wang",
          "hidden": false
        },
        {
          "_id": "68c37f86fc1747b9124039e7",
          "name": "Ke Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-09-11T08:50:01.000Z",
      "submittedOnDailyAt": "2025-09-12T00:36:00.113Z",
      "title": "Harnessing Uncertainty: Entropy-Modulated Policy Gradients for\n  Long-Horizon LLM Agents",
      "submittedOnDailyBy": {
        "_id": "64060b49a577649430bf6974",
        "avatarUrl": "/avatars/74d0d6ed656b593e4c101b09edf18c7a.svg",
        "isPro": false,
        "fullname": "Jiawei Wang",
        "user": "Jarvis1111",
        "type": "user"
      },
      "summary": "In long-horizon tasks, recent agents based on Large Language Models (LLMs)\nface a significant challenge that sparse, outcome-based rewards make it\ndifficult to assign credit to intermediate steps. Previous methods mainly focus\non creating dense reward signals to guide learning, either through traditional\nreinforcement learning techniques like inverse reinforcement learning or by\nusing Process Reward Models for step-by-step feedback. In this paper, we\nidentify a fundamental problem in the learning dynamics of LLMs: the magnitude\nof policy gradients is inherently coupled with the entropy, which leads to\ninefficient small updates for confident correct actions and potentially\ndestabilizes large updates for uncertain ones. To resolve this, we propose\nEntropy-Modulated Policy Gradients (EMPG), a framework that re-calibrates the\nlearning signal based on step-wise uncertainty and the final task outcome. EMPG\namplifies updates for confident correct actions, penalizes confident errors,\nand attenuates updates from uncertain steps to stabilize exploration. We\nfurther introduce a bonus term for future clarity that encourages agents to\nfind more predictable solution paths. Through comprehensive experiments on\nthree challenging agent tasks, WebShop, ALFWorld, and Deep Search, we\ndemonstrate that EMPG achieves substantial performance gains and significantly\noutperforms strong policy gradient baselines. Project page is at\nhttps://empgseed-seed.github.io/",
      "upvotes": 3,
      "discussionId": "68c37f86fc1747b9124039e8",
      "projectPage": "https://empgseed-seed.github.io/",
      "ai_summary": "Entropy-Modulated Policy Gradients (EMPG) addresses learning dynamics issues in LLMs by recalibrating policy gradients based on uncertainty and task outcomes, leading to improved performance in long-horizon tasks.",
      "ai_keywords": [
        "Large Language Models (LLMs)",
        "sparse rewards",
        "dense reward signals",
        "inverse reinforcement learning",
        "Process Reward Models",
        "policy gradients",
        "entropy",
        "Entropy-Modulated Policy Gradients (EMPG)",
        "WebShop",
        "ALFWorld",
        "Deep Search"
      ]
    },
    "publishedAt": "2025-09-11T04:50:01.000Z",
    "title": "Harnessing Uncertainty: Entropy-Modulated Policy Gradients for\n  Long-Horizon LLM Agents",
    "summary": "In long-horizon tasks, recent agents based on Large Language Models (LLMs)\nface a significant challenge that sparse, outcome-based rewards make it\ndifficult to assign credit to intermediate steps. Previous methods mainly focus\non creating dense reward signals to guide learning, either through traditional\nreinforcement learning techniques like inverse reinforcement learning or by\nusing Process Reward Models for step-by-step feedback. In this paper, we\nidentify a fundamental problem in the learning dynamics of LLMs: the magnitude\nof policy gradients is inherently coupled with the entropy, which leads to\ninefficient small updates for confident correct actions and potentially\ndestabilizes large updates for uncertain ones. To resolve this, we propose\nEntropy-Modulated Policy Gradients (EMPG), a framework that re-calibrates the\nlearning signal based on step-wise uncertainty and the final task outcome. EMPG\namplifies updates for confident correct actions, penalizes confident errors,\nand attenuates updates from uncertain steps to stabilize exploration. We\nfurther introduce a bonus term for future clarity that encourages agents to\nfind more predictable solution paths. Through comprehensive experiments on\nthree challenging agent tasks, WebShop, ALFWorld, and Deep Search, we\ndemonstrate that EMPG achieves substantial performance gains and significantly\noutperforms strong policy gradient baselines. Project page is at\nhttps://empgseed-seed.github.io/",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.09265.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64060b49a577649430bf6974",
      "avatarUrl": "/avatars/74d0d6ed656b593e4c101b09edf18c7a.svg",
      "fullname": "Jiawei Wang",
      "name": "Jarvis1111",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2509.09680",
      "authors": [
        {
          "_id": "68c37e4cfc1747b9124039ab",
          "name": "Rongyao Fang",
          "hidden": false
        },
        {
          "_id": "68c37e4cfc1747b9124039ac",
          "name": "Aldrich Yu",
          "hidden": false
        },
        {
          "_id": "68c37e4cfc1747b9124039ad",
          "name": "Chengqi Duan",
          "hidden": false
        },
        {
          "_id": "68c37e4cfc1747b9124039ae",
          "name": "Linjiang Huang",
          "hidden": false
        },
        {
          "_id": "68c37e4cfc1747b9124039af",
          "name": "Shuai Bai",
          "hidden": false
        },
        {
          "_id": "68c37e4cfc1747b9124039b0",
          "name": "Yuxuan Cai",
          "hidden": false
        },
        {
          "_id": "68c37e4cfc1747b9124039b1",
          "name": "Kun Wang",
          "hidden": false
        },
        {
          "_id": "68c37e4cfc1747b9124039b2",
          "name": "Si Liu",
          "hidden": false
        },
        {
          "_id": "68c37e4cfc1747b9124039b3",
          "name": "Xihui Liu",
          "hidden": false
        },
        {
          "_id": "68c37e4cfc1747b9124039b4",
          "name": "Hongsheng Li",
          "hidden": false
        }
      ],
      "publishedAt": "2025-09-11T17:59:59.000Z",
      "submittedOnDailyAt": "2025-09-12T00:28:54.761Z",
      "title": "FLUX-Reason-6M & PRISM-Bench: A Million-Scale Text-to-Image Reasoning\n  Dataset and Comprehensive Benchmark",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "The advancement of open-source text-to-image (T2I) models has been hindered\nby the absence of large-scale, reasoning-focused datasets and comprehensive\nevaluation benchmarks, resulting in a performance gap compared to leading\nclosed-source systems. To address this challenge, We introduce FLUX-Reason-6M\nand PRISM-Bench (Precise and Robust Image Synthesis Measurement Benchmark).\nFLUX-Reason-6M is a massive dataset consisting of 6 million high-quality\nFLUX-generated images and 20 million bilingual (English and Chinese)\ndescriptions specifically designed to teach complex reasoning. The image are\norganized according to six key characteristics: Imagination, Entity, Text\nrendering, Style, Affection, and Composition, and design explicit Generation\nChain-of-Thought (GCoT) to provide detailed breakdowns of image generation\nsteps. The whole data curation takes 15,000 A100 GPU days, providing the\ncommunity with a resource previously unattainable outside of large industrial\nlabs. PRISM-Bench offers a novel evaluation standard with seven distinct\ntracks, including a formidable Long Text challenge using GCoT. Through\ncarefully designed prompts, it utilizes advanced vision-language models for\nnuanced human-aligned assessment of prompt-image alignment and image\naesthetics. Our extensive evaluation of 19 leading models on PRISM-Bench\nreveals critical performance gaps and highlights specific areas requiring\nimprovement. Our dataset, benchmark, and evaluation code are released to\ncatalyze the next wave of reasoning-oriented T2I generation. Project page:\nhttps://flux-reason-6m.github.io/ .",
      "upvotes": 2,
      "discussionId": "68c37e4dfc1747b9124039b5",
      "projectPage": "https://flux-reason-6m.github.io/",
      "githubRepo": "https://github.com/rongyaofang/prism-bench",
      "ai_summary": "FLUX-Reason-6M and PRISM-Bench address the lack of reasoning-focused datasets and benchmarks for text-to-image models, providing a large-scale dataset and evaluation standard to improve model performance.",
      "ai_keywords": [
        "text-to-image (T2I) models",
        "FLUX-Reason-6M",
        "PRISM-Bench",
        "Generation Chain-of-Thought (GCoT)",
        "vision-language models",
        "prompt-image alignment",
        "image aesthetics"
      ],
      "githubStars": 5
    },
    "publishedAt": "2025-09-11T13:59:59.000Z",
    "title": "FLUX-Reason-6M & PRISM-Bench: A Million-Scale Text-to-Image Reasoning\n  Dataset and Comprehensive Benchmark",
    "summary": "The advancement of open-source text-to-image (T2I) models has been hindered\nby the absence of large-scale, reasoning-focused datasets and comprehensive\nevaluation benchmarks, resulting in a performance gap compared to leading\nclosed-source systems. To address this challenge, We introduce FLUX-Reason-6M\nand PRISM-Bench (Precise and Robust Image Synthesis Measurement Benchmark).\nFLUX-Reason-6M is a massive dataset consisting of 6 million high-quality\nFLUX-generated images and 20 million bilingual (English and Chinese)\ndescriptions specifically designed to teach complex reasoning. The image are\norganized according to six key characteristics: Imagination, Entity, Text\nrendering, Style, Affection, and Composition, and design explicit Generation\nChain-of-Thought (GCoT) to provide detailed breakdowns of image generation\nsteps. The whole data curation takes 15,000 A100 GPU days, providing the\ncommunity with a resource previously unattainable outside of large industrial\nlabs. PRISM-Bench offers a novel evaluation standard with seven distinct\ntracks, including a formidable Long Text challenge using GCoT. Through\ncarefully designed prompts, it utilizes advanced vision-language models for\nnuanced human-aligned assessment of prompt-image alignment and image\naesthetics. Our extensive evaluation of 19 leading models on PRISM-Bench\nreveals critical performance gaps and highlights specific areas requiring\nimprovement. Our dataset, benchmark, and evaluation code are released to\ncatalyze the next wave of reasoning-oriented T2I generation. Project page:\nhttps://flux-reason-6m.github.io/ .",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.09680.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 103
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2509.09674",
      "authors": [
        {
          "_id": "68c37bb3fc1747b912403994",
          "name": "Haozhan Li",
          "hidden": false
        },
        {
          "_id": "68c37bb3fc1747b912403995",
          "name": "Yuxin Zuo",
          "hidden": false
        },
        {
          "_id": "68c37bb3fc1747b912403996",
          "name": "Jiale Yu",
          "hidden": false
        },
        {
          "_id": "68c37bb3fc1747b912403997",
          "name": "Yuhao Zhang",
          "hidden": false
        },
        {
          "_id": "68c37bb3fc1747b912403998",
          "name": "Zhaohui Yang",
          "hidden": false
        },
        {
          "_id": "68c37bb3fc1747b912403999",
          "name": "Kaiyan Zhang",
          "hidden": false
        },
        {
          "_id": "68c37bb3fc1747b91240399a",
          "name": "Xuekai Zhu",
          "hidden": false
        },
        {
          "_id": "68c37bb3fc1747b91240399b",
          "name": "Yuchen Zhang",
          "hidden": false
        },
        {
          "_id": "68c37bb3fc1747b91240399c",
          "name": "Tianxing Chen",
          "hidden": false
        },
        {
          "_id": "68c37bb3fc1747b91240399d",
          "name": "Ganqu Cui",
          "hidden": false
        },
        {
          "_id": "68c37bb3fc1747b91240399e",
          "name": "Dehui Wang",
          "hidden": false
        },
        {
          "_id": "68c37bb3fc1747b91240399f",
          "name": "Dingxiang Luo",
          "hidden": false
        },
        {
          "_id": "68c37bb3fc1747b9124039a0",
          "name": "Yuchen Fan",
          "hidden": false
        },
        {
          "_id": "68c37bb3fc1747b9124039a1",
          "name": "Youbang Sun",
          "hidden": false
        },
        {
          "_id": "68c37bb3fc1747b9124039a2",
          "name": "Jia Zeng",
          "hidden": false
        },
        {
          "_id": "68c37bb3fc1747b9124039a3",
          "name": "Jiangmiao Pang",
          "hidden": false
        },
        {
          "_id": "68c37bb3fc1747b9124039a4",
          "name": "Shanghang Zhang",
          "hidden": false
        },
        {
          "_id": "68c37bb3fc1747b9124039a5",
          "name": "Yu Wang",
          "hidden": false
        },
        {
          "_id": "68c37bb3fc1747b9124039a6",
          "name": "Yao Mu",
          "hidden": false
        },
        {
          "_id": "68c37bb3fc1747b9124039a7",
          "name": "Bowen Zhou",
          "hidden": false
        },
        {
          "_id": "68c37bb3fc1747b9124039a8",
          "name": "Ning Ding",
          "hidden": false
        }
      ],
      "publishedAt": "2025-09-11T17:59:17.000Z",
      "submittedOnDailyAt": "2025-09-12T00:32:42.595Z",
      "title": "SimpleVLA-RL: Scaling VLA Training via Reinforcement Learning",
      "submittedOnDailyBy": {
        "_id": "662f638ba9891e43cc4c5125",
        "avatarUrl": "/avatars/77c22de5511f9b85d98ec75fb0b5e9be.svg",
        "isPro": true,
        "fullname": "Li Haozhan",
        "user": "Haozhan72",
        "type": "user"
      },
      "summary": "Vision-Language-Action (VLA) models have recently emerged as a powerful\nparadigm for robotic manipulation. Despite substantial progress enabled by\nlarge-scale pretraining and supervised fine-tuning (SFT), these models face two\nfundamental challenges: (i) the scarcity and high cost of large-scale\nhuman-operated robotic trajectories required for SFT scaling, and (ii) limited\ngeneralization to tasks involving distribution shift. Recent breakthroughs in\nLarge Reasoning Models (LRMs) demonstrate that reinforcement learning (RL) can\ndramatically enhance step-by-step reasoning capabilities, raising a natural\nquestion: Can RL similarly improve the long-horizon step-by-step action\nplanning of VLA? In this work, we introduce SimpleVLA-RL, an efficient RL\nframework tailored for VLA models. Building upon veRL, we introduce\nVLA-specific trajectory sampling, scalable parallelization, multi-environment\nrendering, and optimized loss computation. When applied to OpenVLA-OFT,\nSimpleVLA-RL achieves SoTA performance on LIBERO and even outperforms pi_0\non RoboTwin 1.0\\&2.0 with the exploration-enhancing strategies we introduce.\nSimpleVLA-RL not only reduces dependence on large-scale data and enables robust\ngeneralization, but also remarkably surpasses SFT in real-world tasks.\nMoreover, we identify a novel phenomenon ``pushcut'' during RL training,\nwherein the policy discovers previously unseen patterns beyond those seen in\nthe previous training process. Github: https://github.com/PRIME-RL/SimpleVLA-RL",
      "upvotes": 2,
      "discussionId": "68c37bb3fc1747b9124039a9",
      "ai_summary": "SimpleVLA-RL, an RL framework for VLA models, enhances long-horizon action planning, achieves state-of-the-art performance, and discovers novel patterns during training.",
      "ai_keywords": [
        "reinforcement learning",
        "RL",
        "VLA models",
        "trajectory sampling",
        "parallelization",
        "multi-environment rendering",
        "loss computation",
        "LIBERO",
        "RoboTwin",
        "step-by-step reasoning",
        "distribution shift",
        "pushcut"
      ]
    },
    "publishedAt": "2025-09-11T13:59:17.000Z",
    "title": "SimpleVLA-RL: Scaling VLA Training via Reinforcement Learning",
    "summary": "Vision-Language-Action (VLA) models have recently emerged as a powerful\nparadigm for robotic manipulation. Despite substantial progress enabled by\nlarge-scale pretraining and supervised fine-tuning (SFT), these models face two\nfundamental challenges: (i) the scarcity and high cost of large-scale\nhuman-operated robotic trajectories required for SFT scaling, and (ii) limited\ngeneralization to tasks involving distribution shift. Recent breakthroughs in\nLarge Reasoning Models (LRMs) demonstrate that reinforcement learning (RL) can\ndramatically enhance step-by-step reasoning capabilities, raising a natural\nquestion: Can RL similarly improve the long-horizon step-by-step action\nplanning of VLA? In this work, we introduce SimpleVLA-RL, an efficient RL\nframework tailored for VLA models. Building upon veRL, we introduce\nVLA-specific trajectory sampling, scalable parallelization, multi-environment\nrendering, and optimized loss computation. When applied to OpenVLA-OFT,\nSimpleVLA-RL achieves SoTA performance on LIBERO and even outperforms pi_0\non RoboTwin 1.0\\&2.0 with the exploration-enhancing strategies we introduce.\nSimpleVLA-RL not only reduces dependence on large-scale data and enables robust\ngeneralization, but also remarkably surpasses SFT in real-world tasks.\nMoreover, we identify a novel phenomenon ``pushcut'' during RL training,\nwherein the policy discovers previously unseen patterns beyond those seen in\nthe previous training process. Github: https://github.com/PRIME-RL/SimpleVLA-RL",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.09674.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "662f638ba9891e43cc4c5125",
      "avatarUrl": "/avatars/77c22de5511f9b85d98ec75fb0b5e9be.svg",
      "fullname": "Li Haozhan",
      "name": "Haozhan72",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2509.09614",
      "authors": [
        {
          "_id": "68c381befc1747b9124039f9",
          "name": "Jielin Qiu",
          "hidden": false
        },
        {
          "_id": "68c381befc1747b9124039fa",
          "name": "Zuxin Liu",
          "hidden": false
        },
        {
          "_id": "68c381befc1747b9124039fb",
          "name": "Zhiwei Liu",
          "hidden": false
        },
        {
          "_id": "68c381befc1747b9124039fc",
          "name": "Rithesh Murthy",
          "hidden": false
        },
        {
          "_id": "68c381befc1747b9124039fd",
          "name": "Jianguo Zhang",
          "hidden": false
        },
        {
          "_id": "68c381befc1747b9124039fe",
          "name": "Haolin Chen",
          "hidden": false
        },
        {
          "_id": "68c381befc1747b9124039ff",
          "name": "Shiyu Wang",
          "hidden": false
        },
        {
          "_id": "68c381befc1747b912403a00",
          "name": "Ming Zhu",
          "hidden": false
        },
        {
          "_id": "68c381befc1747b912403a01",
          "name": "Liangwei Yang",
          "hidden": false
        },
        {
          "_id": "68c381befc1747b912403a02",
          "name": "Juntao Tan",
          "hidden": false
        },
        {
          "_id": "68c381befc1747b912403a03",
          "name": "Zhepeng Cen",
          "hidden": false
        },
        {
          "_id": "68c381befc1747b912403a04",
          "name": "Cheng Qian",
          "hidden": false
        },
        {
          "_id": "68c381befc1747b912403a05",
          "name": "Shelby Heinecke",
          "hidden": false
        },
        {
          "_id": "68c381befc1747b912403a06",
          "name": "Weiran Yao",
          "hidden": false
        },
        {
          "_id": "68c381befc1747b912403a07",
          "name": "Silvio Savarese",
          "hidden": false
        },
        {
          "_id": "68c381befc1747b912403a08",
          "name": "Caiming Xiong",
          "hidden": false
        },
        {
          "_id": "68c381befc1747b912403a09",
          "name": "Huan Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-09-11T16:55:04.000Z",
      "submittedOnDailyAt": "2025-09-12T00:43:38.492Z",
      "title": "LoCoBench: A Benchmark for Long-Context Large Language Models in Complex\n  Software Engineering",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "The emergence of long-context language models with context windows extending\nto millions of tokens has created new opportunities for sophisticated code\nunderstanding and software development evaluation. We propose LoCoBench, a\ncomprehensive benchmark specifically designed to evaluate long-context LLMs in\nrealistic, complex software development scenarios. Unlike existing code\nevaluation benchmarks that focus on single-function completion or short-context\ntasks, LoCoBench addresses the critical evaluation gap for long-context\ncapabilities that require understanding entire codebases, reasoning across\nmultiple files, and maintaining architectural consistency across large-scale\nsoftware systems. Our benchmark provides 8,000 evaluation scenarios\nsystematically generated across 10 programming languages, with context lengths\nspanning 10K to 1M tokens, a 100x variation that enables precise assessment of\nlong-context performance degradation in realistic software development\nsettings. LoCoBench introduces 8 task categories that capture essential\nlong-context capabilities: architectural understanding, cross-file refactoring,\nmulti-session development, bug investigation, feature implementation, code\ncomprehension, integration testing, and security analysis. Through a 5-phase\npipeline, we create diverse, high-quality scenarios that challenge LLMs to\nreason about complex codebases at unprecedented scale. We introduce a\ncomprehensive evaluation framework with 17 metrics across 4 dimensions,\nincluding 8 new evaluation metrics, combined in a LoCoBench Score (LCBS). Our\nevaluation of state-of-the-art long-context models reveals substantial\nperformance gaps, demonstrating that long-context understanding in complex\nsoftware development represents a significant unsolved challenge that demands\nmore attention. LoCoBench is released at:\nhttps://github.com/SalesforceAIResearch/LoCoBench.",
      "upvotes": 1,
      "discussionId": "68c381bffc1747b912403a0a",
      "githubRepo": "https://github.com/SalesforceAIResearch/LoCoBench",
      "ai_summary": "LoCoBench evaluates long-context language models in complex software development scenarios, addressing the gap in understanding entire codebases and maintaining architectural consistency across large-scale systems.",
      "ai_keywords": [
        "long-context language models",
        "LoCoBench",
        "code evaluation benchmarks",
        "long-context capabilities",
        "codebases",
        "cross-file refactoring",
        "multi-session development",
        "bug investigation",
        "feature implementation",
        "code comprehension",
        "integration testing",
        "security analysis",
        "LoCoBench Score (LCBS)"
      ],
      "githubStars": 2
    },
    "publishedAt": "2025-09-11T12:55:04.000Z",
    "title": "LoCoBench: A Benchmark for Long-Context Large Language Models in Complex\n  Software Engineering",
    "summary": "The emergence of long-context language models with context windows extending\nto millions of tokens has created new opportunities for sophisticated code\nunderstanding and software development evaluation. We propose LoCoBench, a\ncomprehensive benchmark specifically designed to evaluate long-context LLMs in\nrealistic, complex software development scenarios. Unlike existing code\nevaluation benchmarks that focus on single-function completion or short-context\ntasks, LoCoBench addresses the critical evaluation gap for long-context\ncapabilities that require understanding entire codebases, reasoning across\nmultiple files, and maintaining architectural consistency across large-scale\nsoftware systems. Our benchmark provides 8,000 evaluation scenarios\nsystematically generated across 10 programming languages, with context lengths\nspanning 10K to 1M tokens, a 100x variation that enables precise assessment of\nlong-context performance degradation in realistic software development\nsettings. LoCoBench introduces 8 task categories that capture essential\nlong-context capabilities: architectural understanding, cross-file refactoring,\nmulti-session development, bug investigation, feature implementation, code\ncomprehension, integration testing, and security analysis. Through a 5-phase\npipeline, we create diverse, high-quality scenarios that challenge LLMs to\nreason about complex codebases at unprecedented scale. We introduce a\ncomprehensive evaluation framework with 17 metrics across 4 dimensions,\nincluding 8 new evaluation metrics, combined in a LoCoBench Score (LCBS). Our\nevaluation of state-of-the-art long-context models reveals substantial\nperformance gaps, demonstrating that long-context understanding in complex\nsoftware development represents a significant unsolved challenge that demands\nmore attention. LoCoBench is released at:\nhttps://github.com/SalesforceAIResearch/LoCoBench.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.09614.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 103
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2509.09595",
      "authors": [
        {
          "_id": "68c37f08fc1747b9124039ce",
          "name": "Yikang Ding",
          "hidden": false
        },
        {
          "_id": "68c37f08fc1747b9124039cf",
          "name": "Jiwen Liu",
          "hidden": false
        },
        {
          "_id": "68c37f08fc1747b9124039d0",
          "name": "Wenyuan Zhang",
          "hidden": false
        },
        {
          "_id": "68c37f08fc1747b9124039d1",
          "name": "Zekun Wang",
          "hidden": false
        },
        {
          "_id": "68c37f08fc1747b9124039d2",
          "name": "Wentao Hu",
          "hidden": false
        },
        {
          "_id": "68c37f08fc1747b9124039d3",
          "name": "Liyuan Cui",
          "hidden": false
        },
        {
          "_id": "68c37f08fc1747b9124039d4",
          "name": "Mingming Lao",
          "hidden": false
        },
        {
          "_id": "68c37f08fc1747b9124039d5",
          "name": "Yingchao Shao",
          "hidden": false
        },
        {
          "_id": "68c37f08fc1747b9124039d6",
          "name": "Hui Liu",
          "hidden": false
        },
        {
          "_id": "68c37f08fc1747b9124039d7",
          "name": "Xiaohan Li",
          "hidden": false
        },
        {
          "_id": "68c37f08fc1747b9124039d8",
          "name": "Ming Chen",
          "hidden": false
        },
        {
          "_id": "68c37f08fc1747b9124039d9",
          "name": "Xiaoqiang Liu",
          "hidden": false
        },
        {
          "_id": "68c37f08fc1747b9124039da",
          "name": "Yu-Shen Liu",
          "hidden": false
        },
        {
          "_id": "68c37f08fc1747b9124039db",
          "name": "Pengfei Wan",
          "hidden": false
        }
      ],
      "publishedAt": "2025-09-11T16:34:57.000Z",
      "submittedOnDailyAt": "2025-09-12T00:31:52.896Z",
      "title": "Kling-Avatar: Grounding Multimodal Instructions for Cascaded\n  Long-Duration Avatar Animation Synthesis",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Recent advances in audio-driven avatar video generation have significantly\nenhanced audio-visual realism. However, existing methods treat instruction\nconditioning merely as low-level tracking driven by acoustic or visual cues,\nwithout modeling the communicative purpose conveyed by the instructions. This\nlimitation compromises their narrative coherence and character expressiveness.\nTo bridge this gap, we introduce Kling-Avatar, a novel cascaded framework that\nunifies multimodal instruction understanding with photorealistic portrait\ngeneration. Our approach adopts a two-stage pipeline. In the first stage, we\ndesign a multimodal large language model (MLLM) director that produces a\nblueprint video conditioned on diverse instruction signals, thereby governing\nhigh-level semantics such as character motion and emotions. In the second\nstage, guided by blueprint keyframes, we generate multiple sub-clips in\nparallel using a first-last frame strategy. This global-to-local framework\npreserves fine-grained details while faithfully encoding the high-level intent\nbehind multimodal instructions. Our parallel architecture also enables fast and\nstable generation of long-duration videos, making it suitable for real-world\napplications such as digital human livestreaming and vlogging. To\ncomprehensively evaluate our method, we construct a benchmark of 375 curated\nsamples covering diverse instructions and challenging scenarios. Extensive\nexperiments demonstrate that Kling-Avatar is capable of generating vivid,\nfluent, long-duration videos at up to 1080p and 48 fps, achieving superior\nperformance in lip synchronization accuracy, emotion and dynamic\nexpressiveness, instruction controllability, identity preservation, and\ncross-domain generalization. These results establish Kling-Avatar as a new\nbenchmark for semantically grounded, high-fidelity audio-driven avatar\nsynthesis.",
      "upvotes": 1,
      "discussionId": "68c37f08fc1747b9124039dc",
      "ai_summary": "Kling-Avatar, a cascaded framework, enhances audio-driven avatar video generation by integrating multimodal instruction understanding with photorealistic portrait generation, resulting in high-fidelity, semantically grounded videos.",
      "ai_keywords": [
        "multimodal large language model",
        "blueprint video",
        "high-level semantics",
        "character motion",
        "emotions",
        "blueprint keyframes",
        "first-last frame strategy",
        "global-to-local framework",
        "parallel architecture",
        "lip synchronization accuracy",
        "emotion and dynamic expressiveness",
        "instruction controllability",
        "identity preservation",
        "cross-domain generalization"
      ]
    },
    "publishedAt": "2025-09-11T12:34:57.000Z",
    "title": "Kling-Avatar: Grounding Multimodal Instructions for Cascaded\n  Long-Duration Avatar Animation Synthesis",
    "summary": "Recent advances in audio-driven avatar video generation have significantly\nenhanced audio-visual realism. However, existing methods treat instruction\nconditioning merely as low-level tracking driven by acoustic or visual cues,\nwithout modeling the communicative purpose conveyed by the instructions. This\nlimitation compromises their narrative coherence and character expressiveness.\nTo bridge this gap, we introduce Kling-Avatar, a novel cascaded framework that\nunifies multimodal instruction understanding with photorealistic portrait\ngeneration. Our approach adopts a two-stage pipeline. In the first stage, we\ndesign a multimodal large language model (MLLM) director that produces a\nblueprint video conditioned on diverse instruction signals, thereby governing\nhigh-level semantics such as character motion and emotions. In the second\nstage, guided by blueprint keyframes, we generate multiple sub-clips in\nparallel using a first-last frame strategy. This global-to-local framework\npreserves fine-grained details while faithfully encoding the high-level intent\nbehind multimodal instructions. Our parallel architecture also enables fast and\nstable generation of long-duration videos, making it suitable for real-world\napplications such as digital human livestreaming and vlogging. To\ncomprehensively evaluate our method, we construct a benchmark of 375 curated\nsamples covering diverse instructions and challenging scenarios. Extensive\nexperiments demonstrate that Kling-Avatar is capable of generating vivid,\nfluent, long-duration videos at up to 1080p and 48 fps, achieving superior\nperformance in lip synchronization accuracy, emotion and dynamic\nexpressiveness, instruction controllability, identity preservation, and\ncross-domain generalization. These results establish Kling-Avatar as a new\nbenchmark for semantically grounded, high-fidelity audio-driven avatar\nsynthesis.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.09595.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 103
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2509.09676",
      "authors": [
        {
          "_id": "68c37eb7fc1747b9124039bd",
          "name": "Jiahao Wang",
          "hidden": false
        },
        {
          "_id": "68c37eb7fc1747b9124039be",
          "name": "Yufeng Yuan",
          "hidden": false
        },
        {
          "_id": "68c37eb7fc1747b9124039bf",
          "name": "Rujie Zheng",
          "hidden": false
        },
        {
          "_id": "68c37eb7fc1747b9124039c0",
          "name": "Youtian Lin",
          "hidden": false
        },
        {
          "_id": "68c37eb7fc1747b9124039c1",
          "name": "Jian Gao",
          "hidden": false
        },
        {
          "_id": "68c37eb7fc1747b9124039c2",
          "name": "Lin-Zhuo Chen",
          "hidden": false
        },
        {
          "_id": "68c37eb7fc1747b9124039c3",
          "name": "Yajie Bao",
          "hidden": false
        },
        {
          "_id": "68c37eb7fc1747b9124039c4",
          "name": "Yi Zhang",
          "hidden": false
        },
        {
          "_id": "68c37eb7fc1747b9124039c5",
          "name": "Chang Zeng",
          "hidden": false
        },
        {
          "_id": "68c37eb7fc1747b9124039c6",
          "name": "Yanxi Zhou",
          "hidden": false
        },
        {
          "_id": "68c37eb7fc1747b9124039c7",
          "name": "Xiaoxiao Long",
          "hidden": false
        },
        {
          "_id": "68c37eb7fc1747b9124039c8",
          "name": "Hao Zhu",
          "hidden": false
        },
        {
          "_id": "68c37eb7fc1747b9124039c9",
          "name": "Zhaoxiang Zhang",
          "hidden": false
        },
        {
          "_id": "68c37eb7fc1747b9124039ca",
          "name": "Xun Cao",
          "hidden": false
        },
        {
          "_id": "68c37eb7fc1747b9124039cb",
          "name": "Yao Yao",
          "hidden": false
        }
      ],
      "publishedAt": "2025-09-11T17:59:31.000Z",
      "submittedOnDailyAt": "2025-09-12T00:30:37.532Z",
      "title": "SpatialVID: A Large-Scale Video Dataset with Spatial Annotations",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Significant progress has been made in spatial intelligence, spanning both\nspatial reconstruction and world exploration. However, the scalability and\nreal-world fidelity of current models remain severely constrained by the\nscarcity of large-scale, high-quality training data. While several datasets\nprovide camera pose information, they are typically limited in scale,\ndiversity, and annotation richness, particularly for real-world dynamic scenes\nwith ground-truth camera motion. To this end, we collect SpatialVID, a\ndataset consists of a large corpus of in-the-wild videos with diverse scenes,\ncamera movements and dense 3D annotations such as per-frame camera poses,\ndepth, and motion instructions. Specifically, we collect more than 21,000 hours\nof raw video, and process them into 2.7 million clips through a hierarchical\nfiltering pipeline, totaling 7,089 hours of dynamic content. A subsequent\nannotation pipeline enriches these clips with detailed spatial and semantic\ninformation, including camera poses, depth maps, dynamic masks, structured\ncaptions, and serialized motion instructions. Analysis of SpatialVID's data\nstatistics reveals a richness and diversity that directly foster improved model\ngeneralization and performance, establishing it as a key asset for the video\nand 3D vision research community.",
      "upvotes": 0,
      "discussionId": "68c37eb8fc1747b9124039cc",
      "projectPage": "https://nju-3dv.github.io/projects/SpatialVID/",
      "ai_summary": "SpatialVID, a large-scale dataset with diverse videos and dense 3D annotations, enhances model generalization and performance in video and 3D vision research.",
      "ai_keywords": [
        "spatialVID",
        "camera poses",
        "depth maps",
        "dynamic masks",
        "structured captions",
        "motion instructions",
        "video and 3D vision"
      ]
    },
    "publishedAt": "2025-09-11T13:59:31.000Z",
    "title": "SpatialVID: A Large-Scale Video Dataset with Spatial Annotations",
    "summary": "Significant progress has been made in spatial intelligence, spanning both\nspatial reconstruction and world exploration. However, the scalability and\nreal-world fidelity of current models remain severely constrained by the\nscarcity of large-scale, high-quality training data. While several datasets\nprovide camera pose information, they are typically limited in scale,\ndiversity, and annotation richness, particularly for real-world dynamic scenes\nwith ground-truth camera motion. To this end, we collect SpatialVID, a\ndataset consists of a large corpus of in-the-wild videos with diverse scenes,\ncamera movements and dense 3D annotations such as per-frame camera poses,\ndepth, and motion instructions. Specifically, we collect more than 21,000 hours\nof raw video, and process them into 2.7 million clips through a hierarchical\nfiltering pipeline, totaling 7,089 hours of dynamic content. A subsequent\nannotation pipeline enriches these clips with detailed spatial and semantic\ninformation, including camera poses, depth maps, dynamic masks, structured\ncaptions, and serialized motion instructions. Analysis of SpatialVID's data\nstatistics reveals a richness and diversity that directly foster improved model\ngeneralization and performance, establishing it as a key asset for the video\nand 3D vision research community.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.09676.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 103
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2509.09174",
      "authors": [
        {
          "_id": "68c38053fc1747b9124039ea",
          "name": "Yuhao Zhang",
          "hidden": false
        },
        {
          "_id": "68c38053fc1747b9124039eb",
          "name": "Yuhao Du",
          "hidden": false
        },
        {
          "_id": "68c38053fc1747b9124039ec",
          "name": "Zhanchen Dai",
          "hidden": false
        },
        {
          "_id": "68c38053fc1747b9124039ed",
          "name": "Xiangnan Ma",
          "hidden": false
        },
        {
          "_id": "68c38053fc1747b9124039ee",
          "name": "Kaiqi Kou",
          "hidden": false
        },
        {
          "_id": "68c38053fc1747b9124039ef",
          "name": "Benyou Wang",
          "hidden": false
        },
        {
          "_id": "68c38053fc1747b9124039f0",
          "name": "Haizhou Li",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/66975b9f8031bf92b428e138/fpbLmvS4cUyXEqNZlqSY3.mp4"
      ],
      "publishedAt": "2025-09-11T06:17:59.000Z",
      "submittedOnDailyAt": "2025-09-12T00:46:24.478Z",
      "title": "EchoX: Towards Mitigating Acoustic-Semantic Gap via Echo Training for\n  Speech-to-Speech LLMs",
      "submittedOnDailyBy": {
        "_id": "66975b9f8031bf92b428e138",
        "avatarUrl": "/avatars/3254281a7bac1c8ddde1d6bc7e518b2f.svg",
        "isPro": false,
        "fullname": "Yuhao Zhang",
        "user": "Yoohao",
        "type": "user"
      },
      "summary": "Speech-to-speech large language models (SLLMs) are attracting increasing\nattention. Derived from text-based large language models (LLMs), SLLMs often\nexhibit degradation in knowledge and reasoning capabilities. We hypothesize\nthat this limitation arises because current training paradigms for SLLMs fail\nto bridge the acoustic-semantic gap in the feature representation space. To\naddress this issue, we propose EchoX, which leverages semantic representations\nand dynamically generates speech training targets. This approach integrates\nboth acoustic and semantic learning, enabling EchoX to preserve strong\nreasoning abilities as a speech LLM. Experimental results demonstrate that\nEchoX, with about six thousand hours of training data, achieves advanced\nperformance on multiple knowledge-based question-answering benchmarks. The\nproject is available at https://github.com/FreedomIntelligence/EchoX.",
      "upvotes": 0,
      "discussionId": "68c38053fc1747b9124039f1",
      "ai_summary": "EchoX, a speech-to-speech large language model, addresses the acoustic-semantic gap by integrating semantic representations, preserving reasoning abilities, and achieving advanced performance on knowledge-based benchmarks.",
      "ai_keywords": [
        "speech-to-speech large language models",
        "SLLMs",
        "text-based large language models",
        "LLMs",
        "acoustic-semantic gap",
        "semantic representations",
        "speech training targets",
        "acoustic learning",
        "semantic learning",
        "knowledge-based question-answering benchmarks"
      ]
    },
    "publishedAt": "2025-09-11T02:17:59.000Z",
    "title": "EchoX: Towards Mitigating Acoustic-Semantic Gap via Echo Training for\n  Speech-to-Speech LLMs",
    "summary": "Speech-to-speech large language models (SLLMs) are attracting increasing\nattention. Derived from text-based large language models (LLMs), SLLMs often\nexhibit degradation in knowledge and reasoning capabilities. We hypothesize\nthat this limitation arises because current training paradigms for SLLMs fail\nto bridge the acoustic-semantic gap in the feature representation space. To\naddress this issue, we propose EchoX, which leverages semantic representations\nand dynamically generates speech training targets. This approach integrates\nboth acoustic and semantic learning, enabling EchoX to preserve strong\nreasoning abilities as a speech LLM. Experimental results demonstrate that\nEchoX, with about six thousand hours of training data, achieves advanced\nperformance on multiple knowledge-based question-answering benchmarks. The\nproject is available at https://github.com/FreedomIntelligence/EchoX.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/66975b9f8031bf92b428e138/fpbLmvS4cUyXEqNZlqSY3.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.09174.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66975b9f8031bf92b428e138",
      "avatarUrl": "/avatars/3254281a7bac1c8ddde1d6bc7e518b2f.svg",
      "fullname": "Yuhao Zhang",
      "name": "Yoohao",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  }
]