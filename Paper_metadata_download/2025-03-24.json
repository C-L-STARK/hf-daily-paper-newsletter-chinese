[
  {
    "paper": {
      "id": "2503.16430",
      "authors": [
        {
          "_id": "67e0bd81b04d9e836829c468",
          "name": "Yuqing Wang",
          "hidden": false
        },
        {
          "_id": "67e0bd81b04d9e836829c469",
          "name": "Zhijie Lin",
          "hidden": false
        },
        {
          "_id": "67e0bd81b04d9e836829c46a",
          "name": "Yao Teng",
          "hidden": false
        },
        {
          "_id": "67e0bd81b04d9e836829c46b",
          "name": "Yuanzhi Zhu",
          "hidden": false
        },
        {
          "_id": "67e0bd81b04d9e836829c46c",
          "name": "Shuhuai Ren",
          "hidden": false
        },
        {
          "_id": "67e0bd81b04d9e836829c46d",
          "name": "Jiashi Feng",
          "hidden": false
        },
        {
          "_id": "67e0bd81b04d9e836829c46e",
          "name": "Xihui Liu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-20T17:59:59.000Z",
      "submittedOnDailyAt": "2025-03-24T00:50:05.627Z",
      "title": "Bridging Continuous and Discrete Tokens for Autoregressive Visual\n  Generation",
      "submittedOnDailyBy": {
        "_id": "63ea23b9dedfeebe54d02bdf",
        "avatarUrl": "/avatars/4d9f9a546aa8c63e277161ea700075c4.svg",
        "isPro": false,
        "fullname": "Yuqing Wang",
        "user": "Epiphqny",
        "type": "user"
      },
      "summary": "Autoregressive visual generation models typically rely on tokenizers to\ncompress images into tokens that can be predicted sequentially. A fundamental\ndilemma exists in token representation: discrete tokens enable straightforward\nmodeling with standard cross-entropy loss, but suffer from information loss and\ntokenizer training instability; continuous tokens better preserve visual\ndetails, but require complex distribution modeling, complicating the generation\npipeline. In this paper, we propose TokenBridge, which bridges this gap by\nmaintaining the strong representation capacity of continuous tokens while\npreserving the modeling simplicity of discrete tokens. To achieve this, we\ndecouple discretization from the tokenizer training process through\npost-training quantization that directly obtains discrete tokens from\ncontinuous representations. Specifically, we introduce a dimension-wise\nquantization strategy that independently discretizes each feature dimension,\npaired with a lightweight autoregressive prediction mechanism that efficiently\nmodel the resulting large token space. Extensive experiments show that our\napproach achieves reconstruction and generation quality on par with continuous\nmethods while using standard categorical prediction. This work demonstrates\nthat bridging discrete and continuous paradigms can effectively harness the\nstrengths of both approaches, providing a promising direction for high-quality\nvisual generation with simple autoregressive modeling. Project page:\nhttps://yuqingwang1029.github.io/TokenBridge.",
      "upvotes": 9,
      "discussionId": "67e0bd85b04d9e836829c55f",
      "projectPage": "https://yuqingwang1029.github.io/TokenBridge/",
      "githubRepo": "https://github.com/YuqingWang1029/TokenBridge",
      "ai_keywords": [
        "autoregressive visual generation models",
        "tokenizers",
        "tokens",
        "discrete tokens",
        "continuous tokens",
        "cross-entropy loss",
        "tokenizer training",
        "TokenBridge",
        "post-training quantization",
        "dimension-wise quantization",
        "lightweight autoregressive prediction mechanism",
        "reconstruction quality",
        "generation quality"
      ]
    },
    "publishedAt": "2025-03-20T13:59:59.000Z",
    "title": "Bridging Continuous and Discrete Tokens for Autoregressive Visual\n  Generation",
    "summary": "Autoregressive visual generation models typically rely on tokenizers to\ncompress images into tokens that can be predicted sequentially. A fundamental\ndilemma exists in token representation: discrete tokens enable straightforward\nmodeling with standard cross-entropy loss, but suffer from information loss and\ntokenizer training instability; continuous tokens better preserve visual\ndetails, but require complex distribution modeling, complicating the generation\npipeline. In this paper, we propose TokenBridge, which bridges this gap by\nmaintaining the strong representation capacity of continuous tokens while\npreserving the modeling simplicity of discrete tokens. To achieve this, we\ndecouple discretization from the tokenizer training process through\npost-training quantization that directly obtains discrete tokens from\ncontinuous representations. Specifically, we introduce a dimension-wise\nquantization strategy that independently discretizes each feature dimension,\npaired with a lightweight autoregressive prediction mechanism that efficiently\nmodel the resulting large token space. Extensive experiments show that our\napproach achieves reconstruction and generation quality on par with continuous\nmethods while using standard categorical prediction. This work demonstrates\nthat bridging discrete and continuous paradigms can effectively harness the\nstrengths of both approaches, providing a promising direction for high-quality\nvisual generation with simple autoregressive modeling. Project page:\nhttps://yuqingwang1029.github.io/TokenBridge.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.16430.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63ea23b9dedfeebe54d02bdf",
      "avatarUrl": "/avatars/4d9f9a546aa8c63e277161ea700075c4.svg",
      "fullname": "Yuqing Wang",
      "name": "Epiphqny",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.17352",
      "authors": [
        {
          "_id": "67e0bcc9e5fa0da84e121032",
          "name": "Yihe Deng",
          "hidden": false
        },
        {
          "_id": "67e0bcc9e5fa0da84e121033",
          "name": "Hritik Bansal",
          "hidden": false
        },
        {
          "_id": "67e0bcc9e5fa0da84e121034",
          "name": "Fan Yin",
          "hidden": false
        },
        {
          "_id": "67e0bcc9e5fa0da84e121035",
          "name": "Nanyun Peng",
          "hidden": false
        },
        {
          "_id": "67e0bcc9e5fa0da84e121036",
          "name": "Wei Wang",
          "hidden": false
        },
        {
          "_id": "67e0bcc9e5fa0da84e121037",
          "name": "Kai-Wei Chang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-21T17:52:43.000Z",
      "submittedOnDailyAt": "2025-03-24T00:31:06.884Z",
      "title": "OpenVLThinker: An Early Exploration to Complex Vision-Language Reasoning\n  via Iterative Self-Improvement",
      "submittedOnDailyBy": {
        "_id": "642f4c789b2484d7d8551a93",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/642f4c789b2484d7d8551a93/0lH4YXcbZa-Xlzj6ESo7F.jpeg",
        "isPro": true,
        "fullname": "Yihe Deng",
        "user": "ydeng9",
        "type": "user"
      },
      "summary": "Recent advancements demonstrated by DeepSeek-R1 have shown that complex\nreasoning abilities in large language models (LLMs), including sophisticated\nbehaviors such as self-verification and self-correction, can be achieved by RL\nwith verifiable rewards and significantly improves model performance on\nchallenging tasks such as AIME. Motivated by these findings, our study\ninvestigates whether similar reasoning capabilities can be successfully\nintegrated into large vision-language models (LVLMs) and assesses their impact\non challenging multimodal reasoning tasks. We consider an approach that\niteratively leverages supervised fine-tuning (SFT) on lightweight training data\nand Reinforcement Learning (RL) to further improve model generalization.\nInitially, reasoning capabilities were distilled from pure-text R1 models by\ngenerating reasoning steps using high-quality captions of the images sourced\nfrom diverse visual datasets. Subsequently, iterative RL training further\nenhance reasoning skills, with each iteration's RL-improved model generating\nrefined SFT datasets for the next round. This iterative process yielded\nOpenVLThinker, a LVLM exhibiting consistently improved reasoning performance on\nchallenging benchmarks such as MathVista, MathVerse, and MathVision,\ndemonstrating the potential of our strategy for robust vision-language\nreasoning. The code, model and data are held at\nhttps://github.com/yihedeng9/OpenVLThinker.",
      "upvotes": 2,
      "discussionId": "67e0bccae5fa0da84e121079",
      "projectPage": "https://yihe-deng.notion.site/openvlthinker",
      "githubRepo": "https://github.com/yihedeng9/OpenVLThinker",
      "ai_keywords": [
        "Reinforcement Learning (RL)",
        "verifiable rewards",
        "large language models (LLMs)",
        "self-verification",
        "self-correction",
        "large vision-language models (LVLMs)",
        "multimodal reasoning tasks",
        "supervised fine-tuning (SFT)",
        "lightweight training data",
        "reasoning steps",
        "high-quality captions",
        "diversity",
        "visual datasets",
        "iterative process",
        "OpenVLThinker",
        "reasoning performance",
        "challenging benchmarks",
        "MathVista",
        "MathVerse",
        "MathVision",
        "robust vision-language reasoning"
      ]
    },
    "publishedAt": "2025-03-21T13:52:43.000Z",
    "title": "OpenVLThinker: An Early Exploration to Complex Vision-Language Reasoning\n  via Iterative Self-Improvement",
    "summary": "Recent advancements demonstrated by DeepSeek-R1 have shown that complex\nreasoning abilities in large language models (LLMs), including sophisticated\nbehaviors such as self-verification and self-correction, can be achieved by RL\nwith verifiable rewards and significantly improves model performance on\nchallenging tasks such as AIME. Motivated by these findings, our study\ninvestigates whether similar reasoning capabilities can be successfully\nintegrated into large vision-language models (LVLMs) and assesses their impact\non challenging multimodal reasoning tasks. We consider an approach that\niteratively leverages supervised fine-tuning (SFT) on lightweight training data\nand Reinforcement Learning (RL) to further improve model generalization.\nInitially, reasoning capabilities were distilled from pure-text R1 models by\ngenerating reasoning steps using high-quality captions of the images sourced\nfrom diverse visual datasets. Subsequently, iterative RL training further\nenhance reasoning skills, with each iteration's RL-improved model generating\nrefined SFT datasets for the next round. This iterative process yielded\nOpenVLThinker, a LVLM exhibiting consistently improved reasoning performance on\nchallenging benchmarks such as MathVista, MathVerse, and MathVision,\ndemonstrating the potential of our strategy for robust vision-language\nreasoning. The code, model and data are held at\nhttps://github.com/yihedeng9/OpenVLThinker.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.17352.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "642f4c789b2484d7d8551a93",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/642f4c789b2484d7d8551a93/0lH4YXcbZa-Xlzj6ESo7F.jpeg",
      "fullname": "Yihe Deng",
      "name": "ydeng9",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 8
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.16921",
      "authors": [
        {
          "_id": "67e0bb1665e294ad989334ea",
          "name": "Lingfan Zhang",
          "hidden": false
        },
        {
          "_id": "67e0bb1665e294ad989334eb",
          "name": "Chen Liu",
          "hidden": false
        },
        {
          "_id": "67e0bb1665e294ad989334ec",
          "name": "Chengming Xu",
          "hidden": false
        },
        {
          "_id": "67e0bb1665e294ad989334ed",
          "name": "Kai Hu",
          "hidden": false
        },
        {
          "_id": "67e0bb1665e294ad989334ee",
          "name": "Donghao Luo",
          "hidden": false
        },
        {
          "_id": "67e0bb1665e294ad989334ef",
          "name": "Chengjie Wang",
          "hidden": false
        },
        {
          "_id": "67e0bb1665e294ad989334f0",
          "name": "Yanwei Fu",
          "hidden": false
        },
        {
          "_id": "67e0bb1665e294ad989334f1",
          "name": "Yuan Yao",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-21T07:33:44.000Z",
      "submittedOnDailyAt": "2025-03-24T00:24:41.729Z",
      "title": "When Preferences Diverge: Aligning Diffusion Models with Minority-Aware\n  Adaptive DPO",
      "submittedOnDailyBy": {
        "_id": "652fab9d04a34a9282bf29d6",
        "avatarUrl": "/avatars/cd5967b37ebb1225e9ae1d46f196e2e2.svg",
        "isPro": false,
        "fullname": "Chengming Xu",
        "user": "ChengmingX",
        "type": "user"
      },
      "summary": "In recent years, the field of image generation has witnessed significant\nadvancements, particularly in fine-tuning methods that align models with\nuniversal human preferences. This paper explores the critical role of\npreference data in the training process of diffusion models, particularly in\nthe context of Diffusion-DPO and its subsequent adaptations. We investigate the\ncomplexities surrounding universal human preferences in image generation,\nhighlighting the subjective nature of these preferences and the challenges\nposed by minority samples in preference datasets. Through pilot experiments, we\ndemonstrate the existence of minority samples and their detrimental effects on\nmodel performance. We propose Adaptive-DPO -- a novel approach that\nincorporates a minority-instance-aware metric into the DPO objective. This\nmetric, which includes intra-annotator confidence and inter-annotator\nstability, distinguishes between majority and minority samples. We introduce an\nAdaptive-DPO loss function which improves the DPO loss in two ways: enhancing\nthe model's learning of majority labels while mitigating the negative impact of\nminority samples. Our experiments demonstrate that this method effectively\nhandles both synthetic minority data and real-world preference data, paving the\nway for more effective training methodologies in image generation tasks.",
      "upvotes": 1,
      "discussionId": "67e0bb1a65e294ad9893361c",
      "ai_keywords": [
        "diffusion models",
        "Diffusion-DPO",
        "adaptive-DPO",
        "intra-annotator confidence",
        "inter-annotator stability",
        "DPO objective",
        "Adaptive-DPO loss function"
      ]
    },
    "publishedAt": "2025-03-21T03:33:44.000Z",
    "title": "When Preferences Diverge: Aligning Diffusion Models with Minority-Aware\n  Adaptive DPO",
    "summary": "In recent years, the field of image generation has witnessed significant\nadvancements, particularly in fine-tuning methods that align models with\nuniversal human preferences. This paper explores the critical role of\npreference data in the training process of diffusion models, particularly in\nthe context of Diffusion-DPO and its subsequent adaptations. We investigate the\ncomplexities surrounding universal human preferences in image generation,\nhighlighting the subjective nature of these preferences and the challenges\nposed by minority samples in preference datasets. Through pilot experiments, we\ndemonstrate the existence of minority samples and their detrimental effects on\nmodel performance. We propose Adaptive-DPO -- a novel approach that\nincorporates a minority-instance-aware metric into the DPO objective. This\nmetric, which includes intra-annotator confidence and inter-annotator\nstability, distinguishes between majority and minority samples. We introduce an\nAdaptive-DPO loss function which improves the DPO loss in two ways: enhancing\nthe model's learning of majority labels while mitigating the negative impact of\nminority samples. Our experiments demonstrate that this method effectively\nhandles both synthetic minority data and real-world preference data, paving the\nway for more effective training methodologies in image generation tasks.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.16921.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "652fab9d04a34a9282bf29d6",
      "avatarUrl": "/avatars/cd5967b37ebb1225e9ae1d46f196e2e2.svg",
      "fullname": "Chengming Xu",
      "name": "ChengmingX",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.17287",
      "authors": [
        {
          "_id": "67e0bfcc8fb92b0edaa78dc0",
          "name": "Mingyang Song",
          "hidden": false
        },
        {
          "_id": "67e0bfcc8fb92b0edaa78dc1",
          "name": "Mao Zheng",
          "hidden": false
        },
        {
          "_id": "67e0bfcc8fb92b0edaa78dc2",
          "name": "Zheng Li",
          "hidden": false
        },
        {
          "_id": "67e0bfcc8fb92b0edaa78dc3",
          "name": "Wenjie Yang",
          "hidden": false
        },
        {
          "_id": "67e0bfcc8fb92b0edaa78dc4",
          "name": "Xuan Luo",
          "hidden": false
        },
        {
          "_id": "67e0bfcc8fb92b0edaa78dc5",
          "name": "Yue Pan",
          "hidden": false
        },
        {
          "_id": "67e0bfcc8fb92b0edaa78dc6",
          "name": "Feng Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-21T16:35:31.000Z",
      "submittedOnDailyAt": "2025-03-24T00:43:41.116Z",
      "title": "FastCuRL: Curriculum Reinforcement Learning with Progressive Context\n  Extension for Efficient Training R1-like Reasoning Models",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": false,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "In this paper, we propose \\textsc{FastCuRL}, a simple yet efficient\nCurriculum Reinforcement Learning approach with\ncontext window extending strategy to accelerate the reinforcement learning\ntraining efficiency for R1-like reasoning models while enhancing their\nperformance in tackling complex reasoning tasks with long chain-of-thought\nrationales, particularly with a 1.5B parameter language model.\n\\textsc{FastCuRL} consists of two main procedures: length-aware\ntraining data segmentation and context window extension training. Specifically,\nthe former first splits the original training data into three different levels\nby the input prompt length, and then the latter leverages segmented training\ndatasets with a progressively increasing context window length to train the\nreasoning model. Experimental results demonstrate that\n\\textsc{FastCuRL}-1.5B-Preview surpasses DeepScaleR-1.5B-Preview\nacross all five datasets (including MATH 500, AIME 2024, AMC 2023, Minerva\nMath, and OlympiadBench) while only utilizing 50\\% of training steps.\nFurthermore, all training stages for FastCuRL-1.5B-Preview are completed using\njust a single node with 8 GPUs.",
      "upvotes": 0,
      "discussionId": "67e0bfcd8fb92b0edaa78e17",
      "githubRepo": "https://github.com/nick7nlp/FastCuRL",
      "ai_keywords": [
        "Curriculum Reinforcement Learning",
        "context window",
        "reinforcement learning",
        "training efficiency",
        "R1-like reasoning models",
        "long chain-of-thought",
        "rationales",
        "length-aware training",
        "training data segmentation",
        "context window extension",
        "progressively increasing context window length",
        "DeepScaleR-1.5B-Preview",
        "MATH 500",
        "AIME 2024",
        "AMC 2023",
        "Minerva Math",
        "OlympiadBench",
        "training steps"
      ]
    },
    "publishedAt": "2025-03-21T12:35:31.000Z",
    "title": "FastCuRL: Curriculum Reinforcement Learning with Progressive Context\n  Extension for Efficient Training R1-like Reasoning Models",
    "summary": "In this paper, we propose \\textsc{FastCuRL}, a simple yet efficient\nCurriculum Reinforcement Learning approach with\ncontext window extending strategy to accelerate the reinforcement learning\ntraining efficiency for R1-like reasoning models while enhancing their\nperformance in tackling complex reasoning tasks with long chain-of-thought\nrationales, particularly with a 1.5B parameter language model.\n\\textsc{FastCuRL} consists of two main procedures: length-aware\ntraining data segmentation and context window extension training. Specifically,\nthe former first splits the original training data into three different levels\nby the input prompt length, and then the latter leverages segmented training\ndatasets with a progressively increasing context window length to train the\nreasoning model. Experimental results demonstrate that\n\\textsc{FastCuRL}-1.5B-Preview surpasses DeepScaleR-1.5B-Preview\nacross all five datasets (including MATH 500, AIME 2024, AMC 2023, Minerva\nMath, and OlympiadBench) while only utilizing 50\\% of training steps.\nFurthermore, all training stages for FastCuRL-1.5B-Preview are completed using\njust a single node with 8 GPUs.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.17287.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isHfAdmin": true,
      "isMod": false,
      "followerCount": 6438
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.17126",
      "authors": [
        {
          "_id": "67e0beb474fc794321fb4ad7",
          "name": "John Joon Young Chung",
          "hidden": false
        },
        {
          "_id": "67e0beb474fc794321fb4ad8",
          "name": "Vishakh Padmakumar",
          "hidden": false
        },
        {
          "_id": "67e0beb474fc794321fb4ad9",
          "name": "Melissa Roemmele",
          "hidden": false
        },
        {
          "_id": "67e0beb474fc794321fb4ada",
          "name": "Yuqian Sun",
          "hidden": false
        },
        {
          "_id": "67e0beb474fc794321fb4adb",
          "name": "Max Kreminski",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-21T13:21:45.000Z",
      "submittedOnDailyAt": "2025-03-24T00:39:24.717Z",
      "title": "Modifying Large Language Model Post-Training for Diverse Creative\n  Writing",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": false,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "As creative writing tasks do not have singular correct answers, large\nlanguage models (LLMs) trained to perform these tasks should be able to\ngenerate diverse valid outputs. However, LLM post-training often focuses on\nimproving generation quality but neglects to facilitate output diversity.\nHence, in creative writing generation, we investigate post-training approaches\nto promote both output diversity and quality. Our core idea is to include\ndeviation -- the degree of difference between a training sample and all other\nsamples with the same prompt -- in the training objective to facilitate\nlearning from rare high-quality instances. By adopting our approach to direct\npreference optimization (DPO) and odds ratio preference optimization (ORPO), we\ndemonstrate that we can promote the output diversity of trained models while\nminimally decreasing quality. Our best model with 8B parameters could achieve\non-par diversity as a human-created dataset while having output quality similar\nto the best instruction-tuned models we examined, GPT-4o and DeepSeek-R1. We\nfurther validate our approaches with a human evaluation, an ablation, and a\ncomparison to an existing diversification approach, DivPO.",
      "upvotes": 0,
      "discussionId": "67e0beb574fc794321fb4b04",
      "ai_keywords": [
        "direct preference optimization (DPO)",
        "odds ratio preference optimization (ORPO)",
        "parameter-efficient fine-tuning"
      ]
    },
    "publishedAt": "2025-03-21T09:21:45.000Z",
    "title": "Modifying Large Language Model Post-Training for Diverse Creative\n  Writing",
    "summary": "As creative writing tasks do not have singular correct answers, large\nlanguage models (LLMs) trained to perform these tasks should be able to\ngenerate diverse valid outputs. However, LLM post-training often focuses on\nimproving generation quality but neglects to facilitate output diversity.\nHence, in creative writing generation, we investigate post-training approaches\nto promote both output diversity and quality. Our core idea is to include\ndeviation -- the degree of difference between a training sample and all other\nsamples with the same prompt -- in the training objective to facilitate\nlearning from rare high-quality instances. By adopting our approach to direct\npreference optimization (DPO) and odds ratio preference optimization (ORPO), we\ndemonstrate that we can promote the output diversity of trained models while\nminimally decreasing quality. Our best model with 8B parameters could achieve\non-par diversity as a human-created dataset while having output quality similar\nto the best instruction-tuned models we examined, GPT-4o and DeepSeek-R1. We\nfurther validate our approaches with a human evaluation, an ablation, and a\ncomparison to an existing diversification approach, DivPO.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.17126.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isHfAdmin": true,
      "isMod": false,
      "followerCount": 6438
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.16905",
      "authors": [
        {
          "_id": "67e0c13fe5fa0da84e134581",
          "name": "Jian Zhang",
          "hidden": false
        },
        {
          "_id": "67e0c13fe5fa0da84e134582",
          "name": "Zhiyuan Wang",
          "hidden": false
        },
        {
          "_id": "67e0c13fe5fa0da84e134583",
          "name": "Zhangqi Wang",
          "hidden": false
        },
        {
          "_id": "67e0c13fe5fa0da84e134584",
          "name": "Xinyu Zhang",
          "hidden": false
        },
        {
          "_id": "67e0c13fe5fa0da84e134585",
          "name": "Fangzhi Xu",
          "hidden": false
        },
        {
          "_id": "67e0c13fe5fa0da84e134586",
          "user": {
            "_id": "66ac77011cfb12c087605acb",
            "avatarUrl": "/avatars/54c06bd1c4c9d491470ed4162c2301ae.svg",
            "isPro": false,
            "fullname": "Lin",
            "user": "Qika",
            "type": "user"
          },
          "name": "Qika Lin",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-03-24T02:19:51.913Z",
          "hidden": false
        },
        {
          "_id": "67e0c13fe5fa0da84e134587",
          "name": "Rui Mao",
          "hidden": false
        },
        {
          "_id": "67e0c13fe5fa0da84e134588",
          "name": "Erik Cambria",
          "hidden": false
        },
        {
          "_id": "67e0c13fe5fa0da84e134589",
          "name": "Jun Liu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-21T07:13:45.000Z",
      "submittedOnDailyAt": "2025-03-24T00:51:41.644Z",
      "title": "MAPS: A Multi-Agent Framework Based on Big Seven Personality and\n  Socratic Guidance for Multimodal Scientific Problem Solving",
      "submittedOnDailyBy": {
        "_id": "658be7fe135580745c510323",
        "avatarUrl": "/avatars/830e5cec4565efdc23226a86a0fcef0e.svg",
        "isPro": false,
        "fullname": "Jian Zhang",
        "user": "VentureZJ",
        "type": "user"
      },
      "summary": "Multimodal scientific problems (MSPs) involve complex issues that require the\nintegration of multiple modalities, such as text and diagrams, presenting a\nsignificant challenge in artificial intelligence. While progress has been made\nin addressing traditional scientific problems, MSPs still face two primary\nissues: the challenge of multi-modal comprehensive reasoning in scientific\nproblem-solving and the lack of reflective and rethinking capabilities. To\naddress these issues, we introduce a Multi-Agent framework based on the Big\nSeven Personality and Socratic guidance (MAPS). This framework employs seven\ndistinct agents that leverage feedback mechanisms and the Socratic method to\nguide the resolution of MSPs. To tackle the first issue, we propose a\nprogressive four-agent solving strategy, where each agent focuses on a specific\nstage of the problem-solving process. For the second issue, we introduce a\nCritic agent, inspired by Socratic questioning, which prompts critical thinking\nand stimulates autonomous learning. We conduct extensive experiments on the\nEMMA, Olympiad, and MathVista datasets, achieving promising results that\noutperform the current SOTA model by 15.84% across all tasks. Meanwhile, the\nadditional analytical experiments also verify the model's progress as well as\ngeneralization ability.",
      "upvotes": 0,
      "discussionId": "67e0c147e5fa0da84e1347f5"
    },
    "publishedAt": "2025-03-21T03:13:45.000Z",
    "title": "MAPS: A Multi-Agent Framework Based on Big Seven Personality and\n  Socratic Guidance for Multimodal Scientific Problem Solving",
    "summary": "Multimodal scientific problems (MSPs) involve complex issues that require the\nintegration of multiple modalities, such as text and diagrams, presenting a\nsignificant challenge in artificial intelligence. While progress has been made\nin addressing traditional scientific problems, MSPs still face two primary\nissues: the challenge of multi-modal comprehensive reasoning in scientific\nproblem-solving and the lack of reflective and rethinking capabilities. To\naddress these issues, we introduce a Multi-Agent framework based on the Big\nSeven Personality and Socratic guidance (MAPS). This framework employs seven\ndistinct agents that leverage feedback mechanisms and the Socratic method to\nguide the resolution of MSPs. To tackle the first issue, we propose a\nprogressive four-agent solving strategy, where each agent focuses on a specific\nstage of the problem-solving process. For the second issue, we introduce a\nCritic agent, inspired by Socratic questioning, which prompts critical thinking\nand stimulates autonomous learning. We conduct extensive experiments on the\nEMMA, Olympiad, and MathVista datasets, achieving promising results that\noutperform the current SOTA model by 15.84% across all tasks. Meanwhile, the\nadditional analytical experiments also verify the model's progress as well as\ngeneralization ability.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.16905.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "658be7fe135580745c510323",
      "avatarUrl": "/avatars/830e5cec4565efdc23226a86a0fcef0e.svg",
      "fullname": "Jian Zhang",
      "name": "VentureZJ",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  }
]