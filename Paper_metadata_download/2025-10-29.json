[
  {
    "paper": {
      "id": "2510.24320",
      "authors": [
        {
          "_id": "69017878646208eac0d1f3aa",
          "name": "Zhiheng Xi",
          "hidden": false
        },
        {
          "_id": "69017878646208eac0d1f3ab",
          "name": "Jixuan Huang",
          "hidden": false
        },
        {
          "_id": "69017878646208eac0d1f3ac",
          "name": "Xin Guo",
          "hidden": false
        },
        {
          "_id": "69017878646208eac0d1f3ad",
          "name": "Boyang Hong",
          "hidden": false
        },
        {
          "_id": "69017878646208eac0d1f3ae",
          "name": "Dingwen Yang",
          "hidden": false
        },
        {
          "_id": "69017878646208eac0d1f3af",
          "name": "Xiaoran Fan",
          "hidden": false
        },
        {
          "_id": "69017878646208eac0d1f3b0",
          "name": "Shuo Li",
          "hidden": false
        },
        {
          "_id": "69017878646208eac0d1f3b1",
          "name": "Zehui Chen",
          "hidden": false
        },
        {
          "_id": "69017878646208eac0d1f3b2",
          "name": "Junjie Ye",
          "hidden": false
        },
        {
          "_id": "69017878646208eac0d1f3b3",
          "name": "Siyu Yuan",
          "hidden": false
        },
        {
          "_id": "69017878646208eac0d1f3b4",
          "name": "Zhengyin Du",
          "hidden": false
        },
        {
          "_id": "69017878646208eac0d1f3b5",
          "name": "Xuesong Yao",
          "hidden": false
        },
        {
          "_id": "69017878646208eac0d1f3b6",
          "name": "Yufei Xu",
          "hidden": false
        },
        {
          "_id": "69017878646208eac0d1f3b7",
          "name": "Jiecao Chen",
          "hidden": false
        },
        {
          "_id": "69017878646208eac0d1f3b8",
          "name": "Rui Zheng",
          "hidden": false
        },
        {
          "_id": "69017878646208eac0d1f3b9",
          "name": "Tao Gui",
          "hidden": false
        },
        {
          "_id": "69017878646208eac0d1f3ba",
          "name": "Qi Zhang",
          "hidden": false
        },
        {
          "_id": "69017878646208eac0d1f3bb",
          "name": "Xuanjing Huang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-28T11:37:01.000Z",
      "submittedOnDailyAt": "2025-10-29T00:47:25.750Z",
      "title": "Critique-RL: Training Language Models for Critiquing through Two-Stage\n  Reinforcement Learning",
      "submittedOnDailyBy": {
        "_id": "653a6e5cae155b92bae77b74",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/653a6e5cae155b92bae77b74/TA5FWKAUsB249ux4MzD_R.jpeg",
        "isPro": false,
        "fullname": "Zhiheng Xi",
        "user": "WooooDyy",
        "type": "user"
      },
      "summary": "Training critiquing language models to assess and provide feedback on model\noutputs is a promising way to improve LLMs for complex reasoning tasks.\nHowever, existing approaches typically rely on stronger supervisors for\nannotating critique data. To address this, we propose Critique-RL, an online RL\napproach for developing critiquing language models without stronger\nsupervision. Our approach operates on a two-player paradigm: the actor\ngenerates a response, the critic provides feedback, and the actor refines the\nresponse accordingly. We first reveal that relying solely on indirect reward\nsignals from the actor's outputs for RL optimization often leads to\nunsatisfactory critics: while their helpfulness (i.e., providing constructive\nfeedback) improves, the discriminability (i.e., determining whether a response\nis high-quality or not) remains poor, resulting in marginal performance gains.\nTo overcome this, Critique-RL adopts a two-stage optimization strategy. In\nstage I, it reinforces the discriminability of the critic with direct\nrule-based reward signals; in stage II, it introduces indirect rewards based on\nactor refinement to improve the critic's helpfulness, while maintaining its\ndiscriminability via appropriate regularization. Extensive experiments across\nvarious tasks and models show that Critique-RL delivers substantial performance\nimprovements. For example, it achieves a 9.02% gain on in-domain tasks and a\n5.70% gain on out-of-domain tasks for Qwen2.5-7B, highlighting its potential.",
      "upvotes": 10,
      "discussionId": "69017878646208eac0d1f3bc",
      "githubRepo": "https://github.com/WooooDyy/Critique-RL",
      "ai_summary": "Critique-RL is an online reinforcement learning approach for developing critiquing language models without strong supervision, using a two-stage optimization strategy to improve both the critic's discriminability and helpfulness.",
      "ai_keywords": [
        "reinforcement learning",
        "RL",
        "actor",
        "critic",
        "indirect reward signals",
        "direct rule-based reward signals",
        "discriminability",
        "helpfulness",
        "Qwen2.5-7B"
      ],
      "organization": {
        "_id": "6447a7db3e7b3c11be6205f1",
        "name": "FudanNLP",
        "fullname": "Fudan NLP Lab",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6447a74f6ffed6ece1fd0288/6P6gS4tCPu3idUj7RSrq1.jpeg"
      }
    },
    "publishedAt": "2025-10-28T07:37:01.000Z",
    "title": "Critique-RL: Training Language Models for Critiquing through Two-Stage\n  Reinforcement Learning",
    "summary": "Training critiquing language models to assess and provide feedback on model\noutputs is a promising way to improve LLMs for complex reasoning tasks.\nHowever, existing approaches typically rely on stronger supervisors for\nannotating critique data. To address this, we propose Critique-RL, an online RL\napproach for developing critiquing language models without stronger\nsupervision. Our approach operates on a two-player paradigm: the actor\ngenerates a response, the critic provides feedback, and the actor refines the\nresponse accordingly. We first reveal that relying solely on indirect reward\nsignals from the actor's outputs for RL optimization often leads to\nunsatisfactory critics: while their helpfulness (i.e., providing constructive\nfeedback) improves, the discriminability (i.e., determining whether a response\nis high-quality or not) remains poor, resulting in marginal performance gains.\nTo overcome this, Critique-RL adopts a two-stage optimization strategy. In\nstage I, it reinforces the discriminability of the critic with direct\nrule-based reward signals; in stage II, it introduces indirect rewards based on\nactor refinement to improve the critic's helpfulness, while maintaining its\ndiscriminability via appropriate regularization. Extensive experiments across\nvarious tasks and models show that Critique-RL delivers substantial performance\nimprovements. For example, it achieves a 9.02% gain on in-domain tasks and a\n5.70% gain on out-of-domain tasks for Qwen2.5-7B, highlighting its potential.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.24320.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "653a6e5cae155b92bae77b74",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/653a6e5cae155b92bae77b74/TA5FWKAUsB249ux4MzD_R.jpeg",
      "fullname": "Zhiheng Xi",
      "name": "WooooDyy",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "organization": {
      "_id": "6447a7db3e7b3c11be6205f1",
      "name": "FudanNLP",
      "fullname": "Fudan NLP Lab",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6447a74f6ffed6ece1fd0288/6P6gS4tCPu3idUj7RSrq1.jpeg"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.24701",
      "authors": [
        {
          "_id": "69017244646208eac0d1f30e",
          "name": "Tongyi DeepResearch Team",
          "hidden": false
        },
        {
          "_id": "69017244646208eac0d1f30f",
          "name": "Baixuan Li",
          "hidden": false
        },
        {
          "_id": "69017244646208eac0d1f310",
          "name": "Bo Zhang",
          "hidden": false
        },
        {
          "_id": "69017244646208eac0d1f311",
          "name": "Dingchu Zhang",
          "hidden": false
        },
        {
          "_id": "69017244646208eac0d1f312",
          "name": "Fei Huang",
          "hidden": false
        },
        {
          "_id": "69017244646208eac0d1f313",
          "name": "Guangyu Li",
          "hidden": false
        },
        {
          "_id": "69017244646208eac0d1f314",
          "name": "Guoxin Chen",
          "hidden": false
        },
        {
          "_id": "69017244646208eac0d1f315",
          "name": "Huifeng Yin",
          "hidden": false
        },
        {
          "_id": "69017244646208eac0d1f316",
          "name": "Jialong Wu",
          "hidden": false
        },
        {
          "_id": "69017244646208eac0d1f317",
          "name": "Jingren Zhou",
          "hidden": false
        },
        {
          "_id": "69017244646208eac0d1f318",
          "name": "Kuan Li",
          "hidden": false
        },
        {
          "_id": "69017244646208eac0d1f319",
          "name": "Liangcai Su",
          "hidden": false
        },
        {
          "_id": "69017244646208eac0d1f31a",
          "name": "Litu Ou",
          "hidden": false
        },
        {
          "_id": "69017244646208eac0d1f31b",
          "name": "Liwen Zhang",
          "hidden": false
        },
        {
          "_id": "69017244646208eac0d1f31c",
          "name": "Pengjun Xie",
          "hidden": false
        },
        {
          "_id": "69017244646208eac0d1f31d",
          "name": "Rui Ye",
          "hidden": false
        },
        {
          "_id": "69017244646208eac0d1f31e",
          "name": "Wenbiao Yin",
          "hidden": false
        },
        {
          "_id": "69017244646208eac0d1f31f",
          "name": "Xinmiao Yu",
          "hidden": false
        },
        {
          "_id": "69017244646208eac0d1f320",
          "name": "Xinyu Wang",
          "hidden": false
        },
        {
          "_id": "69017244646208eac0d1f321",
          "name": "Xixi Wu",
          "hidden": false
        },
        {
          "_id": "69017244646208eac0d1f322",
          "name": "Xuanzhong Chen",
          "hidden": false
        },
        {
          "_id": "69017244646208eac0d1f323",
          "name": "Yida Zhao",
          "hidden": false
        },
        {
          "_id": "69017244646208eac0d1f324",
          "name": "Zhen Zhang",
          "hidden": false
        },
        {
          "_id": "69017244646208eac0d1f325",
          "name": "Zhengwei Tao",
          "hidden": false
        },
        {
          "_id": "69017244646208eac0d1f326",
          "name": "Zhongwang Zhang",
          "hidden": false
        },
        {
          "_id": "69017244646208eac0d1f327",
          "name": "Zile Qiao",
          "hidden": false
        },
        {
          "_id": "69017244646208eac0d1f328",
          "name": "Chenxi Wang",
          "hidden": false
        },
        {
          "_id": "69017244646208eac0d1f329",
          "name": "Donglei Yu",
          "hidden": false
        },
        {
          "_id": "69017244646208eac0d1f32a",
          "name": "Gang Fu",
          "hidden": false
        },
        {
          "_id": "69017244646208eac0d1f32b",
          "name": "Haiyang Shen",
          "hidden": false
        },
        {
          "_id": "69017244646208eac0d1f32c",
          "name": "Jiayin Yang",
          "hidden": false
        },
        {
          "_id": "69017244646208eac0d1f32d",
          "name": "Jun Lin",
          "hidden": false
        },
        {
          "_id": "69017244646208eac0d1f32e",
          "name": "Junkai Zhang",
          "hidden": false
        },
        {
          "_id": "69017244646208eac0d1f32f",
          "name": "Kui Zeng",
          "hidden": false
        },
        {
          "_id": "69017244646208eac0d1f330",
          "name": "Li Yang",
          "hidden": false
        },
        {
          "_id": "69017244646208eac0d1f331",
          "name": "Hailong Yin",
          "hidden": false
        },
        {
          "_id": "69017244646208eac0d1f332",
          "name": "Maojia Song",
          "hidden": false
        },
        {
          "_id": "69017244646208eac0d1f333",
          "name": "Ming Yan",
          "hidden": false
        },
        {
          "_id": "69017244646208eac0d1f334",
          "name": "Peng Xia",
          "hidden": false
        },
        {
          "_id": "69017244646208eac0d1f335",
          "name": "Qian Xiao",
          "hidden": false
        },
        {
          "_id": "69017244646208eac0d1f336",
          "name": "Rui Min",
          "hidden": false
        },
        {
          "_id": "69017244646208eac0d1f337",
          "name": "Ruixue Ding",
          "hidden": false
        },
        {
          "_id": "69017244646208eac0d1f338",
          "name": "Runnan Fang",
          "hidden": false
        },
        {
          "_id": "69017244646208eac0d1f339",
          "name": "Shaowei Chen",
          "hidden": false
        },
        {
          "_id": "69017244646208eac0d1f33a",
          "name": "Shen Huang",
          "hidden": false
        },
        {
          "_id": "69017244646208eac0d1f33b",
          "name": "Shihang Wang",
          "hidden": false
        },
        {
          "_id": "69017244646208eac0d1f33c",
          "name": "Shihao Cai",
          "hidden": false
        },
        {
          "_id": "69017244646208eac0d1f33d",
          "name": "Weizhou Shen",
          "hidden": false
        },
        {
          "_id": "69017244646208eac0d1f33e",
          "name": "Xiaobin Wang",
          "hidden": false
        },
        {
          "_id": "69017244646208eac0d1f33f",
          "name": "Xin Guan",
          "hidden": false
        },
        {
          "_id": "69017244646208eac0d1f340",
          "name": "Xinyu Geng",
          "hidden": false
        },
        {
          "_id": "69017244646208eac0d1f341",
          "name": "Yingcheng Shi",
          "hidden": false
        },
        {
          "_id": "69017244646208eac0d1f342",
          "name": "Yuning Wu",
          "hidden": false
        },
        {
          "_id": "69017244646208eac0d1f343",
          "name": "Zhuo Chen",
          "hidden": false
        },
        {
          "_id": "69017244646208eac0d1f344",
          "name": "Zijian Li",
          "hidden": false
        },
        {
          "_id": "69017244646208eac0d1f345",
          "name": "Yong Jiang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-28T17:53:02.000Z",
      "submittedOnDailyAt": "2025-10-29T00:18:14.015Z",
      "title": "Tongyi DeepResearch Technical Report",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "We present Tongyi DeepResearch, an agentic large language model, which is\nspecifically designed for long-horizon, deep information-seeking research\ntasks. To incentivize autonomous deep research agency, Tongyi DeepResearch is\ndeveloped through an end-to-end training framework that combines agentic\nmid-training and agentic post-training, enabling scalable reasoning and\ninformation seeking across complex tasks. We design a highly scalable data\nsynthesis pipeline that is fully automatic, without relying on costly human\nannotation, and empowers all training stages. By constructing customized\nenvironments for each stage, our system enables stable and consistent\ninteractions throughout. Tongyi DeepResearch, featuring 30.5 billion total\nparameters, with only 3.3 billion activated per token, achieves\nstate-of-the-art performance across a range of agentic deep research\nbenchmarks, including Humanity's Last Exam, BrowseComp, BrowseComp-ZH,\nWebWalkerQA, xbench-DeepSearch, FRAMES and xbench-DeepSearch-2510. We\nopen-source the model, framework, and complete solutions to empower the\ncommunity.",
      "upvotes": 7,
      "discussionId": "69017244646208eac0d1f346",
      "projectPage": "https://tongyi-agent.github.io/blog/introducing-tongyi-deep-research/",
      "ai_summary": "Tongyi DeepResearch, a large language model with agentic capabilities, achieves top performance in various deep research tasks through an end-to-end training framework and automated data synthesis.",
      "ai_keywords": [
        "agentic large language model",
        "end-to-end training framework",
        "agentic mid-training",
        "agentic post-training",
        "scalable reasoning",
        "information seeking",
        "data synthesis pipeline",
        "customized environments",
        "Humanity's Last Exam",
        "BrowseComp",
        "BrowseComp-ZH",
        "WebWalkerQA",
        "xbench-DeepSearch",
        "FRAMES",
        "xbench-DeepSearch-2510"
      ],
      "organization": {
        "_id": "67d15cca6e2cf0e062dbfb54",
        "name": "AlibabaTongyiLab",
        "fullname": "TongyiLab",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67d1502bfabfe9974d1f77bb/XdUSVf6HqBzE7zFBfSDQP.png"
      }
    },
    "publishedAt": "2025-10-28T13:53:02.000Z",
    "title": "Tongyi DeepResearch Technical Report",
    "summary": "We present Tongyi DeepResearch, an agentic large language model, which is\nspecifically designed for long-horizon, deep information-seeking research\ntasks. To incentivize autonomous deep research agency, Tongyi DeepResearch is\ndeveloped through an end-to-end training framework that combines agentic\nmid-training and agentic post-training, enabling scalable reasoning and\ninformation seeking across complex tasks. We design a highly scalable data\nsynthesis pipeline that is fully automatic, without relying on costly human\nannotation, and empowers all training stages. By constructing customized\nenvironments for each stage, our system enables stable and consistent\ninteractions throughout. Tongyi DeepResearch, featuring 30.5 billion total\nparameters, with only 3.3 billion activated per token, achieves\nstate-of-the-art performance across a range of agentic deep research\nbenchmarks, including Humanity's Last Exam, BrowseComp, BrowseComp-ZH,\nWebWalkerQA, xbench-DeepSearch, FRAMES and xbench-DeepSearch-2510. We\nopen-source the model, framework, and complete solutions to empower the\ncommunity.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.24701.png",
    "numComments": 3,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 148
    },
    "organization": {
      "_id": "67d15cca6e2cf0e062dbfb54",
      "name": "AlibabaTongyiLab",
      "fullname": "TongyiLab",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67d1502bfabfe9974d1f77bb/XdUSVf6HqBzE7zFBfSDQP.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.24514",
      "authors": [
        {
          "_id": "690173b7646208eac0d1f36c",
          "name": "Huanyu Zhang",
          "hidden": false
        },
        {
          "_id": "690173b7646208eac0d1f36d",
          "name": "Wenshan Wu",
          "hidden": false
        },
        {
          "_id": "690173b7646208eac0d1f36e",
          "name": "Chengzu Li",
          "hidden": false
        },
        {
          "_id": "690173b7646208eac0d1f36f",
          "name": "Ning Shang",
          "hidden": false
        },
        {
          "_id": "690173b7646208eac0d1f370",
          "name": "Yan Xia",
          "hidden": false
        },
        {
          "_id": "690173b7646208eac0d1f371",
          "name": "Yangyu Huang",
          "hidden": false
        },
        {
          "_id": "690173b7646208eac0d1f372",
          "name": "Yifan Zhang",
          "hidden": false
        },
        {
          "_id": "690173b7646208eac0d1f373",
          "name": "Li Dong",
          "hidden": false
        },
        {
          "_id": "690173b7646208eac0d1f374",
          "name": "Zhang Zhang",
          "hidden": false
        },
        {
          "_id": "690173b7646208eac0d1f375",
          "name": "Liang Wang",
          "hidden": false
        },
        {
          "_id": "690173b7646208eac0d1f376",
          "name": "Tieniu Tan",
          "hidden": false
        },
        {
          "_id": "690173b7646208eac0d1f377",
          "name": "Furu Wei",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-28T15:26:20.000Z",
      "submittedOnDailyAt": "2025-10-29T01:05:14.260Z",
      "title": "Latent Sketchpad: Sketching Visual Thoughts to Elicit Multimodal\n  Reasoning in MLLMs",
      "submittedOnDailyBy": {
        "_id": "65e816bbcfd12cd15b052a0e",
        "avatarUrl": "/avatars/4d92da469afdba8cd7dc645b98236011.svg",
        "isPro": false,
        "fullname": "Huanyu_Zhang",
        "user": "huanyu112",
        "type": "user"
      },
      "summary": "While Multimodal Large Language Models (MLLMs) excel at visual understanding,\nthey often struggle in complex scenarios that require visual planning and\nimagination. Inspired by how humans use sketching as a form of visual thinking\nto develop and communicate ideas, we introduce Latent Sketchpad, a framework\nthat equips MLLMs with an internal visual scratchpad. The internal visual\nrepresentations of MLLMs have traditionally been confined to perceptual\nunderstanding. We repurpose them to support generative visual thought without\ncompromising reasoning ability. Building on frontier MLLMs, our approach\nintegrates visual generation directly into their native autoregressive\nreasoning process. It allows the model to interleave textual reasoning with the\ngeneration of visual latents. These latents guide the internal thought process\nand can be translated into sketch images for interpretability. To realize this,\nwe introduce two components: a Context-Aware Vision Head autoregressively\nproduces visual representations, and a pretrained Sketch Decoder renders these\ninto human-interpretable images. We evaluate the framework on our new dataset\nMazePlanning. Experiments across various MLLMs show that Latent Sketchpad\ndelivers comparable or even superior reasoning performance to their backbone.\nIt further generalizes across distinct frontier MLLMs, including Gemma3 and\nQwen2.5-VL. By extending model's textual reasoning to visual thinking, our\nframework opens new opportunities for richer human-computer interaction and\nbroader applications. More details and resources are available on our project\npage: https://latent-sketchpad.github.io/.",
      "upvotes": 5,
      "discussionId": "690173b8646208eac0d1f378",
      "projectPage": "https://latent-sketchpad.github.io/",
      "githubRepo": "https://github.com/hwanyu112/Latent-Sketchpad",
      "ai_summary": "Latent Sketchpad enhances Multimodal Large Language Models with an internal visual scratchpad, enabling generative visual thought and improved reasoning performance.",
      "ai_keywords": [
        "Multimodal Large Language Models",
        "Latent Sketchpad",
        "visual planning",
        "imagination",
        "sketching",
        "visual thinking",
        "internal visual scratchpad",
        "perceptual understanding",
        "generative visual thought",
        "autoregressive reasoning",
        "visual latents",
        "sketch images",
        "Context-Aware Vision Head",
        "Sketch Decoder",
        "MazePlanning",
        "Gemma3",
        "Qwen2.5-VL",
        "human-computer interaction"
      ],
      "organization": {
        "_id": "5e6485f787403103f9f1055e",
        "name": "microsoft",
        "fullname": "Microsoft",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1583646260758-5e64858c87403103f9f1055d.png"
      }
    },
    "publishedAt": "2025-10-28T11:26:20.000Z",
    "title": "Latent Sketchpad: Sketching Visual Thoughts to Elicit Multimodal\n  Reasoning in MLLMs",
    "summary": "While Multimodal Large Language Models (MLLMs) excel at visual understanding,\nthey often struggle in complex scenarios that require visual planning and\nimagination. Inspired by how humans use sketching as a form of visual thinking\nto develop and communicate ideas, we introduce Latent Sketchpad, a framework\nthat equips MLLMs with an internal visual scratchpad. The internal visual\nrepresentations of MLLMs have traditionally been confined to perceptual\nunderstanding. We repurpose them to support generative visual thought without\ncompromising reasoning ability. Building on frontier MLLMs, our approach\nintegrates visual generation directly into their native autoregressive\nreasoning process. It allows the model to interleave textual reasoning with the\ngeneration of visual latents. These latents guide the internal thought process\nand can be translated into sketch images for interpretability. To realize this,\nwe introduce two components: a Context-Aware Vision Head autoregressively\nproduces visual representations, and a pretrained Sketch Decoder renders these\ninto human-interpretable images. We evaluate the framework on our new dataset\nMazePlanning. Experiments across various MLLMs show that Latent Sketchpad\ndelivers comparable or even superior reasoning performance to their backbone.\nIt further generalizes across distinct frontier MLLMs, including Gemma3 and\nQwen2.5-VL. By extending model's textual reasoning to visual thinking, our\nframework opens new opportunities for richer human-computer interaction and\nbroader applications. More details and resources are available on our project\npage: https://latent-sketchpad.github.io/.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.24514.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65e816bbcfd12cd15b052a0e",
      "avatarUrl": "/avatars/4d92da469afdba8cd7dc645b98236011.svg",
      "fullname": "Huanyu_Zhang",
      "name": "huanyu112",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "organization": {
      "_id": "5e6485f787403103f9f1055e",
      "name": "microsoft",
      "fullname": "Microsoft",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1583646260758-5e64858c87403103f9f1055d.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.24698",
      "authors": [
        {
          "_id": "69017d30646208eac0d1f3e7",
          "name": "Baixuan Li",
          "hidden": false
        },
        {
          "_id": "69017d30646208eac0d1f3e8",
          "name": "Dingchu Zhang",
          "hidden": false
        },
        {
          "_id": "69017d30646208eac0d1f3e9",
          "name": "Jialong Wu",
          "hidden": false
        },
        {
          "_id": "69017d30646208eac0d1f3ea",
          "name": "Wenbiao Yin",
          "hidden": false
        },
        {
          "_id": "69017d30646208eac0d1f3eb",
          "name": "Zhengwei Tao",
          "hidden": false
        },
        {
          "_id": "69017d30646208eac0d1f3ec",
          "name": "Yida Zhao",
          "hidden": false
        },
        {
          "_id": "69017d30646208eac0d1f3ed",
          "name": "Liwen Zhang",
          "hidden": false
        },
        {
          "_id": "69017d30646208eac0d1f3ee",
          "name": "Haiyang Shen",
          "hidden": false
        },
        {
          "_id": "69017d30646208eac0d1f3ef",
          "name": "Runnan Fang",
          "hidden": false
        },
        {
          "_id": "69017d30646208eac0d1f3f0",
          "name": "Pengjun Xie",
          "hidden": false
        },
        {
          "_id": "69017d30646208eac0d1f3f1",
          "name": "Jingren Zhou",
          "hidden": false
        },
        {
          "_id": "69017d30646208eac0d1f3f2",
          "name": "Yong Jiang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-28T17:51:50.000Z",
      "submittedOnDailyAt": "2025-10-29T01:04:44.983Z",
      "title": "ParallelMuse: Agentic Parallel Thinking for Deep Information Seeking",
      "submittedOnDailyBy": {
        "_id": "644a4fbc2166258fccc664bc",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/8k3b44MbhQiWuo6i8BnYl.jpeg",
        "isPro": false,
        "fullname": "Jialong Wu",
        "user": "callanwu",
        "type": "user"
      },
      "summary": "Parallel thinking expands exploration breadth, complementing the deep\nexploration of information-seeking (IS) agents to further enhance\nproblem-solving capability. However, conventional parallel thinking faces two\nkey challenges in this setting: inefficiency from repeatedly rolling out from\nscratch, and difficulty in integrating long-horizon reasoning trajectories\nduring answer generation, as limited context capacity prevents full\nconsideration of the reasoning process. To address these issues, we propose\nParallelMuse, a two-stage paradigm designed for deep IS agents. The first\nstage, Functionality-Specified Partial Rollout, partitions generated sequences\ninto functional regions and performs uncertainty-guided path reuse and\nbranching to enhance exploration efficiency. The second stage, Compressed\nReasoning Aggregation, exploits reasoning redundancy to losslessly compress\ninformation relevant to answer derivation and synthesize a coherent final\nanswer. Experiments across multiple open-source agents and benchmarks\ndemonstrate up to 62% performance improvement with a 10--30% reduction in\nexploratory token consumption.",
      "upvotes": 2,
      "discussionId": "69017d31646208eac0d1f3f3",
      "ai_summary": "ParallelMuse enhances problem-solving by efficiently reusing paths and compressing reasoning in deep information-seeking agents, improving performance and reducing token consumption.",
      "ai_keywords": [
        "ParallelMuse",
        "Functionality-Specified Partial Rollout",
        "uncertainty-guided path reuse",
        "branch",
        "Compressed Reasoning Aggregation",
        "reasoning redundancy",
        "information compression",
        "exploratory token consumption"
      ],
      "organization": {
        "_id": "67d15cca6e2cf0e062dbfb54",
        "name": "AlibabaTongyiLab",
        "fullname": "TongyiLab",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67d1502bfabfe9974d1f77bb/XdUSVf6HqBzE7zFBfSDQP.png"
      }
    },
    "publishedAt": "2025-10-28T13:51:50.000Z",
    "title": "ParallelMuse: Agentic Parallel Thinking for Deep Information Seeking",
    "summary": "Parallel thinking expands exploration breadth, complementing the deep\nexploration of information-seeking (IS) agents to further enhance\nproblem-solving capability. However, conventional parallel thinking faces two\nkey challenges in this setting: inefficiency from repeatedly rolling out from\nscratch, and difficulty in integrating long-horizon reasoning trajectories\nduring answer generation, as limited context capacity prevents full\nconsideration of the reasoning process. To address these issues, we propose\nParallelMuse, a two-stage paradigm designed for deep IS agents. The first\nstage, Functionality-Specified Partial Rollout, partitions generated sequences\ninto functional regions and performs uncertainty-guided path reuse and\nbranching to enhance exploration efficiency. The second stage, Compressed\nReasoning Aggregation, exploits reasoning redundancy to losslessly compress\ninformation relevant to answer derivation and synthesize a coherent final\nanswer. Experiments across multiple open-source agents and benchmarks\ndemonstrate up to 62% performance improvement with a 10--30% reduction in\nexploratory token consumption.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.24698.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "644a4fbc2166258fccc664bc",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/8k3b44MbhQiWuo6i8BnYl.jpeg",
      "fullname": "Jialong Wu",
      "name": "callanwu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 25
    },
    "organization": {
      "_id": "67d15cca6e2cf0e062dbfb54",
      "name": "AlibabaTongyiLab",
      "fullname": "TongyiLab",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67d1502bfabfe9974d1f77bb/XdUSVf6HqBzE7zFBfSDQP.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.24699",
      "authors": [
        {
          "_id": "69017ce1646208eac0d1f3d6",
          "name": "Rui Ye",
          "hidden": false
        },
        {
          "_id": "69017ce1646208eac0d1f3d7",
          "name": "Zhongwang Zhang",
          "hidden": false
        },
        {
          "_id": "69017ce1646208eac0d1f3d8",
          "name": "Kuan Li",
          "hidden": false
        },
        {
          "_id": "69017ce1646208eac0d1f3d9",
          "name": "Huifeng Yin",
          "hidden": false
        },
        {
          "_id": "69017ce1646208eac0d1f3da",
          "name": "Zhengwei Tao",
          "hidden": false
        },
        {
          "_id": "69017ce1646208eac0d1f3db",
          "name": "Yida Zhao",
          "hidden": false
        },
        {
          "_id": "69017ce1646208eac0d1f3dc",
          "name": "Liangcai Su",
          "hidden": false
        },
        {
          "_id": "69017ce1646208eac0d1f3dd",
          "name": "Liwen Zhang",
          "hidden": false
        },
        {
          "_id": "69017ce1646208eac0d1f3de",
          "name": "Zile Qiao",
          "hidden": false
        },
        {
          "_id": "69017ce1646208eac0d1f3df",
          "name": "Xinyu Wang",
          "hidden": false
        },
        {
          "_id": "69017ce1646208eac0d1f3e0",
          "name": "Pengjun Xie",
          "hidden": false
        },
        {
          "_id": "69017ce1646208eac0d1f3e1",
          "name": "Fei Huang",
          "hidden": false
        },
        {
          "_id": "69017ce1646208eac0d1f3e2",
          "name": "Siheng Chen",
          "hidden": false
        },
        {
          "_id": "69017ce1646208eac0d1f3e3",
          "name": "Jingren Zhou",
          "hidden": false
        },
        {
          "_id": "69017ce1646208eac0d1f3e4",
          "name": "Yong Jiang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-28T17:51:50.000Z",
      "submittedOnDailyAt": "2025-10-29T01:03:50.549Z",
      "title": "AgentFold: Long-Horizon Web Agents with Proactive Context Management",
      "submittedOnDailyBy": {
        "_id": "644a4fbc2166258fccc664bc",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/8k3b44MbhQiWuo6i8BnYl.jpeg",
        "isPro": false,
        "fullname": "Jialong Wu",
        "user": "callanwu",
        "type": "user"
      },
      "summary": "LLM-based web agents show immense promise for information seeking, yet their\neffectiveness on long-horizon tasks is hindered by a fundamental trade-off in\ncontext management. Prevailing ReAct-based agents suffer from context\nsaturation as they accumulate noisy, raw histories, while methods that fixedly\nsummarize the full history at each step risk the irreversible loss of critical\ndetails. Addressing these, we introduce AgentFold, a novel agent paradigm\ncentered on proactive context management, inspired by the human cognitive\nprocess of retrospective consolidation. AgentFold treats its context as a\ndynamic cognitive workspace to be actively sculpted, rather than a passive log\nto be filled. At each step, it learns to execute a `folding' operation, which\nmanages its historical trajectory at multiple scales: it can perform granular\ncondensations to preserve vital, fine-grained details, or deep consolidations\nto abstract away entire multi-step sub-tasks. The results on prominent\nbenchmarks are striking: with simple supervised fine-tuning (without continual\npre-training or RL), our AgentFold-30B-A3B agent achieves 36.2% on BrowseComp\nand 47.3% on BrowseComp-ZH. Notably, this performance not only surpasses or\nmatches open-source models of a dramatically larger scale, such as the\nDeepSeek-V3.1-671B-A37B, but also surpasses leading proprietary agents like\nOpenAI's o4-mini.",
      "upvotes": 2,
      "discussionId": "69017ce2646208eac0d1f3e5",
      "ai_summary": "AgentFold, a novel proactive context management paradigm for LLM-based web agents, achieves superior performance on long-horizon tasks through dynamic context folding, surpassing larger models and proprietary agents.",
      "ai_keywords": [
        "LLM-based web agents",
        "ReAct-based agents",
        "context saturation",
        "context management",
        "cognitive workspace",
        "folding operation",
        "granular condensations",
        "deep consolidations",
        "BrowseComp",
        "BrowseComp-ZH",
        "DeepSeek-V3.1-671B-A37B",
        "OpenAI's o4-mini"
      ],
      "organization": {
        "_id": "67d15cca6e2cf0e062dbfb54",
        "name": "AlibabaTongyiLab",
        "fullname": "TongyiLab",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67d1502bfabfe9974d1f77bb/XdUSVf6HqBzE7zFBfSDQP.png"
      }
    },
    "publishedAt": "2025-10-28T13:51:50.000Z",
    "title": "AgentFold: Long-Horizon Web Agents with Proactive Context Management",
    "summary": "LLM-based web agents show immense promise for information seeking, yet their\neffectiveness on long-horizon tasks is hindered by a fundamental trade-off in\ncontext management. Prevailing ReAct-based agents suffer from context\nsaturation as they accumulate noisy, raw histories, while methods that fixedly\nsummarize the full history at each step risk the irreversible loss of critical\ndetails. Addressing these, we introduce AgentFold, a novel agent paradigm\ncentered on proactive context management, inspired by the human cognitive\nprocess of retrospective consolidation. AgentFold treats its context as a\ndynamic cognitive workspace to be actively sculpted, rather than a passive log\nto be filled. At each step, it learns to execute a `folding' operation, which\nmanages its historical trajectory at multiple scales: it can perform granular\ncondensations to preserve vital, fine-grained details, or deep consolidations\nto abstract away entire multi-step sub-tasks. The results on prominent\nbenchmarks are striking: with simple supervised fine-tuning (without continual\npre-training or RL), our AgentFold-30B-A3B agent achieves 36.2% on BrowseComp\nand 47.3% on BrowseComp-ZH. Notably, this performance not only surpasses or\nmatches open-source models of a dramatically larger scale, such as the\nDeepSeek-V3.1-671B-A37B, but also surpasses leading proprietary agents like\nOpenAI's o4-mini.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.24699.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "644a4fbc2166258fccc664bc",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/8k3b44MbhQiWuo6i8BnYl.jpeg",
      "fullname": "Jialong Wu",
      "name": "callanwu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 25
    },
    "organization": {
      "_id": "67d15cca6e2cf0e062dbfb54",
      "name": "AlibabaTongyiLab",
      "fullname": "TongyiLab",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67d1502bfabfe9974d1f77bb/XdUSVf6HqBzE7zFBfSDQP.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.24697",
      "authors": [
        {
          "_id": "69017d55646208eac0d1f400",
          "name": "Zhengwei Tao",
          "hidden": false
        },
        {
          "_id": "69017d55646208eac0d1f401",
          "name": "Haiyang Shen",
          "hidden": false
        },
        {
          "_id": "69017d55646208eac0d1f402",
          "name": "Baixuan Li",
          "hidden": false
        },
        {
          "_id": "69017d55646208eac0d1f403",
          "name": "Wenbiao Yin",
          "hidden": false
        },
        {
          "_id": "69017d55646208eac0d1f404",
          "name": "Jialong Wu",
          "hidden": false
        },
        {
          "_id": "69017d55646208eac0d1f405",
          "name": "Kuan Li",
          "hidden": false
        },
        {
          "_id": "69017d55646208eac0d1f406",
          "name": "Zhongwang Zhang",
          "hidden": false
        },
        {
          "_id": "69017d55646208eac0d1f407",
          "name": "Huifeng Yin",
          "hidden": false
        },
        {
          "_id": "69017d55646208eac0d1f408",
          "name": "Rui Ye",
          "hidden": false
        },
        {
          "_id": "69017d55646208eac0d1f409",
          "name": "Liwen Zhang",
          "hidden": false
        },
        {
          "_id": "69017d55646208eac0d1f40a",
          "name": "Xinyu Wang",
          "hidden": false
        },
        {
          "_id": "69017d55646208eac0d1f40b",
          "name": "Pengjun Xie",
          "hidden": false
        },
        {
          "_id": "69017d55646208eac0d1f40c",
          "name": "Jingren Zhou",
          "hidden": false
        },
        {
          "_id": "69017d55646208eac0d1f40d",
          "name": "Yong Jiang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-28T17:51:42.000Z",
      "submittedOnDailyAt": "2025-10-29T01:05:29.846Z",
      "title": "WebLeaper: Empowering Efficiency and Efficacy in WebAgent via Enabling\n  Info-Rich Seeking",
      "submittedOnDailyBy": {
        "_id": "644a4fbc2166258fccc664bc",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/8k3b44MbhQiWuo6i8BnYl.jpeg",
        "isPro": false,
        "fullname": "Jialong Wu",
        "user": "callanwu",
        "type": "user"
      },
      "summary": "Large Language Model (LLM)-based agents have emerged as a transformative\napproach for open-ended problem solving, with information seeking (IS) being a\ncore capability that enables autonomous reasoning and decision-making. While\nprior research has largely focused on improving retrieval depth, we observe\nthat current IS agents often suffer from low search efficiency, which in turn\nconstrains overall performance. A key factor underlying this inefficiency is\nthe sparsity of target entities in training tasks, which limits opportunities\nfor agents to learn and generalize efficient search behaviors. To address these\nchallenges, we propose WebLeaper, a framework for constructing high-coverage IS\ntasks and generating efficient solution trajectories. We formulate IS as a\ntree-structured reasoning problem, enabling a substantially larger set of\ntarget entities to be embedded within a constrained context. Leveraging curated\nWikipedia tables, we propose three variants for synthesizing IS tasks, Basic,\nUnion, and Reverse-Union, to systematically increase both IS efficiency and\nefficacy. Finally, we curate training trajectories by retaining only those that\nare simultaneously accurate and efficient, ensuring that the model is optimized\nfor both correctness and search performance. Extensive experiments on both\nbasic and comprehensive settings, conducted on five IS benchmarks, BrowserComp,\nGAIA, xbench-DeepSearch, WideSearch, and Seal-0, demonstrate that our method\nconsistently achieves improvements in both effectiveness and efficiency over\nstrong baselines.",
      "upvotes": 2,
      "discussionId": "69017d55646208eac0d1f40e",
      "ai_summary": "WebLeaper framework improves information seeking efficiency and effectiveness by constructing high-coverage tasks and generating efficient solution trajectories using tree-structured reasoning and curated Wikipedia tables.",
      "ai_keywords": [
        "Large Language Model (LLM)",
        "information seeking (IS)",
        "search efficiency",
        "target entities",
        "tree-structured reasoning",
        "WebLeaper",
        "Basic",
        "Union",
        "Reverse-Union",
        "training trajectories",
        "BrowserComp",
        "GAIA",
        "xbench-DeepSearch",
        "WideSearch",
        "Seal-0"
      ],
      "organization": {
        "_id": "67d15cca6e2cf0e062dbfb54",
        "name": "AlibabaTongyiLab",
        "fullname": "TongyiLab",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67d1502bfabfe9974d1f77bb/XdUSVf6HqBzE7zFBfSDQP.png"
      }
    },
    "publishedAt": "2025-10-28T13:51:42.000Z",
    "title": "WebLeaper: Empowering Efficiency and Efficacy in WebAgent via Enabling\n  Info-Rich Seeking",
    "summary": "Large Language Model (LLM)-based agents have emerged as a transformative\napproach for open-ended problem solving, with information seeking (IS) being a\ncore capability that enables autonomous reasoning and decision-making. While\nprior research has largely focused on improving retrieval depth, we observe\nthat current IS agents often suffer from low search efficiency, which in turn\nconstrains overall performance. A key factor underlying this inefficiency is\nthe sparsity of target entities in training tasks, which limits opportunities\nfor agents to learn and generalize efficient search behaviors. To address these\nchallenges, we propose WebLeaper, a framework for constructing high-coverage IS\ntasks and generating efficient solution trajectories. We formulate IS as a\ntree-structured reasoning problem, enabling a substantially larger set of\ntarget entities to be embedded within a constrained context. Leveraging curated\nWikipedia tables, we propose three variants for synthesizing IS tasks, Basic,\nUnion, and Reverse-Union, to systematically increase both IS efficiency and\nefficacy. Finally, we curate training trajectories by retaining only those that\nare simultaneously accurate and efficient, ensuring that the model is optimized\nfor both correctness and search performance. Extensive experiments on both\nbasic and comprehensive settings, conducted on five IS benchmarks, BrowserComp,\nGAIA, xbench-DeepSearch, WideSearch, and Seal-0, demonstrate that our method\nconsistently achieves improvements in both effectiveness and efficiency over\nstrong baselines.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.24697.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "644a4fbc2166258fccc664bc",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/8k3b44MbhQiWuo6i8BnYl.jpeg",
      "fullname": "Jialong Wu",
      "name": "callanwu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 25
    },
    "organization": {
      "_id": "67d15cca6e2cf0e062dbfb54",
      "name": "AlibabaTongyiLab",
      "fullname": "TongyiLab",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67d1502bfabfe9974d1f77bb/XdUSVf6HqBzE7zFBfSDQP.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.23763",
      "authors": [
        {
          "_id": "690168ee646208eac0d1f2fb",
          "name": "Siyin Wang",
          "hidden": false
        },
        {
          "_id": "690168ee646208eac0d1f2fc",
          "name": "Jinlan Fu",
          "hidden": false
        },
        {
          "_id": "690168ee646208eac0d1f2fd",
          "name": "Feihong Liu",
          "hidden": false
        },
        {
          "_id": "690168ee646208eac0d1f2fe",
          "name": "Xinzhe He",
          "hidden": false
        },
        {
          "_id": "690168ee646208eac0d1f2ff",
          "name": "Huangxuan Wu",
          "hidden": false
        },
        {
          "_id": "690168ee646208eac0d1f300",
          "name": "Junhao Shi",
          "hidden": false
        },
        {
          "_id": "690168ee646208eac0d1f301",
          "name": "Kexin Huang",
          "hidden": false
        },
        {
          "_id": "690168ee646208eac0d1f302",
          "name": "Zhaoye Fei",
          "hidden": false
        },
        {
          "_id": "690168ee646208eac0d1f303",
          "name": "Jingjing Gong",
          "hidden": false
        },
        {
          "_id": "690168ee646208eac0d1f304",
          "name": "Zuxuan Wu",
          "hidden": false
        },
        {
          "_id": "690168ee646208eac0d1f305",
          "name": "Yugang Jiang",
          "hidden": false
        },
        {
          "_id": "690168ee646208eac0d1f306",
          "name": "See-Kiong Ng",
          "hidden": false
        },
        {
          "_id": "690168ee646208eac0d1f307",
          "name": "Tat-Seng Chua",
          "hidden": false
        },
        {
          "_id": "690168ee646208eac0d1f308",
          "name": "Xipeng Qiu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-27T18:49:03.000Z",
      "submittedOnDailyAt": "2025-10-29T01:05:28.984Z",
      "title": "RoboOmni: Proactive Robot Manipulation in Omni-modal Context",
      "submittedOnDailyBy": {
        "_id": "64c3c631e77ea9f28111172a",
        "avatarUrl": "/avatars/495dbb73b69c399bae780da3118e332f.svg",
        "isPro": false,
        "fullname": "Siyin Wang (SII)",
        "user": "sinwang",
        "type": "user"
      },
      "summary": "Recent advances in Multimodal Large Language Models (MLLMs) have driven rapid\nprogress in Vision-Language-Action (VLA) models for robotic manipulation.\nAlthough effective in many scenarios, current approaches largely rely on\nexplicit instructions, whereas in real-world interactions, humans rarely issue\ninstructions directly. Effective collaboration requires robots to infer user\nintentions proactively. In this work, we introduce cross-modal contextual\ninstructions, a new setting where intent is derived from spoken dialogue,\nenvironmental sounds, and visual cues rather than explicit commands. To address\nthis new setting, we present RoboOmni, a Perceiver-Thinker-Talker-Executor\nframework based on end-to-end omni-modal LLMs that unifies intention\nrecognition, interaction confirmation, and action execution. RoboOmni fuses\nauditory and visual signals spatiotemporally for robust intention recognition,\nwhile supporting direct speech interaction. To address the absence of training\ndata for proactive intention recognition in robotic manipulation, we build\nOmniAction, comprising 140k episodes, 5k+ speakers, 2.4k event sounds, 640\nbackgrounds, and six contextual instruction types. Experiments in simulation\nand real-world settings show that RoboOmni surpasses text- and ASR-based\nbaselines in success rate, inference speed, intention recognition, and\nproactive assistance.",
      "upvotes": 2,
      "discussionId": "690168ee646208eac0d1f309",
      "projectPage": "https://OpenMOSS.github.io/RoboOmni",
      "githubRepo": "https://github.com/OpenMOSS/RoboOmni",
      "ai_summary": "RoboOmni, a Perceiver-Thinker-Talker-Executor framework using end-to-end omni-modal LLMs, improves robotic manipulation by inferring user intentions from spoken dialogue, environmental sounds, and visual cues.",
      "ai_keywords": [
        "Multimodal Large Language Models",
        "Vision-Language-Action models",
        "cross-modal contextual instructions",
        "Perceiver-Thinker-Talker-Executor",
        "end-to-end omni-modal LLMs",
        "intention recognition",
        "interaction confirmation",
        "action execution",
        "spatiotemporal fusion",
        "OmniAction",
        "proactive intention recognition",
        "text-based baselines",
        "ASR-based baselines"
      ],
      "organization": {
        "_id": "613b0dee83ec35d460684607",
        "name": "fnlp",
        "fullname": "OpenMOSS (SII, Fudan NLP)",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61457b8deff2c9fdb4de4988/xM_PjniEZ9fmDKtJN7PAG.png"
      }
    },
    "publishedAt": "2025-10-27T14:49:03.000Z",
    "title": "RoboOmni: Proactive Robot Manipulation in Omni-modal Context",
    "summary": "Recent advances in Multimodal Large Language Models (MLLMs) have driven rapid\nprogress in Vision-Language-Action (VLA) models for robotic manipulation.\nAlthough effective in many scenarios, current approaches largely rely on\nexplicit instructions, whereas in real-world interactions, humans rarely issue\ninstructions directly. Effective collaboration requires robots to infer user\nintentions proactively. In this work, we introduce cross-modal contextual\ninstructions, a new setting where intent is derived from spoken dialogue,\nenvironmental sounds, and visual cues rather than explicit commands. To address\nthis new setting, we present RoboOmni, a Perceiver-Thinker-Talker-Executor\nframework based on end-to-end omni-modal LLMs that unifies intention\nrecognition, interaction confirmation, and action execution. RoboOmni fuses\nauditory and visual signals spatiotemporally for robust intention recognition,\nwhile supporting direct speech interaction. To address the absence of training\ndata for proactive intention recognition in robotic manipulation, we build\nOmniAction, comprising 140k episodes, 5k+ speakers, 2.4k event sounds, 640\nbackgrounds, and six contextual instruction types. Experiments in simulation\nand real-world settings show that RoboOmni surpasses text- and ASR-based\nbaselines in success rate, inference speed, intention recognition, and\nproactive assistance.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.23763.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64c3c631e77ea9f28111172a",
      "avatarUrl": "/avatars/495dbb73b69c399bae780da3118e332f.svg",
      "fullname": "Siyin Wang (SII)",
      "name": "sinwang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "organization": {
      "_id": "613b0dee83ec35d460684607",
      "name": "fnlp",
      "fullname": "OpenMOSS (SII, Fudan NLP)",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61457b8deff2c9fdb4de4988/xM_PjniEZ9fmDKtJN7PAG.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.24695",
      "authors": [
        {
          "_id": "69017da8646208eac0d1f419",
          "name": "Xuanzhong Chen",
          "hidden": false
        },
        {
          "_id": "69017da8646208eac0d1f41a",
          "name": "Zile Qiao",
          "hidden": false
        },
        {
          "_id": "69017da8646208eac0d1f41b",
          "name": "Guoxin Chen",
          "hidden": false
        },
        {
          "_id": "69017da8646208eac0d1f41c",
          "name": "Liangcai Su",
          "hidden": false
        },
        {
          "_id": "69017da8646208eac0d1f41d",
          "name": "Zhen Zhang",
          "hidden": false
        },
        {
          "_id": "69017da8646208eac0d1f41e",
          "name": "Xinyu Wang",
          "hidden": false
        },
        {
          "_id": "69017da8646208eac0d1f41f",
          "name": "Pengjun Xie",
          "hidden": false
        },
        {
          "_id": "69017da8646208eac0d1f420",
          "name": "Fei Huang",
          "hidden": false
        },
        {
          "_id": "69017da8646208eac0d1f421",
          "name": "Jingren Zhou",
          "hidden": false
        },
        {
          "_id": "69017da8646208eac0d1f422",
          "name": "Yong Jiang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-28T17:50:47.000Z",
      "submittedOnDailyAt": "2025-10-29T01:07:28.731Z",
      "title": "AgentFrontier: Expanding the Capability Frontier of LLM Agents with\n  ZPD-Guided Data Synthesis",
      "submittedOnDailyBy": {
        "_id": "644a4fbc2166258fccc664bc",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/8k3b44MbhQiWuo6i8BnYl.jpeg",
        "isPro": false,
        "fullname": "Jialong Wu",
        "user": "callanwu",
        "type": "user"
      },
      "summary": "Training large language model agents on tasks at the frontier of their\ncapabilities is key to unlocking advanced reasoning. We introduce a data\nsynthesis approach inspired by the educational theory of the Zone of Proximal\nDevelopment (ZPD), which defines this frontier as tasks an LLM cannot solve\nalone but can master with guidance. To operationalize this, we present the\nAgentFrontier Engine, an automated pipeline that synthesizes high-quality,\nmultidisciplinary data situated precisely within the LLM's ZPD. This engine\nsupports both continued pre-training with knowledge-intensive data and targeted\npost-training on complex reasoning tasks. From the same framework, we derive\nthe ZPD Exam, a dynamic and automated benchmark designed to evaluate agent\ncapabilities on these frontier tasks. We train AgentFrontier-30B-A3B model on\nour synthesized data, which achieves state-of-the-art results on demanding\nbenchmarks like Humanity's Last Exam, even surpassing some leading proprietary\nagents. Our work demonstrates that a ZPD-guided approach to data synthesis\noffers a scalable and effective path toward building more capable LLM agents.",
      "upvotes": 1,
      "discussionId": "69017da8646208eac0d1f423",
      "ai_summary": "A ZPD-guided data synthesis approach enhances large language model capabilities by training them on tasks just beyond their current abilities, leading to state-of-the-art performance on complex benchmarks.",
      "ai_keywords": [
        "Zone of Proximal Development (ZPD)",
        "AgentFrontier Engine",
        "high-quality",
        "multidisciplinary data",
        "continued pre-training",
        "targeted post-training",
        "ZPD Exam",
        "dynamic benchmark",
        "complex reasoning tasks",
        "AgentFrontier-30B-A3B model",
        "Humanity's Last Exam"
      ],
      "organization": {
        "_id": "67d15cca6e2cf0e062dbfb54",
        "name": "AlibabaTongyiLab",
        "fullname": "TongyiLab",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67d1502bfabfe9974d1f77bb/XdUSVf6HqBzE7zFBfSDQP.png"
      }
    },
    "publishedAt": "2025-10-28T13:50:47.000Z",
    "title": "AgentFrontier: Expanding the Capability Frontier of LLM Agents with\n  ZPD-Guided Data Synthesis",
    "summary": "Training large language model agents on tasks at the frontier of their\ncapabilities is key to unlocking advanced reasoning. We introduce a data\nsynthesis approach inspired by the educational theory of the Zone of Proximal\nDevelopment (ZPD), which defines this frontier as tasks an LLM cannot solve\nalone but can master with guidance. To operationalize this, we present the\nAgentFrontier Engine, an automated pipeline that synthesizes high-quality,\nmultidisciplinary data situated precisely within the LLM's ZPD. This engine\nsupports both continued pre-training with knowledge-intensive data and targeted\npost-training on complex reasoning tasks. From the same framework, we derive\nthe ZPD Exam, a dynamic and automated benchmark designed to evaluate agent\ncapabilities on these frontier tasks. We train AgentFrontier-30B-A3B model on\nour synthesized data, which achieves state-of-the-art results on demanding\nbenchmarks like Humanity's Last Exam, even surpassing some leading proprietary\nagents. Our work demonstrates that a ZPD-guided approach to data synthesis\noffers a scalable and effective path toward building more capable LLM agents.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.24695.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "644a4fbc2166258fccc664bc",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/8k3b44MbhQiWuo6i8BnYl.jpeg",
      "fullname": "Jialong Wu",
      "name": "callanwu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 25
    },
    "organization": {
      "_id": "67d15cca6e2cf0e062dbfb54",
      "name": "AlibabaTongyiLab",
      "fullname": "TongyiLab",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67d1502bfabfe9974d1f77bb/XdUSVf6HqBzE7zFBfSDQP.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.24694",
      "authors": [
        {
          "_id": "69017e04646208eac0d1f42e",
          "name": "Yida Zhao",
          "hidden": false
        },
        {
          "_id": "69017e04646208eac0d1f42f",
          "name": "Kuan Li",
          "hidden": false
        },
        {
          "_id": "69017e04646208eac0d1f430",
          "name": "Xixi Wu",
          "hidden": false
        },
        {
          "_id": "69017e04646208eac0d1f431",
          "name": "Liwen Zhang",
          "hidden": false
        },
        {
          "_id": "69017e04646208eac0d1f432",
          "name": "Dingchu Zhang",
          "hidden": false
        },
        {
          "_id": "69017e04646208eac0d1f433",
          "name": "Baixuan Li",
          "hidden": false
        },
        {
          "_id": "69017e04646208eac0d1f434",
          "name": "Maojia Song",
          "hidden": false
        },
        {
          "_id": "69017e04646208eac0d1f435",
          "name": "Zhuo Chen",
          "hidden": false
        },
        {
          "_id": "69017e04646208eac0d1f436",
          "name": "Chenxi Wang",
          "hidden": false
        },
        {
          "_id": "69017e04646208eac0d1f437",
          "name": "Xinyu Wang",
          "hidden": false
        },
        {
          "_id": "69017e04646208eac0d1f438",
          "name": "Kewei Tu",
          "hidden": false
        },
        {
          "_id": "69017e04646208eac0d1f439",
          "name": "Pengjun Xie",
          "hidden": false
        },
        {
          "_id": "69017e04646208eac0d1f43a",
          "name": "Jingren Zhou",
          "hidden": false
        },
        {
          "_id": "69017e04646208eac0d1f43b",
          "name": "Yong Jiang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-28T17:50:40.000Z",
      "submittedOnDailyAt": "2025-10-29T01:08:19.441Z",
      "title": "Repurposing Synthetic Data for Fine-grained Search Agent Supervision",
      "submittedOnDailyBy": {
        "_id": "644a4fbc2166258fccc664bc",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/8k3b44MbhQiWuo6i8BnYl.jpeg",
        "isPro": false,
        "fullname": "Jialong Wu",
        "user": "callanwu",
        "type": "user"
      },
      "summary": "LLM-based search agents are increasingly trained on entity-centric synthetic\ndata to solve complex, knowledge-intensive tasks. However, prevailing training\nmethods like Group Relative Policy Optimization (GRPO) discard this rich entity\ninformation, relying instead on sparse, outcome-based rewards. This critical\nlimitation renders them unable to distinguish informative \"near-miss\"\nsamples-those with substantially correct reasoning but a flawed final\nanswer-from complete failures, thus discarding valuable learning signals. We\naddress this by leveraging the very entities discarded during training. Our\nempirical analysis reveals a strong positive correlation between the number of\nground-truth entities identified during an agent's reasoning process and final\nanswer accuracy. Building on this insight, we introduce Entity-aware Group\nRelative Policy Optimization (E-GRPO), a novel framework that formulates a\ndense entity-aware reward function. E-GRPO assigns partial rewards to incorrect\nsamples proportional to their entity match rate, enabling the model to\neffectively learn from these \"near-misses\". Experiments on diverse\nquestion-answering (QA) and deep research benchmarks show that E-GRPO\nconsistently and significantly outperforms the GRPO baseline. Furthermore, our\nanalysis reveals that E-GRPO not only achieves superior accuracy but also\ninduces more efficient reasoning policies that require fewer tool calls,\ndemonstrating a more effective and sample-efficient approach to aligning search\nagents.",
      "upvotes": 1,
      "discussionId": "69017e04646208eac0d1f43c",
      "ai_summary": "Entity-aware Group Relative Policy Optimization (E-GRPO) enhances search agents by incorporating entity information into the reward function, improving accuracy and efficiency in knowledge-intensive tasks.",
      "ai_keywords": [
        "Group Relative Policy Optimization (GRPO)",
        "Entity-aware Group Relative Policy Optimization (E-GRPO)",
        "entity-centric synthetic data",
        "entity match rate",
        "near-misses",
        "question-answering (QA)",
        "deep research benchmarks",
        "tool calls"
      ],
      "organization": {
        "_id": "67d15cca6e2cf0e062dbfb54",
        "name": "AlibabaTongyiLab",
        "fullname": "TongyiLab",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67d1502bfabfe9974d1f77bb/XdUSVf6HqBzE7zFBfSDQP.png"
      }
    },
    "publishedAt": "2025-10-28T13:50:40.000Z",
    "title": "Repurposing Synthetic Data for Fine-grained Search Agent Supervision",
    "summary": "LLM-based search agents are increasingly trained on entity-centric synthetic\ndata to solve complex, knowledge-intensive tasks. However, prevailing training\nmethods like Group Relative Policy Optimization (GRPO) discard this rich entity\ninformation, relying instead on sparse, outcome-based rewards. This critical\nlimitation renders them unable to distinguish informative \"near-miss\"\nsamples-those with substantially correct reasoning but a flawed final\nanswer-from complete failures, thus discarding valuable learning signals. We\naddress this by leveraging the very entities discarded during training. Our\nempirical analysis reveals a strong positive correlation between the number of\nground-truth entities identified during an agent's reasoning process and final\nanswer accuracy. Building on this insight, we introduce Entity-aware Group\nRelative Policy Optimization (E-GRPO), a novel framework that formulates a\ndense entity-aware reward function. E-GRPO assigns partial rewards to incorrect\nsamples proportional to their entity match rate, enabling the model to\neffectively learn from these \"near-misses\". Experiments on diverse\nquestion-answering (QA) and deep research benchmarks show that E-GRPO\nconsistently and significantly outperforms the GRPO baseline. Furthermore, our\nanalysis reveals that E-GRPO not only achieves superior accuracy but also\ninduces more efficient reasoning policies that require fewer tool calls,\ndemonstrating a more effective and sample-efficient approach to aligning search\nagents.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.24694.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "644a4fbc2166258fccc664bc",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/8k3b44MbhQiWuo6i8BnYl.jpeg",
      "fullname": "Jialong Wu",
      "name": "callanwu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 25
    },
    "organization": {
      "_id": "67d15cca6e2cf0e062dbfb54",
      "name": "AlibabaTongyiLab",
      "fullname": "TongyiLab",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67d1502bfabfe9974d1f77bb/XdUSVf6HqBzE7zFBfSDQP.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.24563",
      "authors": [
        {
          "_id": "690172eb646208eac0d1f357",
          "name": "Hongrui Jia",
          "hidden": false
        },
        {
          "_id": "690172eb646208eac0d1f358",
          "name": "Jitong Liao",
          "hidden": false
        },
        {
          "_id": "690172eb646208eac0d1f359",
          "name": "Xi Zhang",
          "hidden": false
        },
        {
          "_id": "690172eb646208eac0d1f35a",
          "name": "Haiyang Xu",
          "hidden": false
        },
        {
          "_id": "690172eb646208eac0d1f35b",
          "name": "Tianbao Xie",
          "hidden": false
        },
        {
          "_id": "690172eb646208eac0d1f35c",
          "name": "Chaoya Jiang",
          "hidden": false
        },
        {
          "_id": "690172eb646208eac0d1f35d",
          "name": "Ming Yan",
          "hidden": false
        },
        {
          "_id": "690172eb646208eac0d1f35e",
          "name": "Si Liu",
          "hidden": false
        },
        {
          "_id": "690172eb646208eac0d1f35f",
          "name": "Wei Ye",
          "hidden": false
        },
        {
          "_id": "690172eb646208eac0d1f360",
          "name": "Fei Huang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-28T15:56:36.000Z",
      "submittedOnDailyAt": "2025-10-29T00:20:59.791Z",
      "title": "OSWorld-MCP: Benchmarking MCP Tool Invocation In Computer-Use Agents",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "With advances in decision-making and reasoning capabilities, multimodal\nagents show strong potential in computer application scenarios. Past\nevaluations have mainly assessed GUI interaction skills, while tool invocation\nabilities, such as those enabled by the Model Context Protocol (MCP), have been\nlargely overlooked. Comparing agents with integrated tool invocation to those\nevaluated only on GUI interaction is inherently unfair. We present OSWorld-MCP,\nthe first comprehensive and fair benchmark for assessing computer-use agents'\ntool invocation, GUI operation, and decision-making abilities in a real-world\nenvironment. We design a novel automated code-generation pipeline to create\ntools and combine them with a curated selection from existing tools. Rigorous\nmanual validation yields 158 high-quality tools (covering 7 common\napplications), each verified for correct functionality, practical\napplicability, and versatility. Extensive evaluations of state-of-the-art\nmultimodal agents on OSWorld-MCP show that MCP tools generally improve task\nsuccess rates (e.g., from 8.3% to 20.4% for OpenAI o3 at 15 steps, from 40.1%\nto 43.3% for Claude 4 Sonnet at 50 steps), underscoring the importance of\nassessing tool invocation capabilities. However, even the strongest models have\nrelatively low tool invocation rates, Only 36.3%, indicating room for\nimprovement and highlighting the benchmark's challenge. By explicitly measuring\nMCP tool usage skills, OSWorld-MCP deepens understanding of multimodal agents\nand sets a new standard for evaluating performance in complex, tool-assisted\nenvironments. Our code, environment, and data are publicly available at\nhttps://osworld-mcp.github.io.",
      "upvotes": 1,
      "discussionId": "690172eb646208eac0d1f361",
      "projectPage": "https://osworld-mcp.github.io/",
      "ai_summary": "OSWorld-MCP is a benchmark that evaluates multimodal agents' tool invocation, GUI operation, and decision-making abilities, highlighting the importance of assessing tool usage in real-world scenarios.",
      "ai_keywords": [
        "multimodal agents",
        "Model Context Protocol (MCP)",
        "OSWorld-MCP",
        "automated code-generation pipeline",
        "GUI operation",
        "task success rates",
        "tool invocation rates"
      ],
      "organization": {
        "_id": "67d15cca6e2cf0e062dbfb54",
        "name": "AlibabaTongyiLab",
        "fullname": "TongyiLab",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67d1502bfabfe9974d1f77bb/XdUSVf6HqBzE7zFBfSDQP.png"
      }
    },
    "publishedAt": "2025-10-28T11:56:36.000Z",
    "title": "OSWorld-MCP: Benchmarking MCP Tool Invocation In Computer-Use Agents",
    "summary": "With advances in decision-making and reasoning capabilities, multimodal\nagents show strong potential in computer application scenarios. Past\nevaluations have mainly assessed GUI interaction skills, while tool invocation\nabilities, such as those enabled by the Model Context Protocol (MCP), have been\nlargely overlooked. Comparing agents with integrated tool invocation to those\nevaluated only on GUI interaction is inherently unfair. We present OSWorld-MCP,\nthe first comprehensive and fair benchmark for assessing computer-use agents'\ntool invocation, GUI operation, and decision-making abilities in a real-world\nenvironment. We design a novel automated code-generation pipeline to create\ntools and combine them with a curated selection from existing tools. Rigorous\nmanual validation yields 158 high-quality tools (covering 7 common\napplications), each verified for correct functionality, practical\napplicability, and versatility. Extensive evaluations of state-of-the-art\nmultimodal agents on OSWorld-MCP show that MCP tools generally improve task\nsuccess rates (e.g., from 8.3% to 20.4% for OpenAI o3 at 15 steps, from 40.1%\nto 43.3% for Claude 4 Sonnet at 50 steps), underscoring the importance of\nassessing tool invocation capabilities. However, even the strongest models have\nrelatively low tool invocation rates, Only 36.3%, indicating room for\nimprovement and highlighting the benchmark's challenge. By explicitly measuring\nMCP tool usage skills, OSWorld-MCP deepens understanding of multimodal agents\nand sets a new standard for evaluating performance in complex, tool-assisted\nenvironments. Our code, environment, and data are publicly available at\nhttps://osworld-mcp.github.io.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.24563.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 148
    },
    "organization": {
      "_id": "67d15cca6e2cf0e062dbfb54",
      "name": "AlibabaTongyiLab",
      "fullname": "TongyiLab",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67d1502bfabfe9974d1f77bb/XdUSVf6HqBzE7zFBfSDQP.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.22099",
      "authors": [
        {
          "_id": "6901760b646208eac0d1f37a",
          "name": "Xuanming Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-25T00:50:47.000Z",
      "submittedOnDailyAt": "2025-10-29T00:35:45.044Z",
      "title": "Generalization or Memorization: Dynamic Decoding for Mode Steering",
      "submittedOnDailyBy": {
        "_id": "65fc5109899083a2aad987c5",
        "avatarUrl": "/avatars/289dbb8128746d931118cff6f6871a45.svg",
        "isPro": false,
        "fullname": "XUANMING ZHANG",
        "user": "XUANMINGZHANG",
        "type": "user"
      },
      "summary": "Large Language Models (LLMs) exhibit a troubling duality, capable of both\nremarkable generalization and brittle, verbatim memorization of their training\ndata. This unpredictability undermines their reliability in high-stakes\napplications. In this work, we propose a unified framework to understand,\nidentify, and control these distinct reasoning modes. First, we introduce a\ntheoretical model based on the Information Bottleneck (IB) principle,\nformalizing generalization as the learning of a compressed, task-relevant\nrepresentation and memorization as a failure to compress. Building on this\ntheory, we develop Dynamic Mode Steering (DMS), a novel inference-time\nalgorithm which comprises two components: (1) a lightweight, causally-grounded\nlinear probe that identifies the model's instantaneous reliance on\nmemorization, and (2) a dynamic activation steering mechanism that nudges the\nmodel's computation towards pre-identified generalization circuits. We frame\nDMS as a form of adaptive, self-contrastive decoding. Experiments on reasoning\nand faithfulness tasks demonstrate that DMS significantly improves logical\nconsistency and factual accuracy, thereby offering a principled approach to\nenhancing LLM reliability.",
      "upvotes": 1,
      "discussionId": "6901760c646208eac0d1f37b",
      "ai_summary": "A framework using the Information Bottleneck principle and Dynamic Mode Steering algorithm improves the reliability of Large Language Models by balancing generalization and memorization.",
      "ai_keywords": [
        "Information Bottleneck",
        "Dynamic Mode Steering",
        "linear probe",
        "dynamic activation steering",
        "adaptive",
        "self-contrastive decoding",
        "generalization",
        "memorization",
        "logical consistency",
        "factual accuracy"
      ],
      "organization": {
        "_id": "6112d84f8c2e1f4060908c9e",
        "name": "stanfordnlp",
        "fullname": "Stanford NLP",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1628624969199-6032802e1f993496bc14d9e3.png"
      }
    },
    "publishedAt": "2025-10-24T20:50:47.000Z",
    "title": "Generalization or Memorization: Dynamic Decoding for Mode Steering",
    "summary": "Large Language Models (LLMs) exhibit a troubling duality, capable of both\nremarkable generalization and brittle, verbatim memorization of their training\ndata. This unpredictability undermines their reliability in high-stakes\napplications. In this work, we propose a unified framework to understand,\nidentify, and control these distinct reasoning modes. First, we introduce a\ntheoretical model based on the Information Bottleneck (IB) principle,\nformalizing generalization as the learning of a compressed, task-relevant\nrepresentation and memorization as a failure to compress. Building on this\ntheory, we develop Dynamic Mode Steering (DMS), a novel inference-time\nalgorithm which comprises two components: (1) a lightweight, causally-grounded\nlinear probe that identifies the model's instantaneous reliance on\nmemorization, and (2) a dynamic activation steering mechanism that nudges the\nmodel's computation towards pre-identified generalization circuits. We frame\nDMS as a form of adaptive, self-contrastive decoding. Experiments on reasoning\nand faithfulness tasks demonstrate that DMS significantly improves logical\nconsistency and factual accuracy, thereby offering a principled approach to\nenhancing LLM reliability.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.22099.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65fc5109899083a2aad987c5",
      "avatarUrl": "/avatars/289dbb8128746d931118cff6f6871a45.svg",
      "fullname": "XUANMING ZHANG",
      "name": "XUANMINGZHANG",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "organization": {
      "_id": "6112d84f8c2e1f4060908c9e",
      "name": "stanfordnlp",
      "fullname": "Stanford NLP",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1628624969199-6032802e1f993496bc14d9e3.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.21323",
      "authors": [
        {
          "_id": "6900561a22d452aac6dd4439",
          "user": {
            "_id": "660229531737e5cd4a6e7948",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/_S1dkCnhREr1XchhdPEGM.png",
            "isPro": false,
            "fullname": "Shufan Shen",
            "user": "shufanshen",
            "type": "user"
          },
          "name": "Shufan Shen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-10-28T15:35:14.639Z",
          "hidden": false
        },
        {
          "_id": "6900561a22d452aac6dd443a",
          "name": "Junshu Sun",
          "hidden": false
        },
        {
          "_id": "6900561a22d452aac6dd443b",
          "name": "Qingming Huang",
          "hidden": false
        },
        {
          "_id": "6900561a22d452aac6dd443c",
          "name": "Shuhui Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-24T10:29:31.000Z",
      "submittedOnDailyAt": "2025-10-29T00:24:20.855Z",
      "title": "VL-SAE: Interpreting and Enhancing Vision-Language Alignment with a\n  Unified Concept Set",
      "submittedOnDailyBy": {
        "_id": "660229531737e5cd4a6e7948",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/_S1dkCnhREr1XchhdPEGM.png",
        "isPro": false,
        "fullname": "Shufan Shen",
        "user": "shufanshen",
        "type": "user"
      },
      "summary": "The alignment of vision-language representations endows current\nVision-Language Models (VLMs) with strong multi-modal reasoning capabilities.\nHowever, the interpretability of the alignment component remains uninvestigated\ndue to the difficulty in mapping the semantics of multi-modal representations\ninto a unified concept set. To address this problem, we propose VL-SAE, a\nsparse autoencoder that encodes vision-language representations into its hidden\nactivations. Each neuron in its hidden layer correlates to a concept\nrepresented by semantically similar images and texts, thereby interpreting\nthese representations with a unified concept set. To establish the\nneuron-concept correlation, we encourage semantically similar representations\nto exhibit consistent neuron activations during self-supervised training.\nFirst, to measure the semantic similarity of multi-modal representations, we\nperform their alignment in an explicit form based on cosine similarity. Second,\nwe construct the VL-SAE with a distance-based encoder and two modality-specific\ndecoders to ensure the activation consistency of semantically similar\nrepresentations. Experiments across multiple VLMs (e.g., CLIP, LLaVA)\ndemonstrate the superior capability of VL-SAE in interpreting and enhancing the\nvision-language alignment. For interpretation, the alignment between vision and\nlanguage representations can be understood by comparing their semantics with\nconcepts. For enhancement, the alignment can be strengthened by aligning\nvision-language representations at the concept level, contributing to\nperformance improvements in downstream tasks, including zero-shot image\nclassification and hallucination elimination. Codes are available at\nhttps://github.com/ssfgunner/VL-SAE.",
      "upvotes": 1,
      "discussionId": "6900561b22d452aac6dd443d",
      "githubRepo": "https://github.com/ssfgunner/VL-SAE",
      "ai_summary": "VL-SAE, a sparse autoencoder, enhances vision-language alignment by correlating neurons to unified concepts, improving interpretability and performance in tasks like zero-shot image classification and hallucination elimination.",
      "ai_keywords": [
        "sparse autoencoder",
        "vision-language representations",
        "multi-modal reasoning",
        "neuron-concept correlation",
        "cosine similarity",
        "distance-based encoder",
        "modality-specific decoders",
        "zero-shot image classification",
        "hallucination elimination"
      ],
      "githubStars": 3,
      "organization": {
        "_id": "632fea4a9c9aa2bfdf5982f8",
        "name": "UCAS",
        "fullname": "ucas",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/632fe99f2a6ef6fb4ad7ba08/QiMtq1UkcKsI9yy1ZCU1m.jpeg"
      }
    },
    "publishedAt": "2025-10-24T06:29:31.000Z",
    "title": "VL-SAE: Interpreting and Enhancing Vision-Language Alignment with a\n  Unified Concept Set",
    "summary": "The alignment of vision-language representations endows current\nVision-Language Models (VLMs) with strong multi-modal reasoning capabilities.\nHowever, the interpretability of the alignment component remains uninvestigated\ndue to the difficulty in mapping the semantics of multi-modal representations\ninto a unified concept set. To address this problem, we propose VL-SAE, a\nsparse autoencoder that encodes vision-language representations into its hidden\nactivations. Each neuron in its hidden layer correlates to a concept\nrepresented by semantically similar images and texts, thereby interpreting\nthese representations with a unified concept set. To establish the\nneuron-concept correlation, we encourage semantically similar representations\nto exhibit consistent neuron activations during self-supervised training.\nFirst, to measure the semantic similarity of multi-modal representations, we\nperform their alignment in an explicit form based on cosine similarity. Second,\nwe construct the VL-SAE with a distance-based encoder and two modality-specific\ndecoders to ensure the activation consistency of semantically similar\nrepresentations. Experiments across multiple VLMs (e.g., CLIP, LLaVA)\ndemonstrate the superior capability of VL-SAE in interpreting and enhancing the\nvision-language alignment. For interpretation, the alignment between vision and\nlanguage representations can be understood by comparing their semantics with\nconcepts. For enhancement, the alignment can be strengthened by aligning\nvision-language representations at the concept level, contributing to\nperformance improvements in downstream tasks, including zero-shot image\nclassification and hallucination elimination. Codes are available at\nhttps://github.com/ssfgunner/VL-SAE.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.21323.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "660229531737e5cd4a6e7948",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/_S1dkCnhREr1XchhdPEGM.png",
      "fullname": "Shufan Shen",
      "name": "shufanshen",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "organization": {
      "_id": "632fea4a9c9aa2bfdf5982f8",
      "name": "UCAS",
      "fullname": "ucas",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/632fe99f2a6ef6fb4ad7ba08/QiMtq1UkcKsI9yy1ZCU1m.jpeg"
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2510.24591",
      "authors": [
        {
          "_id": "690172c7646208eac0d1f348",
          "name": "Christine Ye",
          "hidden": false
        },
        {
          "_id": "690172c7646208eac0d1f349",
          "name": "Sihan Yuan",
          "hidden": false
        },
        {
          "_id": "690172c7646208eac0d1f34a",
          "name": "Suchetha Cooray",
          "hidden": false
        },
        {
          "_id": "690172c7646208eac0d1f34b",
          "name": "Steven Dillmann",
          "hidden": false
        },
        {
          "_id": "690172c7646208eac0d1f34c",
          "name": "Ian L. V. Roque",
          "hidden": false
        },
        {
          "_id": "690172c7646208eac0d1f34d",
          "name": "Dalya Baron",
          "hidden": false
        },
        {
          "_id": "690172c7646208eac0d1f34e",
          "name": "Philipp Frank",
          "hidden": false
        },
        {
          "_id": "690172c7646208eac0d1f34f",
          "name": "Sergio Martin-Alvarez",
          "hidden": false
        },
        {
          "_id": "690172c7646208eac0d1f350",
          "name": "Nolan Koblischke",
          "hidden": false
        },
        {
          "_id": "690172c7646208eac0d1f351",
          "name": "Frank J Qu",
          "hidden": false
        },
        {
          "_id": "690172c7646208eac0d1f352",
          "name": "Diyi Yang",
          "hidden": false
        },
        {
          "_id": "690172c7646208eac0d1f353",
          "name": "Risa Wechsler",
          "hidden": false
        },
        {
          "_id": "690172c7646208eac0d1f354",
          "name": "Ioana Ciuca",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-28T16:21:19.000Z",
      "submittedOnDailyAt": "2025-10-29T00:20:08.707Z",
      "title": "ReplicationBench: Can AI Agents Replicate Astrophysics Research Papers?",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Frontier AI agents show increasing promise as scientific research assistants,\nand may eventually be useful for extended, open-ended research workflows.\nHowever, in order to use agents for novel research, we must first assess the\nunderlying faithfulness and correctness of their work. To evaluate agents as\nresearch assistants, we introduce ReplicationBench, an evaluation framework\nthat tests whether agents can replicate entire research papers drawn from the\nastrophysics literature. Astrophysics, where research relies heavily on\narchival data and computational study while requiring little real-world\nexperimentation, is a particularly useful testbed for AI agents in scientific\nresearch. We split each paper into tasks which require agents to replicate the\npaper's core contributions, including the experimental setup, derivations, data\nanalysis, and codebase. Each task is co-developed with the original paper\nauthors and targets a key scientific result, enabling objective evaluation of\nboth faithfulness (adherence to original methods) and correctness (technical\naccuracy of results). ReplicationBench is extremely challenging for current\nfrontier language models: even the best-performing language models score under\n20%. We analyze ReplicationBench trajectories in collaboration with domain\nexperts and find a rich, diverse set of failure modes for agents in scientific\nresearch. ReplicationBench establishes the first benchmark of paper-scale,\nexpert-validated astrophysics research tasks, reveals insights about agent\nperformance generalizable to other domains of data-driven science, and provides\na scalable framework for measuring AI agents' reliability in scientific\nresearch.",
      "upvotes": 0,
      "discussionId": "690172c8646208eac0d1f355",
      "ai_summary": "ReplicationBench evaluates AI agents' ability to replicate astrophysics research papers, providing insights into their faithfulness and correctness in scientific research tasks.",
      "ai_keywords": [
        "ReplicationBench",
        "AI agents",
        "scientific research assistants",
        "faithfulness",
        "correctness",
        "research papers",
        "astrophysics",
        "experimental setup",
        "derivations",
        "data analysis",
        "codebase",
        "language models",
        "benchmark",
        "expert-validated",
        "data-driven science",
        "reliability"
      ]
    },
    "publishedAt": "2025-10-28T12:21:19.000Z",
    "title": "ReplicationBench: Can AI Agents Replicate Astrophysics Research Papers?",
    "summary": "Frontier AI agents show increasing promise as scientific research assistants,\nand may eventually be useful for extended, open-ended research workflows.\nHowever, in order to use agents for novel research, we must first assess the\nunderlying faithfulness and correctness of their work. To evaluate agents as\nresearch assistants, we introduce ReplicationBench, an evaluation framework\nthat tests whether agents can replicate entire research papers drawn from the\nastrophysics literature. Astrophysics, where research relies heavily on\narchival data and computational study while requiring little real-world\nexperimentation, is a particularly useful testbed for AI agents in scientific\nresearch. We split each paper into tasks which require agents to replicate the\npaper's core contributions, including the experimental setup, derivations, data\nanalysis, and codebase. Each task is co-developed with the original paper\nauthors and targets a key scientific result, enabling objective evaluation of\nboth faithfulness (adherence to original methods) and correctness (technical\naccuracy of results). ReplicationBench is extremely challenging for current\nfrontier language models: even the best-performing language models score under\n20%. We analyze ReplicationBench trajectories in collaboration with domain\nexperts and find a rich, diverse set of failure modes for agents in scientific\nresearch. ReplicationBench establishes the first benchmark of paper-scale,\nexpert-validated astrophysics research tasks, reveals insights about agent\nperformance generalizable to other domains of data-driven science, and provides\na scalable framework for measuring AI agents' reliability in scientific\nresearch.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.24591.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 148
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.24448",
      "authors": [
        {
          "_id": "6901734b646208eac0d1f363",
          "name": "Pablo Acuaviva",
          "hidden": false
        },
        {
          "_id": "6901734b646208eac0d1f364",
          "name": "Aram Davtyan",
          "hidden": false
        },
        {
          "_id": "6901734b646208eac0d1f365",
          "name": "Mariam Hassan",
          "hidden": false
        },
        {
          "_id": "6901734b646208eac0d1f366",
          "name": "Sebastian Stapf",
          "hidden": false
        },
        {
          "_id": "6901734b646208eac0d1f367",
          "name": "Ahmad Rahimi",
          "hidden": false
        },
        {
          "_id": "6901734b646208eac0d1f368",
          "name": "Alexandre Alahi",
          "hidden": false
        },
        {
          "_id": "6901734b646208eac0d1f369",
          "name": "Paolo Favaro",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-28T14:12:11.000Z",
      "submittedOnDailyAt": "2025-10-29T00:50:57.135Z",
      "title": "Rethinking Visual Intelligence: Insights from Video Pretraining",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Large language models (LLMs) have demonstrated that large-scale pretraining\nenables systems to adapt rapidly to new problems with little supervision in the\nlanguage domain. This success, however, has not translated as effectively to\nthe visual domain, where models, including LLMs, continue to struggle with\ncompositional understanding, sample efficiency, and general-purpose\nproblem-solving. We investigate Video Diffusion Models (VDMs) as a promising\ndirection for bridging this gap. Pretraining on spatiotemporal data endows\nthese models with strong inductive biases for structure and dynamics, which we\nhypothesize can support broad task adaptability. To test this, we design a\ncontrolled evaluation in which both a pretrained LLM and a pretrained VDM are\nequipped with lightweight adapters and presented with tasks in their natural\nmodalities. Across benchmarks including ARC-AGI, ConceptARC, visual games,\nroute planning, and cellular automata, VDMs demonstrate higher data efficiency\nthan their language counterparts. Taken together, our results indicate that\nvideo pretraining offers inductive biases that support progress toward visual\nfoundation models.",
      "upvotes": 0,
      "discussionId": "6901734b646208eac0d1f36a",
      "ai_summary": "Video Diffusion Models (VDMs) show higher data efficiency than large language models across various visual tasks, suggesting video pretraining can enhance visual foundation models.",
      "ai_keywords": [
        "Video Diffusion Models",
        "VDMs",
        "large language models",
        "LLMs",
        "spatiotemporal data",
        "inductive biases",
        "ARC-AGI",
        "ConceptARC",
        "visual games",
        "route planning",
        "cellular automata",
        "visual foundation models"
      ]
    },
    "publishedAt": "2025-10-28T10:12:11.000Z",
    "title": "Rethinking Visual Intelligence: Insights from Video Pretraining",
    "summary": "Large language models (LLMs) have demonstrated that large-scale pretraining\nenables systems to adapt rapidly to new problems with little supervision in the\nlanguage domain. This success, however, has not translated as effectively to\nthe visual domain, where models, including LLMs, continue to struggle with\ncompositional understanding, sample efficiency, and general-purpose\nproblem-solving. We investigate Video Diffusion Models (VDMs) as a promising\ndirection for bridging this gap. Pretraining on spatiotemporal data endows\nthese models with strong inductive biases for structure and dynamics, which we\nhypothesize can support broad task adaptability. To test this, we design a\ncontrolled evaluation in which both a pretrained LLM and a pretrained VDM are\nequipped with lightweight adapters and presented with tasks in their natural\nmodalities. Across benchmarks including ARC-AGI, ConceptARC, visual games,\nroute planning, and cellular automata, VDMs demonstrate higher data efficiency\nthan their language counterparts. Taken together, our results indicate that\nvideo pretraining offers inductive biases that support progress toward visual\nfoundation models.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.24448.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 148
    },
    "isAuthorParticipating": false
  }
]