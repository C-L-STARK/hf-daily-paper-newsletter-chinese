[
  {
    "paper": {
      "id": "2509.01656",
      "authors": [
        {
          "_id": "68beda55c123124955ef6267",
          "name": "Zetong Zhou",
          "hidden": false
        },
        {
          "_id": "68beda55c123124955ef6268",
          "name": "Dongping Chen",
          "hidden": false
        },
        {
          "_id": "68beda55c123124955ef6269",
          "name": "Zixian Ma",
          "hidden": false
        },
        {
          "_id": "68beda55c123124955ef626a",
          "name": "Zhihan Hu",
          "hidden": false
        },
        {
          "_id": "68beda55c123124955ef626b",
          "name": "Mingyang Fu",
          "hidden": false
        },
        {
          "_id": "68beda55c123124955ef626c",
          "name": "Sinan Wang",
          "hidden": false
        },
        {
          "_id": "68beda55c123124955ef626d",
          "name": "Yao Wan",
          "hidden": false
        },
        {
          "_id": "68beda55c123124955ef626e",
          "name": "Zhou Zhao",
          "hidden": false
        },
        {
          "_id": "68beda55c123124955ef626f",
          "name": "Ranjay Krishna",
          "hidden": false
        }
      ],
      "publishedAt": "2025-09-01T17:57:49.000Z",
      "submittedOnDailyAt": "2025-09-09T00:10:59.899Z",
      "title": "Reinforced Visual Perception with Tools",
      "submittedOnDailyBy": {
        "_id": "643be8879f5d314db2d9ed23",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/643be8879f5d314db2d9ed23/VrW2UtJ7ppOnGIYjTWd7b.png",
        "isPro": false,
        "fullname": "Chen Dongping",
        "user": "shuaishuaicdp",
        "type": "user"
      },
      "summary": "Visual reasoning, a cornerstone of human intelligence, encompasses complex\nperceptual and logical processes essential for solving diverse visual problems.\nWhile advances in computer vision have produced powerful models for various\nperceptual tasks, leveraging these for general visual reasoning remains\nchallenging. Prior work demonstrates that augmenting LLMs with vision models\nvia supervised finetuning improves performance, but faces key limitations such\nas expensive data generation, reliance on careful data filtering, and poor\ngeneralization. To address these issues, we propose ReVPT to enhance\nmulti-modal LLMs' abilities to reason about and use visual tools through\nreinforcement learning. We introduce a novel RL algorithm based on GRPO,\ndesigned to train models to reason with a suite of four visual tools. Through\nextensive experiments, we show that our method achieves state-of-the-art\nperformance on several perception-heavy benchmarks, including SAT, CV-Bench,\nBLINK and MMStar, significantly outperforming the supervised and text-based RL\nfinetuning baselines. Notably, Our ReVPT-3B and ReVPT-7B outperform the\ninstruct models by 9.03% and 9.44% on CV-Bench. Finally, we bring to the\ncommunity new insights on RL-based visual tool-usage through extensive\nablations. Our code is available at https://github.com/ls-kelvin/REVPT.",
      "upvotes": 10,
      "discussionId": "68beda55c123124955ef6270",
      "githubRepo": "https://github.com/ls-kelvin/REVPT",
      "ai_summary": "ReVPT enhances multi-modal LLMs' visual reasoning capabilities using reinforcement learning, achieving state-of-the-art performance on visual benchmarks.",
      "ai_keywords": [
        "LLMs",
        "vision models",
        "supervised finetuning",
        "reinforcement learning",
        "GRPO",
        "visual tools",
        "SAT",
        "CV-Bench",
        "BLINK",
        "MMStar",
        "instruct models"
      ],
      "githubStars": 17
    },
    "publishedAt": "2025-09-01T13:57:49.000Z",
    "title": "Reinforced Visual Perception with Tools",
    "summary": "Visual reasoning, a cornerstone of human intelligence, encompasses complex\nperceptual and logical processes essential for solving diverse visual problems.\nWhile advances in computer vision have produced powerful models for various\nperceptual tasks, leveraging these for general visual reasoning remains\nchallenging. Prior work demonstrates that augmenting LLMs with vision models\nvia supervised finetuning improves performance, but faces key limitations such\nas expensive data generation, reliance on careful data filtering, and poor\ngeneralization. To address these issues, we propose ReVPT to enhance\nmulti-modal LLMs' abilities to reason about and use visual tools through\nreinforcement learning. We introduce a novel RL algorithm based on GRPO,\ndesigned to train models to reason with a suite of four visual tools. Through\nextensive experiments, we show that our method achieves state-of-the-art\nperformance on several perception-heavy benchmarks, including SAT, CV-Bench,\nBLINK and MMStar, significantly outperforming the supervised and text-based RL\nfinetuning baselines. Notably, Our ReVPT-3B and ReVPT-7B outperform the\ninstruct models by 9.03% and 9.44% on CV-Bench. Finally, we bring to the\ncommunity new insights on RL-based visual tool-usage through extensive\nablations. Our code is available at https://github.com/ls-kelvin/REVPT.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.01656.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "643be8879f5d314db2d9ed23",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/643be8879f5d314db2d9ed23/VrW2UtJ7ppOnGIYjTWd7b.png",
      "fullname": "Chen Dongping",
      "name": "shuaishuaicdp",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 9
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2509.06949",
      "authors": [
        {
          "_id": "68bf87ff207285de11b07b39",
          "name": "Yinjie Wang",
          "hidden": false
        },
        {
          "_id": "68bf87ff207285de11b07b3a",
          "name": "Ling Yang",
          "hidden": false
        },
        {
          "_id": "68bf87ff207285de11b07b3b",
          "name": "Bowen Li",
          "hidden": false
        },
        {
          "_id": "68bf87ff207285de11b07b3c",
          "name": "Ye Tian",
          "hidden": false
        },
        {
          "_id": "68bf87ff207285de11b07b3d",
          "name": "Ke Shen",
          "hidden": false
        },
        {
          "_id": "68bf87ff207285de11b07b3e",
          "name": "Mengdi Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-09-08T17:58:06.000Z",
      "submittedOnDailyAt": "2025-09-09T00:25:38.536Z",
      "title": "Revolutionizing Reinforcement Learning Framework for Diffusion Large\n  Language Models",
      "submittedOnDailyBy": {
        "_id": "64fde4e252e82dd432b74ce9",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64fde4e252e82dd432b74ce9/-CQZbBP7FsPPyawYrsi4z.jpeg",
        "isPro": false,
        "fullname": "Ling Yang",
        "user": "Lingaaaaaaa",
        "type": "user"
      },
      "summary": "We propose TraceRL, a trajectory-aware reinforcement learning framework for\ndiffusion language models (DLMs) that incorporates preferred inference\ntrajectory into post-training, and is applicable across different\narchitectures. Equipped with a diffusion-based value model that enhances\ntraining stability, we demonstrate improved reasoning performance on complex\nmath and coding tasks. Besides, it can also be applied to adapt block-specific\nmodels to larger blocks, which improves sampling flexibility. Employing\nTraceRL, we derive a series of state-of-the-art diffusion language models,\nnamely TraDo. Although smaller than 7B-scale AR models, TraDo-4B-Instruct still\nconsistently outperforms them across complex math reasoning tasks.\nTraDo-8B-Instruct achieves relative accuracy improvements of 6.1% over\nQwen2.5-7B-Instruct and 51.3% over Llama3.1-8B-Instruct on mathematical\nreasoning benchmarks. Through curriculum learning, we also derive the first\nlong-CoT DLM, outperforming Qwen2.5-7B-Instruct on MATH500 with an 18.1%\nrelative accuracy gain. To facilitate reproducible research and practical\napplications, we release a comprehensive open-source framework for building,\ntraining, and deploying diffusion LLMs across diverse architectures. The\nframework integrates accelerated KV-cache techniques and inference engines for\nboth inference and reinforcement learning, and includes implementations of\nvarious supervised fine-tuning and RL methods for mathematics, coding, and\ngeneral tasks. Code and Models: https://github.com/Gen-Verse/dLLM-RL",
      "upvotes": 5,
      "discussionId": "68bf87ff207285de11b07b3f",
      "projectPage": "https://huggingface.co/collections/Gen-Verse/trado-series-68beb6cd6a26c27cde9fe3af",
      "githubRepo": "https://github.com/Gen-Verse/dLLM-RL",
      "ai_summary": "TraceRL enhances diffusion language models with trajectory-aware reinforcement learning, improving reasoning performance on complex tasks and enabling flexible sampling.",
      "ai_keywords": [
        "trajectory-aware reinforcement learning",
        "diffusion language models",
        "diffusion-based value model",
        "sampling flexibility",
        "curriculum learning",
        "long-CoT DLM",
        "KV-cache techniques",
        "inference engines",
        "supervised fine-tuning",
        "RL methods"
      ],
      "githubStars": 5
    },
    "publishedAt": "2025-09-08T13:58:06.000Z",
    "title": "Revolutionizing Reinforcement Learning Framework for Diffusion Large\n  Language Models",
    "summary": "We propose TraceRL, a trajectory-aware reinforcement learning framework for\ndiffusion language models (DLMs) that incorporates preferred inference\ntrajectory into post-training, and is applicable across different\narchitectures. Equipped with a diffusion-based value model that enhances\ntraining stability, we demonstrate improved reasoning performance on complex\nmath and coding tasks. Besides, it can also be applied to adapt block-specific\nmodels to larger blocks, which improves sampling flexibility. Employing\nTraceRL, we derive a series of state-of-the-art diffusion language models,\nnamely TraDo. Although smaller than 7B-scale AR models, TraDo-4B-Instruct still\nconsistently outperforms them across complex math reasoning tasks.\nTraDo-8B-Instruct achieves relative accuracy improvements of 6.1% over\nQwen2.5-7B-Instruct and 51.3% over Llama3.1-8B-Instruct on mathematical\nreasoning benchmarks. Through curriculum learning, we also derive the first\nlong-CoT DLM, outperforming Qwen2.5-7B-Instruct on MATH500 with an 18.1%\nrelative accuracy gain. To facilitate reproducible research and practical\napplications, we release a comprehensive open-source framework for building,\ntraining, and deploying diffusion LLMs across diverse architectures. The\nframework integrates accelerated KV-cache techniques and inference engines for\nboth inference and reinforcement learning, and includes implementations of\nvarious supervised fine-tuning and RL methods for mathematics, coding, and\ngeneral tasks. Code and Models: https://github.com/Gen-Verse/dLLM-RL",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.06949.png",
    "numComments": 4,
    "submittedBy": {
      "_id": "64fde4e252e82dd432b74ce9",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64fde4e252e82dd432b74ce9/-CQZbBP7FsPPyawYrsi4z.jpeg",
      "fullname": "Ling Yang",
      "name": "Lingaaaaaaa",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 10
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2509.06155",
      "authors": [
        {
          "_id": "68bf827d207285de11b07b2d",
          "name": "Duomin Wang",
          "hidden": false
        },
        {
          "_id": "68bf827d207285de11b07b2e",
          "name": "Wei Zuo",
          "hidden": false
        },
        {
          "_id": "68bf827d207285de11b07b2f",
          "name": "Aojie Li",
          "hidden": false
        },
        {
          "_id": "68bf827d207285de11b07b30",
          "name": "Ling-Hao Chen",
          "hidden": false
        },
        {
          "_id": "68bf827d207285de11b07b31",
          "name": "Xinyao Liao",
          "hidden": false
        },
        {
          "_id": "68bf827d207285de11b07b32",
          "name": "Deyu Zhou",
          "hidden": false
        },
        {
          "_id": "68bf827d207285de11b07b33",
          "name": "Zixin Yin",
          "hidden": false
        },
        {
          "_id": "68bf827d207285de11b07b34",
          "name": "Xili Dai",
          "hidden": false
        },
        {
          "_id": "68bf827d207285de11b07b35",
          "name": "Daxin Jiang",
          "hidden": false
        },
        {
          "_id": "68bf827d207285de11b07b36",
          "name": "Gang Yu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-09-07T17:55:03.000Z",
      "submittedOnDailyAt": "2025-09-09T00:08:23.306Z",
      "title": "UniVerse-1: Unified Audio-Video Generation via Stitching of Experts",
      "submittedOnDailyBy": {
        "_id": "64ae9b88a22a179fc4d07992",
        "avatarUrl": "/avatars/c9065f04a1188ea3129e56a90328ffd3.svg",
        "isPro": false,
        "fullname": "wang",
        "user": "dorni",
        "type": "user"
      },
      "summary": "We introduce UniVerse-1, a unified, Veo-3-like model capable of\nsimultaneously generating coordinated audio and video. To enhance training\nefficiency, we bypass training from scratch and instead employ a stitching of\nexperts (SoE) technique. This approach deeply fuses the corresponding blocks of\npre-trained video and music generation experts models, thereby fully leveraging\ntheir foundational capabilities. To ensure accurate annotations and temporal\nalignment for both ambient sounds and speech with video content, we developed\nan online annotation pipeline that processes the required training data and\ngenerates labels during training process. This strategy circumvents the\nperformance degradation often caused by misalignment text-based annotations.\nThrough the synergy of these techniques, our model, after being finetuned on\napproximately 7,600 hours of audio-video data, produces results with\nwell-coordinated audio-visuals for ambient sounds generation and strong\nalignment for speech generation. To systematically evaluate our proposed\nmethod, we introduce Verse-Bench, a new benchmark dataset. In an effort to\nadvance research in audio-video generation and to close the performance gap\nwith state-of-the-art models such as Veo3, we make our model and code publicly\navailable. We hope this contribution will benefit the broader research\ncommunity. Project page: https://dorniwang.github.io/UniVerse-1/.",
      "upvotes": 4,
      "discussionId": "68bf827d207285de11b07b37",
      "projectPage": "https://dorniwang.github.io/UniVerse-1/",
      "githubRepo": "https://github.com/Dorniwang/UniVerse-1-code/",
      "ai_summary": "UniVerse-1, a unified audio-video generation model, uses a stitching of experts technique to combine pre-trained video and music models, ensuring accurate temporal alignment and producing high-quality audio-visual outputs.",
      "ai_keywords": [
        "unified model",
        "Veo-3-like model",
        "stitching of experts (SoE)",
        "pre-trained video generation",
        "pre-trained music generation",
        "online annotation pipeline",
        "temporal alignment",
        "ambient sounds",
        "speech generation",
        "Verse-Bench",
        "audio-video generation"
      ],
      "githubStars": 7
    },
    "publishedAt": "2025-09-07T13:55:03.000Z",
    "title": "UniVerse-1: Unified Audio-Video Generation via Stitching of Experts",
    "summary": "We introduce UniVerse-1, a unified, Veo-3-like model capable of\nsimultaneously generating coordinated audio and video. To enhance training\nefficiency, we bypass training from scratch and instead employ a stitching of\nexperts (SoE) technique. This approach deeply fuses the corresponding blocks of\npre-trained video and music generation experts models, thereby fully leveraging\ntheir foundational capabilities. To ensure accurate annotations and temporal\nalignment for both ambient sounds and speech with video content, we developed\nan online annotation pipeline that processes the required training data and\ngenerates labels during training process. This strategy circumvents the\nperformance degradation often caused by misalignment text-based annotations.\nThrough the synergy of these techniques, our model, after being finetuned on\napproximately 7,600 hours of audio-video data, produces results with\nwell-coordinated audio-visuals for ambient sounds generation and strong\nalignment for speech generation. To systematically evaluate our proposed\nmethod, we introduce Verse-Bench, a new benchmark dataset. In an effort to\nadvance research in audio-video generation and to close the performance gap\nwith state-of-the-art models such as Veo3, we make our model and code publicly\navailable. We hope this contribution will benefit the broader research\ncommunity. Project page: https://dorniwang.github.io/UniVerse-1/.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.06155.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64ae9b88a22a179fc4d07992",
      "avatarUrl": "/avatars/c9065f04a1188ea3129e56a90328ffd3.svg",
      "fullname": "wang",
      "name": "dorni",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  }
]